{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":211599040,"authorName":"Jeff Clune","from":"Jeff Clune &lt;jclune@...&gt;","profile":"jeffreyclune","replyTo":"LIST","senderId":"EkvVYnvI-e7ujRRoMjU7Z3kTPzaXYkcHbo28L4IJIgzYN6GC9CAjXGFi7MaBBMmACd9JogCtDS-SCIIKssPpgvpF","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [neat] Re: Machine Learning and the Long View of AI","postDate":"1209421046","msgId":4002,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PEM0M0JDNTM2LjIyN0YyJWpjbHVuZUBtc3UuZWR1Pg==","inReplyToHeader":"PGZ2MnJsMSs5cWdjQGVHcm91cHMuY29tPg=="},"prevInTopic":4001,"nextInTopic":4003,"prevInTime":4001,"nextInTime":4003,"topicId":3955,"numMessagesInTopic":49,"msgSnippet":"Ken- Thank you for explaining these arguments at length. I always learn a lot when we have these sorts of discussions, and there is no exception in this case.","rawEmail":"Return-Path: &lt;jclune@...&gt;\r\nX-Sender: jclune@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 42410 invoked from network); 28 Apr 2008 22:17:30 -0000\r\nX-Received: from unknown (66.218.67.96)\n  by m36.grp.scd.yahoo.com with QMQP; 28 Apr 2008 22:17:30 -0000\r\nX-Received: from unknown (HELO hs-out-0708.google.com) (64.233.178.244)\n  by mta17.grp.scd.yahoo.com with SMTP; 28 Apr 2008 22:17:30 -0000\r\nX-Received: by hs-out-0708.google.com with SMTP id n78so3842608hsc.8\n        for &lt;neat@yahoogroups.com&gt;; Mon, 28 Apr 2008 15:17:29 -0700 (PDT)\r\nX-Received: by 10.90.87.7 with SMTP id k7mr232935agb.25.1209421049603;\n        Mon, 28 Apr 2008 15:17:29 -0700 (PDT)\r\nReturn-Path: &lt;jclune@...&gt;\r\nX-Received: from ?192.168.2.2? ( [67.167.130.112])\n        by mx.google.com with ESMTPS id q15sm8597829qbq.0.2008.04.28.15.17.28\n        (version=TLSv1/SSLv3 cipher=OTHER);\n        Mon, 28 Apr 2008 15:17:28 -0700 (PDT)\r\nUser-Agent: Microsoft-Entourage/12.1.0.080305\r\nDate: Mon, 28 Apr 2008 18:17:26 -0400\r\nTo: &quot;neat@yahoogroups.com&quot; &lt;neat@yahoogroups.com&gt;\r\nMessage-ID: &lt;C43BC536.227F2%jclune@...&gt;\r\nThread-Topic: [neat] Re: Machine Learning and the Long View of AI\r\nThread-Index: AcipfaONxO5Zbq9iVE6oSR36rmg6bg==\r\nIn-Reply-To: &lt;fv2rl1+9qgc@...&gt;\r\nMime-version: 1.0\r\nContent-type: text/plain;\n\tcharset=&quot;US-ASCII&quot;\r\nContent-transfer-encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Jeff Clune &lt;jclune@...&gt;\r\nSubject: Re: [neat] Re: Machine Learning and the Long View of AI\r\nX-Yahoo-Group-Post: member; u=211599040; y=bPf8Fbe-zU963QPlbt5ThlTMGqgvbTVo4HgSodP8t6_BI5Oaqo7R\r\nX-Yahoo-Profile: jeffreyclune\r\n\r\nKen-\n\nThank you for explaining these arguments at length. I always learn a lot\nwhen we have these sorts of discussions, and there is no exception in this\ncase. I agree with most of what you write below, especially that\nconstraining/biasing evolution is very important. I guess the only place we\ndisagree is that I believe there is a middle ground of constraint that we\nshould shoot for, whereas you seem to feel &#39;the more the better&#39;.\n\nYou write:\n\n&gt;  Therefore, progress is NE should in part be measured with respect to\n&gt; progress in constraining the problem to make such a discovery more\n&gt; likely.  When an NE algorithm is improved to allow us to tell it more\n&gt; about the world in which its output will be situated, that is good\n&gt; news for the long view.  In short, we don&#39;t care at all how NE\n&gt; produced a brain as long as it really does.\n\nThis reminds me of something that Hod Lipson says repeatedly. Whenever\nsomeone evolves something impressive the first question to ask is, &quot;How big\nare your building blocks?&quot; I am going to provide a straw man of your\nargument. Hopefully the fact that I admit that up front will make it less\nobjectionable. Imagine that Kasparov and a neural net engineer teamed up and\nhand-created a neural  net (call it &#39;DeepNet&#39;)  that played chess at a\ngrandmaster level. Now imagine that we create an NE algorithm for learning\nchess playing that was otherwise identical to NEAT, but had one extra\nmutation operator, which was &#39;clear out the current phenotype and replace it\nwith DeepNet&#39;. In this case we would have highly constrained the problem to\nfind a good chess playing solution. We would have also successfully injected\nour a priori knowledge of the problem. However, it would be very\nunimpressive as an accomplishment in the field of evolutionary computation.\nThe credit goes to the humans that designed DeepNet, not for the\nevolutionary algorithm that recreated it.\n\nAs I said, this is an unfair caricature of your view. However, I think it\nmight reveal what I have been trying to say. In my mind, the goal is to\nprovide smaller and smaller building blocks because then we know it is\nevolution that is doing the work, and not us. There is a sweet spot in the\nmiddle. If we humans don&#39;t do any work in biasing the search, then evolution\nwill perform terribly. But if we provide building blocks that are too large,\nthen evolution did not really do the heavy lifting. So, as opposed to saying\n&#39;the more constraint the better,&#39; I think it is interesting to try to\nprovide smaller building blocks while still gaining high levels of\nperformance. As I have said before, I also think that if we make progress on\nthis front, the evolutionary algorithm (not its product) will be more likely\nto generalize to solving other problems. The long term goal, of course, is\nto have our algorithms solve problems and create things where we either\ndon&#39;t know how to solve the problems, or can&#39;t be bothered to do so. For\nexample, the NE that produced DeepNet would not do very well at race car\ndriving. But an algorithm that was constrained in a more abstract way to\nexploit regularities in its environment might do better on both car racing\nand chess. \n\nI guess I start from the recognition that evolution produced humans without\nany bias from a conscious entity. How it did that is one of the most\nfascinating and open questions both in our field and in biology. We agree\nthat trying to emulate ways in which natural evolution did things like bias\nitself, and thus allow the evolution of modularity, is the way forward for\nour field. HyperNEAT represents such amazing progress because it employed\nthis strategy. But it strikes me that nature was not told a priori how many\nleg modules it should make or learn to control. Nor was it told how many\nneural modules it should create in the brain. It figured that stuff out on\nits own, and probably performed better as a result because it could learn to\ntailor the number of modules it needed to the regularity of the problems it\nfaced. I guess I don&#39;t think we will make it very far towards evolving\nbrains that are generally intelligent if our evolutionary algorithms cannot\ndo likewise. It seems that something is majorly lacking if we have to tell\nit each time what the regularities are in the environment, and how to go\nabout exploiting them.\n\nApologies for the straw man argument. I do think there is a lot of merit to\nthe general thrust of what you say. I may be overreacting in focusing on the\nextremes\n\n\n\nCheers,\nJeff Clune\n\nDigital Evolution Lab, Michigan State University\n\njclune@...\n\n\n\n\n&gt; From: Kenneth Stanley &lt;kstanley@...&gt;\n&gt; Reply-To: &quot;neat@yahoogroups.com&quot; &lt;neat@yahoogroups.com&gt;\n&gt; Date: Sun, 27 Apr 2008 21:36:33 -0000\n&gt; To: &quot;neat@yahoogroups.com&quot; &lt;neat@yahoogroups.com&gt;\n&gt; Subject: [neat] Re: Machine Learning and the Long View of AI\n&gt; \n&gt; --- In neat@yahoogroups.com, &quot;Derek James&quot; &lt;djames@...&gt; wrote:\n&gt; \n&gt;&gt; \n&gt;&gt;&gt;  In RL, in contrast, the long view is almost the opposite: They\n&gt; want to\n&gt;&gt;&gt;  remove all constraints and still learn nevertheless.\n&gt;&gt; \n&gt;&gt; I&#39;m not sure what you mean by this, Ken. Could you elaborate a little?\n&gt;&gt; \n&gt; \n&gt; Sure.  I think the problem is that I can&#39;t find a way to explain my\n&gt; point concisely.  As I try to explain it, it starts taking up too much\n&gt; text so I shorten it and then it loses its meaning.  Let me give it a\n&gt; try again...\n&gt; \n&gt; I think the difference between the goals of RL and NE is an\n&gt; interesting topic because they are almost always conflated, as if they\n&gt; are trying to solve the same problem.\n&gt; \n&gt; The RL community (e.g. value-function approaches) is trying to build\n&gt; something that learns like a natural brain.  They are saying, through\n&gt; analytic means we can deduce how a brain can learn from sparse\n&gt; reinforcement and formalize that process in an algorithm.  The hope, I\n&gt; would think, is to eventually build the &quot;general intelligence&quot; that\n&gt; aligns with the holy grail of AI.  So each step along the way is an\n&gt; improvement in that general ability.\n&gt; \n&gt; So if that is your goal, then the benchmarks you choose have to be\n&gt; designed to measure progress to that goal.  So what they need to do is\n&gt; show that their designed intelligence can work largely independently\n&gt; of a priori &quot;cheats&quot; that provide the meat of the solution.  Because,\n&gt; after all, how can it be a general intelligence if it needs you to\n&gt; tell it something that it is supposed to be able to figure out?  This\n&gt; perspective, I believe, is aligned with Jeff&#39;s view.\n&gt; \n&gt; However, NE as a long-term pursuit is involved in something different,\n&gt; even though it can be applied to the same problems.  NE is not an\n&gt; attempt to formalize how people learn with sparse reinforcement.\n&gt; Rather, it is an attempt to formalize how evolution can build a brain.\n&gt;  So RL is formalizing the brain itself and NE is formalizing how\n&gt; evolution succeeds in creating a brain.  NE is therefore one step removed.\n&gt; \n&gt; This difference is ultimately a philosophical difference on the best\n&gt; approach to creating a full-blown AI.  The instrumental issue is\n&gt; whether you think it&#39;s easier to build it yourself or to design an\n&gt; algorithm that can build it.  The confusion and hence conflation of\n&gt; the two approaches arises in part because they do indeed both aim at\n&gt; the same long view goal: a general AI.  But they are coming at it from\n&gt; very different angles.\n&gt; \n&gt; And because of this stark difference, the *metric* of progress should\n&gt; be quite different.  We cannot measure our progress in building a\n&gt; general intelligence directly in the same way that we measure our\n&gt; progress in creating an evolutionary algorithm that itself will\n&gt; someday output one.\n&gt; \n&gt; This distinction is potentially subtle and confusing so let me try to\n&gt; make it clearer:  Human brains aren&#39;t designed to build yet more human\n&gt; brains.  We are good at a lot of things, and we learn generally, but\n&gt; we do not build 100-trillion part devices that are more complex than\n&gt; any known object in the universe.  I&#39;m not saying we won&#39;t ever be\n&gt; able to do it, but if you want to simulate a human brain, your first\n&gt; thought would not be that it needs to be capable of designing yet\n&gt; another brain by itself.  Your first thought is about things like\n&gt; object recognition or pursuit and evasion.\n&gt; \n&gt; In contrast, building brains is exactly what natural evolution did,\n&gt; and it did it quite well.  Natural evolution does not perform object\n&gt; recognition; it does not communicate with language; it does not run\n&gt; away from predators or hunt for prey.  Yet it does build brains that\n&gt; themselves do those things.  And that is the aspect of it we wish to\n&gt; harness- a very specific niche kind of skill (though radically\n&gt; impressive)- not a general skill.\n&gt; \n&gt; So the two pursuits are really quite different.  And therefore they\n&gt; deserve different metrics to judge their progress with respect to the\n&gt; long term goal.  That is, unless we conflate them to be the same\n&gt; thing, which we often do without thinking about it.\n&gt; \n&gt; For example, we could just say, well, both NE and RL are learning\n&gt; techniques, and after all, we can apply them to the same problems, so\n&gt; why make a big distinction in how we judge them?  Let&#39;s just compare\n&gt; them directly on the same benchmarks and get on with it.\n&gt; \n&gt; That&#39;s fine for the short-term view, i.e. let&#39;s just improve our\n&gt; ability to tackle practical problems, but for the long view, they\n&gt; cannot be judged in the same way.  If I improve at my ability to\n&gt; balance on one foot is that a sign that I will be able to build a\n&gt; brain someday?  If evolution evolves a brain that plays checkers, is\n&gt; that a sign that evolution *itself* is on the road to performing\n&gt; object recognition?  These are totally different pursuits.\n&gt; \n&gt; So in that context, how should they be judged with respect to long\n&gt; term goals?  Well, I think RL deserves to be judged based on its\n&gt; increasing ability to learn more generally.  And in that sense,\n&gt; exactly Jeff&#39;s criteria should apply to it: We should be interested in\n&gt; whether it &quot;needs&quot; a priori information to learn.  In other words, the\n&gt; less we need to constrain the problem for the learner, the more\n&gt; impressed we deserve to be.  That shows progress towards more and more\n&gt; general AI and ML.\n&gt; \n&gt; But if evolution is not *itself* supposed to be a general learner\n&gt; (rather, we just want it to concentrate on one very specific skill:\n&gt; brain building), then those considerations are orthogonal to its\n&gt; greatest promise.  Its promise is to evolve a brain itself, and as\n&gt; such, neuroevolutionary algorithms deserve to be judged on our ability\n&gt; to *constrain* the problem so that they can accomplish exactly that.\n&gt; In other words, the problem NE *algorithms* face is leaps and bounds\n&gt; beyond what RL algorithms face.  RL algorithms just need to be able to\n&gt; do as well as brains; NE has to be able to discover brains themselves.\n&gt;  Therefore, progress is NE should in part be measured with respect to\n&gt; progress in constraining the problem to make such a discovery more\n&gt; likely.  When an NE algorithm is improved to allow us to tell it more\n&gt; about the world in which its output will be situated, that is good\n&gt; news for the long view.  In short, we don&#39;t care at all how NE\n&gt; produced a brain as long as it really does.  Will anyone complain if a\n&gt; human brain pops out of a system that was a priori given the concept\n&gt; of symmetry?  Rather, we should be glad that such a priori context was\n&gt; possible to provide in the first place, because it may have saved us a\n&gt; year of wasted computation in figuring it out needlessly.\n&gt; \n&gt; This distinction is almost completely ignored when NE and RL are\n&gt; compared directly.  Therefore, the implications of any such comparison\n&gt; are fuzzy and lacking context with respect to the long view.  I am not\n&gt; sure if I should care or not if RL solves something better than NE, or\n&gt; vice versa, because the author doesn&#39;t explain how the result aligns\n&gt; with the long-term goals of the fields.  Long term goals seem like\n&gt; unwelcome guests these days in AI, which is why I probably won&#39;t be\n&gt; writing about any of this in a publication any time soon.\n&gt; \n&gt; ...\n&gt; \n&gt; So Derek what you are saying about NE being good at &quot;hard-wired&quot;\n&gt; solutions and RL being appropriate for ontogenetic lifetime learning,\n&gt; while true, is not what I think of as the primary long-view issue.\n&gt; \n&gt; In the long view, NE will be used to evolve structures that do learn\n&gt; over their lifetime, i.e. not hardwired at all.  The only reason that\n&gt; it tends to be used to evolve hardwired solutions today is because we\n&gt; are trying to get a foothold on how to evolve certain types of complex\n&gt; structures.   Once we get very good at it, focus will naturally shift\n&gt; to evolving dynamic brains (and of course there is already work along\n&gt; these lines today, much from Floreano).  I do not even think that we\n&gt; will need to include stock learning algorithms like Hebbian learning.\n&gt;  When we achieve our long-term goals, those *themselves* will be left\n&gt; up to evolution because after all there may be something even better.\n&gt;  \n&gt;&gt;&gt; My aim is to design an\n&gt;&gt;&gt;  algorithm that will output a brain, not to design the brain itself.\n&gt;&gt; \n&gt;&gt; But what kind of brain are you wanting to output?\n&gt;&gt; \n&gt; \n&gt; Note that I&#39;m speaking purely about the long view for these different\n&gt; fields here.  Of course on a day-to-day basis I am not solely focused\n&gt; on what will happen 100 years from now.  On a practical day-to-day\n&gt; basis, of course I want to make NE better capable to tackle problems\n&gt; that e.g. RL tackles.  So in the short-term context, I just want to\n&gt; output something that works for the problem at hand.\n&gt; \n&gt; But in the long view, which we were talking about, I think the\n&gt; ultimate goal would be to output a full-fledged adaptive system with\n&gt; astronomical complexity and the power and subtlety of human reasoning.\n&gt;  On that path, constraint is the only hope, unless you want to wait\n&gt; three billion years and just hope in the meantime that the initial\n&gt; conditions were set up correctly.  Therefore, demonstrations of the\n&gt; power of constraint deserve to be judged as evidence of the promise of\n&gt; and progress towards the long term goal in NE.\n&gt; \n&gt; ken\n&gt; \n&gt; \n\n\n\n"}}