{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":115403844,"authorName":"John Arrowwood","from":"&quot;John Arrowwood&quot; &lt;jarrowwx@...&gt;","profile":"jarrowwx","replyTo":"LIST","senderId":"lSd2RfYQbOdzu08iOHwouZ2NrqcrY-yNZsZsvUn0CGX4eVYdY4sZ5MHnLfd90Ik-otrueMm2ZV3nTQPsijbiuhEa_8d_9OWxJCuPO4-0","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] Re: Computation Time","postDate":"1086378278","msgId":999,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PEJBWTItRjI3b2xwbG1Va0ptSTcwMDA0NDQxNEBob3RtYWlsLmNvbT4="},"prevInTopic":993,"nextInTopic":1018,"prevInTime":998,"nextInTime":1000,"topicId":845,"numMessagesInTopic":99,"msgSnippet":"... What I intend to do is the equivalent of a partial-evaluator.  For a given network, the weights are fixed.  At least, for now...  So, I can leverage that","rawEmail":"Return-Path: &lt;jarrowwx@...&gt;\r\nX-Sender: jarrowwx@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 81953 invoked from network); 4 Jun 2004 19:45:01 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m2.grp.scd.yahoo.com with QMQP; 4 Jun 2004 19:45:01 -0000\r\nReceived: from unknown (HELO hotmail.com) (65.54.247.27)\n  by mta5.grp.scd.yahoo.com with SMTP; 4 Jun 2004 19:45:01 -0000\r\nReceived: from mail pickup service by hotmail.com with Microsoft SMTPSVC;\n\t Fri, 4 Jun 2004 12:44:38 -0700\r\nReceived: from 64.122.44.102 by by2fd.bay2.hotmail.msn.com with HTTP;\n\tFri, 04 Jun 2004 19:44:38 GMT\r\nX-Originating-Email: [jarrowwx@...]\r\nX-Sender: jarrowwx@...\r\nTo: neat@yahoogroups.com\r\nBcc: \r\nDate: Fri, 04 Jun 2004 12:44:38 -0700\r\nMime-Version: 1.0\r\nContent-Type: text/plain; format=flowed\r\nMessage-ID: &lt;BAY2-F27olplmUkJmI700044414@...&gt;\r\nX-OriginalArrivalTime: 04 Jun 2004 19:44:38.0892 (UTC) FILETIME=[5F4B72C0:01C44A6C]\r\nX-eGroups-Remote-IP: 65.54.247.27\r\nFrom: &quot;John Arrowwood&quot; &lt;jarrowwx@...&gt;\r\nReply-To: john@...\r\nSubject: Re: [neat] Re: Computation Time\r\nX-Yahoo-Group-Post: member; u=115403844\r\nX-Yahoo-Profile: jarrowwx\r\n\r\n&gt;From: Ian Badcoe &lt;ian_badcoe@...&gt;\n\n&gt;At 15:38 04/06/2004, you wrote:\n&gt; &gt; &gt;From: Ian Badcoe &lt;ian_badcoe@...&gt;\n&gt; &gt;\n&gt; &gt; &gt;Why are we discussing this BTW, do people have serious speed problems?\n&gt; &gt;\n&gt; &gt;The problem for which I got interested in NEAT is image enlargement.  The\n&gt; &gt;evaluation of a single network for fitness will require it to be \n&gt;activated\n&gt; &gt;*billions* of times.  So in that case, every little bit helps. :)\n&gt;\n&gt;OK, a few thoughts:\n&gt;\n&gt;1) it matters how much data you have, memory-access is still far slower\n&gt;than calculation (unless the whole thing fits in the data cache...) -- by\n&gt;data I mean all data in RAM: images, weights, node-values, all of it\n&gt;\n&gt;2) ANN evaluation is essentially like a classical &quot;gather&quot; problem.  e.g.\n&gt;data has to be gathered from multiple locations (the previous node values),\n&gt;processed and written into a single location (one new node value)\n\nWhat I intend to do is the equivalent of a partial-evaluator.  For a given \nnetwork, the weights are fixed.  At least, for now...  So, I can leverage \nthat knowledge.  I will write a script that takes in the definition of a \nnetwork and writes out a function (or maybe a class) that calculates the \nresult.  It will take in an array of node values (just doubles, not \nstructures).  It won&#39;t have to look up the weights, the function will know \nthe weights, they will be hard-coded.  It will not use any loops, it will \nfetch the value of a node, multiply it by the known weight, and add it to an \naccumulator.  Once it has finished the accumulation for one node, it will \napply the appropriate (e.g. sigmoid) function to it, then save that as that \nnode&#39;s new value.  The processing order will be such that an entire \nfeed-forward network is calculated in a single pass.\n\nI think the FASTEST way to do it is:\n\ninitialize a register variable to 0\nget the value of the first node, multiply times weight, add to register\ndo same for second, third, and fourth node.\n(weights and relative address of node values are hard-coded in function)\napply sigmoid\nsave variable to node value\n(repeat for all nodes)\n\n&gt;3) when they were rolling out the extended SIMD instruction sets (MMX and\n&gt;the later floating-point version, which I think was called SMME -- for\n&gt;streaming multi-media extensions) they covered some problems quite like\n&gt;this in the tutorials they gave away.  They took a purely intel-centric\n&gt;view, of course, but many of their general points should apply.\n\nIf there is a way to do it using those fancy extension instructions that \nmight be faster, I&#39;d be happy to give it a try.  Though that might mean that \nmy laptop (a P2) couldn&#39;t participate in the network evaluation.  But my \nworkhorse system, a dual P4 xeon, should be able to, and if it gives a \nsignificant speed boost, I&#39;d use it there.\n\nYou know where I can get my hands on one of those tutorials?\n\n&gt;4) since you are running the same network many times with different input\n&gt;data then you might get a big speed-up if you can re-arrange the order of\n&gt;processing.  I assume that at the moment...[snip]\n\nAt the moment, I still haven&#39;t built it yet.  I&#39;m building an implementation \nof NEAT (in perl) that is conducive to distributed processing.  Once that&#39;s \ndone, and tested using XOR and maybe pole-balencing, THEN I&#39;ll start working \non the image enlargement experiment.\n\n&gt;you evaluate a whole network, then\n&gt;another whole network, then another, etc etc.  My first thought it to\n&gt;reorder this so that one link is evaluated for every point in the image,\n&gt;then the next link and so on.  This way you are performing a simple\n&gt;operation on big arrays (and it is important to sort the data so that it is\n&gt;in contiguous arrays).  Thus, if you had a trivial net:\n&gt;\n&gt;IMAGE --w1-&gt; NODE --w2-&gt; OUTPUT\n&gt;\n&gt;Then instead of.\n&gt;\n&gt;for (x,y) = every_point_in_image\n&gt;          NODE = IMAGE[x] * w[1];\n&gt;          NODE = ActivationFunction(NODE);\n&gt;          OUTPUT[x] = NODE * w[2];\n&gt;next x\n&gt;\n&gt;-- which has a random data access pattern (OK, so not very random but the\n&gt;more nodes you add the more it is) --\n\nThe network will have 81 pixel inputs, a 9x9 grid, extraced from the image \nto be enlarged.  Technically, they won&#39;t be pixels, though, they will be \n&#39;relative brightness&#39; that have been scaled so that the brightest pixel is 1 \nand the darkest is 0, with some exceptions.\n\nAnd it will have 4 other inputs, whose role is to specify a region in that \nspace (upper-left to lower-right).  The network must approximate a function \nthat is represented by the 81 pixel inputs, and then as output return the \naverage of the area of that function represented by the region inputs.\n\nIn order to encourage robustness, I will evaluate the network once for each \nof several different variations of each input.  Normal, negative/inverse, \nrotated, and mirrored.  I believe that there are 16 different possible \ncombinations, or maybe it was 8.  For each such input, it will have to \ncalculate a result given a large number of variations of the region, from \nlarger to smaller, closer to center to farther to the outside.  I will \nweight the result by distance from the center.  I expect the network to be \nbetter at predicting the center-most pixels than it will be at the edges.\n\nIt&#39;s a HUGE problem domain.  And it&#39;s a huge amount of work to evaluate the \nfitness of one single potential network.    I&#39;m going to have to take some \nshortcuts.  There are billions of possible input/output pairs possible \nwithin a single image, and I have thousands of images available to me.  So \nI&#39;m going to have to simplify.\n\nFirst of all, the network should be able to recreate its inputs.  If it \ncan&#39;t do that, there&#39;s no chance it can do enlargement.  if I specify \n0,0,1,1 as the output region, it should output  a single value that \ncorresponds to the average of the 81 inputs.  If I specify 1/8,1/8, 7/8,7/8, \nit should output a value corresponding to the average of the inner 49 \ninputs.  Repeat down to the center 1 pixel.  If the fitness on that task was \ntoo low (not better than random), the testing on that input is done, and the \nresult is 1 plus a fraction corresponding to how close it was.\n\nOtherwise, it continues evaluation, examining each possible 8x8, 7x7, 6x6, \n... 1x1 areas of the inputs, weighted by distance from center.  If it can&#39;t \ndo better than random, it returns 2 plus a fraction.\n\nNow, the original image / expected results will be 144x144 pixels.  The \ninputs will be the 9x9 scaled down version of that.  Once it has gotten this \nfar, it starts over, but looks more carefully.  instead of 1/8,1/8, 7/8,7/8, \nand repeating down towards the center, it can start at \n1/144,1/144,143/144,143/144 and work its way down.  If it can&#39;t do better \nthan random, it returns 3 plus a fraction.\n\nNow, if it is still better than random, it evaluates the network for each of \nthe 144x144 expected pixels, weighting the results by distance from center.  \nIf it can&#39;t do better than random, it returns 4 plus a fraction.\n\nIf it got this far, it&#39;s a good network.  It manages to do a better than \nrandom job of 16x enlargement.  So to differentiate it from others that are \nequally good...\n\nEvaluate it for every possible 2x2 area of the 144x144 expected space.  And \n3x3, and 4x4, and 5x5...15x15.  That will take a long time.The fitness then \nbecomes 5 plus a fraction, where the fraction is between 0 and 1, where 0 is \nabout the same as random, and 1 is &#39;perfect score!&#39;\n\nAnd of course, the same network is evaluated against a mirrored, rotated, \nand negated version of that one input sample, making an average fitness for \nthat one input.\n\nThere are TRILLIONS of those 9x9/144x144 test data pairs to choose from.  \nObviously, I can&#39;t evaluate each network against all of them.  And I should \nprobably evaluate each network against the same ones.  So what I should \nprobably do is:\n\npick an input sample at random.\nevaluate all networks against that input sample.\ndo\n    pick another at random\n    evaluate all networks against that input sample\n    calculate the average fitness of each network so far\n    order the organisms by their relative fitness values\n    calculate who would pass on their genes to the next generation\nwhile the list of who passes on their genes has changed\n\nInitially, no one passes on their genes, so there will be a minimum of 3 \ninput evaluations per generation.\n\nNow, since the input and the structure AND the weights are all known for a \nLARGE number of activations of the network, I may decide to do further \noptimization of the evaluation of one network.  I may generate code based on \nconstants for input values, so that the compiler can optimize the heck out \nof the function.  Yeah...I like it...  :)\n\nHave a script generate a .c file that contains the dynamic &#39;evaluate \nnetwork&#39; function given the expected output range.  The rest of the C source \nis a static algorithm like what was described above.  Initially, since \nfitnesses will be low and execution times will therefore be shorter, I just \ncompile and run it locally.  Once a solution begins to appear, I can make it \nwork in a distributed fashion, bzipping the source and shipping it out to \nclient machines which compile it, run it, and return the results.  The \ncloser we get to a solution, the slower it progresses, but the more \nresources I can employ in the search.\n\nSo, there&#39;s a &quot;brain dump&quot; of my thoughts on how to go about this \nexperiment.  Feedback on any aspect of it is welcome, not just on optimizing \nthe network evaluation execution time.  :)\n\n\n\n&gt;You do this:\n&gt;\n&gt;// DATA contains IMAGE sorted into a linear array\n&gt;\n&gt;for x = 0 to number_of_pixels\n&gt;          NODE[x] = DATA[x] * w[1]\n&gt;next x\n&gt;for x = 0 to number_of_pixels\n&gt;          NODE[x] = ActivationFunction(NODE[x])\n&gt;next x\n&gt;for x = 0 to number_of_pixels\n&gt;          OUTPUT[x] = NODE[x] * w(2)\n&gt;next x\n&gt;\n&gt;-- which accesses data as three straight runs through a set of contiguous\n&gt;arrays.\n&gt;\n&gt;The change that makes to data caching can be considerable.  If you like,\n&gt;I&#39;ll explain in more detail off list, unless this is of general\n&gt;interest?  It&#39;s one of the things people used to do to get oomph out of the\n&gt;vector-pipelines on supercomputers...\n\nI&#39;d love to hear more details, on- or off-list, depending on other people&#39;s \ninterest.\n\n&gt;p.s. What language are you using?\n\nProbably C, for speed.  Except for the actual NEAT code, which will be Perl, \nfor coding convenience.   I&#39;d generate ASM code if I thought it could help! \n:)\n\n\n\n"}}