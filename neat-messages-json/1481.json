{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"3w9x9R2Zm7DZQSnnGe-OcybI1qmHP6K-IhGfLPihwfNF03rEEVoRqhZLo9AlmWfhSPZKsr6gMkNYssg82lNFTUQyVVIBen7fLheN-KRJKTO2","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: IEX musings","postDate":"1093965442","msgId":1481,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGNoMjRxMitmMmY5QGVHcm91cHMuY29tPg==","inReplyToHeader":"PEJBWTItRjI4bkUwOFgyYklRamIwMDBjOWYwNEBob3RtYWlsLmNvbT4="},"prevInTopic":1468,"nextInTopic":1482,"prevInTime":1480,"nextInTime":1482,"topicId":1468,"numMessagesInTopic":15,"msgSnippet":"... complicated ... decision ... actual ... minutes. ... back to the ... do an ... begin, though, ... idea of ... optimize the ... passed ... that take ... ","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 15294 invoked from network); 31 Aug 2004 15:18:36 -0000\r\nReceived: from unknown (66.218.66.218)\n  by m24.grp.scd.yahoo.com with QMQP; 31 Aug 2004 15:18:36 -0000\r\nReceived: from unknown (HELO n1.grp.scd.yahoo.com) (66.218.66.64)\n  by mta3.grp.scd.yahoo.com with SMTP; 31 Aug 2004 15:18:36 -0000\r\nReceived: from [66.218.67.160] by n1.grp.scd.yahoo.com with NNFMP; 31 Aug 2004 15:17:23 -0000\r\nDate: Tue, 31 Aug 2004 15:17:22 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;ch24q2+f2f9@...&gt;\r\nIn-Reply-To: &lt;BAY2-F28nE08X2bIQjb000c9f04@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Length: 8569\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Remote-IP: 66.218.66.64\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: Re: IEX musings\r\nX-Yahoo-Group-Post: member; u=54567749\r\nX-Yahoo-Profile: kenstanley01\r\n\r\n--- In neat@yahoogroups.com, &quot;John Arrowwood&quot; &lt;jarrowwx@h...&gt; wrote:\n&gt; First, a quick update.  I decided to try starting with more \ncomplicated \n&gt; networks, to see how well NEAT could optimize it.  Problem was, my \ndecision \n&gt; to generate optimized code doesn&#39;t scale well.  The generation and \n&gt; compilation of the code can take almost 30 minutes, while the \nactual \n&gt; activations required for a single generation can take a couple of \nminutes.  \n&gt; That just won&#39;t do...  So now I need to re-think things, and go \nback to the \n&gt; more general approach to network activations.  I&#39;ll probably still \ndo an \n&gt; &#39;activation order determination&#39; pass on the network before I \nbegin, though, \n&gt; just to keep it simple and fast.\n&gt; \n&gt; But for scalability reasons, I&#39;m beginning to lean towards the \nidea of \n&gt; having NEAT add topology, and then having the client machine \noptimize the \n&gt; weights by whatever means necessary.  Those optimized weights are \npassed \n&gt; back to NEAT, which are then used as the basis for any mutations \nthat take \n&gt; place based on that topology.  Which started me researching weight \n&gt; optimization techniques.  Which leads me here...  This is \nbasically me just \n&gt; capturing some thoughts on the subject and sharing them with the \ngroup in \n&gt; the hopes that some of you will have some constructive insights \ninto it.\n&gt; \n&gt; First off, what I&#39;m working on is not a controller.  It&#39;s more of \na function \n&gt; approximation, which means I may be able to take some shortcuts I \nwouldn&#39;t \n&gt; be able to, otherwise.  Second, understanding of how a network \nsolves a \n&gt; problem might be insightful in finding a way to speed up weight \n&gt; optimization.  So I&#39;ll give myself a quick review of how a neural \nnetwork \n&gt; works, first...\n&gt; \n&gt; A single neuron is a linear classifier.  Either the inputs meet \nsome \n&gt; criteria, or they don&#39;t.  Generally speaking, in order for a \nsingle neuron \n&gt; to do its job, some of its inputs should indicate that the neuron \nshould \n&gt; have an output in one direction (e.g. 1), and another set of \ninputs indicate \n&gt; that the output should go the other way (e.g. 0 or -1).  The \nweights simply \n&gt; work to segregate the inputs, which must be linearly separable.  \nIf they \n&gt; aren&#39;t, the network can&#39;t find a solution.\n&gt; \n&gt; However, those &#39;inputs&#39; can be anything.  They can be raw inputs, \nor they \n&gt; can be the outputs of other classifiers.  Each neuron in the first \nhidden \n&gt; &#39;layer&#39; is essentially a &#39;feature extractor.&#39;  The weights of each \nof those \n&gt; neurons is used to divide the raw inputs into a particular (linear \n&gt; separable) class.  So a value of 1 on the hidden node means that \nthe input \n&gt; HAS the characteristic that it is looking for.\n&gt; \n&gt; The combination of 1st-layer hidden nodes is, in essence, a \nfeature map.  If \n&gt; these two hidden nodes are active, but the rest are not, that \nmeans the \n&gt; inputs contain these two features.  But what features are they?  \nWhich \n&gt; features of the inputs should that first layer be characterizing?  \nThat \n&gt; really depends on the problem, of course.\n&gt; \n&gt; But if that first layer is basically a feature map, why couldn&#39;t \nthe weights \n&gt; be initially set the same way a self-organizing map is?  Then, \noptimization \n&gt; of the weights of the first layer is a fairly straight-forward \nproblem with \n&gt; a provable (?) solution guaranteed (?) to converge fairly quickly \n(?).  You \n&gt; can also apply domain-specific knowledge to the initial \nconfiguration of \n&gt; those weights.  For example, for the image enlargement domain, you \ncould \n&gt; define the initial weights in such a way that each node tends to \nlook for a \n&gt; particular shape in the inputs, like a gradient at a particular \nangle.  \n&gt; Those initial weights are randomly perturbed slightly, to avoid \nsymetry \n&gt; causing multiple nodes to be &#39;best&#39; during training of the SOM.  \nThen just \n&gt; learn those weights as a SOM.  Next, train the second-level \nweights via \n&gt; back-propagation with the first layer weights clamped.  Once \noptimal as they \n&gt; can be, unclamp the first layer weights, and use back-propagation \nto zero in \n&gt; on an optimal solution from a starting point that is more likely \nthan a \n&gt; random configuration of being able to find the global minimum.  Or \nat least, \n&gt; it seems intuitive that it would be more likely, simply because \nyou know \n&gt; that the first layer is actually dividing up the inputs in a \nreasonable way.\n&gt; \n&gt; The above technique should apply to networks with multiple hidden \nlayers, as \n&gt; well.  For each subsequent &#39;layer&#39; that is not the output layer, \nfirst set \n&gt; the weights as an SOM.  Then move on to the next hidden layer.  \nOnce you \n&gt; reach the output layer, set the initial weights via back-\npropagation (or \n&gt; some other gradient-descent technique).  Then start unclamping \nweights one \n&gt; layer at a time, starting at the lowest level and working your way \nback up, \n&gt; optimizing each layer at a time until the whole network is \noptimized, but \n&gt; from a starting position that may well already be near the optimum.\n&gt; \n&gt; A key to success of that approach is being able to treat the layer \nas a \n&gt; self-organizing map.  But I suspect that a generalized approach to \ndoing so \n&gt; is feasible.  Anyone have any thoughts on this idea?\n&gt; \n\nYou can definitely train a SOM on an input vector to get higher-\nlevel features that are further input into something else, such as a \nNEAT network.  In fact, you can also layer SOMs on top of each other \nas you suggest.  The Constructivist Learning Architecture (CLA), \ndeveloped in our NN research group at UT by Harold Chaput, actually \nimplements this idea already:\n\nhttp://www.cs.utexas.edu/users/chaput/cla/\n\nHowever, I&#39;m not sure you can further backprop on SOM weights.  I \ndon&#39;t remember all the details but the SOM activation function and \nlearning mechanism may mean that it develops different weights than \na backprop network would.  Maybe they can be made compatible though, \nbut you need to make sure.\n\nI&#39;ve played with inputting SOM nodes into a NEAT network before with \nsome success.  Also, it is sometimes possible to do Hebbian learning \noff of a SOM.  In other words,  if the SOM captures things at the \nright level, you don&#39;t need any more hidden nodes so you don&#39;t need \nto use a complete backprop procedure.  Of course, that&#39;s a big if.\n\n&gt; But there is another aspect of the problem of image enlargement \nthat is not \n&gt; even addressed by this discussion.  That is, the &#39;input window&#39; \nparameters.\n&gt; \n&gt; In a desire to create a network that can do arbitrary scale image \n&gt; enlargement, I designed a network where one input represents the \nleft-most \n&gt; edge of the area of interest, one represents the right-most, one \nrepresents \n&gt; the top, and one the bottom.  The output of the network is \nsupposed to \n&gt; represent the average height of the curve within that region.  \nThat \n&gt; requirement may well be the primary reason I&#39;ve been unable to \nevolve a \n&gt; solution so far...\n&gt; \n&gt; Since there is only one output node, and since a neuron/node is a \nlinear \n&gt; classifier, the network must have a structure that presents inputs \nto that \n&gt; output node that are in essense a simple linearly separable \nproblem.  Can \n&gt; such a thing even exist for what I&#39;m trying to do?  And would it \nrequire me \n&gt; to first have a network that enlarges to some high scale, and then \nuses \n&gt; those inputs to &#39;look up&#39; the ouput value, thereby making me \nbetter off just \n&gt; doing a fixed scale enlargement?  I&#39;m going to have to figure that \nout \n&gt; before I will know if what I have set out to do is even feasible.\n&gt; \n&gt; Let&#39;s analyze the problem a bit.  Suppose the inputs are this:\n&gt; \n&gt; 1 2 3\n&gt; 2 2 3\n&gt; 3 3 3\n&gt; \n&gt; Suppose the &#39;position&#39; inputs were 0,0, 1/3, 1/3.  The expected \noutput is 1. \n&gt;   At 1/3,0,2/3,1/3, the  expected output is 2.  But at \n0,0,2/3,2/3, the \n&gt; expected output is 7/4.  And at 0,0,1,1, the expected output is \n22/9.\n&gt; \n\nCan you remind me what the coordinates mean in the &quot;position?&quot;  It&#39;s \nbeen a while since I&#39;ve looked at all this and I was distracted with \nmy dissertation.  Maybe you can show the result that these position \ninputs would produce graphically somehow to put it in context.\n\nken\n\n&gt; Perhaps what I should really do at this point is see if I can \nevolve a \n&gt; network that does this.  One set of inputs, 14 expected outputs, \nand see \n&gt; what, if anything, NEAT can evolve that would solve the problem.  \nIf NEAT \n&gt; can&#39;t evolve a solution to that, then I know that NEAT can&#39;t \nevolve an image \n&gt; enlargement network that utilizes this technique.  I&#39;ll let you \nknow what I \n&gt; find out...\n&gt; \n&gt; -- John\n\n\n"}}