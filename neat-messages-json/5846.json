{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Ken","from":"&quot;Ken&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"sEhZFil9C4Rh6ubZXT2pvSx_Dw2om6zj1n56QrCLdkAIdmxFgDt6Rya9qj4F3QjoLSA4EeQb7yO3xQzlTHK1AP3JWV9b","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: Lessons From Data Mining - The Power of Ensembles","postDate":"1346310297","msgId":5846,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGsxbjNhcCtjbzBzQGVHcm91cHMuY29tPg==","inReplyToHeader":"PENBRTBNK1lmOEoyWmNEYjJGOXZEUE83d3RqUnlKeUVXZTc9WEZUTjU3SldxbVdYVWY4d0BtYWlsLmdtYWlsLmNvbT4="},"prevInTopic":5845,"nextInTopic":5847,"prevInTime":5845,"nextInTime":5847,"topicId":5844,"numMessagesInTopic":4,"msgSnippet":"Hi Colin, yes the success of these ensembles at obtaining accurate results is interesting.  Actually I played with decision tree ensembles a bit a long time","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 85033 invoked from network); 30 Aug 2012 07:04:58 -0000\r\nX-Received: from unknown (98.137.35.160)\n  by m8.grp.sp2.yahoo.com with QMQP; 30 Aug 2012 07:04:58 -0000\r\nX-Received: from unknown (HELO ng16-ip1.bullet.mail.ne1.yahoo.com) (98.138.215.243)\n  by mta4.grp.sp2.yahoo.com with SMTP; 30 Aug 2012 07:04:58 -0000\r\nX-Received: from [98.138.217.177] by ng16.bullet.mail.ne1.yahoo.com with NNFMP; 30 Aug 2012 07:04:57 -0000\r\nX-Received: from [98.137.34.119] by tg2.bullet.mail.ne1.yahoo.com with NNFMP; 30 Aug 2012 07:04:57 -0000\r\nDate: Thu, 30 Aug 2012 07:04:57 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;k1n3ap+co0s@...&gt;\r\nIn-Reply-To: &lt;CAE0M+Yf8J2ZcDb2F9vDPO7wtjRyJyEWe7=XFTN57JWqmWXUf8w@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: &quot;Ken&quot; &lt;kstanley@...&gt;\r\nSubject: Re: Lessons From Data Mining - The Power of Ensembles\r\nX-Yahoo-Group-Post: member; u=54567749; y=jAKaRnVGjQxY5q_e6Vy2vFTmCNDaxNr7YlVFoEiTI39TPyr3O5Mi\r\nX-Yahoo-Profile: kenstanley01\r\n\r\n\n\nHi Colin, yes the success of these ensembles at obtaining accurate result=\r\ns is interesting.  Actually I played with decision tree ensembles a bit a l=\r\nong time ago - one of the few papers I&#39;ve written that has nothing to do wi=\r\nth evolution introduced an algorithm called the &quot;Concept Drift Committee&quot; (=\r\nCDC) for tracking a drifting concept with a &quot;committee&quot; (really an ensemble=\r\n) of decision trees.  It seemed to work pretty well at the time compared to=\r\n other drift trackers.  I wrote it up only as a tech report when I was in g=\r\nrad school:\n\nhttp://nn.cs.utexas.edu/downloads/papers/stanley.utcstr03-302.=\r\npdf\n\nIf I understand your idea right, it&#39;s to make an individual in an evol=\r\nving population itself a kind of network ensemble that exhibits the benefic=\r\nial ensemble averaging effect.  The thing I&#39;m trying to figure out about th=\r\nis idea is whether tying the ensemble together as an individual is an advan=\r\ntage or a disadvantage.  For example, in an alternative conception, you cou=\r\nld instead cooperatively coevolve separate classifier networks that then co=\r\nme together into transient ensembles to make decisions.  Which is going to =\r\nwork better and capture the nice ensemble effect: the algorithm that causes=\r\n them the ensemble component networks to  stick together or the one that br=\r\neaks them apart and mixes them up during training?\n\nSticking them together =\r\nin one genome does imply role differentiation and cooperation of a sort tha=\r\nt is less likely in a totally free-for-all coevolution.  On the other hand =\r\nif you had separate subpopulations of networks from which individuals are p=\r\nut together into ensembles (like in the ESP algorithm) then each subpopulat=\r\nion could differentiate into its own kind of role.  \n\nBut I&#39;m not sure that=\r\n really role differentiation towards specific functionalities is what makes=\r\n these Random Forest-type systems work?  Or is it more just about the idea =\r\nthat having a bunch of amateurs is better than a single expert (but the ama=\r\nteurs aren&#39;t particular &quot;specialized&quot; themselves)?  In that case, role diff=\r\nerentiation would not be that important compared to just some statistical v=\r\nariation in general behavior.\n\nTo confuse things even more, maybe indirect =\r\nencoding can apply to this kind of thing after all - for example multiagent=\r\n HyperNEAT can generate a whole team (you could call it an ensemble) of neu=\r\nral networks that share some regularities but differ in other ways.  Whethe=\r\nr that is good or bad to get the kind of ensemble effect we&#39;re discussing i=\r\ns unclear to me, but maybe there would be some benefit in the fact that the=\r\n indirect encoding ensures the networks in the ensemble share some properti=\r\nes.\n\nWhat do you think about these issues?\n\nken\n\n\n\n--- In neat@yahoogroups.=\r\ncom, Colin Green &lt;colin.green1@...&gt; wrote:\n&gt;\n&gt; Hi all,\n&gt; \n&gt; I&#39;m guessing th=\r\nat many on here will by now be away of the data mining\n&gt; competition web si=\r\nte - kaggle. If not then I&#39;d recommend checking it\n&gt; out as there are some =\r\ninteresting data sets being put up there now on\n&gt; a regular basis.\n&gt; \n&gt; One=\r\n of the the main lessons for me from competing in kaggle\n&gt; competitions has=\r\n been that no matter which underlying technique I use\n&gt; to model the data i=\r\nt generally isn&#39;t competitive. To make that leap\n&gt; into competitive modelli=\r\nng requires ensembles of models. And ensembles\n&gt; of many weak models can an=\r\nd do result in strong models. In fact the\n&gt; main method in use on kaggle is=\r\n Random Forests (ensemble of Decision\n&gt; Trees), in which each decision tree=\r\n is generally a really poor model\n&gt; all by itself -  but they are very fast=\r\n to construct, so you can build\n&gt; an ensemble of hundreds or thousands in a=\r\n reasonable span of time.\n&gt; \n&gt; Si I&#39;d like to throw out some ideas regardin=\r\ng what the EC community\n&gt; can learn from this, if anything. Does the power =\r\nof ensembles apply to\n&gt; EC in any way?\n&gt; \n&gt; \n&gt; The general approach is to t=\r\nake an underlying technique - for me this\n&gt; is usually a gradient descent a=\r\npproach - and to apply some randomness\n&gt; to it, such that each time you run=\r\n it the model produced will be\n&gt; different. One of the most common techniqu=\r\nes is to use a random\n&gt; sub-set of the training data to learn against (e.g.=\r\n bootstrap\n&gt; aggregation) and/or to use a random sub-set of variables to mo=\r\ndel.\n&gt; Basically any sources of randomness you can think of that don&#39;t\n&gt; co=\r\nmpletely break the model.\n&gt; \n&gt; You can imaging that each model in the ensem=\r\nble is modelling the data\n&gt; in a slightly different way, but a no less corr=\r\nect way overall (they\n&gt; all score ok), and therefore when we combine them t=\r\nhe errors that each\n&gt; of them make will tend to be averaged down - making a=\r\n better overall\n&gt; model.\n&gt; \n&gt; I want to apply this idea NEAT but won&#39;t have=\r\n time to work on it for a\n&gt; while so I thought I would throw the idea out t=\r\nhere for feedback and\n&gt; maybe it will spark some discussion. My initial tho=\r\nughts on how this\n&gt; could apply to NEAT are as follows:\n&gt; \n&gt; \n&gt; 1) Each gen=\r\nome will now be made up of multiple distinct parts, each\n&gt; part representin=\r\ng a distinct ANN, so basically the new genome is a\n&gt; collection of traditio=\r\nnal NEAT genomes. Lets call the distinct parts\n&gt; chromosomes for now - mayb=\r\ne there is a better word to use there?\n&gt; \n&gt; 2) We evaluate a genome by deco=\r\nding each chromosome in turn into a\n&gt; network and then combing the networks=\r\n into an ensemble of networks .\n&gt; Note that this is functionally different =\r\nto combining them into one\n&gt; giant network, *if* you allow connections from=\r\n the output nodes back\n&gt; into the hidden nodes - that would mean that the s=\r\nub-networks would be\n&gt; actually connected up and passing signals between ea=\r\nch other.\n&gt; \n&gt; 3) We can put the ensemble behind an interface such as INetw=\r\nork, such\n&gt; that existing code that handles and evaluates INetworks(s) cont=\r\ninues\n&gt; as before.\n&gt; \n&gt; 4) When performing crossover we can now choose to d=\r\no this at the\n&gt; sub-network level as an extra option. So swap in a differen=\r\nt\n&gt; sub-network(chromosome), or add and remove sub-networks, or add a new\n&gt;=\r\n randomly created sub-network.\n&gt; \n&gt; \n&gt; You might think that this is not qua=\r\nlitatively different to what we\n&gt; have. The ensemble could be represented w=\r\nith one large ANN and\n&gt; therefore our existing genome structure (barring th=\r\ne issue about\n&gt; sharing of output nodes). My suspicion is that there *is* a=\r\n\n&gt; qualitative difference here, that because the sub-nets can be copied\n&gt; i=\r\nn their entirety that they will co-evolve and each will focus on\n&gt; specific=\r\n functionality. It also possibly provides another mechanism\n&gt; for allowing =\r\nlarge scale changes to the ensemble ANN that are good and\n&gt; sound changes, =\r\nrather than relying on fitness sharing and speciation\n&gt; to allow a chain of=\r\n small bad mutations that lead to a larger positive\n&gt; change.\n&gt; \n&gt; I could =\r\ngo into this a bit more but hopefully I&#39;ve said enough to\n&gt; convey the idea=\r\n.\n&gt; \n&gt; Thoughts?\n&gt; \n&gt; Colin\n&gt; \n&gt; P.S. I figure this approach is orthogonal =\r\nto much of the current\n&gt; research such as novelty search and indirect encod=\r\nings.\n&gt;\n\n\n\n"}}