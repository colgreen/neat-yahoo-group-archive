{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":283334584,"authorName":"petar_chervenski","from":"&quot;petar_chervenski&quot; &lt;petar_chervenski@...&gt;","profile":"petar_chervenski","replyTo":"LIST","senderId":"RZIY1epQIiXLyDQzQ7Yy8ZgFLAmiwv9cBZZZ6ChSGPmmfhxfno1tRPRQMtOORigplcVDsIbPwPlWiUY_PXyMajP6Y1rBEY8CeUSWBYWJBpJ5pmcpPKg","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Backpropagation and NEAT","postDate":"1205460007","msgId":3867,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGZyY203OCtncDQ0QGVHcm91cHMuY29tPg==","inReplyToHeader":"PGZyYzE2OCtodGE3QGVHcm91cHMuY29tPg=="},"prevInTopic":3865,"nextInTopic":3868,"prevInTime":3866,"nextInTime":3868,"topicId":3846,"numMessagesInTopic":41,"msgSnippet":"Given the simplest topology (a perceptron structure), the local minima is just one. Perceptrons are always guaranteed to converge on correct weights. But","rawEmail":"Return-Path: &lt;petar_chervenski@...&gt;\r\nX-Sender: petar_chervenski@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 70045 invoked from network); 14 Mar 2008 02:00:10 -0000\r\nX-Received: from unknown (66.218.67.94)\n  by m42.grp.scd.yahoo.com with QMQP; 14 Mar 2008 02:00:10 -0000\r\nX-Received: from unknown (HELO n51b.bullet.mail.sp1.yahoo.com) (66.163.168.165)\n  by mta15.grp.scd.yahoo.com with SMTP; 14 Mar 2008 02:00:09 -0000\r\nX-Received: from [216.252.122.217] by n51.bullet.mail.sp1.yahoo.com with NNFMP; 14 Mar 2008 02:00:09 -0000\r\nX-Received: from [66.218.69.1] by t2.bullet.sp1.yahoo.com with NNFMP; 14 Mar 2008 02:00:09 -0000\r\nX-Received: from [66.218.66.73] by t1.bullet.scd.yahoo.com with NNFMP; 14 Mar 2008 02:00:09 -0000\r\nDate: Fri, 14 Mar 2008 02:00:07 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;frcm78+gp44@...&gt;\r\nIn-Reply-To: &lt;frc168+hta7@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;petar_chervenski&quot; &lt;petar_chervenski@...&gt;\r\nSubject: Re: Backpropagation and NEAT\r\nX-Yahoo-Group-Post: member; u=283334584; y=P_ztS20b4pAeAReLszgLuPVBuohH9-SGByIdTa1ETKg08Sq5wJfZZNOB-w\r\nX-Yahoo-Profile: petar_chervenski\r\n\r\nGiven the simplest topology (a perceptron structure), the local \nminima is =\r\njust one. Perceptrons are always guaranteed to converge on \ncorrect weights=\r\n. But increasing the dimentionality of the solution \nincreases the error su=\r\nrface&#39;s curvature as well. So more dimentions \nmeans more complex error sur=\r\nface. The coolest thing in NEAT is that \nwhen it increases the dimentionali=\r\nty of the solution, the individuals \nare already located in a promising are=\r\na of the new space. In fact \nthose spaces are related to each other - you d=\r\non&#39;t know how the error \nsurface is going to look like when you enter the n=\r\new space with more \ndimentions. There are unlimited possibilities. \nSo what=\r\n local gradient search will do in essence is pushing the \nweights towards t=\r\nhe *local* minumim.. It is not guaranteed that this \nis the *solution*! It =\r\nis simply because you don&#39;t know the solution&#39;s \ndimentionality at first. I=\r\nt may require 3 or 21342532 dimentions. \nDon&#39;t forget that NEAT complexifie=\r\ns solutions incrementaly. \n\nPeter\n\n--- In neat@yahoogroups.com, &quot;afcarl2&quot; &lt;=\r\na.carl@...&gt; wrote:\n&gt;\n&gt; In fact, it may be that a substancial portion of the=\r\n value-added of \n&gt; speciation and niche protection of infant organisms, is =\r\nassociated \n&gt; with providing opportunity to accumulate sufficient neighborh=\r\nood \n&gt; evaluations to &quot;discover&quot; the same local minimia over multiple \n&gt; ge=\r\nnerations, that a local search may discover in one generation. \nAnd \n&gt; main=\r\ntaining multiple species in hope that one of the local minimia \n&gt; will in f=\r\nact also be the global minimia.\n&gt; \n&gt; --- In neat@yahoogroups.com, &quot;afcarl2&quot;=\r\n &lt;a.carl@&gt; wrote:\n&gt; &gt;\n&gt; &gt; If &quot;most individuals in a species represented by =\r\na given \ntopology&quot; \n&gt; &gt; ended up in &quot;the same local minimia&quot;, one could arg=\r\nue that the \n&gt; &gt; subject specie&#39;s logical end point was the same local mini=\r\nmia, \nand \n&gt; &gt; that the cost of maintaining more than one organism was \n&gt; &gt;=\r\n computationally wasteful. Better to know sooner and breed \n&gt; additional \n&gt;=\r\n &gt; organisms of differing topology so as to maintain the population \n&gt; size=\r\n \n&gt; &gt; and maximize the population&#39;s &quot;effective&quot; diversity.\n&gt; &gt; \n&gt; &gt; Paying =\r\nmore for the same answer does not make it a better answer.\n&gt; &gt; \n&gt; &gt; --- In =\r\nneat@yahoogroups.com, &quot;petar_chervenski&quot; \n&gt; &gt; &lt;petar_chervenski@&gt; wrote:\n&gt; =\r\n&gt; &gt;\n&gt; &gt; &gt; Well I think that encoding the resulting weights back to the \n&gt; g=\r\nenome \n&gt; &gt; &gt; would somehow hurt the population weight diversity, since most=\r\n \n&gt; &gt; &gt; individuals in a species represented by a given topology can \nend \n=\r\n&gt; up \n&gt; &gt; &gt; in the same local minima, thus leaving out a species with the \n=\r\n&gt; &gt; nearly \n&gt; &gt; &gt; same individuals, i.e. clones. \n&gt; &gt; &gt; This is why I think=\r\n that backprop should be applied occasionaly \n&gt; &gt; after \n&gt; &gt; &gt; long periods=\r\n of stagnation, for example the cases where delta-\n&gt; &gt; coding \n&gt; &gt; &gt; kicks =\r\nin, when it focuses the search in the most promising \nareas \n&gt; of \n&gt; &gt; &gt; th=\r\ne search space. \n&gt; &gt; &gt; I am still trying to re-implement RTRL myself, thoug=\r\nh.. Then \nI&#39;ll \n&gt; &gt; see \n&gt; &gt; &gt; if it is going to actually enhance performan=\r\nce. \n&gt; &gt; &gt; \n&gt; &gt; &gt; Peter\n&gt; &gt; &gt; \n&gt; &gt; &gt; --- In neat@yahoogroups.com, &quot;Kenneth =\r\nStanley&quot; &lt;kstanley@&gt; \nwrote:\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; Rafael, thank you for pointing=\r\n out the connection to memetic \n&gt; &gt; &gt; &gt; algorithms.  That is good to point =\r\nout that such a \ncombination \n&gt; &gt; &gt; falls \n&gt; &gt; &gt; &gt; under that category.\n&gt; &gt;=\r\n &gt; &gt; \n&gt; &gt; &gt; &gt; However, there are still those who would argue that the local=\r\n \n&gt; &gt; &gt; search \n&gt; &gt; &gt; &gt; method should not be encoded back into the genome, =\r\nthat is, \n&gt; that \n&gt; &gt; &gt; &gt; evolution should simply search for the best start=\r\ning point \nfrom \n&gt; &gt; &gt; which \n&gt; &gt; &gt; &gt; a local search would depart.  Because=\r\n of the Baldwin Effect, \n&gt; that \n&gt; &gt; &gt; may \n&gt; &gt; &gt; &gt; even work better.\n&gt; &gt; &gt;=\r\n &gt; \n&gt; &gt; &gt; &gt; Personally, I do not know which approach would work better \nbut=\r\n \n&gt; &gt; both \n&gt; &gt; &gt; &gt; are viable and it is probably domain dependent.\n&gt; &gt; &gt; &gt;=\r\n \n&gt; &gt; &gt; &gt; ken\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; --- In neat@yahoogroups.com, &quot;Rafael C.P.&quot; &lt;=\r\nkurama.youko.br@&gt; \n&gt; &gt; &gt; &gt; wrote:\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; Ken, it doesn&#39;t fit p=\r\nure evolution but it fits memetic \n&gt; &gt; &gt; algorithms, \n&gt; &gt; &gt; &gt; that\n&gt; &gt; &gt; &gt; =\r\n&gt; consists exactly of evolution alternated with local search \n&gt; &gt; &gt; methods=\r\n \n&gt; &gt; &gt; &gt; for fine\n&gt; &gt; &gt; &gt; &gt; tunning (just few steps). NEAT+BP may become a=\r\n good memetic \n&gt; &gt; &gt; &gt; algorithm for\n&gt; &gt; &gt; &gt; &gt; neural networks.\n&gt; &gt; &gt; &gt; &gt; \n=\r\n&gt; &gt; &gt; &gt; &gt; On Mon, Mar 10, 2008 at 2:19 PM, Kenneth Stanley &lt;kstanley@&gt;\n&gt; &gt; =\r\n&gt; &gt; &gt; wrote:\n&gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt;   Peter, I believe that backprop can po=\r\ntentially improve \nthe\n&gt; &gt; &gt; &gt; &gt; &gt; accuracy. It has been shown to work effe=\r\nctively with \n&gt; &gt; &gt; neurevolution\n&gt; &gt; &gt; &gt; &gt; &gt; in classification tasks in th=\r\ne past. So in principle it \n&gt; could\n&gt; &gt; &gt; &gt; &gt; &gt; help. Of course, there is a=\r\nlways the chance that it will \nnot\n&gt; &gt; &gt; &gt; &gt; &gt; enhance performance as well.=\r\n\n&gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; One issue I would also consider is that some peopl=\r\ne \n&gt; disagree \n&gt; &gt; on\n&gt; &gt; &gt; &gt; &gt; &gt; whether the changes to weights from backp=\r\nrop should be \n&gt; &gt; encoded \n&gt; &gt; &gt; &gt; back\n&gt; &gt; &gt; &gt; &gt; &gt; into the genome or not=\r\n. If it is actually encoded back \ninto \n&gt; &gt; the\n&gt; &gt; &gt; &gt; &gt; &gt; genome, that is=\r\n &quot;Lamarckian&quot; evolution because in effect \n&gt; what \n&gt; &gt; &gt; the\n&gt; &gt; &gt; &gt; &gt; &gt; or=\r\nganism learned over its lifetime is encoded into its own\n&gt; &gt; &gt; &gt; &gt; &gt; offspr=\r\ning. That is obviously not how real evolution works.\n&gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; =\r\n&gt; However, of course, it doesn&#39;t have to work like real \n&gt; &gt; evolution \n&gt; &gt;=\r\n &gt; &gt; and\n&gt; &gt; &gt; &gt; &gt; &gt; some people believe that Lamarckian evolution will wor=\r\nk \n&gt; &gt; better.\n&gt; &gt; &gt; &gt; &gt; &gt; However, there are arguments that in fact it wor=\r\nks worse \n&gt; &gt; &gt; because \n&gt; &gt; &gt; &gt; it\n&gt; &gt; &gt; &gt; &gt; &gt; hurts the diversity of the =\r\npopulation. Because of the \n&gt; Baldwin\n&gt; &gt; &gt; &gt; &gt; &gt; effect, some would argue =\r\nthat evolution+backprop is most \n&gt; &gt; &gt; powerful \n&gt; &gt; &gt; &gt; if\n&gt; &gt; &gt; &gt; &gt; &gt; the=\r\n learned weights are not encoded back into the genome. \n&gt; &gt; This \n&gt; &gt; &gt; &gt; t=\r\nopic\n&gt; &gt; &gt; &gt; &gt; &gt; is fairly extensive. A lot is written about the &quot;Baldwin \n=\r\n&gt; &gt; &gt; effect.&quot;\n&gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; ken\n&gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; =\r\n&gt; &gt; --- In neat@yahoogroups.com &lt;neat%\n&gt; &gt; &gt; &gt; 40yahoogroups.com&gt;, &quot;petar_c=\r\nhervenski&quot;\n&gt; &gt; &gt; &gt; &gt; &gt; &lt;petar_chervenski@&gt; wrote:\n&gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt;=\r\n &gt; Hi Ken,\n&gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; I am evolving time series predictors=\r\n, in fact even a \n&gt; &gt; &gt; simplified\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; version of time series pre=\r\ndictors, where the network \nhas \n&gt; to \n&gt; &gt; &gt; &gt; answer\n&gt; &gt; &gt; &gt; &gt; &gt; is\n&gt; &gt; &gt; =\r\n&gt; &gt; &gt; &gt; the future value going up or down. The actual output \n&gt; neuron \n&gt; &gt;=\r\n &gt; is a\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; simple step function, but back-prop can be applied if=\r\n \nit \n&gt; is \n&gt; &gt; &gt; &gt; turned\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; out to a sigmoid with a very steep=\r\n slope.\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; The networks are allowed to have any topology and the=\r\ny \nare\n&gt; &gt; &gt; &gt; &gt; &gt; evaluated\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; on the run, meaning that on each=\r\n timestep, an error is \n&gt; being\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; calculated (being 0 or 1, dep=\r\nending on the prediction \n&gt; made).\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; First of all, do you think=\r\n that applying back-prop to \n&gt; these\n&gt; &gt; &gt; &gt; &gt; &gt; networks\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; may=\r\n bring any accuracy improvement? I know that it is \n&gt; going \n&gt; &gt; &gt; to \n&gt; &gt; =\r\n&gt; &gt; eat\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; the CPU resourses, so it can be applied at regular \n&gt;=\r\n &gt; intervals, \n&gt; &gt; &gt; &gt; say\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; each 50 generations, to push the n=\r\networks&#39;s weights in \n&gt; the \n&gt; &gt; &gt; right\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; direction, a kind of=\r\n a hint to the search. I am still \n&gt; &gt; thinking\n&gt; &gt; &gt; &gt; &gt; &gt; of &quot;is\n&gt; &gt; &gt; &gt; =\r\n&gt; &gt; &gt; it worth it?&quot;..\n&gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; Peter\n&gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt;=\r\n &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; --- In neat@yahoogroups.com &lt;neat%\n&gt; &gt;=\r\n &gt; 40yahoogroups.com&gt;, &quot;Kenneth \n&gt; &gt; &gt; &gt; Stanley&quot;\n&gt; &gt; &gt; &gt; &gt; &gt; &lt;kstanley@&gt; w=\r\nrote:\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; A number of people have programmed ba=\r\nckprop into \nNEAT. \n&gt; &gt; Chris\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; Christenson did a Masters the=\r\nsis on combining NEAT \nand \n&gt; &gt; &gt; &gt; backprop;\n&gt; &gt; &gt; &gt; &gt; &gt; a\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;=\r\n paper based on this work is actually in the files \n&gt; section \n&gt; &gt; of\n&gt; &gt; &gt;=\r\n &gt; &gt; &gt; this\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; group:\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; =\r\n&gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; \n&gt; \nhttp://f1.grp.yahoofs.com/v1/UP7SR=\r\n8rDovimxlLlvcmGOziLUBIVncb2Tfr7sruo\n&gt; &gt; &gt; &gt; &gt; &gt; B\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; 8b\n&gt; &gt; &gt; &gt; =\r\n&gt; &gt; &gt; &gt; taAfELU62JLyQ9XCxXF_Akhcmi-\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt;=\r\n &gt; \n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; \n&gt; \nTH4gpVHIikwnzB59ArOMQfPOAzyw25/Evolving_Trainab=\r\nle_Neural_Networks_6_p\n&gt; &gt; &gt; &gt; &gt; &gt; a\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; ge\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; s.doc=\r\n\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; Shimon Whiteson implemented it as part of =\r\nhis NEAT+Q\n&gt; &gt; &gt; &gt; &gt; &gt; reinforcement\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; learning method:\n&gt; &gt; &gt;=\r\n &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; \n&gt; \nhttp://staff.science.uv=\r\na.nl/~whiteson/pubs/whitesonaaai06.pdf&lt;http://s\n&gt; &gt; &gt; &gt; taff.science.uva.nl=\r\n/%7Ewhiteson/pubs/whitesonaaai06.pdf&gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; There=\r\n has been a lot written on backprop in NEAT in \nthe \n&gt; &gt; &gt; &gt; archives\n&gt; &gt; &gt;=\r\n &gt; &gt; &gt; of\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; this group: just search for &quot;backprop&quot; from the y=\r\nahoo \n&gt; &gt; page \n&gt; &gt; &gt; for\n&gt; &gt; &gt; &gt; &gt; &gt; this\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; group and many m=\r\nessages will pop up.\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; In general, if you do =\r\nnot allow recurrence then I \n&gt; believe \n&gt; &gt; &gt; &gt; there\n&gt; &gt; &gt; &gt; &gt; &gt; is\n&gt; &gt; &gt; =\r\n&gt; &gt; &gt; &gt; no\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; special change needed in the traditional backpro=\r\np \n&gt; &gt; algorithm.\n&gt; &gt; &gt; &gt; &gt; &gt; With\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; recurrence you would nee=\r\nd something like recurrent \n&gt; &gt; backprop \n&gt; &gt; &gt; &gt; like\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; Derek\n=\r\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; suggested. But let&#39;s just say you are evolving \n&gt; &gt; nonrecu=\r\nrrent\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; networks-\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; is there a particular problem=\r\n you have in mind that \n&gt; comes \n&gt; &gt; up\n&gt; &gt; &gt; &gt; &gt; &gt; with\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; ap=\r\nplying backprop to such networks?\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; ken\n&gt; &gt; &gt;=\r\n &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; --- In neat@yahoogroups.com &lt;neat%40yahoogroups.=\r\ncom&gt;,\n&gt; &gt; &gt; &gt; &gt; &gt; &quot;petar_chervenski&quot;\n&gt; &gt; &gt; &gt; &gt; &gt; &lt;petar_chervenski@&gt;\n&gt; &gt; &gt; =\r\n&gt; &gt; &gt; &gt; &gt; wrote:\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; Hello there.\n&gt; &gt; &gt; &gt; &gt;=\r\n &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; I am looking for any back-propagation algorithm =\r\n\nthat \n&gt; &gt; can \n&gt; &gt; &gt; &gt; work\n&gt; &gt; &gt; &gt; &gt; &gt; on\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; networks with=\r\n arbitrary topology such as these that \n&gt; NEAT\n&gt; &gt; &gt; &gt; &gt; &gt; evolves.\n&gt; &gt; &gt; &gt;=\r\n &gt; &gt; &gt; All\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; libraries I found so far either assume layered=\r\n \n&gt; networks \n&gt; &gt; or\n&gt; &gt; &gt; &gt; &gt; &gt; only\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; feed-\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;=\r\n forward ones.. I am confused. Is there any source \n&gt; code \n&gt; &gt; &gt; that\n&gt; &gt; =\r\n&gt; &gt; &gt; &gt; might\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; help\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; me? Any back-prop impl=\r\nementation that can work on \nNEAT\n&gt; &gt; &gt; &gt; &gt; &gt; networks\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; such\n&gt;=\r\n &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; that it can easily be integrated. Or maybe some \n&gt; papers =\r\n\n&gt; &gt; on \n&gt; &gt; &gt; &gt; the\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; topic?\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; I appreciate an=\r\ny help from the community.\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; Peter\n&gt; &gt; &gt; =\r\n&gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt;  \n&gt; &gt; &gt; &gt;=\r\n &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; -- \n&gt; &gt; &gt; &gt; &gt; =3D=3D=3D=3D=\r\n=3D=3D=3D=3D=3D\n&gt; &gt; &gt; &gt; &gt; Rafael C.P.\n&gt; &gt; &gt; &gt; &gt; =3D=3D=3D=3D=3D=3D=3D=3D=3D=\r\n\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt;\n&gt;\n\n\n\n"}}