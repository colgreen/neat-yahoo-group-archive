{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":288313973,"authorName":"aimike002","from":"&quot;aimike002&quot; &lt;aimike002@...&gt;","profile":"aimike002","replyTo":"LIST","senderId":"TuU3pyPCHavivpev1QD474QfV3AesYyUTBOoqSzTskQYocIh0681c9WTEAYMVgvtd3L7xrQfF7cOWJb90sdQ4QE2IMJNrYnc","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Some questions from beginner in NEAT","postDate":"1164044249","msgId":2836,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGVqc3A0cCtwNHBkQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGVqc21rNys5dWtnQGVHcm91cHMuY29tPg=="},"prevInTopic":2834,"nextInTopic":2837,"prevInTime":2835,"nextInTime":2837,"topicId":2819,"numMessagesInTopic":9,"msgSnippet":"Thanks. My main reason for thinking of a hybrid EANN & BP approach is that I read that using GAs to evolve the weights is good at avoiding the ANN getting","rawEmail":"Return-Path: &lt;aimike002@...&gt;\r\nX-Sender: aimike002@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 6613 invoked from network); 20 Nov 2006 17:39:36 -0000\r\nReceived: from unknown (66.218.66.172)\n  by m40.grp.scd.yahoo.com with QMQP; 20 Nov 2006 17:39:36 -0000\r\nReceived: from unknown (HELO n12c.bullet.sp1.yahoo.com) (69.147.64.111)\n  by mta4.grp.scd.yahoo.com with SMTP; 20 Nov 2006 17:39:36 -0000\r\nReceived: from [216.252.122.219] by n12.bullet.sp1.yahoo.com with NNFMP; 20 Nov 2006 17:37:30 -0000\r\nReceived: from [66.218.69.4] by t4.bullet.sp1.yahoo.com with NNFMP; 20 Nov 2006 17:37:30 -0000\r\nReceived: from [66.218.66.77] by t4.bullet.scd.yahoo.com with NNFMP; 20 Nov 2006 17:37:30 -0000\r\nDate: Mon, 20 Nov 2006 17:37:29 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;ejsp4p+p4pd@...&gt;\r\nIn-Reply-To: &lt;ejsmk7+9ukg@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0\r\nFrom: &quot;aimike002&quot; &lt;aimike002@...&gt;\r\nSubject: Re: Some questions from beginner in NEAT\r\nX-Yahoo-Group-Post: member; u=288313973; y=L2z4v0uPBiW5qbMXpMPpVV2iPv3ZBUxQHafaRhF5sZHExbbW\r\nX-Yahoo-Profile: aimike002\r\n\r\nThanks. My main reason for thinking of a hybrid EANN & BP approach is\nthat =\r\nI read that using GAs to evolve the weights is good at avoiding\nthe ANN get=\r\nting stuck in local optima, but the downside is that it can \noscillate a lo=\r\nt toward end. I read that one approach is to use the\nEANN for the bulk of t=\r\nhe work, but switch to a standard BP approach\nfor some fine tuning toward t=\r\nhe end. \nI guess I could always take whatever weights & topology the NEAT\ns=\r\nystem has evolved , pass that to one of the standard BP packages &\nsee if a=\r\nccuracy can be fine tuned/improved.\n\n\n--- In neat@yahoogroups.com, &quot;cpchris=\r\ntenson&quot; &lt;cpchristenson@...&gt; wrote:\n&gt;\n&gt; I still read, just never comment.  I=\r\n did have success combining \n&gt; evolution and learning.  However, unlike wha=\r\nt you described, my \n&gt; thesis included the learning within the life of the =\r\nnetwork rather \n&gt; than after the evolution is complete.  So, evolution woul=\r\nd create a \n&gt; network, then that network would learn to perform a task, the=\r\nn it \n&gt; was evaluated based on its ability to learn the task.  The learned =\r\n\n&gt; weights were NOT passed onto the next generation.  The idea was to \n&gt; ev=\r\nolve a network that was better at learning.  I was able to show \n&gt; this was=\r\n possible using the task of learning to solve polynomials.  \n&gt; A very simpl=\r\ne example, but functional.\n&gt; \n&gt; --- In neat@yahoogroups.com, &quot;Kenneth Stanl=\r\ney&quot; &lt;kstanley@&gt; wrote:\n&gt; &gt;\n&gt; &gt; I know people have combined NEAT with BP in =\r\nthe past.  For \n&gt; example, \n&gt; &gt; Chris Christenson had some success with it,=\r\n but I&#39;m not sure he&#39;s \n&gt; &gt; still reading this group.\n&gt; &gt; \n&gt; &gt; In any case,=\r\n the number of generations (and hence the length of \n&gt; &gt; time) it takes to =\r\nsolve a problem depends on problem difficulty, \n&gt; so \n&gt; &gt; it&#39;s difficult to=\r\n give an accurate estimate, but 7 hours is \n&gt; probably \n&gt; &gt; a reasonable ex=\r\npectation for the problem you cite.  \n&gt; &gt; \n&gt; &gt; In fact, supervised classifi=\r\ncation problems tend to be faster than \n&gt; &gt; control problems since they don=\r\n&#39;t involve a domain simulator, so \n&gt; you \n&gt; &gt; can expect it to be possibly =\r\na lot faster.  However, if you use BP \n&gt; &gt; you have to factor in the time t=\r\nhat adds to each evaluation, \n&gt; &gt; depending how you fold it into your proce=\r\ndure.\n&gt; &gt; \n&gt; &gt; I&#39;m also guessing you will not need two hidden layers of 8 h=\r\nidden \n&gt; &gt; nodes each to solve your problem.  NEAT usually discovers that i=\r\nt \n&gt; &gt; takes a lot less than you expect to solve problems that seem like \n&gt;=\r\n &gt; they should be difficult.\n&gt; &gt; \n&gt; &gt; Finally, NEAT has been applied to rea=\r\nl world industrial problems \n&gt; in \n&gt; &gt; simulation, but I&#39;m not sure if it&#39;s=\r\n been literally used in e.g a \n&gt; &gt; real factory or something like that.  Bu=\r\nt a 300 instance \n&gt; &gt; classification sounds fairly straightforward. \n&gt; &gt; \n&gt;=\r\n &gt; ken \n&gt; &gt; \n&gt; &gt; --- In neat@yahoogroups.com, &quot;aimike002&quot; &lt;aimike002@&gt; wrot=\r\ne:\n&gt; &gt; &gt;\n&gt; &gt; &gt; Sorry , just to clarify for (3) if we took an example with \n=\r\n&gt; &gt; &gt; say 300 training samples, with say 100 samples in the test set.\n&gt; &gt; &gt;=\r\n Just to try and get an idea.\n&gt; &gt; &gt; \n&gt; &gt; &gt; Mike\n&gt; &gt; &gt; \n&gt; &gt; &gt; --- In neat@ya=\r\nhoogroups.com, &quot;aimike002&quot; &lt;aimike002@&gt; wrote:\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; Hi I&#39;m takin=\r\ng a look for possibly using in for my Masters \n&gt; &gt; project,\n&gt; &gt; &gt; &gt; which  =\r\nneeds evolving the architecture for a neural net for a \n&gt; &gt; real\n&gt; &gt; &gt; &gt; wo=\r\nrld application. \n&gt; &gt; &gt; &gt; I have read some of Ken Stanley&#39;s papers on the w=\r\neb but beyond \n&gt; &gt; that\n&gt; &gt; &gt; &gt; I&#39;m a beginner, so if I may ask some beginn=\r\ner&#39;s questions:\n&gt; &gt; &gt; &gt; 1) I think my neural net may be evolved using GAs ,=\r\n but fine \n&gt; &gt; tuned to\n&gt; &gt; &gt; &gt; optimise using standard BP (ie hybrid appro=\r\nach). Has anybody \n&gt; &gt; else been\n&gt; &gt; &gt; &gt; down that path using NEAT, taking =\r\ne.g. the weights evolved \n&gt; using \n&gt; &gt; NEAT\n&gt; &gt; &gt; &gt; & feeding them into som=\r\ne standard BP software?\n&gt; &gt; &gt; &gt; 2) Has NEAT been used in any &#39;real world&#39;/i=\r\nndustrial \n&gt; &gt; applications yet?\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; and a &#39;how long is a piec=\r\ne of string question&#39;...\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; 3)  As yet  have no feel for how =\r\nlong the evolution might \n&gt; take. \n&gt; &gt; I&#39;ve\n&gt; &gt; &gt; &gt; ran a few of the downlo=\r\naded sample executables they seem to \n&gt; take \n&gt; &gt; quite\n&gt; &gt; &gt; &gt; a while to =\r\nrun. My neural net has 8 inputs, one output and if \n&gt; we \n&gt; &gt; guess\n&gt; &gt; &gt; &gt;=\r\n at a final neural net of 2 hidden layers with 8 nodes per \n&gt; layer. \n&gt; &gt; I=\r\nf we\n&gt; &gt; &gt; &gt; used say a 2GHz PC doing nothing else - could we guess at 7 \n&gt;=\r\n hours\n&gt; &gt; &gt; &gt; (i.e. an overnight run) to evolve or is it more likely to be=\r\n \n&gt; &gt; several\n&gt; &gt; &gt; &gt; days? Would appreciate any best guesses or experience=\r\n on this.\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; Thanks in advance\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; Mike\n&gt; &gt; &gt; &gt;\n=\r\n&gt; &gt; &gt;\n&gt; &gt;\n&gt;\n\n\n\n\n"}}