{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"5n7DjZIflwNE2jyoT9xPs7mClF145BQfHz3gp_88KFREm2KfmemF9FxU51A8s8xAdbGGOOWm9ylE3uAIBQuyxtCLbumgcWOu2VHqo5CaWP_-","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Machine Learning and the Long View of AI","postDate":"1209332193","msgId":3999,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGZ2MnJsMSs5cWdjQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDE5YjEwZDUxMDgwNDI3MDg0Nm03MWU2NDlhY3llZTc3N2RkYWIzZjU0YjBlQG1haWwuZ21haWwuY29tPg=="},"prevInTopic":3998,"nextInTopic":4000,"prevInTime":3998,"nextInTime":4000,"topicId":3955,"numMessagesInTopic":49,"msgSnippet":"... want to ... Sure.  I think the problem is that I can t find a way to explain my point concisely.  As I try to explain it, it starts taking up too much text","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 40218 invoked from network); 27 Apr 2008 21:36:35 -0000\r\nX-Received: from unknown (66.218.67.95)\n  by m47.grp.scd.yahoo.com with QMQP; 27 Apr 2008 21:36:35 -0000\r\nX-Received: from unknown (HELO n42a.bullet.mail.sp1.yahoo.com) (66.163.168.136)\n  by mta16.grp.scd.yahoo.com with SMTP; 27 Apr 2008 21:36:35 -0000\r\nX-Received: from [216.252.122.217] by n42.bullet.mail.sp1.yahoo.com with NNFMP; 27 Apr 2008 21:36:35 -0000\r\nX-Received: from [209.73.164.86] by t2.bullet.sp1.yahoo.com with NNFMP; 27 Apr 2008 21:36:35 -0000\r\nX-Received: from [66.218.66.73] by t8.bullet.scd.yahoo.com with NNFMP; 27 Apr 2008 21:36:35 -0000\r\nDate: Sun, 27 Apr 2008 21:36:33 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;fv2rl1+9qgc@...&gt;\r\nIn-Reply-To: &lt;19b10d510804270846m71e649acyee777ddab3f54b0e@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: Re: Machine Learning and the Long View of AI\r\nX-Yahoo-Group-Post: member; u=54567749; y=oIRe8r0hO4brqFZQqJIFrCBgn7wcQI1vGTU0biEocOFJZVvb-jdw\r\nX-Yahoo-Profile: kenstanley01\r\n\r\n--- In neat@yahoogroups.com, &quot;Derek James&quot; &lt;djames@...&gt; wrote:\n\n&gt; \n&gt; &gt;  In =\r\nRL, in contrast, the long view is almost the opposite: They\nwant to\n&gt; &gt;  re=\r\nmove all constraints and still learn nevertheless.\n&gt; \n&gt; I&#39;m not sure what y=\r\nou mean by this, Ken. Could you elaborate a little?\n&gt;\n\nSure.  I think the p=\r\nroblem is that I can&#39;t find a way to explain my\npoint concisely.  As I try =\r\nto explain it, it starts taking up too much\ntext so I shorten it and then i=\r\nt loses its meaning.  Let me give it a\ntry again...\n\nI think the difference=\r\n between the goals of RL and NE is an\ninteresting topic because they are al=\r\nmost always conflated, as if they\nare trying to solve the same problem.\n\nTh=\r\ne RL community (e.g. value-function approaches) is trying to build\nsomethin=\r\ng that learns like a natural brain.  They are saying, through\nanalytic mean=\r\ns we can deduce how a brain can learn from sparse\nreinforcement and formali=\r\nze that process in an algorithm.  The hope, I\nwould think, is to eventually=\r\n build the &quot;general intelligence&quot; that\naligns with the holy grail of AI.  S=\r\no each step along the way is an\nimprovement in that general ability.\n\nSo if=\r\n that is your goal, then the benchmarks you choose have to be\ndesigned to m=\r\neasure progress to that goal.  So what they need to do is\nshow that their d=\r\nesigned intelligence can work largely independently\nof a priori &quot;cheats&quot; th=\r\nat provide the meat of the solution.  Because,\nafter all, how can it be a g=\r\neneral intelligence if it needs you to\ntell it something that it is suppose=\r\nd to be able to figure out?  This\nperspective, I believe, is aligned with J=\r\neff&#39;s view.\n\nHowever, NE as a long-term pursuit is involved in something di=\r\nfferent,\neven though it can be applied to the same problems.  NE is not an\n=\r\nattempt to formalize how people learn with sparse reinforcement. \nRather, i=\r\nt is an attempt to formalize how evolution can build a brain.\n So RL is for=\r\nmalizing the brain itself and NE is formalizing how\nevolution succeeds in c=\r\nreating a brain.  NE is therefore one step removed.\n\nThis difference is ult=\r\nimately a philosophical difference on the best\napproach to creating a full-=\r\nblown AI.  The instrumental issue is\nwhether you think it&#39;s easier to build=\r\n it yourself or to design an\nalgorithm that can build it.  The confusion an=\r\nd hence conflation of\nthe two approaches arises in part because they do ind=\r\need both aim at\nthe same long view goal: a general AI.  But they are coming=\r\n at it from\nvery different angles.\n\nAnd because of this stark difference, t=\r\nhe *metric* of progress should\nbe quite different.  We cannot measure our p=\r\nrogress in building a\ngeneral intelligence directly in the same way that we=\r\n measure our\nprogress in creating an evolutionary algorithm that itself wil=\r\nl\nsomeday output one.  \n\nThis distinction is potentially subtle and confusi=\r\nng so let me try to\nmake it clearer:  Human brains aren&#39;t designed to build=\r\n yet more human\nbrains.  We are good at a lot of things, and we learn gener=\r\nally, but\nwe do not build 100-trillion part devices that are more complex t=\r\nhan\nany known object in the universe.  I&#39;m not saying we won&#39;t ever be\nable=\r\n to do it, but if you want to simulate a human brain, your first\nthought wo=\r\nuld not be that it needs to be capable of designing yet\nanother brain by it=\r\nself.  Your first thought is about things like\nobject recognition or pursui=\r\nt and evasion.\n\nIn contrast, building brains is exactly what natural evolut=\r\nion did,\nand it did it quite well.  Natural evolution does not perform obje=\r\nct\nrecognition; it does not communicate with language; it does not run\naway=\r\n from predators or hunt for prey.  Yet it does build brains that\nthemselves=\r\n do those things.  And that is the aspect of it we wish to\nharness- a very =\r\nspecific niche kind of skill (though radically\nimpressive)- not a general s=\r\nkill.\n\nSo the two pursuits are really quite different.  And therefore they\n=\r\ndeserve different metrics to judge their progress with respect to the\nlong =\r\nterm goal.  That is, unless we conflate them to be the same\nthing, which we=\r\n often do without thinking about it.\n\nFor example, we could just say, well,=\r\n both NE and RL are learning\ntechniques, and after all, we can apply them t=\r\no the same problems, so\nwhy make a big distinction in how we judge them?  L=\r\net&#39;s just compare\nthem directly on the same benchmarks and get on with it.\n=\r\n\nThat&#39;s fine for the short-term view, i.e. let&#39;s just improve our\nability t=\r\no tackle practical problems, but for the long view, they\ncannot be judged i=\r\nn the same way.  If I improve at my ability to\nbalance on one foot is that =\r\na sign that I will be able to build a\nbrain someday?  If evolution evolves =\r\na brain that plays checkers, is\nthat a sign that evolution *itself* is on t=\r\nhe road to performing\nobject recognition?  These are totally different purs=\r\nuits.\n\nSo in that context, how should they be judged with respect to long\nt=\r\nerm goals?  Well, I think RL deserves to be judged based on its\nincreasing =\r\nability to learn more generally.  And in that sense,\nexactly Jeff&#39;s criteri=\r\na should apply to it: We should be interested in\nwhether it &quot;needs&quot; a prior=\r\ni information to learn.  In other words, the\nless we need to constrain the =\r\nproblem for the learner, the more\nimpressed we deserve to be.  That shows p=\r\nrogress towards more and more\ngeneral AI and ML.\n\nBut if evolution is not *=\r\nitself* supposed to be a general learner\n(rather, we just want it to concen=\r\ntrate on one very specific skill:\nbrain building), then those consideration=\r\ns are orthogonal to its\ngreatest promise.  Its promise is to evolve a brain=\r\n itself, and as\nsuch, neuroevolutionary algorithms deserve to be judged on =\r\nour ability\nto *constrain* the problem so that they can accomplish exactly =\r\nthat. \nIn other words, the problem NE *algorithms* face is leaps and bounds=\r\n\nbeyond what RL algorithms face.  RL algorithms just need to be able to\ndo =\r\nas well as brains; NE has to be able to discover brains themselves.\n Theref=\r\nore, progress is NE should in part be measured with respect to\nprogress in =\r\nconstraining the problem to make such a discovery more\nlikely.  When an NE =\r\nalgorithm is improved to allow us to tell it more\nabout the world in which =\r\nits output will be situated, that is good\nnews for the long view.  In short=\r\n, we don&#39;t care at all how NE\nproduced a brain as long as it really does.  =\r\nWill anyone complain if a\nhuman brain pops out of a system that was a prior=\r\ni given the concept\nof symmetry?  Rather, we should be glad that such a pri=\r\nori context was\npossible to provide in the first place, because it may have=\r\n saved us a\nyear of wasted computation in figuring it out needlessly.\n\nThis=\r\n distinction is almost completely ignored when NE and RL are\ncompared direc=\r\ntly.  Therefore, the implications of any such comparison\nare fuzzy and lack=\r\ning context with respect to the long view.  I am not\nsure if I should care =\r\nor not if RL solves something better than NE, or\nvice versa, because the au=\r\nthor doesn&#39;t explain how the result aligns\nwith the long-term goals of the =\r\nfields.  Long term goals seem like\nunwelcome guests these days in AI, which=\r\n is why I probably won&#39;t be\nwriting about any of this in a publication any =\r\ntime soon.  \n\n...\n\nSo Derek what you are saying about NE being good at &quot;har=\r\nd-wired&quot;\nsolutions and RL being appropriate for ontogenetic lifetime learni=\r\nng,\nwhile true, is not what I think of as the primary long-view issue.\n\nIn =\r\nthe long view, NE will be used to evolve structures that do learn\nover thei=\r\nr lifetime, i.e. not hardwired at all.  The only reason that\nit tends to be=\r\n used to evolve hardwired solutions today is because we\nare trying to get a=\r\n foothold on how to evolve certain types of complex\nstructures.   Once we g=\r\net very good at it, focus will naturally shift\nto evolving dynamic brains (=\r\nand of course there is already work along\nthese lines today, much from Flor=\r\neano).  I do not even think that we\nwill need to include stock learning alg=\r\norithms like Hebbian learning.\n When we achieve our long-term goals, those =\r\n*themselves* will be left\nup to evolution because after all there may be so=\r\nmething even better.\n \n&gt; &gt; My aim is to design an\n&gt; &gt;  algorithm that will =\r\noutput a brain, not to design the brain itself.\n&gt; \n&gt; But what kind of brain=\r\n are you wanting to output?\n&gt; \n\nNote that I&#39;m speaking purely about the lon=\r\ng view for these different\nfields here.  Of course on a day-to-day basis I =\r\nam not solely focused\non what will happen 100 years from now.  On a practic=\r\nal day-to-day\nbasis, of course I want to make NE better capable to tackle p=\r\nroblems\nthat e.g. RL tackles.  So in the short-term context, I just want to=\r\n\noutput something that works for the problem at hand.\n\nBut in the long view=\r\n, which we were talking about, I think the\nultimate goal would be to output=\r\n a full-fledged adaptive system with\nastronomical complexity and the power =\r\nand subtlety of human reasoning.\n On that path, constraint is the only hope=\r\n, unless you want to wait\nthree billion years and just hope in the meantime=\r\n that the initial\nconditions were set up correctly.  Therefore, demonstrati=\r\nons of the\npower of constraint deserve to be judged as evidence of the prom=\r\nise of\nand progress towards the long term goal in NE.\n\nken\n\n\n\n"}}