{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"shuXMeLW54JtFfhrT089Jpxl38NpklR-qbtZr7U1C0rUAPnWlbCkZZAVu0JQeWgZMkbHckdS_35Nqn0U0FhiaRmMGJuh1jXlN7TzoA4_GGU6","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: Warning: Capping the maximum and minimum link weight may be important!","postDate":"1083013833","msgId":719,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGM2anRzOSs1a2tiQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDQwOEQ3NkRBLjgwNDAzMDFAZHNsLnBpcGV4LmNvbT4="},"prevInTopic":718,"nextInTopic":723,"prevInTime":718,"nextInTime":720,"topicId":672,"numMessagesInTopic":13,"msgSnippet":"A few comments... First, normalizing weights is not really feasible.  The relative values of the weights does *not* scale because the sigmoid function is ","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 68429 invoked from network); 26 Apr 2004 21:11:03 -0000\r\nReceived: from unknown (66.218.66.167)\n  by m24.grp.scd.yahoo.com with QMQP; 26 Apr 2004 21:11:03 -0000\r\nReceived: from unknown (HELO n9.grp.scd.yahoo.com) (66.218.66.93)\n  by mta6.grp.scd.yahoo.com with SMTP; 26 Apr 2004 21:11:03 -0000\r\nReceived: from [66.218.67.178] by n9.grp.scd.yahoo.com with NNFMP; 26 Apr 2004 21:10:35 -0000\r\nDate: Mon, 26 Apr 2004 21:10:33 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;c6jts9+5kkb@...&gt;\r\nIn-Reply-To: &lt;408D76DA.8040301@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Length: 4336\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-eGroups-Remote-IP: 66.218.66.93\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: Re: Warning: Capping the maximum and minimum link weight may be important!\r\nX-Yahoo-Group-Post: member; u=54567749\r\nX-Yahoo-Profile: kenstanley01\r\n\r\nA few comments...\n\nFirst, normalizing weights is not really feasible.  The relative\nvalues of the weights does *not* scale because the sigmoid function is\nabsolute, i.e. it would not be scaled along with the weights.  If you\ntry to normalize weights based on the relative strengths you will\neliminate the functionality of the network.\n\nSecond, with a simple network like XOR, it looks fine to have the\nweights go up unbounded, and indeed the original NEAT was distributed\nwithout such a cap.  But once you start evolving networks with\nhundreds of connections or more, it doesn&#39;t work the way you guys are\nthinking.  A connection that evolves to &quot;drown out&quot; other connections\nthen can never be surmounted.  In other words, new connections have no\nchance of having an effect, since they are drowned out as soon as they\nappear, so you stop getting new functionality.\n\nNow one thing you can do about this perhaps is find the max and min\nweights in the network and generate all new connections within that\nrange.  However, that will cause the weights of the entire network to\ngradually gravitate to larger and larger magnitudes, which also means\nit will becomes a binary network.\n\nI think the right thing to do for long runs that are producing very\ncomplex networks is indeed to cap the weights.  This also means it is\neasier to break out of a local optimum, depending on the weight\nmutation power.\n\nken\n\n--- In neat@yahoogroups.com, Colin Green &lt;cgreen@d...&gt; wrote:\n&gt; Jim O&#39;Flaherty, Jr. wrote:\n&gt; \n&gt; &gt; Ken,\n&gt; &gt;  \n&gt; &gt; I have been thinking about this particular area.  It is possible\nthat \n&gt; &gt; you *do* want those larger values.  They may indicate that a \n&gt; &gt; particular feature (or set of features) are specifically higher\nin \n&gt; &gt; priority.  And the elevated weight values are a way for that node\n(or \n&gt; &gt; set of nodes) to drown out noise to refine the &quot;abstraction&quot;.\n&gt; &gt;  \n&gt; &gt; Now what this *might* mean is that the other weights would be\nuseful \n&gt; &gt; to &quot;remove&quot; so there is less need for the weight (or subset of \n&gt; &gt; weights) to have their particular values advance so high to\ncompensate \n&gt; &gt; for the noise.\n&gt; &gt;  \n&gt; &gt; If the value range of the weights is clamped, it could produce a\nless \n&gt; &gt; efficient search.  When the weight that would just keep\n&quot;elevating&quot; \n&gt; &gt; hits the cap, to keep it&#39;s relative value to the other weights\nfeeding \n&gt; &gt; the node, all the other weights would need to go down\nproportionally \n&gt; &gt; at the same time.  However, the odds of that happening are just\nabout \n&gt; &gt; nil.  As such, I sense that that node will start to lose efficacy\nand \n&gt; &gt; the search will wander away from what might have been a &quot;key \n&gt; &gt; abstraction&quot; or &quot;feature discovery&quot;.\n&gt; &gt;  \n&gt; &gt; I cannot prove any of this.  I am just using my intuition here.\n&gt; &gt;\n&gt; \n&gt; This is a good point. As an example take a look at the XOR network\nI \n&gt; posted a while ago:\n&gt; \n&gt; ------------------------------------------------\n&gt;     neuron id=&quot;0&quot; type=&quot;bias&quot;\n&gt;     neuron id=&quot;1&quot; type=&quot;in&quot;\n&gt;     neuron id=&quot;2&quot; type=&quot;in&quot;\n&gt;     neuron id=&quot;3&quot; type=&quot;out&quot;\n&gt; \n&gt;     connection src-id=&quot;0&quot; tgt-id=&quot;3&quot; weight=&quot;1.64327624043105&quot;\n&gt;     connection src-id=&quot;1&quot; tgt-id=&quot;3&quot; weight=&quot;-3.3017311279049&quot;\n&gt;     connection src-id=&quot;2&quot; tgt-id=&quot;3&quot; weight=&quot;-3.3025163810043&quot;\n&gt;     connection src-id=&quot;3&quot; tgt-id=&quot;3&quot; weight=&quot;-7.44947553716588&quot;\n&gt; ------------------------------------------------\n&gt; \n&gt; The large recurrent connection on neuron 3 &#39;drowns-out&#39; the signals\nfrom other connections from the 2nd timestep onwards, and because the\nactivation function had an output range of 0 to 1 the large negative\nconnection weight pulled the output down towards 0.\n&gt; \n&gt; An alternative stragegy might be to not cap the connection weights\nbut to periodically normalize the connection weights. That way you\nallow for the sometimes useful large weights but bring them back into\nthe range that the weight mutation function is designed for. One\nproblem with this approach though would be that the smaller weights in\nthe rest of the network would be reduced to within a tight range -\nthus reducing the effectiveness of the search.\n&gt; \n&gt; I think the correct thing to do here would be to perform some\nstatistical analysis of the network weights, if just one or two are\nlarge then normalization isn&#39;t required, but if a large proportion of\nthem are then we should normalize to bring them back in line.\n&gt; \n&gt; Colin.\n\n\n"}}