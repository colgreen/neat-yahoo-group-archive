{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":151231063,"authorName":"Joseph Reisinger","from":"Joseph Reisinger &lt;joeraii@...&gt;","profile":"joeraii","replyTo":"LIST","senderId":"mj63hPX5WzTCmuBuCqbALDvQL50fwdYRQukfyJCJy9soh4pY8AMz_yYnjaudZ1BVZ7D4vOd8rBYocc9xmmSDMGNY923-9C5joNrVru4smw","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] Re: HyperNEAT and No Free Lunch","postDate":"1177958434","msgId":3226,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDcyODQxNzRGLTczRjctNEIyRS04NTAzLTQ1NkQ1NzRDOUU2Q0Bjcy51dGV4YXMuZWR1Pg==","inReplyToHeader":"PGYxNDlubSthOXRwQGVHcm91cHMuY29tPg==","referencesHeader":"PGYxNDlubSthOXRwQGVHcm91cHMuY29tPg=="},"prevInTopic":3223,"nextInTopic":3228,"prevInTime":3225,"nextInTime":3227,"topicId":3214,"numMessagesInTopic":27,"msgSnippet":"... Yeah I m trying to bring to light a more subtle point. You are trying to claim that the way prior knowledge can be included in HyperNEAT is somehow better","rawEmail":"Return-Path: &lt;joeraii@...&gt;\r\nX-Sender: joeraii@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 17610 invoked from network); 30 Apr 2007 18:41:31 -0000\r\nReceived: from unknown (66.218.67.34)\n  by m46.grp.scd.yahoo.com with QMQP; 30 Apr 2007 18:41:31 -0000\r\nReceived: from unknown (HELO spunkymail-a20.g.dreamhost.com) (208.97.132.74)\n  by mta8.grp.scd.yahoo.com with SMTP; 30 Apr 2007 18:41:31 -0000\r\nReceived: from [128.62.110.198] (w-mob101-128-62-110-198.public.utexas.edu [128.62.110.198])\n\tby spunkymail-a20.g.dreamhost.com (Postfix) with ESMTP id 6BD7CFE8BB\n\tfor &lt;neat@yahoogroups.com&gt;; Mon, 30 Apr 2007 11:40:37 -0700 (PDT)\r\nMime-Version: 1.0 (Apple Message framework v752.2)\r\nIn-Reply-To: &lt;f149nm+a9tp@...&gt;\r\nReferences: &lt;f149nm+a9tp@...&gt;\r\nContent-Type: text/plain; charset=US-ASCII; delsp=yes; format=flowed\r\nMessage-Id: &lt;7284174F-73F7-4B2E-8503-456D574C9E6C@...&gt;\r\nContent-Transfer-Encoding: 7bit\r\nDate: Mon, 30 Apr 2007 13:40:34 -0500\r\nTo: neat@yahoogroups.com\r\nX-Mailer: Apple Mail (2.752.2)\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Joseph Reisinger &lt;joeraii@...&gt;\r\nSubject: Re: [neat] Re: HyperNEAT and No Free Lunch\r\nX-Yahoo-Group-Post: member; u=151231063; y=fV4bJYKTubXSaUO4gDsHUZ6Le0hKzKxf0e_IFJ1khMJ9tA\r\nX-Yahoo-Profile: joeraii\r\n\r\n&gt; Hmm, yes, that could be interesting.  I see where you&#39;re going.\n&gt; Maybe there is a theoretically clean way to make the point.  It could\n&gt; be nice.  I would definitely not rule it out.\n&gt;\n&gt; But I need more guidance toward a statement more concrete than what\n&gt; I&#39;ve already said.  You suggest that I misapplied NFL, but I don&#39;t\n&gt; think I was really applying NFL at all.  I was just saying something\n&gt; that is already well-accepted about NFL, which is that if you provide\n&gt; a priori bias you are no longer in an NFL situation vs. something\n&gt; that lacks such bias.  I was not so much making a new assertion as\n&gt; just bringing it up as a reminder of a generally accepted concept.\n\nYeah I&#39;m trying to bring to light a more subtle point. You are trying  \nto claim that the way prior knowledge can be included in HyperNEAT is  \nsomehow better than other algorithms. I just wanted to amend that  \nstatement somewhat with the idea that the experimenter could be wrong  \nin guessing the geometry, and thus inadvertently make search more  \ndifficult for HyperNEAT. So there is both good and bad with having  \nmore knobs to control. So appealing simply to NFL here as validation  \nof HyperNEAT being better than NEAT is a little misleading. Really  \nall you can say is that NFL tells us nothing about how HyperNEAT and  \nNEAT are related.\n\nYou were applying NFL in the sense that you said that HyperNEAT falls  \noutside of the purview of NFL because it can employ prior knowledge.  \nThis is true, but you can also use prior knowledge in NEAT, albeit to  \nin some sense a lesser extent. So would you say that NEAT also falls  \noutside of NFL? No, I don&#39;t think anyone would try to make this  \nargument, rather, the use of prior knowledge is just sort of swept  \nunder the rug when doing any NFL-related comparisons.\n\nDerek actually summarizes this point really well with his comment: &quot;I  \ndon&#39;t understand how it&#39;s possible *not* to include a priori  \nknowledge into any optimization algorithm in practice.&quot; In general  \nyou can&#39;t really exclude all prior knowledge. Thats one of the main  \npractical reasons that we never see evidence of NFL in the real world.\n\n\n&gt; I don&#39;t know the details on Sherstov and Stone; I&#39;ll check that out.\n&gt; But tile coding is not a good example.  It actually does the exact\n&gt; opposite of what HyperNEAT does.  HyperNEAT encodes geometric\n&gt; relationships among inputs and outputs as variations along Cartesian\n&gt; dimensions.  Tile coding, on the other hand, willfully destroys\n&gt; geometric relationships by breaking their respective dimensions into\n&gt; tiny little pieces that no longer have anything to tie them\n&gt; together.  The idea that left is in opposition to right is only\n&gt; possible to leverage if left and right are variants of a single\n&gt; dimension (i.e. something that HyperNEAT stives to capture).  In\n&gt; constrast, tile coding purposefully separates such relationships into\n&gt; broken bits that no longer vary along a dimension.\n\nSure, but tile-coding does respect at least one form of geometry:  \nNearby elements in the state space are known to be nearby, and thus  \nare grouped in the same tile. I don&#39;t want to argue whether or not  \ntile-coding is better than HyperNEAT, it probably is better in a few  \ncases, but worse in more. Rather, I just wanted to point it out as an  \nexample of a line of research that built in some geometric assumptions.\n\n&gt; In fact, I think it&#39;s a great example of why it&#39;s important to\n&gt; characterize what is new about HyperNEAT.  On the face of it, one\n&gt; might think (among other things), oh it&#39;s just another way to\n&gt; optimize the input representation like tile coding.  But that\n&gt; superficial connection hides the fact that these are perfectly\n&gt; opposite ideas.   Tile coding could be the most aggregious violation\n&gt; of the integrity of input geometry that there is, even worse than\n&gt; regular ANN input encoding.  It basically says, let&#39;s throw up our\n&gt; hands in the air and just give up on exploiting regularities\n&gt; entirely.  Instead we&#39;ll just try to solve every problem in tiny\n&gt; little bits.  It&#39;s actually a good indicator of how impotent some\n&gt; approaches are that their cleverest means to make problems easier is\n&gt; to break up all their inherent relationships so that the learning\n&gt; algorithm becomes even more blind than it already is.\n&gt;\n\nI actually completely agree with this, btw. I&#39;ve talked a bit with  \nShimon and I think there is a lot of room for research into value  \nfunctions that incorporate geometric knowledge, or at least are  \ncapable of learning and exploiting it. That would be really cool.\n\n\n&gt; ken\n&gt;\n&gt;&gt; -- Joe\n&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; ken\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; --- In neat@yahoogroups.com, Joseph Reisinger &lt;joeraii@&gt; wrote:\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; (I&#39;ve changed the thread title so that this post will be easier\n&gt; to\n&gt;&gt;&gt;&gt; ignore.)\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; Sorry for dragging us deeper into a discussion of NFL, but I\n&gt; think\n&gt;&gt;&gt;&gt; this point is really important, especially if you are going to be\n&gt;&gt;&gt;&gt; making these claims in front of a broader audience. Also I think\n&gt;&gt;&gt; it\n&gt;&gt;&gt;&gt; is helpful for the audience here to be crystal clear on the NFL\n&gt;&gt;&gt;&gt; issue. Its a very important theorem and it has been quite often\n&gt;&gt;&gt;&gt; misapplied (for instance, there is some evidence that Wolpert\n&gt;&gt;&gt;&gt; initially came up with NFL specifically to critique GAs).\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; That&#39;s why I said it &quot;may&quot; be better for evolving very large\n&gt;&gt;&gt; scale\n&gt;&gt;&gt;&gt;&gt; brains.\n&gt;&gt;&gt;&gt;&gt; The &quot;may&quot; hinges on the issues you bring up and others.\n&gt; However,\n&gt;&gt;&gt;&gt;&gt; even the opportunity to be better is an advantage over having no\n&gt;&gt;&gt; such\n&gt;&gt;&gt;&gt;&gt; opportunity.  The opportunity of course can be squandered with\n&gt; the\n&gt;&gt;&gt;&gt;&gt; wrong a priori knowledge.   Yet part of my point is that it will\n&gt;&gt;&gt;&gt;&gt; often be the case that the natural geometry of a task is all you\n&gt;&gt;&gt; need\n&gt;&gt;&gt;&gt;&gt; to provide a powerful bias (or at least a bias that is better\n&gt; than\n&gt;&gt;&gt;&gt;&gt; nothing), so it will often be possible to seize the opportunity\n&gt;&gt;&gt;&gt;&gt; without a great deal of effort.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; Ok, note your use of the word &quot;often&quot; instead of the\n&gt;&gt;&gt; word &quot;always.&quot;\n&gt;&gt;&gt;&gt; Just that word substitution means that you aren&#39;t within the\n&gt; realm\n&gt;&gt;&gt; of\n&gt;&gt;&gt;&gt; situations that NFL covers. I totally agree with you that the\n&gt;&gt;&gt;&gt; application of such geometry can be useful. I just don&#39;t think\n&gt; you\n&gt;&gt;&gt;&gt; can leverage NFL to back up the argument you are making.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; It is true too that NEAT and other neural network algorithms do\n&gt;&gt;&gt;&gt;&gt; indeed allow for some inclusion of prior knowledge through their\n&gt;&gt;&gt;&gt;&gt; input/output encoding.  However, note how I phrased my\n&gt;&gt;&gt;&gt;&gt; claim: &quot;HyperNEAT is not subject to the No Free Lunch theorem\n&gt; when\n&gt;&gt;&gt;&gt;&gt; comparing to algorithms that do not allow injecting such a\n&gt; priori\n&gt;&gt;&gt;&gt;&gt; knowledge.&quot;  That is, among algorithms that allow you to decide\n&gt;&gt;&gt; on an\n&gt;&gt;&gt;&gt;&gt; input encoding in the traditional way, the provision of such\n&gt;&gt;&gt; encoding\n&gt;&gt;&gt;&gt;&gt; does not give one algorithm a leg up over another since they all\n&gt;&gt;&gt;&gt;&gt; allow for such knowledge to be included.  HyperNEAT, on the\n&gt; other\n&gt;&gt;&gt;&gt;&gt; hand, allows a new kind of knowledge (i.e. geometry) to be\n&gt;&gt;&gt; included\n&gt;&gt;&gt;&gt;&gt; and therefore does have a potential leg up on that class of\n&gt;&gt;&gt;&gt;&gt; algorithms.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; But this leg up would necessarily be different for each problem,\n&gt;&gt;&gt;&gt; hence my original critique. You can&#39;t build the experimenter into\n&gt;&gt;&gt; the\n&gt;&gt;&gt;&gt; system and still talk about NFL. Its just not applicable because\n&gt;&gt;&gt; you\n&gt;&gt;&gt;&gt; are no longer doing any generalization across classes of\n&gt; problems.\n&gt;&gt;&gt;&gt; Therefore you can&#39;t use it to build your case.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; Your argument runs something like: &quot;HyperNEAT has more parameters\n&gt;&gt;&gt;&gt; than the experimenter can set than other Non-Hyper NEATs that\n&gt;&gt;&gt; allow\n&gt;&gt;&gt;&gt; him/her to build in even better prior knowledge.&quot; But you are\n&gt;&gt;&gt;&gt; neglecting the fact that the experimenter now has more possible\n&gt;&gt;&gt;&gt; parameters to set incorrectly! At the very least, you should\n&gt; limit\n&gt;&gt;&gt;&gt; your statement just to problems where the geometry is known /to\n&gt;&gt;&gt; the\n&gt;&gt;&gt;&gt; experimenter/.  In the case where the experimenter knows the\n&gt;&gt;&gt; geometry\n&gt;&gt;&gt;&gt; a priori, and he is correct in that knowledge, then yes,\n&gt; HyperNEAT\n&gt;&gt;&gt;&gt; would outperform other Non-Hyper algorithms in the NFL sense.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; Of course it depends on how well the user takes\n&gt;&gt;&gt;&gt;&gt; advantage of the opportunity, but the opportunity is now there.\n&gt;&gt;&gt; This\n&gt;&gt;&gt;&gt;&gt; fact does indeed mean that statements about HyperNEAT vs. other\n&gt;&gt;&gt;&gt;&gt; neuroevolution (or even machine learning) algorithms can cite an\n&gt;&gt;&gt;&gt;&gt; opportunity to genuinely be better on average, which in effect\n&gt;&gt;&gt; brings\n&gt;&gt;&gt;&gt;&gt; it outside NFL in one particular sense.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; It can be better on average, if the experimenter always makes the\n&gt;&gt;&gt;&gt; correct choices w.r.t. the geometry. But I don&#39;t think you can\n&gt; use\n&gt;&gt;&gt;&gt; such &quot;oracle&quot; experimenters as a basis for comparison. And\n&gt; anyway,\n&gt;&gt;&gt;&gt; NFL still holds, in a broader sense: For example, imagine the\n&gt;&gt;&gt; class\n&gt;&gt;&gt;&gt; of problems that /seem/ to have an underlying geometry feature,\n&gt;&gt;&gt; lets\n&gt;&gt;&gt;&gt; call it &quot;A.&quot; There could possibly be instances of this class of\n&gt;&gt;&gt;&gt; problems where naively exploiting A would actually cause the\n&gt;&gt;&gt; learning\n&gt;&gt;&gt;&gt; algorithm to perform worse than not exploiting it. And in fact we\n&gt;&gt;&gt;&gt; could probably construct such a class.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; Again, to sum up, I think the way you are framing the benefits of\n&gt;&gt;&gt;&gt; HyperNEAT&#39;s geometry exploiting features rely solely on an\n&gt;&gt;&gt;&gt; intelligent experimenter, and in that sense do sort of bypass\n&gt; NFL.\n&gt;&gt;&gt;&gt; But making such a statement is meaningless, because it basically\n&gt;&gt;&gt;&gt; assumes that the experimenter always guesses right. And if I were\n&gt;&gt;&gt;&gt; that experimenter, then I wouldn&#39;t be here talking to you, I\n&gt; would\n&gt;&gt;&gt;&gt; have already solved strong AI :)\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; Humbly,\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; -- Joe\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; Yahoo! Groups Links\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt; Yahoo! Groups Links\n&gt;\n&gt;\n&gt;\n\n\n"}}