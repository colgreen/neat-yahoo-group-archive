{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":274910130,"authorName":"David D&#39;Ambrosio","from":"&quot;David D&#39;Ambrosio&quot; &lt;ddambro84@...&gt;","profile":"ddambroeplex","replyTo":"LIST","senderId":"XG1NLrEleRura2nNdnLdbXnVmo17PhwIumawzRw-9Ls3bmq5MH1U-YWqID7eHBH2lCzfs-k6whQ1-yRf0-NDSPS7wDGCY_v7-PcCfmGJRKjh","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Another New Paper:  Multiagent HyperNEAT","postDate":"1209058179","msgId":3980,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGZ1cWcyMytxdWFiQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGZ1cDd2YytoazJiQGVHcm91cHMuY29tPg=="},"prevInTopic":3979,"nextInTopic":3981,"prevInTime":3979,"nextInTime":3981,"topicId":3955,"numMessagesInTopic":49,"msgSnippet":"I actually did run some heterogeneous experiments without the repeating frame, although it was purely by accident so I did not do any real analysis to the","rawEmail":"Return-Path: &lt;ddambro84@...&gt;\r\nX-Sender: ddambro84@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 89135 invoked from network); 24 Apr 2008 17:29:40 -0000\r\nX-Received: from unknown (66.218.67.97)\n  by m53.grp.scd.yahoo.com with QMQP; 24 Apr 2008 17:29:40 -0000\r\nX-Received: from unknown (HELO n48c.bullet.mail.sp1.yahoo.com) (66.163.168.182)\n  by mta18.grp.scd.yahoo.com with SMTP; 24 Apr 2008 17:29:40 -0000\r\nX-Received: from [216.252.122.218] by n48.bullet.mail.sp1.yahoo.com with NNFMP; 24 Apr 2008 17:29:40 -0000\r\nX-Received: from [66.218.69.5] by t3.bullet.sp1.yahoo.com with NNFMP; 24 Apr 2008 17:29:40 -0000\r\nX-Received: from [66.218.66.92] by t5.bullet.scd.yahoo.com with NNFMP; 24 Apr 2008 17:29:40 -0000\r\nDate: Thu, 24 Apr 2008 17:29:39 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;fuqg23+quab@...&gt;\r\nIn-Reply-To: &lt;fup7vc+hk2b@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;David D&#39;Ambrosio&quot; &lt;ddambro84@...&gt;\r\nSubject: Re: Another New Paper:  Multiagent HyperNEAT\r\nX-Yahoo-Group-Post: member; u=274910130; y=LmxZDaeg523CDACkBXDPEkdf2cJPmeC_MLr_9iRCHpBiMyZL\r\nX-Yahoo-Profile: ddambroeplex\r\n\r\nI actually did run some heterogeneous experiments without the\nrepeating fra=\r\nme, although it was purely by accident so I did not do\nany real analysis to=\r\n the results, but what I saw was that without r(X)\nunseeded heterogeneous t=\r\neams performed worse than the other four types\nin this paper.\n\nThe reasonin=\r\ng behind this disparity in performance is, I think, what\nKen suggested: tha=\r\nt there is a kind of &quot;chicken and egg&quot; problem here.\n Finding the pattern o=\r\nf agents doesn&#39;t directly benefit the agents in\nterms of fitness, and simil=\r\narly finding an optimal control policy for\nfive agents doesn&#39;t help unless =\r\nyou can distribute it properly among\nthem.  However, by giving HyperNEAT on=\r\ne of these two things that we\nknow to be correct (the agent locations), it =\r\ncan work on finding the\nother.\n\n--- In neat@yahoogroups.com, &quot;Kenneth Stanl=\r\ney&quot; &lt;kstanley@...&gt; wrote:\n&gt;\n&gt; Jeff, those are good questions.  Actually, Da=\r\nvid can correct me if I\n&gt; am wrong, but I believe we never did try to run t=\r\nhese experiments\n&gt; without providing the repeating coordinate frame.  It is=\r\n interesting\n&gt; that you see that as a natural first step, since our thinkin=\r\ng unfolded\n&gt; in a different order:\n&gt; \n&gt; The idea originated from the though=\r\nt that it is an advantage of\n&gt; HyperNEAT that we can describe to it a prior=\r\ni the locations of\n&gt; different agents on the substrate and that way convey =\r\nto it that there\n&gt; are variations on a theme from the start.  Also, we thou=\r\nght it was\n&gt; tidy that we can perfectly express to the learner from the sta=\r\nrt\n&gt; exactly where those agents&#39; networks begin and end, leaving no room\n&gt; =\r\nfor ambiguity or misalignment.\n&gt; \n&gt; Perhaps one way to explain why we thoug=\r\nht about that first is to\n&gt; consider that one important philosophical motiv=\r\nation behind HyperNEAT\n&gt; is that machine learning needs a way for humans to=\r\n convey to the\n&gt; learner a priori known domain geometry.  In effect, we are=\r\n running\n&gt; away from the black box of No Free Lunch (which is a nasty trap)=\r\n by\n&gt; finding new ways to convey critical a priori domain information. \n&gt; W=\r\nhile arguments can be made that because certain techniques align with\n&gt; cer=\r\ntain problem classes we should not pay too much heed to NFL, why\n&gt; would we=\r\n purposefully move *towards* the black box when we don&#39;t have\n&gt; to?  The re=\r\nal excitement, I think, is to find very general techniques\n&gt; for conveying =\r\nto the learner standard kinds of a priori practical\n&gt; information (or bias)=\r\n, e.g. geometry.\n&gt; \n&gt; That said, if you really did start without the repeat=\r\ning coordinate\n&gt; frames, I am guessing it would perform worse as you predic=\r\nt, though I\n&gt; don&#39;t know by how much.  It is probably worth doing just to s=\r\nee what\n&gt; happens.  Yet my personal view is that there would not be a very =\r\ndeep\n&gt; insight to gain from such a result.  After all, why would we expect =\r\nit\n&gt; to consistently discover the right regularity simply by chance every\n&gt;=\r\n time?  Remember that early in evolution, simply discovering this\n&gt; regular=\r\nity may not even be rewarded; just because it somehow gets\n&gt; lucky and figu=\r\nres out exactly the right repeating frame of reference,\n&gt; that does not nec=\r\nessarily mean that within those coordinate frames it\n&gt; is doing anything us=\r\neful (i.e. it could be a repetition of a bad\n&gt; policy), so the discovery is=\r\n likely to go unnoticed and die out, just\n&gt; as easily as it might be levera=\r\nged and elaborated properly.\n&gt; \n&gt; This problem is related to that discussio=\r\nn we had a while back about\n&gt; &quot;target-based evolution&quot; and the phenomena of=\r\n Picbreeder.  Often the\n&gt; stepping stones (such as discovering the right ba=\r\nsic regularity) are\n&gt; not recognized by the ultimate objective function, so=\r\n it&#39;s pretty much\n&gt; up to luck to find them and keep them around long enoug=\r\nh to take\n&gt; advantage of them.  My feeling is that it is not fruitful for a=\r\nny\n&gt; indirect encoding to try to solve that problem, because it is not a\n&gt; =\r\nproblem with the encoding per say but rather with the way fitness is\n&gt; assi=\r\ngned.\n&gt; \n&gt; With this dilemma in mind, I think it is perhaps most useful to =\r\npoint\n&gt; out that we can simply tell it the regularity that is important and=\r\n\n&gt; see if it can exploit that effectively, which shows that we can\n&gt; provid=\r\ne powerful domain bias.  \n&gt; \n&gt; ken\n&gt; \n&gt; --- In neat@yahoogroups.com, Jeff C=\r\nlune &lt;jclune@&gt; wrote:\n&gt; &gt;\n&gt; &gt; Hello-\n&gt; &gt; \n&gt; &gt; I enjoyed reading this. Thank=\r\ns for posting it.\n&gt; &gt; \n&gt; &gt; A question: how did HyperNEAT perform when you d=\r\nid not provide it\n&gt; with the\n&gt; &gt; repeating coordinate frame for each agent?=\r\n As you mention in the\n&gt; paper, this\n&gt; &gt; is something that HyperNEAT could =\r\nlearn on its own. I assume from\n&gt; the fact\n&gt; &gt; that you added it that Hyper=\r\nNEAT was not doing a good job of\n&gt; learning this.\n&gt; &gt; \n&gt; &gt; If that assumpti=\r\non is right, how bad was it at learning this problem\n&gt; &gt; decomposition? One=\r\n of the touted benefits of HyperNEAT, and generative\n&gt; &gt; encodings in gener=\r\nal, is the ability to evolve a module and reuse it\n&gt; many\n&gt; &gt; times (potent=\r\nially with variation).  Here the modularity of the\n&gt; problem was\n&gt; &gt; cleanl=\r\ny divided, and should have been relatively easy for HyperNEAT to\n&gt; &gt; discov=\r\ner. Do you find it disconcerting that it couldn&#39;t do so?\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt;=\r\n Cheers,\n&gt; &gt; Jeff Clune\n&gt; &gt; \n&gt; &gt; Digital Evolution Lab, Michigan State Univ=\r\nersity\n&gt; &gt; \n&gt; &gt; jclune@\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; &gt; From: Kenneth Stanley &lt;ks=\r\ntanley@&gt;\n&gt; &gt; &gt; Reply-To: &quot;neat@yahoogroups.com&quot; &lt;neat@yahoogroups.com&gt;\n&gt; &gt; =\r\n&gt; Date: Wed, 16 Apr 2008 22:48:44 -0000\n&gt; &gt; &gt; To: &quot;neat@yahoogroups.com&quot; &lt;n=\r\neat@yahoogroups.com&gt;\n&gt; &gt; &gt; Subject: [neat] Another New Paper:  Multiagent H=\r\nyperNEAT\n&gt; &gt; &gt; \n&gt; &gt; &gt; David D&#39;Ambrosio and I discuss the potential for Hype=\r\nrNEAT\n&gt; &gt; &gt; controlling multiple heterogeneous agents in this new\n&gt; &gt; &gt; pap=\r\ner, &quot;Generative Encoding for Multiagent Learning,&quot; to appear at\n&gt; &gt; &gt; GECCO=\r\n 2008:\n&gt; &gt; &gt; \n&gt; &gt; &gt; http://eplex.cs.ucf.edu/index.php?\n&gt; &gt; &gt; option=3Dcom_c=\r\nontent&task=3Dview&id=3D14&Itemid=3D28#dambrosio.gecco08\n&gt; &gt; &gt; \n&gt; &gt; &gt; Direc=\r\nt Link:\n&gt; &gt; &gt; \n&gt; &gt; &gt; http://eplex.cs.ucf.edu/papers/dambrosio_gecco08.pdf\n&gt;=\r\n &gt; &gt; \n&gt; &gt; &gt; We also have a nice sample of videos that depict various evolve=\r\nd\n&gt; &gt; &gt; teams in action:\n&gt; &gt; &gt; \n&gt; &gt; &gt; http://eplex.cs.ucf.edu/multiagenthyp=\r\nerneat\n&gt; &gt; &gt; \n&gt; &gt; &gt; The interesting idea in this paper is that just as a si=\r\nngle\n&gt; &gt; &gt; connective CPPN can encode how a single network varies over spac=\r\ne,\n&gt; &gt; &gt; it can also encode how a *set* of networks (each representing the\n=\r\n&gt; &gt; &gt; policy of one agent on the team) varies over space.  In this way,\n&gt; &gt;=\r\n &gt; HyperNEAT can learn an expression that encodes how policies vary\n&gt; &gt; &gt; o=\r\nver the team geometry.  For example, in a soccer team agents vary\n&gt; &gt; &gt; fro=\r\nm defensive to offensive as you move away from the goal.  Part of\n&gt; &gt; &gt; the=\r\n power of this approach is that it means basic skills can be\n&gt; &gt; &gt; learned =\r\nand shared among the whole team, since the CPPN encodes how\n&gt; &gt; &gt; those ski=\r\nlls vary across the field.\n&gt; &gt; &gt; \n&gt; &gt; &gt; ken\n&gt; &gt; &gt;\n&gt; &gt;\n&gt;\n\n\n\n"}}