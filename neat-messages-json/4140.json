{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":345796568,"authorName":"peterberrington","from":"&quot;peterberrington&quot; &lt;peterberrington@...&gt;","profile":"peterberrington","replyTo":"LIST","senderId":"dbMhjyisEAY1SKcHdQRgF8QCwORyT2pqf8KylJdR9wd8dsGDeYo7_M9-aiTXEDUDJ_NhCshKmNerjx4I9COMhwpAUKw3GRYnQHTeqbA4HNwhXic","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Question about hyperneat, NEVH and hidden layers","postDate":"1213086944","msgId":4140,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGcybGVkMCs4YW05QGVHcm91cHMuY29tPg==","inReplyToHeader":"PGcybDl1Mitkbmo5QGVHcm91cHMuY29tPg=="},"prevInTopic":4139,"nextInTopic":4143,"prevInTime":4139,"nextInTime":4141,"topicId":4138,"numMessagesInTopic":4,"msgSnippet":"Interesting points Petar. It was my understanding from the original hyperneat papers that points on the substrate correspond to nodes in a potential network.","rawEmail":"Return-Path: &lt;peterberrington@...&gt;\r\nX-Sender: peterberrington@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 56232 invoked from network); 10 Jun 2008 08:35:47 -0000\r\nX-Received: from unknown (66.218.67.96)\n  by m57.grp.scd.yahoo.com with QMQP; 10 Jun 2008 08:35:46 -0000\r\nX-Received: from unknown (HELO n19a.bullet.sp1.yahoo.com) (69.147.64.130)\n  by mta17.grp.scd.yahoo.com with SMTP; 10 Jun 2008 08:35:46 -0000\r\nX-Received: from [216.252.122.219] by n19.bullet.sp1.yahoo.com with NNFMP; 10 Jun 2008 08:35:46 -0000\r\nX-Received: from [209.73.164.83] by t4.bullet.sp1.yahoo.com with NNFMP; 10 Jun 2008 08:35:46 -0000\r\nX-Received: from [66.218.66.74] by t7.bullet.scd.yahoo.com with NNFMP; 10 Jun 2008 08:35:46 -0000\r\nDate: Tue, 10 Jun 2008 08:35:44 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;g2led0+8am9@...&gt;\r\nIn-Reply-To: &lt;g2l9u2+dnj9@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;peterberrington&quot; &lt;peterberrington@...&gt;\r\nSubject: Re: Question about hyperneat, NEVH and hidden layers\r\nX-Yahoo-Group-Post: member; u=345796568; y=oHMlqDNfMUiyF5VJvoY4yowNaFNYdYOS-8B2bjrB-xgTM3H9A-p1dHo2\r\nX-Yahoo-Profile: peterberrington\r\n\r\nInteresting points Petar.\nIt was my understanding from the original hyperne=\r\nat papers that points\non the substrate correspond to nodes in a potential n=\r\network. By\nquerying the CPPN you can find whether a connection exists betwe=\r\nen the\ntwo queried points, and if so its weight. Deciding how these nodes a=\r\nre\nlaid out on the substrate is necessary before you can query them; if\nso,=\r\n don&#39;t you need to decide which ones will be hidden and which will\nbe input=\r\ns or outputs a priori? Don&#39;t these decisions (along with the\ngeometric layo=\r\nut) bias the search in a very dramatic way, enough that\nit should be a crit=\r\nical focus of the search?\n\nAnother question: How do you decide if a point o=\r\nn the substrate is\nrecurrent to itself? \n\n I&#39;m interested in the behaviour =\r\ncharacterization you speak of, with\nregards to the joint angles over time. =\r\nIf what Ken is saying is true\n(and I think it sounds far too tempting not t=\r\no explore), the key\ninsight in characterizing joint activations/angles is t=\r\nhe quality of\noscillation. \n\nCan you elaborate more on how you characterize=\r\n a behaviour? You say\nthat if a gait gets far, you can record its joint ang=\r\nles, but how\nwould this benefit novelty search? Recording behaviour as the\n=\r\neuclidean coordinates on termination has the benefit that it sneakily\nincor=\r\nporates the objective function into it: while not DIRECTLY\nsearching for be=\r\nhaviours that maximize distance from origin, you are\neffectively searching =\r\nfor these same behaviours indirectly, because in\norder for a behaviour to b=\r\ne new, it has to be sufficiently far away\nfrom previous behaviours. \n\nEven =\r\nin novelty search, you still want to search for behaviours that\nget far fro=\r\nm the origin in a limited timespan. By recording where\nevery one ends up (l=\r\nocation-wise) and biasing the search in the\ndirection of new end locations,=\r\n you can accomplish this without\nexplicitly setting out to discover behavio=\r\nurs which get far.\n\nHow does this effect become realized by characterizing =\r\nbehaviour as\njoint angles or joint movement? New joint angles will produce\n=\r\ndifferent behaviour, that is certain, but there is nothing in the\nsearch fo=\r\nr new joint angles which suggests we are getting any closer\nto walking. You=\r\n say in your post that you can combine this information\nwith the actual dis=\r\ntance that behaviour acheives, but I can&#39;t see how\nthis would work.\n\nI coul=\r\nd be entirely wrong on this insight, so please share any\nthoughts you have.=\r\n\n\n\nI can&#39;t prove it, but I have a gut feeling that aperiodic or\n&quot;lopsided&quot; =\r\ngaits are not only inefficient, but are also unlikely to be\nstepping stoned=\r\n towards efficient gaits. \n\nPerhaps a useful way of characterizing behaviou=\r\nr will involve\ncharacterizing how the joint angles oscillate, but this migh=\r\nt have\nbeen what you are talking about. \n\nI don&#39;t know how irrlicht fares o=\r\nn this, but a problem with ODE when\ngenerating behaviours for ambulation is=\r\n deception in the form of\n&quot;cheating oscillators&quot;. If you are interested the=\r\nres information on\nthis in this paper:\nhttp://artax.karlin.mff.cuni.cz/~krc=\r\nap1am/ero/doc/report.pdf\nBasically you get the behaviour of one joint activ=\r\nating fully positive\nin one timestep, then on the next time step it activat=\r\nes fully\nnegative. The result is a kind of gliding behaviour as the frictio=\r\nn\nfrom the floor provides a net positive force on the body due to\ninaccurac=\r\ny. \n\nDo you think its worth it trying to remove &quot;cheaters&quot; from the\npopulat=\r\nion, or could that be an error (for example, are cheaters ever\nlikely to be=\r\n a stepping stone to something good? One would think no,\nbut then these thi=\r\nngs are never as clear as that)\n\n\n\n--- In neat@yahoogroups.com, &quot;petar_cher=\r\nvenski&quot; &lt;petar_chervenski@...&gt;\nwrote:\n&gt;\n&gt; Well the substrate in NEVH is not=\r\n a simple single layer perceptron \n&gt; thing, but a Continious Time Recurrent=\r\n Neural Network (CTRNN). Such \n&gt; networks with enough hidden nodes, full in=\r\nterconnectivity and the \n&gt; right time constants, biases and connection weig=\r\nhts, can approximate \n&gt; any dynamic system just like the 2 layer feed-forwa=\r\nrd NNs can \n&gt; approximate any continious function. \n&gt; HyperNEAT is allowed =\r\nto create any connective pattern in the CTRNN \n&gt; substrate and it also &quot;dra=\r\nws&quot; the time constants and biases through \n&gt; the nodes in the substrate. I =\r\ncan name this thing LeakyHyperNEAT \n&gt; because the substrate has more proper=\r\nties but it is essentialy the \n&gt; same HyperNEAT/CPPN-NEAT system at the gen=\r\netic level. \n&gt; Oh and one thing about layers.. I don&#39;t assume there are any=\r\n layers. \n&gt; There are no layers in a fully interconnected network, since ev=\r\nery \n&gt; node may output to any other node, including itself. There are no \n&gt;=\r\n layers in NEAT also, the CPPNs complexify and evolve like in regular \n&gt; NE=\r\nAT. \n&gt; I believe that the layered topology described in the paper mentioned=\r\n \n&gt; is a simplification of the problem based on some kind of priori in \n&gt; t=\r\nhe task domain. You can have a completely interconnected network and \n&gt; not=\r\n assume layered topology but then HyperNEAT may have difficulties \n&gt; discov=\r\nering the right connectivity concept early in evolution. \n&gt; So the short an=\r\nswer to your question is, as long as there are enough \n&gt; hidden nodes in th=\r\ne CTRNN network and HyperNEAT is not restricted to \n&gt; particular types of s=\r\nubstrates (like feed-forward-only), it should be \n&gt; able to learn to walk. =\r\nIt may be hard, though, but with NS applied to \n&gt; it, the problems with fit=\r\nness search disappear. \n&gt; One thing I remember you said before some time, i=\r\nf I characterize the \n&gt; behavior as the joint angles over time steps I woul=\r\nd be unable to \n&gt; distinguish the good behavior. But I realized that this i=\r\ns not true \n&gt; actually. If the humanoid goes forward enough, I can consider=\r\n it a \n&gt; good behavior, no matter that I don&#39;t keep track of the final XYZ =\r\nin \n&gt; the BC. \n&gt; \n&gt; Peter C\n&gt; \n&gt; --- In neat@yahoogroups.com, &quot;peterberring=\r\nton&quot; &lt;peterberrington@&gt; \n&gt; wrote:\n&gt; &gt;\n&gt; &gt; Although the papers on hyperneat =\r\npropose some unique ways of\n&gt; &gt; representing the substrate geometrically, I=\r\n got the impression that\n&gt; &gt; the resulting generated net was always going t=\r\no be 2 layer\n&gt; &gt; feed-forward network (isn&#39;t the cppn used to construct thi=\r\ns?). In \n&gt; the\n&gt; &gt; paper on hyperneat checkers however, Jason Gauci and Ken=\r\n Stanley \n&gt; query\n&gt; &gt; a single cppn to get connection weights for the input=\r\n/hidden layer,\n&gt; &gt; and the hidden/output layer. (A Case Study on the Critic=\r\nal Role of\n&gt; &gt; Geometric Regularity in Machine Learning)\n&gt; &gt; \n&gt; &gt; Given tha=\r\nt single layer feedforward networks were found incapable of\n&gt; &gt; learning li=\r\nnearly inseperable  patterns, it seems like the\n&gt; &gt; functionality of NEVH c=\r\nould be severely hindered by limiting itself \n&gt; to\n&gt; &gt; search the space of =\r\nsingle-layer perceptrons. \n&gt; &gt; \n&gt; &gt; Is there a principled way of extending =\r\nthe method used by Gauci and\n&gt; &gt; Stanley in the checkers paper to making ar=\r\nbitrary amounts hidden\n&gt; &gt; layers? Can anyone provide any insights on wheth=\r\ner limiting the\n&gt; &gt; generated nets to single layer actually inhibits their =\r\nability to\n&gt; &gt; learn in the domain of humanoid bipedal walking?\n&gt; &gt;\n&gt;\n\n\n\n"}}