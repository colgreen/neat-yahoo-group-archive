{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":281645563,"authorName":"afcarl2","from":"&quot;afcarl2&quot; &lt;a.carl@...&gt;","profile":"afcarl2","replyTo":"LIST","senderId":"NptERIS9v3TSWpXYgEtqT1j6ImgYW9r_FzERKHTjO_KwrMeIBH1_ql9rU8dWUyO-y5ReXdICzduuH-sFwKVjUUI","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Machine Learning and the Long View of AI","postDate":"1209422010","msgId":4003,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGZ2NWpicStyZDIwQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGZ2NWhiZit1YmJkQGVHcm91cHMuY29tPg=="},"prevInTopic":4002,"nextInTopic":4004,"prevInTime":4002,"nextInTime":4004,"topicId":3955,"numMessagesInTopic":49,"msgSnippet":"It appears that you are not addressing Peter s observations: a) CPPN weight mutations are more disruptive in effect than in a direct encoding (i.e. greater","rawEmail":"Return-Path: &lt;a.carl@...&gt;\r\nX-Sender: a.carl@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 3074 invoked from network); 28 Apr 2008 22:33:32 -0000\r\nX-Received: from unknown (66.218.67.94)\n  by m43.grp.scd.yahoo.com with QMQP; 28 Apr 2008 22:33:32 -0000\r\nX-Received: from unknown (HELO n19a.bullet.scd.yahoo.com) (66.94.237.48)\n  by mta15.grp.scd.yahoo.com with SMTP; 28 Apr 2008 22:33:32 -0000\r\nX-Received: from [66.218.69.4] by n19.bullet.scd.yahoo.com with NNFMP; 28 Apr 2008 22:33:32 -0000\r\nX-Received: from [66.218.66.86] by t4.bullet.scd.yahoo.com with NNFMP; 28 Apr 2008 22:33:31 -0000\r\nDate: Mon, 28 Apr 2008 22:33:30 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;fv5jbq+rd20@...&gt;\r\nIn-Reply-To: &lt;fv5hbf+ubbd@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;afcarl2&quot; &lt;a.carl@...&gt;\r\nSubject: Re: Machine Learning and the Long View of AI\r\nX-Yahoo-Group-Post: member; u=281645563; y=Bd02zHGTEnUPm12vcDjLutIbaB-tec3VNAoMb-7NRRt18Q\r\nX-Yahoo-Profile: afcarl2\r\n\r\nIt appears that you are not addressing Peter&#39;s observations:\n\na) CPPN weigh=\r\nt mutations are more disruptive in effect than in a \ndirect encoding (i.e. =\r\ngreater changes to the shape of the fitness \nlandscape),\n\nb) the relative c=\r\nomputational cost of &quot;discovering&quot; patterns in a \nCPPN is substantial,\n\nc) =\r\na priori knowledge is still required in the definition of the \nsubstrate re=\r\ngarding size and complexity,\n\nd) the weight threshold of 0.2 appears arbitr=\r\nary,\n\ne) the ability of a CPPN to discover patterns comes at too high a \nco=\r\nmputational cost relative to the direct encoding alternative.\n\nNuclear weap=\r\nons could be described as &quot;holistic&quot; also, but they are \nstill &quot;destructive=\r\n&quot; in nature. \n\nI do not recall seeing any relative computational cost compa=\r\nrisons in \nthe multi-agent paper, from which a comparison on a cost basis m=\r\nay be \ndrawn.\n\nIs there any cost data available?\n\n\n--- In neat@yahoogroups.=\r\ncom, &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt; wrote:\n&gt;\n&gt; Thanks Peter, glad you tho=\r\nught my post was useful.  I agree with the\n&gt; other part of your post as wel=\r\nl:  An important area for research \nwith\n&gt; HyperNEAT is how to automate the=\r\n evolution of the substrate\n&gt; complexity.  Ideally, we would like to see th=\r\ne substrate itself \nbecome\n&gt; increasingly complex/dense over evolutionary t=\r\nime in response to\n&gt; evolutionary pressures.  But if that is going to happe=\r\nn, ideally it\n&gt; should work elegantly and seamlessly, rather than being ad =\r\nhoc. \n&gt; \n&gt; I believe it is possible and it will be achieved sooner or later=\r\n.  I\n&gt; also believe that natural evolution proceeded by occasional \nincreas=\r\nes\n&gt; in holistic brain size, so I think it&#39;s a naturally appealing idea\n&gt; t=\r\nhat would bring us yet another step closer to the power of natural\n&gt; evolut=\r\nion. \n&gt; \n&gt; However, I do not believe that the mutations in HyperNEAT are \na=\r\nctually\n&gt; more destructive than those in NEAT.  It is true that HyperNEAT\n&gt;=\r\n mutations have holistic effects, but those holistic effects are\n&gt; orderly,=\r\n that is, it is not the equivalent of randomizing the \nweights\n&gt; of all the=\r\n connections involved.  Rather, it is a warping of the\n&gt; weight distributio=\r\nn along a dimension of regularity that was \nselected\n&gt; by evolution.   I th=\r\nink there is no a priori reason to believe that\n&gt; such changes are more or =\r\nless destructive than single-weight\n&gt; mutations, as long as the magnitude o=\r\nf the overall change is kept\n&gt; within a reasonable limit, just as with any =\r\nkind of mutation.\n&gt; \n&gt; ken\n&gt; \n&gt; --- In neat@yahoogroups.com, &quot;petar_cherven=\r\nski&quot; &lt;petar_chervenski@&gt;\n&gt; wrote:\n&gt; &gt;\n&gt; &gt; Great post, Ken! I really enjoyed=\r\n reading it. It is just all \ntrue. \n&gt; &gt; \n&gt; &gt; There is really a difference b=\r\netween building a brain and \nevolving \n&gt; &gt; one. No matter if the brain lear=\r\nns in its lifetime or not. In the \n&gt; &gt; case you build a brain yourself that=\r\n doesn&#39;t learn, it is just \n&gt; &gt; conventional programming at all. \n&gt; &gt; \n&gt; &gt; =\r\nI think the constraints for evolution should be very sharp, so to \n&gt; &gt; say,=\r\n because in EC in general the fitness function is the most \n&gt; &gt; important t=\r\nhing as well as the representation/mapping. You can&#39;t \njust \n&gt; &gt; say &quot;be sm=\r\nart!&quot; to an EC algorithm. You have to model \n&gt; &gt; its &quot;environment&quot; as well,=\r\n and the process of evaluation usually \n&gt; &gt; takes a lot of computation time=\r\n for the most interesting \nproblems. \n&gt; &gt; \n&gt; &gt; There is a kind of.. Hm I gu=\r\ness I can&#39;t express myself in english \n&gt; &gt; well. The more complex the task =\r\nis, the more computation time is \n&gt; &gt; required for a proper evaluation. \n&gt; =\r\n&gt; I am maybe not saying anything new to you, Ken, but I just \nmention I \n&gt; =\r\n&gt; understand it. \n&gt; &gt; \n&gt; &gt; HyperNEAT and CPPNs in general opened up an enti=\r\nre new field of \n&gt; &gt; research, that is, the evolution of mathematical compo=\r\nsitions \n&gt; &gt; describing phenotypes of any kind. What I think about it is, t=\r\nhat \n&gt; &gt; mutations are mostly destructive to the networks, while in a \nrobo=\r\ntics \n&gt; &gt; experiments with direct representation, one weight change is not =\r\n\nthat \n&gt; &gt; bad, so to say. But change one weight of a CPPN and you get a \nt=\r\notally \n&gt; &gt; different thing. In HyperNEAT this is not just a minor change, =\r\n\nbut a \n&gt; &gt; total change of the network&#39;s behaviour. So there is a great de=\r\nal \nof \n&gt; &gt; computation time required to discover some concepts. In fact th=\r\ne \n&gt; &gt; fitness landscape in CPPN-based evolution is totally different \nthan=\r\n \n&gt; &gt; other approaches to the same problem. \n&gt; &gt; \n&gt; &gt; Another thing is that=\r\n the geometry itself does not provide \n&gt; &gt; information about the phenotype =\r\ncomplexity at all. I mean that \neven a \n&gt; &gt; network of 1000000000 connectio=\r\nns can be generated by a \nconnective \n&gt; &gt; CPPN but the bias is usually towa=\r\nrds minimal solutions. I know \nthat \n&gt; &gt; complexification is a property of =\r\nthe genotype space, but why to \n&gt; &gt; waste computation time evaluating indiv=\r\niduals with millions of \n&gt; &gt; connections that actually are bad solutions? \n=\r\n&gt; &gt; \n&gt; &gt; So, you may provide the geometry to the search, but you still \ncan=\r\n&#39;t \n&gt; &gt; provide the complexity. You need a priori that the complexity of \nt=\r\nhe \n&gt; &gt; substrate is big enough. \n&gt; &gt; \n&gt; &gt; That 0.2 treshold is like a hard=\r\n-coded hack to me. It may be able \nto \n&gt; &gt; represent any kind of connectivi=\r\nty, but I think the effort for \n&gt; &gt; discovering it is bigger than discoveri=\r\nng the actual regularities \nat \n&gt; &gt; all. \n&gt; &gt; \n&gt; &gt; There should be a way to=\r\n map complexity of the genotype to the \n&gt; &gt; phenotype, but not in such a co=\r\nnstrained way. It should be \n&gt; &gt; increasing. Did you ever see an animal as =\r\nsimple as a worm but \nbig as \n&gt; &gt; a whale? OK size doesn&#39;t matter. :) This =\r\ncomparison was not a \ngood \n&gt; &gt; one. \n&gt; &gt; \n&gt; &gt; I know there is an option th=\r\nat HyperNEAT can evolve the substrate \nby \n&gt; &gt; itself, but how to control i=\r\nt? The dynamics of the neural \nnetworks \n&gt; &gt; has to be taken into account. =\r\n\n&gt; &gt; \n&gt; &gt; Sorry about my scattered around thoughts. That was just a stream =\r\n\nof \n&gt; &gt; conciosness. \n&gt; &gt; \n&gt; &gt; Peter\n&gt; &gt; \n&gt; &gt; --- In neat@yahoogroups.com,=\r\n &quot;Kenneth Stanley&quot; &lt;kstanley@&gt; wrote:\n&gt; &gt; &gt;\n&gt; &gt; &gt; --- In neat@yahoogroups.c=\r\nom, &quot;Derek James&quot; &lt;djames@&gt; wrote:\n&gt; &gt; &gt; \n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt;  In RL, in con=\r\ntrast, the long view is almost the opposite: \nThey\n&gt; &gt; &gt; want to\n&gt; &gt; &gt; &gt; &gt; =\r\n remove all constraints and still learn nevertheless.\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; I&#39;m =\r\nnot sure what you mean by this, Ken. Could you elaborate \na \n&gt; &gt; little?\n&gt; =\r\n&gt; &gt; &gt;\n&gt; &gt; &gt; \n&gt; &gt; &gt; Sure.  I think the problem is that I can&#39;t find a way to=\r\n \nexplain my\n&gt; &gt; &gt; point concisely.  As I try to explain it, it starts taki=\r\nng up \ntoo \n&gt; &gt; much\n&gt; &gt; &gt; text so I shorten it and then it loses its meani=\r\nng.  Let me \ngive it \n&gt; &gt; a\n&gt; &gt; &gt; try again...\n&gt; &gt; &gt; \n&gt; &gt; &gt; I think the dif=\r\nference between the goals of RL and NE is an\n&gt; &gt; &gt; interesting topic becaus=\r\ne they are almost always conflated, as \nif \n&gt; &gt; they\n&gt; &gt; &gt; are trying to so=\r\nlve the same problem.\n&gt; &gt; &gt; \n&gt; &gt; &gt; The RL community (e.g. value-function ap=\r\nproaches) is trying to \nbuild\n&gt; &gt; &gt; something that learns like a natural br=\r\nain.  They are saying, \n&gt; &gt; through\n&gt; &gt; &gt; analytic means we can deduce how =\r\na brain can learn from sparse\n&gt; &gt; &gt; reinforcement and formalize that proces=\r\ns in an algorithm.  The \n&gt; &gt; hope, I\n&gt; &gt; &gt; would think, is to eventually bu=\r\nild the &quot;general intelligence&quot; \nthat\n&gt; &gt; &gt; aligns with the holy grail of AI=\r\n.  So each step along the way \nis an\n&gt; &gt; &gt; improvement in that general abil=\r\nity.\n&gt; &gt; &gt; \n&gt; &gt; &gt; So if that is your goal, then the benchmarks you choose h=\r\nave to \nbe\n&gt; &gt; &gt; designed to measure progress to that goal.  So what they n=\r\need \nto do \n&gt; &gt; is\n&gt; &gt; &gt; show that their designed intelligence can work lar=\r\ngely \nindependently\n&gt; &gt; &gt; of a priori &quot;cheats&quot; that provide the meat of the=\r\n solution.  \n&gt; &gt; Because,\n&gt; &gt; &gt; after all, how can it be a general intellig=\r\nence if it needs you \nto\n&gt; &gt; &gt; tell it something that it is supposed to be =\r\nable to figure \nout?  \n&gt; &gt; This\n&gt; &gt; &gt; perspective, I believe, is aligned wi=\r\nth Jeff&#39;s view.\n&gt; &gt; &gt; \n&gt; &gt; &gt; However, NE as a long-term pursuit is involved=\r\n in something \n&gt; &gt; different,\n&gt; &gt; &gt; even though it can be applied to the sa=\r\nme problems.  NE is not \nan\n&gt; &gt; &gt; attempt to formalize how people learn wit=\r\nh sparse \nreinforcement. \n&gt; &gt; &gt; Rather, it is an attempt to formalize how e=\r\nvolution can build a \n&gt; &gt; brain.\n&gt; &gt; &gt;  So RL is formalizing the brain itse=\r\nlf and NE is formalizing how\n&gt; &gt; &gt; evolution succeeds in creating a brain. =\r\n NE is therefore one \nstep \n&gt; &gt; removed.\n&gt; &gt; &gt; \n&gt; &gt; &gt; This difference is ul=\r\ntimately a philosophical difference on the \nbest\n&gt; &gt; &gt; approach to creating=\r\n a full-blown AI.  The instrumental issue is\n&gt; &gt; &gt; whether you think it&#39;s e=\r\nasier to build it yourself or to design \nan\n&gt; &gt; &gt; algorithm that can build =\r\nit.  The confusion and hence \nconflation of\n&gt; &gt; &gt; the two approaches arises=\r\n in part because they do indeed both \naim at\n&gt; &gt; &gt; the same long view goal:=\r\n a general AI.  But they are coming at \nit \n&gt; &gt; from\n&gt; &gt; &gt; very different a=\r\nngles.\n&gt; &gt; &gt; \n&gt; &gt; &gt; And because of this stark difference, the *metric* of p=\r\nrogress \n&gt; &gt; should\n&gt; &gt; &gt; be quite different.  We cannot measure our progre=\r\nss in building \na\n&gt; &gt; &gt; general intelligence directly in the same way that =\r\nwe measure \nour\n&gt; &gt; &gt; progress in creating an evolutionary algorithm that i=\r\ntself will\n&gt; &gt; &gt; someday output one.  \n&gt; &gt; &gt; \n&gt; &gt; &gt; This distinction is pot=\r\nentially subtle and confusing so let me \ntry \n&gt; &gt; to\n&gt; &gt; &gt; make it clearer:=\r\n  Human brains aren&#39;t designed to build yet \nmore \n&gt; &gt; human\n&gt; &gt; &gt; brains. =\r\n We are good at a lot of things, and we learn \ngenerally, but\n&gt; &gt; &gt; we do n=\r\not build 100-trillion part devices that are more complex \nthan\n&gt; &gt; &gt; any kn=\r\nown object in the universe.  I&#39;m not saying we won&#39;t ever \nbe\n&gt; &gt; &gt; able to=\r\n do it, but if you want to simulate a human brain, your \nfirst\n&gt; &gt; &gt; though=\r\nt would not be that it needs to be capable of designing \nyet\n&gt; &gt; &gt; another =\r\nbrain by itself.  Your first thought is about things \nlike\n&gt; &gt; &gt; object rec=\r\nognition or pursuit and evasion.\n&gt; &gt; &gt; \n&gt; &gt; &gt; In contrast, building brains =\r\nis exactly what natural evolution \ndid,\n&gt; &gt; &gt; and it did it quite well.  Na=\r\ntural evolution does not perform \nobject\n&gt; &gt; &gt; recognition; it does not com=\r\nmunicate with language; it does not \nrun\n&gt; &gt; &gt; away from predators or hunt =\r\nfor prey.  Yet it does build brains \nthat\n&gt; &gt; &gt; themselves do those things.=\r\n  And that is the aspect of it we \nwish to\n&gt; &gt; &gt; harness- a very specific n=\r\niche kind of skill (though radically\n&gt; &gt; &gt; impressive)- not a general skill=\r\n.\n&gt; &gt; &gt; \n&gt; &gt; &gt; So the two pursuits are really quite different.  And therefo=\r\nre \nthey\n&gt; &gt; &gt; deserve different metrics to judge their progress with respe=\r\nct \nto \n&gt; &gt; the\n&gt; &gt; &gt; long term goal.  That is, unless we conflate them to =\r\nbe the same\n&gt; &gt; &gt; thing, which we often do without thinking about it.\n&gt; &gt; &gt;=\r\n \n&gt; &gt; &gt; For example, we could just say, well, both NE and RL are \nlearning\n=\r\n&gt; &gt; &gt; techniques, and after all, we can apply them to the same \nproblems, \n=\r\n&gt; &gt; so\n&gt; &gt; &gt; why make a big distinction in how we judge them?  Let&#39;s just \n=\r\ncompare\n&gt; &gt; &gt; them directly on the same benchmarks and get on with it.\n&gt; &gt; =\r\n&gt; \n&gt; &gt; &gt; That&#39;s fine for the short-term view, i.e. let&#39;s just improve our\n&gt;=\r\n &gt; &gt; ability to tackle practical problems, but for the long view, \nthey\n&gt; &gt;=\r\n &gt; cannot be judged in the same way.  If I improve at my ability to\n&gt; &gt; &gt; b=\r\nalance on one foot is that a sign that I will be able to build \na\n&gt; &gt; &gt; bra=\r\nin someday?  If evolution evolves a brain that plays \ncheckers, is\n&gt; &gt; &gt; th=\r\nat a sign that evolution *itself* is on the road to performing\n&gt; &gt; &gt; object=\r\n recognition?  These are totally different pursuits.\n&gt; &gt; &gt; \n&gt; &gt; &gt; So in tha=\r\nt context, how should they be judged with respect to \nlong\n&gt; &gt; &gt; term goals=\r\n?  Well, I think RL deserves to be judged based on its\n&gt; &gt; &gt; increasing abi=\r\nlity to learn more generally.  And in that sense,\n&gt; &gt; &gt; exactly Jeff&#39;s crit=\r\neria should apply to it: We should be \ninterested \n&gt; &gt; in\n&gt; &gt; &gt; whether it =\r\n&quot;needs&quot; a priori information to learn.  In other \nwords, \n&gt; &gt; the\n&gt; &gt; &gt; les=\r\ns we need to constrain the problem for the learner, the more\n&gt; &gt; &gt; impresse=\r\nd we deserve to be.  That shows progress towards more \nand \n&gt; &gt; more\n&gt; &gt; &gt; =\r\ngeneral AI and ML.\n&gt; &gt; &gt; \n&gt; &gt; &gt; But if evolution is not *itself* supposed t=\r\no be a general \nlearner\n&gt; &gt; &gt; (rather, we just want it to concentrate on on=\r\ne very specific \nskill:\n&gt; &gt; &gt; brain building), then those considerations ar=\r\ne orthogonal to its\n&gt; &gt; &gt; greatest promise.  Its promise is to evolve a bra=\r\nin itself, and \nas\n&gt; &gt; &gt; such, neuroevolutionary algorithms deserve to be j=\r\nudged on our \n&gt; &gt; ability\n&gt; &gt; &gt; to *constrain* the problem so that they can=\r\n accomplish exactly \n&gt; &gt; that. \n&gt; &gt; &gt; In other words, the problem NE *algor=\r\nithms* face is leaps and \nbounds\n&gt; &gt; &gt; beyond what RL algorithms face.  RL =\r\nalgorithms just need to be \nable \n&gt; &gt; to\n&gt; &gt; &gt; do as well as brains; NE has=\r\n to be able to discover brains \n&gt; &gt; themselves.\n&gt; &gt; &gt;  Therefore, progress =\r\nis NE should in part be measured with \nrespect \n&gt; &gt; to\n&gt; &gt; &gt; progress in co=\r\nnstraining the problem to make such a discovery \nmore\n&gt; &gt; &gt; likely.  When a=\r\nn NE algorithm is improved to allow us to tell \nit \n&gt; &gt; more\n&gt; &gt; &gt; about th=\r\ne world in which its output will be situated, that is \ngood\n&gt; &gt; &gt; news for =\r\nthe long view.  In short, we don&#39;t care at all how NE\n&gt; &gt; &gt; produced a brai=\r\nn as long as it really does.  Will anyone \ncomplain \n&gt; &gt; if a\n&gt; &gt; &gt; human b=\r\nrain pops out of a system that was a priori given the \nconcept\n&gt; &gt; &gt; of sym=\r\nmetry?  Rather, we should be glad that such a priori \ncontext \n&gt; &gt; was\n&gt; &gt; =\r\n&gt; possible to provide in the first place, because it may have \nsaved \n&gt; &gt; u=\r\ns a\n&gt; &gt; &gt; year of wasted computation in figuring it out needlessly.\n&gt; &gt; &gt; \n=\r\n&gt; &gt; &gt; This distinction is almost completely ignored when NE and RL are\n&gt; &gt; =\r\n&gt; compared directly.  Therefore, the implications of any such \n&gt; &gt; comparis=\r\non\n&gt; &gt; &gt; are fuzzy and lacking context with respect to the long view.  I \na=\r\nm \n&gt; &gt; not\n&gt; &gt; &gt; sure if I should care or not if RL solves something better=\r\n than \nNE, \n&gt; &gt; or\n&gt; &gt; &gt; vice versa, because the author doesn&#39;t explain how=\r\n the result \naligns\n&gt; &gt; &gt; with the long-term goals of the fields.  Long ter=\r\nm goals seem \nlike\n&gt; &gt; &gt; unwelcome guests these days in AI, which is why I =\r\nprobably \nwon&#39;t be\n&gt; &gt; &gt; writing about any of this in a publication any tim=\r\ne soon.  \n&gt; &gt; &gt; \n&gt; &gt; &gt; ...\n&gt; &gt; &gt; \n&gt; &gt; &gt; So Derek what you are saying about =\r\nNE being good at &quot;hard-wired&quot;\n&gt; &gt; &gt; solutions and RL being appropriate for =\r\nontogenetic lifetime \n&gt; &gt; learning,\n&gt; &gt; &gt; while true, is not what I think o=\r\nf as the primary long-view \nissue.\n&gt; &gt; &gt; \n&gt; &gt; &gt; In the long view, NE will b=\r\ne used to evolve structures that do \nlearn\n&gt; &gt; &gt; over their lifetime, i.e. =\r\nnot hardwired at all.  The only \nreason \n&gt; &gt; that\n&gt; &gt; &gt; it tends to be used=\r\n to evolve hardwired solutions today is \nbecause \n&gt; &gt; we\n&gt; &gt; &gt; are trying t=\r\no get a foothold on how to evolve certain types of \n&gt; &gt; complex\n&gt; &gt; &gt; struc=\r\ntures.   Once we get very good at it, focus will naturally \n&gt; &gt; shift\n&gt; &gt; &gt;=\r\n to evolving dynamic brains (and of course there is already work \n&gt; &gt; along=\r\n\n&gt; &gt; &gt; these lines today, much from Floreano).  I do not even think \nthat w=\r\ne\n&gt; &gt; &gt; will need to include stock learning algorithms like Hebbian \n&gt; &gt; le=\r\narning.\n&gt; &gt; &gt;  When we achieve our long-term goals, those *themselves* will=\r\n \nbe \n&gt; &gt; left\n&gt; &gt; &gt; up to evolution because after all there may be somethi=\r\nng even \n&gt; &gt; better.\n&gt; &gt; &gt;  \n&gt; &gt; &gt; &gt; &gt; My aim is to design an\n&gt; &gt; &gt; &gt; &gt;  al=\r\ngorithm that will output a brain, not to design the \nbrain \n&gt; &gt; itself.\n&gt; &gt;=\r\n &gt; &gt; \n&gt; &gt; &gt; &gt; But what kind of brain are you wanting to output?\n&gt; &gt; &gt; &gt; \n&gt; =\r\n&gt; &gt; \n&gt; &gt; &gt; Note that I&#39;m speaking purely about the long view for these \n&gt; &gt;=\r\n different\n&gt; &gt; &gt; fields here.  Of course on a day-to-day basis I am not sol=\r\nely \n&gt; &gt; focused\n&gt; &gt; &gt; on what will happen 100 years from now.  On a practi=\r\ncal day-to-\nday\n&gt; &gt; &gt; basis, of course I want to make NE better capable to =\r\ntackle \nproblems\n&gt; &gt; &gt; that e.g. RL tackles.  So in the short-term context,=\r\n I just \nwant to\n&gt; &gt; &gt; output something that works for the problem at hand.=\r\n\n&gt; &gt; &gt; \n&gt; &gt; &gt; But in the long view, which we were talking about, I think th=\r\ne\n&gt; &gt; &gt; ultimate goal would be to output a full-fledged adaptive system \nwi=\r\nth\n&gt; &gt; &gt; astronomical complexity and the power and subtlety of human \n&gt; &gt; r=\r\neasoning.\n&gt; &gt; &gt;  On that path, constraint is the only hope, unless you want=\r\n to \nwait\n&gt; &gt; &gt; three billion years and just hope in the meantime that the =\r\n\ninitial\n&gt; &gt; &gt; conditions were set up correctly.  Therefore, demonstrations=\r\n of \nthe\n&gt; &gt; &gt; power of constraint deserve to be judged as evidence of the =\r\n\npromise \n&gt; &gt; of\n&gt; &gt; &gt; and progress towards the long term goal in NE.\n&gt; &gt; &gt;=\r\n \n&gt; &gt; &gt; ken\n&gt; &gt; &gt;\n&gt; &gt;\n&gt;\n\n\n\n"}}