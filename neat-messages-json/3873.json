{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":281645563,"authorName":"afcarl2","from":"&quot;afcarl2&quot; &lt;a.carl@...&gt;","profile":"afcarl2","replyTo":"LIST","senderId":"dr8ffB4FalBCPNwSYTjsKhi2NboSq-Edus2CWUbdVCTLEFJ_ErU2qpsYrDEw_P_leyman35gHzE4625wYq_NtlM","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Backpropagation and NEAT","postDate":"1205655528","msgId":3873,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGZyaWw1OCtrM2I0QGVHcm91cHMuY29tPg==","inReplyToHeader":"PGZyaTA0cytyYzMwQGVHcm91cHMuY29tPg=="},"prevInTopic":3872,"nextInTopic":3874,"prevInTime":3872,"nextInTime":3874,"topicId":3846,"numMessagesInTopic":41,"msgSnippet":"Perhaps the following two quotes from the Dakota Users Manual ( http://www.cs.sandia.gov/DAKOTA/software.html ) may be helpful to bring focus on the salient","rawEmail":"Return-Path: &lt;a.carl@...&gt;\r\nX-Sender: a.carl@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 16327 invoked from network); 16 Mar 2008 08:18:51 -0000\r\nX-Received: from unknown (66.218.67.97)\n  by m46.grp.scd.yahoo.com with QMQP; 16 Mar 2008 08:18:51 -0000\r\nX-Received: from unknown (HELO n24c.bullet.scd.yahoo.com) (66.218.67.214)\n  by mta18.grp.scd.yahoo.com with SMTP; 16 Mar 2008 08:18:51 -0000\r\nX-Received: from [209.73.164.86] by n24.bullet.scd.yahoo.com with NNFMP; 16 Mar 2008 08:18:48 -0000\r\nX-Received: from [66.218.66.48] by t8.bullet.scd.yahoo.com with NNFMP; 16 Mar 2008 08:18:48 -0000\r\nDate: Sun, 16 Mar 2008 08:18:48 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;fril58+k3b4@...&gt;\r\nIn-Reply-To: &lt;fri04s+rc30@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;afcarl2&quot; &lt;a.carl@...&gt;\r\nSubject: Re: Backpropagation and NEAT\r\nX-Yahoo-Group-Post: member; u=281645563; y=dYxzUV_gm0Or-qonEAZj7DzeQYSPyGUsMb7zHL85xCFlkA\r\nX-Yahoo-Profile: afcarl2\r\n\r\nPerhaps the following two quotes from the Dakota Users Manual ( \nhttp://www=\r\n.cs.sandia.gov/DAKOTA/software.html ) may be helpful to \nbring focus on the=\r\n salient issues:\n\n\n1) &quot;Multilevel Hybrid Optimization: This strategy allows=\r\n the user to \nspecify a sequence of optimization methods, with the results =\r\nfrom one \nmethod providing the starting point for the next method in the \ns=\r\nequence. An example which is useful in many engineering design \nproblems in=\r\nvolves the use of a nongradient-based global optimization\nmethod (e.g., gen=\r\netic algorithm) to identify a promising region of \nthe parameter space, whi=\r\nch feeds its results into a gradient-based \nmethod (quasi-Newton, SQP, etc.=\r\n) to perform an efficient local search \nfor the optimum point.&quot;, Sec. 3.8 O=\r\nptimization Strategies, p. 59.\n\n\n2) &quot;Rather, EAs are better suited to optim=\r\nization problems where \nconventional gradient-based optimization fails, suc=\r\nh as situations \nwhere there are multiple local optima and/or gradients are=\r\n not \navailable. In such cases, the computational expense of an EA is \nwarr=\r\nanted since other optimization methods are not applicable or \nimpractical. =\r\nIn many optimization problems, EAs often quickly \nidentify promising region=\r\ns of the design space where the global \nminimum may be located. However, an=\r\n EA can be slow to converge to\nthe optimum. For this reason, it can be an e=\r\nffective approach to \ncombine the global search capabilities of a EA with t=\r\nhe efficient \nlocal search of a gradient-based algorithm in a multilevel hy=\r\nbrid \noptimization strategy. In this approach, the optimization starts by \n=\r\nusing a few iterations of a EA to provide the initial search for a \ngood re=\r\ngion of the parameter space (low objective function and/or \nfeasible constr=\r\naints), and then it switches to a gradient-based\nalgorithm (using the best =\r\ndesign point found by the EA as its \nstarting point) to perform an efficien=\r\nt local search for an optimum \ndesign point.&quot;, Sec. 2.4.7 Nongradient-based=\r\n Optimization via \nEvolutionary Algorithm, p. 43.\n\nMost real world problems=\r\n are multiobjective in nature, with multiple \nnon-linear inequality constra=\r\nints. The determination of the pareto \nfront only magnifies the referenced =\r\nissues.\n\nAnd no, it is not an exageration, it is based upon experience. \nCo=\r\nnsider the computational sequence of events of the two alternatives \nas mea=\r\nsured in total overall fitness evaluations required to achieve \nconvergence=\r\n to a local minima. The robust nature of GA to find a \nglobal minima also w=\r\norks against quick convergence. NEAT is a \nsignificant inpromement over con=\r\nventional GA, but the improvement is \nstill inadequate to overcome the comp=\r\nutational cost in problem \ndomains in which the fitness evaluation is non-t=\r\nrivial.\n\n\n--- In neat@yahoogroups.com, &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt; wro=\r\nte:\n&gt;\n&gt; Andy, I may be misunderstanding some of your points about local \nse=\r\narch.\n&gt; \n&gt; First, the term &quot;local search&quot; itself implies that we are talkin=\r\ng\n&gt; about a method that has been developed exclusively to search in one\n&gt; l=\r\nocal part of the search space.  But what method is really intended\n&gt; for th=\r\nat purpose and why is that a good thing?  The problem in\n&gt; difficult tasks =\r\nin general is usually that whatever algorithm you \nare\n&gt; using gets *trappe=\r\nd* in a local area.  Therefore, generally \nspeaking,\n&gt; an algorithm that is=\r\n designed to stay in one area is probably a bad\n&gt; thing, not a good thing. =\r\n Only the most trivial problems entail \nsimply\n&gt; running up the first hills=\r\nide that you see.  For that, all we would\n&gt; need is hill climbing.  \n&gt; \n&gt; I=\r\nn fact, one of the liabilities of backprop is that it tends to be\n&gt; caught =\r\non local optima.  That is a serious problem and by no means a\n&gt; reason to r=\r\necommend it.  \n&gt; \n&gt; You say, &quot;There are many times in which it is orders of=\r\n magnitude\n&gt; quicker for a non-GA local gradient search method to find a lo=\r\ncal\n&gt; minima, than an equivalent GA to mutate through generations to find\n&gt;=\r\n the same local minima.&quot;\n&gt; \n&gt; Generally speaking finding a local minimum is=\r\n easy for any \nalgorithm.\n&gt;    A gradient technique is likely faster than a=\r\n GA, but &quot;orders of\n&gt; magnitude?&quot;  In general, that sounds exagerated.  In =\r\nfact, in the \n90s\n&gt; before NEAT existed there were a number of papers writt=\r\nen that\n&gt; compared neuroevolution to backprop and concluded that \nneuroevol=\r\nution\n&gt; was faster.  I do not think these results are meaningful because th=\r\ne\n&gt; issue is highly domain-dependent, but it does show that neither\n&gt; appro=\r\nach has a massive advantage over the other in general.  In \nfact,\n&gt; claimin=\r\ng otherwise just winds up running into No Free Lunch.\n&gt; \n&gt; Yet in most prob=\r\nlems what we really care about finding are not just\n&gt; any local optima but =\r\n*particular* local optima (or the global \noptimum)\n&gt; that satisfy a &quot;good e=\r\nnough&quot; criterion.  Thus we need to be able to\n&gt; leave the local neighborhoo=\r\nd entirely and go on a real search.  For\n&gt; that kind of search, you need an=\r\n algorithm that is designed not to\n&gt; simply rush up the nearest hill.  \n&gt;  =\r\n\n&gt; You also say, &quot;But a local search method could just as easily wander\n&gt; f=\r\nree in the local fitness landscape, un-encumbered by organism\n&gt; topology is=\r\nsues, to find the local minima.&quot;\n&gt; \n&gt; I do not understand what you mean by =\r\n&quot;un-encumbered.&quot;  All search is\n&gt; within the confines of a certain topology=\r\n, even if that topology is\n&gt; set by the user.  The topology is what defines=\r\n the search space.  \nYou\n&gt; cannot isolate search from the space being searc=\r\nhed.  Furthermore, \nif\n&gt; there is any encumbrance, it is the inability to c=\r\nhange the topology\n&gt; when it is found wanting.  Thus searching through diff=\r\nerent \ntopologies\n&gt; is the opposite of an encumbrance; it is a liberation. =\r\n \n&gt; \n&gt; Along the same lines, this statement also seems misleading: &quot;The \nmo=\r\nral\n&gt; of the story is that local search does not have to be constrained by\n=\r\n&gt; organism topology, beyond that of providing the start point.&quot;\n&gt; \n&gt; Again,=\r\n local search is always constrained by topology.  Topology\n&gt; defines the di=\r\nmensions of the search. \n&gt; \n&gt; What you may not be considering is that chang=\r\ning the topology is\n&gt; changing the search space itself.  It is an entirely =\r\ndifferent type \nof\n&gt; operation than moving *within* a particular search spa=\r\nce.  Adding a\n&gt; new connection adds a new dimension to space.  That is not =\r\nthe same \nas\n&gt; moving in any particular direction along any particular dime=\r\nnsion,\n&gt; which is what you mean by &quot;local search.&quot;\n&gt; \n&gt; In a broader pictur=\r\ne, gradient search is not the panacea that we \nneed.\n&gt; It is exactly the fa=\r\nilures of such search that are the reason that\n&gt; we are not able to handle =\r\nthese very difficult real world problems\n&gt; that you mention.  The big chall=\r\nenge is that the gradient in a \nmassive\n&gt; multidimensional space on a compl=\r\nex problem is almost certainly \nhighly\n&gt; deceptive, and gradient techniques=\r\n have nothing going for them other\n&gt; than the gradient!  If the gradient is=\r\n all we have to go on, how can\n&gt; that be a good thing?  \n&gt; \n&gt; It will certa=\r\ninly be necessary to change topologies as a part of any\n&gt; effective formula=\r\n for success on extremely hard problems, among \nother\n&gt; ingredients that ar=\r\ne are still being discovered and invented today.\n&gt; \n&gt; ken\n&gt; \n&gt; \n&gt; --- In ne=\r\nat@yahoogroups.com, &quot;afcarl2&quot; &lt;a.carl@&gt; wrote:\n&gt; &gt;\n&gt; &gt; Peter,\n&gt; &gt; \n&gt; &gt; It a=\r\nppears that you are implicitly assuming that that the only \nway \n&gt; &gt; to &quot;re=\r\nach out and touch&quot; the fitness evaluation is via a NEAT \n&gt; &gt; fabricated org=\r\nanism topology. Local search does not have to \nproceed \n&gt; &gt; via organism to=\r\npology. \n&gt; &gt; \n&gt; &gt; Usage of NEAT as a global search method, as part of a \nhi=\r\nerarchical \n&gt; &gt; methodology, surely uses organism topology as the local sea=\r\nrch \nstart \n&gt; &gt; point. And in the case of your proposed backprop search on =\r\n\norganism \n&gt; &gt; weight values, maintains the same number and composition of =\r\nnodes \nand \n&gt; &gt; associated connectivity. But a local search method could ju=\r\nst as \n&gt; &gt; easily wander free in the local fitness landscape, un-encumbered=\r\n \nby \n&gt; &gt; organism topology issues, to find the local minima.\n&gt; &gt; \n&gt; &gt; The =\r\nvery strong point of GA can also be its greatest weakness, in \n&gt; &gt; instance=\r\ns in which the computational resource requirements of the \n&gt; &gt; fitness eval=\r\nuation, in light of the dimensionality and hyper \nvolume \n&gt; &gt; size and comp=\r\nlexity, become non-trivial.\n&gt; &gt; \n&gt; &gt; This is especially true in the case of=\r\n non-adaptive mutation \n&gt; &gt; parameters, as is currently the case with NEAT.=\r\n There are many \ntimes \n&gt; &gt; in which it is orders of magnitude quicker for =\r\na non-GA local \n&gt; &gt; gradient search method to find a local minima, than an =\r\nequivalent \nGA \n&gt; &gt; to mutate through generations to find the same local mi=\r\nnima. \n&gt; &gt; \n&gt; &gt; NEAT speciation and niche protection help to mitigate the p=\r\nroblem \nvia \n&gt; &gt; population size and maintaining multiple species, but at a=\r\n \n&gt; &gt; computational cost. But a hierarchical search methodology can \napply =\r\n\n&gt; &gt; the strengths of both GA and local search, without having to use \n&gt; &gt; =\r\ncomputational power to cover-up GA&#39;s weak points.\n&gt; &gt; \n&gt; &gt; The remaining qu=\r\nestion as to whether to a) re-encode the final \n&gt; &gt; destination of the loca=\r\nl search back into the organism&#39;s \ntopology, or \n&gt; &gt; b) simply take the fin=\r\nal/best fitness derived by the local search \n&gt; &gt; (using the organism&#39;s orig=\r\ninal topology as the start point), and \n&gt; &gt; associate it with the organism =\r\nand it&#39;s original topology, is up \nfor \n&gt; &gt; debate and/or personal preferen=\r\nce.\n&gt; &gt; \n&gt; &gt; The moral of the story is that local search does not have to b=\r\ne \n&gt; &gt; constrained by organism topology, beyond that of providing the \nstar=\r\nt \n&gt; &gt; point. \n&gt; &gt; \n&gt; &gt; Real world problems are so complex in light of curr=\r\nent computer \n&gt; &gt; speeds and fitness computation requirements, to render GA=\r\n alone \nto be \n&gt; &gt; computationally unpractical in many instances, even with=\r\n the \nobvious \n&gt; &gt; benefits that NEAT brings to the table.\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt;=\r\n &gt; --- In neat@yahoogroups.com, &quot;petar_chervenski&quot; \n&gt; &gt; &lt;petar_chervenski@&gt;=\r\n wrote:\n&gt; &gt; &gt;\n&gt; &gt; &gt; Well actually speciation takes care of this. Species ar=\r\ne \nallowed to \n&gt; &gt; &gt; exist until they stagnate for too long time. If some n=\r\new \nstructure \n&gt; &gt; &gt; appears through mutations, the mutated individuals are=\r\n \nseparated in \n&gt; &gt; &gt; another species. Each species is a local protected co=\r\nmpetition \n&gt; &gt; among \n&gt; &gt; &gt; individuals grouped by similarity. Consider it =\r\nas a GA \nperformed on \n&gt; &gt; &gt; near identical topologies. Then you can see NE=\r\nAT as a algorithm \n&gt; &gt; &gt; running multiple GAs. So this is actually what you=\r\n mean by \ndynamic \n&gt; &gt; &gt; programming.. or something. In fact this scheme is=\r\n far better \nthan \n&gt; &gt; &gt; it. \n&gt; &gt; &gt; As for the idea of speculative structur=\r\ne, this is the core of \nNEAT \n&gt; &gt; &gt; and it is actually Ken&#39;s idea :) \n&gt; &gt; &gt;=\r\n Colin Green&#39;s idea is about phased searching, as far as I know. \nIt \n&gt; &gt; i=\r\ns \n&gt; &gt; &gt; that after some structure is added through complexifying, a \n&gt; &gt; &gt;=\r\n simplifying phase kicks in, removing any unnecessary structure, \n&gt; &gt; thus =\r\n\n&gt; &gt; &gt; returning the search down to a baseline low dimentional space \nwhile=\r\n \n&gt; &gt; &gt; retaining the fitness (because of elitism). \n&gt; &gt; &gt; \n&gt; &gt; &gt; Peter\n&gt; &gt;=\r\n &gt; \n&gt; &gt; &gt; --- In neat@yahoogroups.com, c f &lt;christofer_fransson@&gt; wrote:\n&gt; =\r\n&gt; &gt; &gt;\n&gt; &gt; &gt; &gt; In dynamic programming the idea is to divide the\n&gt; &gt; &gt; &gt; solu=\r\ntion in steps and then for each step present a\n&gt; &gt; &gt; &gt; fixed number of poss=\r\nible solutions.\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; Collin Greens idea is that speculative nod=\r\nes are added\n&gt; &gt; &gt; &gt; to the solutions but it might take time/generations\n&gt; =\r\n&gt; &gt; &gt; before an added node are shown to be useful. \n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; Is it =\r\npossible to apply dynamic programming approach\n&gt; &gt; &gt; &gt; to this area, to evo=\r\nlve NEAT driven networks?\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; To combine local optimization an=\r\nd dynamic programming\n&gt; &gt; &gt; &gt; ideas?\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; Br,\n&gt; &gt; &gt; &gt; Christofe=\r\nr\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; \n&gt;=\r\n &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; --- petar_chervenski &lt;petar_cher=\r\nvenski@&gt;\n&gt; &gt; &gt; &gt; wrote:\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; Given the simplest topology (a p=\r\nerceptron\n&gt; &gt; &gt; &gt; &gt; structure), the local \n&gt; &gt; &gt; &gt; &gt; minima is just one. Pe=\r\nrceptrons are always\n&gt; &gt; &gt; &gt; &gt; guaranteed to converge on \n&gt; &gt; &gt; &gt; &gt; correct=\r\n weights. But increasing the dimentionality\n&gt; &gt; &gt; &gt; &gt; of the solution \n&gt; &gt; =\r\n&gt; &gt; &gt; increases the error surface&#39;s curvature as well. So\n&gt; &gt; &gt; &gt; &gt; more di=\r\nmentions \n&gt; &gt; &gt; &gt; &gt; means more complex error surface. The coolest thing\n&gt; &gt;=\r\n &gt; &gt; &gt; in NEAT is that \n&gt; &gt; &gt; &gt; &gt; when it increases the dimentionality of t=\r\nhe\n&gt; &gt; &gt; &gt; &gt; solution, the individuals \n&gt; &gt; &gt; &gt; &gt; are already located in a =\r\npromising area of the new\n&gt; &gt; &gt; &gt; &gt; space. In fact \n&gt; &gt; &gt; &gt; &gt; those spaces =\r\nare related to each other - you don&#39;t\n&gt; &gt; &gt; &gt; &gt; know how the error \n&gt; &gt; &gt; &gt;=\r\n &gt; surface is going to look like when you enter the new\n&gt; &gt; &gt; &gt; &gt; space wit=\r\nh more \n&gt; &gt; &gt; &gt; &gt; dimentions. There are unlimited possibilities. \n&gt; &gt; &gt; &gt; &gt;=\r\n So what local gradient search will do in essence is\n&gt; &gt; &gt; &gt; &gt; pushing the =\r\n\n&gt; &gt; &gt; &gt; &gt; weights towards the *local* minumim.. It is not\n&gt; &gt; &gt; &gt; &gt; guaran=\r\nteed that this \n&gt; &gt; &gt; &gt; &gt; is the *solution*! It is simply because you don&#39;t=\r\n\n&gt; &gt; &gt; &gt; &gt; know the solution&#39;s \n&gt; &gt; &gt; &gt; &gt; dimentionality at first. It may r=\r\nequire 3 or\n&gt; &gt; &gt; &gt; &gt; 21342532 dimentions. \n&gt; &gt; &gt; &gt; &gt; Don&#39;t forget that NEA=\r\nT complexifies solutions\n&gt; &gt; &gt; &gt; &gt; incrementaly. \n&gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; Pete=\r\nr\n&gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; --- In neat@yahoogroups.com, &quot;afcarl2&quot; &lt;a.carl@&gt;\n&gt; &gt;=\r\n &gt; &gt; &gt; wrote:\n&gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; In fact, it may be that a substancial=\r\n portion of\n&gt; &gt; &gt; &gt; &gt; the value-added of \n&gt; &gt; &gt; &gt; &gt; &gt; speciation and niche =\r\nprotection of infant\n&gt; &gt; &gt; &gt; &gt; organisms, is associated \n&gt; &gt; &gt; &gt; &gt; &gt; with p=\r\nroviding opportunity to accumulate\n&gt; &gt; &gt; &gt; &gt; sufficient neighborhood \n&gt; &gt; &gt;=\r\n &gt; &gt; &gt; evaluations to &quot;discover&quot; the same local minimia\n&gt; &gt; &gt; &gt; &gt; over mult=\r\niple \n&gt; &gt; &gt; &gt; &gt; &gt; generations, that a local search may discover in\n&gt; &gt; &gt; &gt; =\r\n&gt; one generation. \n&gt; &gt; &gt; &gt; &gt; And \n&gt; &gt; &gt; &gt; &gt; &gt; maintaining multiple species =\r\nin hope that one of\n&gt; &gt; &gt; &gt; &gt; the local minimia \n&gt; &gt; &gt; &gt; &gt; &gt; will in fact a=\r\nlso be the global minimia.\n&gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; --- In neat@yahoogroups=\r\n.com, &quot;afcarl2&quot; &lt;a.carl@&gt;\n&gt; &gt; &gt; &gt; &gt; wrote:\n&gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; If &quot;=\r\nmost individuals in a species represented by\n&gt; &gt; &gt; &gt; &gt; a given \n&gt; &gt; &gt; &gt; &gt; t=\r\nopology&quot; \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; ended up in &quot;the same local minimia&quot;, one could\n&gt; &gt;=\r\n &gt; &gt; &gt; argue that the \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; subject specie&#39;s logical end point was=\r\n the same\n&gt; &gt; &gt; &gt; &gt; local minimia, \n&gt; &gt; &gt; &gt; &gt; and \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; that the c=\r\nost of maintaining more than one\n&gt; &gt; &gt; &gt; &gt; organism was \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; comp=\r\nutationally wasteful. Better to know sooner\n&gt; &gt; &gt; &gt; &gt; and breed \n&gt; &gt; &gt; &gt; &gt; =\r\n&gt; additional \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; organisms of differing topology so as to\n&gt; &gt; &gt; =\r\n&gt; &gt; maintain the population \n&gt; &gt; &gt; &gt; &gt; &gt; size \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; and maximize t=\r\nhe population&#39;s &quot;effective&quot;\n&gt; &gt; &gt; &gt; &gt; diversity.\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt;=\r\n &gt; Paying more for the same answer does not make it\n&gt; &gt; &gt; &gt; &gt; a better answ=\r\ner.\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; --- In neat@yahoogroups.com, &quot;petar_cherve=\r\nnski&quot; \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &lt;petar_chervenski@&gt; wrote:\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt;=\r\n &gt; &gt; Well I think that encoding the resulting\n&gt; &gt; &gt; &gt; &gt; weights back to the=\r\n \n&gt; &gt; &gt; &gt; &gt; &gt; genome \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; would somehow hurt the population wei=\r\nght\n&gt; &gt; &gt; &gt; &gt; diversity, since most \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; individuals in a speci=\r\nes represented by a\n&gt; &gt; &gt; &gt; &gt; given topology can \n&gt; &gt; &gt; &gt; &gt; end \n&gt; &gt; &gt; &gt; &gt; =\r\n&gt; up \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; in the same local minima, thus leaving out a\n&gt; &gt; &gt; &gt; =\r\n&gt; species with the \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; nearly \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; same individuals,=\r\n i.e. clones. \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; This is why I think that backprop should be\n=\r\n&gt; &gt; &gt; &gt; &gt; applied occasionaly \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; after \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; long pe=\r\nriods of stagnation, for example the\n&gt; &gt; &gt; &gt; &gt; cases where delta-\n&gt; &gt; &gt; &gt; &gt;=\r\n &gt; &gt; coding \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; kicks in, when it focuses the search in the\n&gt; =\r\n&gt; &gt; &gt; &gt; most promising \n&gt; &gt; &gt; &gt; &gt; areas \n&gt; &gt; &gt; &gt; &gt; &gt; of \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; th=\r\ne search space. \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; I am still trying to re-implement RTRL mys=\r\nelf,\n&gt; &gt; &gt; &gt; &gt; though.. Then \n&gt; &gt; &gt; &gt; &gt; I&#39;ll \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; see \n&gt; &gt; &gt; &gt; &gt; =\r\n&gt; &gt; &gt; if it is going to actually enhance\n&gt; &gt; &gt; &gt; &gt; performance. \n&gt; &gt; &gt; &gt; &gt; =\r\n&gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; Peter\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; --- In neat@y=\r\nahoogroups.com, &quot;Kenneth Stanley&quot;\n&gt; &gt; &gt; &gt; &gt; &lt;kstanley@&gt; \n&gt; &gt; &gt; &gt; &gt; wrote:\n&gt;=\r\n &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; Rafael, thank you for pointing out the\n&gt;=\r\n &gt; &gt; &gt; &gt; connection to memetic \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; algorithms.  That is good=\r\n to point out that\n&gt; &gt; &gt; &gt; &gt; such a \n&gt; &gt; &gt; &gt; &gt; combination \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;=\r\n falls \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; under that category.\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt;=\r\n &gt; &gt; &gt; &gt; However, there are still those who would\n&gt; &gt; &gt; &gt; &gt; argue that the =\r\nlocal \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; search \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; method should not be encod=\r\ned back into the\n&gt; &gt; &gt; &gt; &gt; genome, that is, \n&gt; &gt; &gt; &gt; &gt; &gt; that \n&gt; &gt; &gt; &gt; &gt; &gt; =\r\n&gt; &gt; &gt; evolution should simply search for the best\n&gt; &gt; &gt; &gt; &gt; starting point =\r\n\n&gt; &gt; &gt; &gt; &gt; from \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; which \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; a local search wo=\r\nuld depart.  Because of the\n&gt; &gt; &gt; &gt; &gt; Baldwin Effect, \n&gt; &gt; &gt; &gt; &gt; &gt; that \n&gt; =\r\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; may \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; even work better.\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; \n&gt;=\r\n &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; Personally, I do not know which approach\n&gt; &gt; &gt; &gt; &gt; would w=\r\nork better \n&gt; &gt; &gt; &gt; &gt; but \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; both \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; are viable=\r\n and it is probably domain\n&gt; &gt; &gt; &gt; &gt; dependent.\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; =\r\n&gt; &gt; &gt; &gt; &gt; ken\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; --- In neat@yahoogroups.=\r\ncom, &quot;Rafael C.P.&quot;\n&gt; &gt; &gt; &gt; &gt; &lt;kurama.youko.br@&gt; \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; wrote:\n&gt;=\r\n &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; Ken, it doesn&#39;t fit pure evolution b=\r\nut it\n&gt; &gt; &gt; &gt; &gt; fits memetic \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; algorithms, \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; =\r\n&gt; that\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; consists exactly of evolution alternated\n&gt; &gt; &gt; &gt;=\r\n &gt; with local search \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; methods \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; for fine\n&gt;=\r\n &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; tunning (just few steps). NEAT+BP may\n&gt; &gt; &gt; &gt; &gt; become a=\r\n good memetic \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; algorithm for\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; neural n=\r\networks.\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; On Mon, Mar 10, 2008 at 2=\r\n:19 PM, Kenneth\n&gt; &gt; &gt; &gt; &gt; Stanley &lt;kstanley@&gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; wrote:\n&gt; =\r\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;   Peter, I believe that backprop c=\r\nan\n&gt; &gt; &gt; &gt; &gt; potentially improve \n&gt; &gt; &gt; &gt; &gt; the\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; accur=\r\nacy. It has been shown to work\n&gt; &gt; &gt; &gt; &gt; effectively with \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; =\r\nneurevolution\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; in classification tasks in the past. So=\r\n\n&gt; &gt; &gt; &gt; &gt; in principle it \n&gt; &gt; &gt; &gt; &gt; &gt; could\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; help. O=\r\nf course, there is always the\n&gt; &gt; &gt; &gt; &gt; chance that it will \n&gt; &gt; &gt; &gt; &gt; not\n=\r\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; enhance performance as well.\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; =\r\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; One issue I would also consider is that\n&gt; &gt; &gt; &gt; &gt; some =\r\npeople \n&gt; &gt; &gt; &gt; &gt; &gt; disagree \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; on\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; whethe=\r\nr the changes to weights from\n&gt; &gt; &gt; &gt; &gt; backprop should be \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; e=\r\nncoded \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; back\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; into the genome or not=\r\n. If it is\n&gt; &gt; &gt; &gt; &gt; actually encoded back \n&gt; &gt; &gt; &gt; &gt; into \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; t=\r\nhe\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; genome, that is &quot;Lamarckian&quot; evolution\n&gt; &gt; &gt; &gt; &gt; b=\r\necause in effect \n&gt; &gt; &gt; &gt; &gt; &gt; what \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; the\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;=\r\n &gt; organism learned over its lifetime is\n&gt; &gt; &gt; &gt; &gt; encoded into its own\n&gt; &gt;=\r\n &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; =3D=3D=3D message truncated =3D=3D=3D\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; \n&gt; &gt;=\r\n &gt; &gt; \n&gt; &gt; &gt; &gt;       \n&gt; &gt; &gt; \n&gt; &gt; \n__________________________________________=\r\n____________________________\n&gt; &gt; &gt; ______________\n&gt; &gt; &gt; &gt; Never miss a thin=\r\ng.  Make Yahoo your home page. \n&gt; &gt; &gt; &gt; http://www.yahoo.com/r/hs\n&gt; &gt; &gt; &gt;\n&gt;=\r\n &gt; &gt;\n&gt; &gt;\n&gt;\n\n\n\n"}}