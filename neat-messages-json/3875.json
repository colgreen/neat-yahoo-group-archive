{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":281645563,"authorName":"afcarl2","from":"&quot;afcarl2&quot; &lt;a.carl@...&gt;","profile":"afcarl2","replyTo":"LIST","senderId":"I6nQA-wQ35CgwfSecI8kN_423LKZc0dNHoVJp_8C-ftwWrcMlERppgQsOW-QVXDRhjCkKWuN4nqKN8xcc5YheMU","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: Backpropagation and NEAT","postDate":"1205737186","msgId":3875,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGZybDR0MitucWhnQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGZyazVqZCtkazdiQGVHcm91cHMuY29tPg=="},"prevInTopic":3874,"nextInTopic":3876,"prevInTime":3874,"nextInTime":3876,"topicId":3846,"numMessagesInTopic":41,"msgSnippet":"Ken, As I have stated repeatedly, NEAT is a substantial improvement over conventional GA. Furthermore, the underlying concepts provide a robust foundation upon","rawEmail":"Return-Path: &lt;a.carl@...&gt;\r\nX-Sender: a.carl@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 33018 invoked from network); 17 Mar 2008 06:59:50 -0000\r\nX-Received: from unknown (66.218.67.94)\n  by m42.grp.scd.yahoo.com with QMQP; 17 Mar 2008 06:59:50 -0000\r\nX-Received: from unknown (HELO n3.bullet.mail.re1.yahoo.com) (69.147.103.130)\n  by mta15.grp.scd.yahoo.com with SMTP; 17 Mar 2008 06:59:49 -0000\r\nX-Received: from [68.142.237.90] by n3.bullet.mail.re1.yahoo.com with NNFMP; 17 Mar 2008 06:59:49 -0000\r\nX-Received: from [66.218.69.2] by t6.bullet.re3.yahoo.com with NNFMP; 17 Mar 2008 06:59:48 -0000\r\nX-Received: from [66.218.67.195] by t2.bullet.scd.yahoo.com with NNFMP; 17 Mar 2008 06:59:48 -0000\r\nDate: Mon, 17 Mar 2008 06:59:46 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;frl4t2+nqhg@...&gt;\r\nIn-Reply-To: &lt;frk5jd+dk7b@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: &quot;afcarl2&quot; &lt;a.carl@...&gt;\r\nSubject: Re: Backpropagation and NEAT\r\nX-Yahoo-Group-Post: member; u=281645563; y=6PgHS9j0hi7SHJ6nZz-1bmlw87VcvisP5qAHfD-s8-IHAA\r\nX-Yahoo-Profile: afcarl2\r\n\r\nKen,\n\nAs I have stated repeatedly, NEAT is a substantial improvement over \n=\r\nconventional GA. Furthermore, the underlying concepts provide a \nrobust fou=\r\nndation upon which to build. But as a monolithic approach, \nit fundamentall=\r\ny suffers much of the same ills as conventional EA \napproaches.\n\nAlso as pr=\r\neviously stated, NEAT could be considered metaphorically a \nwheel. A wheel =\r\nis a wonderful thing. But they are so much more useful \nwhen you add all of=\r\n the other nuts and bolts required to make up a \nvehicle to get you from po=\r\nint &quot;A&quot; to point &quot;B&quot; in a reliable and \nefficient manner.\n\nIn like fashion,=\r\n the underlying concepts of NEAT are so much more \nuseful when utilized as =\r\na foundation upon which other required \nelements of functionality are added=\r\n in order to provide a reliable \nand efficient tool to deal with the comple=\r\nxities of real world \nproblems comprised of multiple objective functions, n=\r\non-linear \nfeasibility constraints and computation of pareto front solution=\r\n sets.\n\nThe issue is not one of &quot;breakthrough&quot;. It is a matter doing what \n=\r\nneeds to be done to provide a reliable and efficient tool to \nprovide &quot;good=\r\n-enough&quot; solutions to the types of problems needing to \nbe solved.\n\nThe sim=\r\nple fact that it is possible to undermine any approach by an \nimproper appl=\r\nication, is hardly a rational explanation for its \npossible shortcomings an=\r\nd why it should not be used.\n\nThe issues you raise, though valid, appear di=\r\nsingenuous for two \nreasons. First, if you had reviewed the contents and ca=\r\npabilities of \nthe Dakota toolkit, you would have discovered the issues as =\r\nbeing \nessentially addressed. Second, not intending any disrespect, from \ne=\r\nxternal appearances, your efforts appear directed in other \ndirections than=\r\n that of addressing your self admitted areas of \nconcern.\n\nFrom a review of=\r\n literature on the subject, there is a common \nunderstanding that EA is com=\r\nputationally expensive and slow to \nconverge, but robust for global search =\r\nin problem domains with \nmultiple local minima.\n\nAnd that is from people wh=\r\no are working &quot;hard&quot; problems (i.e. multi-\ndiscipline, multiobjective, non-=\r\nlinear inequality constraints, etc.), \nat government laboratories and Fortu=\r\nne 100 defense firms. Not dancing \nrag-dolls and computer-aided art.\n\nHow m=\r\nany times has NEAT, as a monolithic approach, been cited and \nsuccessfully =\r\napplied by government laboratories and Fortune 100 \ndefense firms for these=\r\n type of &quot;hard&quot; problems?\n\nI believe that the NEAT approach, with the addit=\r\nion of appropriate \nother computational elements, is very useful and applic=\r\nable to the \nsolution these types of &quot;hard&quot; problems.\n\nAs also previously s=\r\ntated, we tend to shape tools that are applicable \nto the types of problems=\r\n we deal with. Weak tools for weak problems, \nand strong tools for hard pro=\r\nblems. \n\nThere is a fundamental difference between choosing your problem fo=\r\nr \nyour tool of choice, and being given a problem and being told to \nsolve =\r\nit.\n\nI could further address your issues point specific, but it appears to =\r\n\nbe of no utility since it appears that you have not done your \nhomework re=\r\ngarding review of the larger landscape of \noptimization/search associated w=\r\nith engineering design problems.\n\n\n--- In neat@yahoogroups.com, &quot;Kenneth St=\r\nanley&quot; &lt;kstanley@...&gt; wrote:\n&gt;\n&gt; Andy, all I&#39;m saying is that local gradien=\r\nt-based optimization is \nnot\n&gt; a magic bullet.    It&#39;s certainly useful in =\r\nsome problems, but in\n&gt; others it won&#39;t be a big help.  In any case, the bi=\r\ng-picture point \nI&#39;m\n&gt; making is that if you really want to focus on the mo=\r\nst difficult\n&gt; machine learning problems, then local search is not much of =\r\na\n&gt; breakthrough.  We&#39;re going to need a lot more than that.\n&gt; \n&gt; The probl=\r\nem is that it can be a liability just as much as it can be \nan\n&gt; advantage.=\r\n  Just as it can allow you to precisely tune very \nlocalized\n&gt; values where=\r\n a GA may fail, it can also cause you to prematurely\n&gt; converge.  In combin=\r\nation with a GA, it will sometimes work as you \nare\n&gt; hoping, wherein the G=\r\nA does the global work and the gradient search\n&gt; nicely optimizes each loca=\r\nl area; however, it can just as easily \nwork\n&gt; against that desired dynamic=\r\n: It can cause the population to be \npulled\n&gt; into local optima too early a=\r\nnd thereby thwart the GA&#39;s otherwise\n&gt; useful ability to spread out over th=\r\ne global space.  \n&gt; \n&gt; Basically, any blanket statement one might make woul=\r\nd be an\n&gt; overgeneralization because the structure of individual problems a=\r\nre \nso\n&gt; complex and varied.  \n&gt; \n&gt; In fact, the real operational issue tha=\r\nt you are getting at is just\n&gt; the step size of the search operator, i.e. t=\r\nhe power of mutation and\n&gt; the size of the learning rate constant in backpr=\r\nopagation.  You \ncould\n&gt; even create a topsy-turvy hybrid in which backprop=\r\n has a much higher\n&gt; learning rate and the GA a much smaller mutation power=\r\n!  In that \ncase,\n&gt; the &quot;local&quot; search would actually be more global and th=\r\ne GA would be\n&gt; optimizing locally!  \n&gt; \n&gt; You could even have multilevel G=\r\nAs with different mutation strengths\n&gt; at different granularities.  If you =\r\ncreated a GA with a very tiny\n&gt; mutation power, it will start to look a lot=\r\n like the &quot;local search&quot;\n&gt; that you are talking about, even though it is st=\r\nill a GA.\n&gt; \n&gt; There are plenty of algorithms that do mix among local and g=\r\nlobal\n&gt; search in various ways.  There are GAs that change the mutation \npo=\r\nwer\n&gt; over the course of the run according to stagnation.   There are\n&gt; var=\r\niants of backprop that change the learning rate (as in simulated\n&gt; annealin=\r\ng) to let the solution simmer down into a local area.\n&gt; \n&gt; Yet none of thes=\r\ne are guaranteed to work, and all can be thwarted\n&gt; under the right (i.e. w=\r\nrong) conditions.  That&#39;s No Free Lunch.  And\n&gt; so it&#39;s a little too simple=\r\n to just say our solution is a hybrid.  A\n&gt; hybrid is useful in some cases,=\r\n wrong in others.  \n&gt; \n&gt; If you have experience where a hybrid worked bette=\r\nr than a GA alone,\n&gt; that is problem-specific.  There are counterexamples f=\r\nor every \nexample.  \n&gt; \n&gt; But the really important issue is not whether or =\r\nnot we need\n&gt; global/local search.  That can work fine when applied appropr=\r\niately,\n&gt; but the big issue is that we need *new* ideas if we are really go=\r\ning\n&gt; to tackle problems that are currently beyond our capabilities.  \nWe&#39;r=\r\ne\n&gt; talking about things like information reuse in representation, high\n&gt; d=\r\nimensionality, high complexity, automatically learning \nregularities,\n&gt; and=\r\n restructuring the search space to make it less deceptive.  These\n&gt; are iss=\r\nues that are not addressed by gradient-based search.\n&gt; \n&gt; One thing however=\r\n is clear to me:  The hardest problems are going to\n&gt; require the ability t=\r\no change the dimensionality of the \nrepresentation\n&gt; on the fly.  Of that m=\r\nuch I am confident.\n&gt; \n&gt; ken\n&gt; \n&gt; \n&gt; \n&gt; \n&gt; --- In neat@yahoogroups.com, &quot;af=\r\ncarl2&quot; &lt;a.carl@&gt; wrote:\n&gt; &gt;\n&gt; &gt; Perhaps the following two quotes from the D=\r\nakota Users Manual ( \n&gt; &gt; http://www.cs.sandia.gov/DAKOTA/software.html ) m=\r\nay be helpful to \n&gt; &gt; bring focus on the salient issues:\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; 1) &quot;=\r\nMultilevel Hybrid Optimization: This strategy allows the user \nto \n&gt; &gt; spec=\r\nify a sequence of optimization methods, with the results from \none \n&gt; &gt; met=\r\nhod providing the starting point for the next method in the \n&gt; &gt; sequence. =\r\nAn example which is useful in many engineering design \n&gt; &gt; problems involve=\r\ns the use of a nongradient-based global \noptimization\n&gt; &gt; method (e.g., gen=\r\netic algorithm) to identify a promising region \nof \n&gt; &gt; the parameter space=\r\n, which feeds its results into a gradient-\nbased \n&gt; &gt; method (quasi-Newton,=\r\n SQP, etc.) to perform an efficient local \nsearch \n&gt; &gt; for the optimum poin=\r\nt.&quot;, Sec. 3.8 Optimization Strategies, p. 59.\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; 2) &quot;Rather, EAs=\r\n are better suited to optimization problems where \n&gt; &gt; conventional gradien=\r\nt-based optimization fails, such as \nsituations \n&gt; &gt; where there are multip=\r\nle local optima and/or gradients are not \n&gt; &gt; available. In such cases, the=\r\n computational expense of an EA is \n&gt; &gt; warranted since other optimization =\r\nmethods are not applicable or \n&gt; &gt; impractical. In many optimization proble=\r\nms, EAs often quickly \n&gt; &gt; identify promising regions of the design space w=\r\nhere the global \n&gt; &gt; minimum may be located. However, an EA can be slow to =\r\nconverge to\n&gt; &gt; the optimum. For this reason, it can be an effective approa=\r\nch to \n&gt; &gt; combine the global search capabilities of a EA with the efficien=\r\nt \n&gt; &gt; local search of a gradient-based algorithm in a multilevel hybrid \n&gt;=\r\n &gt; optimization strategy. In this approach, the optimization starts \nby \n&gt; =\r\n&gt; using a few iterations of a EA to provide the initial search for \na \n&gt; &gt; =\r\ngood region of the parameter space (low objective function and/or \n&gt; &gt; feas=\r\nible constraints), and then it switches to a gradient-based\n&gt; &gt; algorithm (=\r\nusing the best design point found by the EA as its \n&gt; &gt; starting point) to =\r\nperform an efficient local search for an \noptimum \n&gt; &gt; design point.&quot;, Sec.=\r\n 2.4.7 Nongradient-based Optimization via \n&gt; &gt; Evolutionary Algorithm, p. 4=\r\n3.\n&gt; &gt; \n&gt; &gt; Most real world problems are multiobjective in nature, with \nmu=\r\nltiple \n&gt; &gt; non-linear inequality constraints. The determination of the \npa=\r\nreto \n&gt; &gt; front only magnifies the referenced issues.\n&gt; &gt; \n&gt; &gt; And no, it i=\r\ns not an exageration, it is based upon experience. \n&gt; &gt; Consider the comput=\r\national sequence of events of the two \nalternatives \n&gt; &gt; as measured in tot=\r\nal overall fitness evaluations required to \nachieve \n&gt; &gt; convergence to a l=\r\nocal minima. The robust nature of GA to find a \n&gt; &gt; global minima also work=\r\ns against quick convergence. NEAT is a \n&gt; &gt; significant inpromement over co=\r\nnventional GA, but the improvement \nis \n&gt; &gt; still inadequate to overcome th=\r\ne computational cost in problem \n&gt; &gt; domains in which the fitness evaluatio=\r\nn is non-trivial.\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; --- In neat@yahoogroups.com, &quot;Kenneth Stanl=\r\ney&quot; &lt;kstanley@&gt; wrote:\n&gt; &gt; &gt;\n&gt; &gt; &gt; Andy, I may be misunderstanding some of =\r\nyour points about local \n&gt; &gt; search.\n&gt; &gt; &gt; \n&gt; &gt; &gt; First, the term &quot;local se=\r\narch&quot; itself implies that we are \ntalking\n&gt; &gt; &gt; about a method that has bee=\r\nn developed exclusively to search in \none\n&gt; &gt; &gt; local part of the search sp=\r\nace.  But what method is really \nintended\n&gt; &gt; &gt; for that purpose and why is=\r\n that a good thing?  The problem in\n&gt; &gt; &gt; difficult tasks in general is usu=\r\nally that whatever algorithm \nyou \n&gt; &gt; are\n&gt; &gt; &gt; using gets *trapped* in a =\r\nlocal area.  Therefore, generally \n&gt; &gt; speaking,\n&gt; &gt; &gt; an algorithm that is=\r\n designed to stay in one area is probably a \nbad\n&gt; &gt; &gt; thing, not a good th=\r\ning.  Only the most trivial problems entail \n&gt; &gt; simply\n&gt; &gt; &gt; running up th=\r\ne first hillside that you see.  For that, all we \nwould\n&gt; &gt; &gt; need is hill =\r\nclimbing.  \n&gt; &gt; &gt; \n&gt; &gt; &gt; In fact, one of the liabilities of backprop is tha=\r\nt it tends to \nbe\n&gt; &gt; &gt; caught on local optima.  That is a serious problem =\r\nand by no \nmeans a\n&gt; &gt; &gt; reason to recommend it.  \n&gt; &gt; &gt; \n&gt; &gt; &gt; You say, &quot;T=\r\nhere are many times in which it is orders of \nmagnitude\n&gt; &gt; &gt; quicker for a=\r\n non-GA local gradient search method to find a \nlocal\n&gt; &gt; &gt; minima, than an=\r\n equivalent GA to mutate through generations to \nfind\n&gt; &gt; &gt; the same local =\r\nminima.&quot;\n&gt; &gt; &gt; \n&gt; &gt; &gt; Generally speaking finding a local minimum is easy fo=\r\nr any \n&gt; &gt; algorithm.\n&gt; &gt; &gt;    A gradient technique is likely faster than a=\r\n GA, but &quot;orders \nof\n&gt; &gt; &gt; magnitude?&quot;  In general, that sounds exagerated.=\r\n  In fact, in \nthe \n&gt; &gt; 90s\n&gt; &gt; &gt; before NEAT existed there were a number o=\r\nf papers written that\n&gt; &gt; &gt; compared neuroevolution to backprop and conclud=\r\ned that \n&gt; &gt; neuroevolution\n&gt; &gt; &gt; was faster.  I do not think these results=\r\n are meaningful \nbecause the\n&gt; &gt; &gt; issue is highly domain-dependent, but it=\r\n does show that neither\n&gt; &gt; &gt; approach has a massive advantage over the oth=\r\ner in general.  In \n&gt; &gt; fact,\n&gt; &gt; &gt; claiming otherwise just winds up runnin=\r\ng into No Free Lunch.\n&gt; &gt; &gt; \n&gt; &gt; &gt; Yet in most problems what we really care=\r\n about finding are not \njust\n&gt; &gt; &gt; any local optima but *particular* local =\r\noptima (or the global \n&gt; &gt; optimum)\n&gt; &gt; &gt; that satisfy a &quot;good enough&quot; crit=\r\nerion.  Thus we need to be \nable to\n&gt; &gt; &gt; leave the local neighborhood enti=\r\nrely and go on a real search.  \nFor\n&gt; &gt; &gt; that kind of search, you need an =\r\nalgorithm that is designed not \nto\n&gt; &gt; &gt; simply rush up the nearest hill.  =\r\n\n&gt; &gt; &gt;  \n&gt; &gt; &gt; You also say, &quot;But a local search method could just as easil=\r\ny \nwander\n&gt; &gt; &gt; free in the local fitness landscape, un-encumbered by organ=\r\nism\n&gt; &gt; &gt; topology issues, to find the local minima.&quot;\n&gt; &gt; &gt; \n&gt; &gt; &gt; I do not=\r\n understand what you mean by &quot;un-encumbered.&quot;  All \nsearch is\n&gt; &gt; &gt; within =\r\nthe confines of a certain topology, even if that \ntopology is\n&gt; &gt; &gt; set by =\r\nthe user.  The topology is what defines the search \nspace.  \n&gt; &gt; You\n&gt; &gt; &gt; =\r\ncannot isolate search from the space being searched.  \nFurthermore, \n&gt; &gt; if=\r\n\n&gt; &gt; &gt; there is any encumbrance, it is the inability to change the \ntopolog=\r\ny\n&gt; &gt; &gt; when it is found wanting.  Thus searching through different \n&gt; &gt; to=\r\npologies\n&gt; &gt; &gt; is the opposite of an encumbrance; it is a liberation.  \n&gt; &gt;=\r\n &gt; \n&gt; &gt; &gt; Along the same lines, this statement also seems \nmisleading: &quot;The=\r\n \n&gt; &gt; moral\n&gt; &gt; &gt; of the story is that local search does not have to be \nco=\r\nnstrained by\n&gt; &gt; &gt; organism topology, beyond that of providing the start po=\r\nint.&quot;\n&gt; &gt; &gt; \n&gt; &gt; &gt; Again, local search is always constrained by topology.  =\r\nTopology\n&gt; &gt; &gt; defines the dimensions of the search. \n&gt; &gt; &gt; \n&gt; &gt; &gt; What you=\r\n may not be considering is that changing the topology is\n&gt; &gt; &gt; changing the=\r\n search space itself.  It is an entirely different \ntype \n&gt; &gt; of\n&gt; &gt; &gt; oper=\r\nation than moving *within* a particular search space.  \nAdding a\n&gt; &gt; &gt; new =\r\nconnection adds a new dimension to space.  That is not the \nsame \n&gt; &gt; as\n&gt; =\r\n&gt; &gt; moving in any particular direction along any particular \ndimension,\n&gt; &gt;=\r\n &gt; which is what you mean by &quot;local search.&quot;\n&gt; &gt; &gt; \n&gt; &gt; &gt; In a broader pict=\r\nure, gradient search is not the panacea that \nwe \n&gt; &gt; need.\n&gt; &gt; &gt; It is exa=\r\nctly the failures of such search that are the reason \nthat\n&gt; &gt; &gt; we are not=\r\n able to handle these very difficult real world \nproblems\n&gt; &gt; &gt; that you me=\r\nntion.  The big challenge is that the gradient in a \n&gt; &gt; massive\n&gt; &gt; &gt; mult=\r\nidimensional space on a complex problem is almost certainly \n&gt; &gt; highly\n&gt; &gt;=\r\n &gt; deceptive, and gradient techniques have nothing going for them \nother\n&gt; =\r\n&gt; &gt; than the gradient!  If the gradient is all we have to go on, \nhow can\n&gt;=\r\n &gt; &gt; that be a good thing?  \n&gt; &gt; &gt; \n&gt; &gt; &gt; It will certainly be necessary to=\r\n change topologies as a part \nof any\n&gt; &gt; &gt; effective formula for success on=\r\n extremely hard problems, among \n&gt; &gt; other\n&gt; &gt; &gt; ingredients that are are s=\r\ntill being discovered and invented \ntoday.\n&gt; &gt; &gt; \n&gt; &gt; &gt; ken\n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt;=\r\n &gt; &gt; --- In neat@yahoogroups.com, &quot;afcarl2&quot; &lt;a.carl@&gt; wrote:\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; =\r\n&gt; Peter,\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; It appears that you are implicitly assuming that =\r\nthat the \nonly \n&gt; &gt; way \n&gt; &gt; &gt; &gt; to &quot;reach out and touch&quot; the fitness evalu=\r\nation is via a NEAT \n&gt; &gt; &gt; &gt; fabricated organism topology. Local search doe=\r\ns not have to \n&gt; &gt; proceed \n&gt; &gt; &gt; &gt; via organism topology. \n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; =\r\n&gt; Usage of NEAT as a global search method, as part of a \n&gt; &gt; hierarchical \n=\r\n&gt; &gt; &gt; &gt; methodology, surely uses organism topology as the local \nsearch \n&gt; =\r\n&gt; start \n&gt; &gt; &gt; &gt; point. And in the case of your proposed backprop search on=\r\n \n&gt; &gt; organism \n&gt; &gt; &gt; &gt; weight values, maintains the same number and compos=\r\nition of \nnodes \n&gt; &gt; and \n&gt; &gt; &gt; &gt; associated connectivity. But a local sear=\r\nch method could just \nas \n&gt; &gt; &gt; &gt; easily wander free in the local fitness l=\r\nandscape, un-\nencumbered \n&gt; &gt; by \n&gt; &gt; &gt; &gt; organism topology issues, to find=\r\n the local minima.\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; The very strong point of GA can also be=\r\n its greatest \nweakness, in \n&gt; &gt; &gt; &gt; instances in which the computational r=\r\nesource requirements of \nthe \n&gt; &gt; &gt; &gt; fitness evaluation, in light of the d=\r\nimensionality and hyper \n&gt; &gt; volume \n&gt; &gt; &gt; &gt; size and complexity, become no=\r\nn-trivial.\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; This is especially true in the case of non-adap=\r\ntive mutation \n&gt; &gt; &gt; &gt; parameters, as is currently the case with NEAT. Ther=\r\ne are \nmany \n&gt; &gt; times \n&gt; &gt; &gt; &gt; in which it is orders of magnitude quicker =\r\nfor a non-GA local \n&gt; &gt; &gt; &gt; gradient search method to find a local minima, =\r\nthan an \nequivalent \n&gt; &gt; GA \n&gt; &gt; &gt; &gt; to mutate through generations to find =\r\nthe same local minima. \n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; NEAT speciation and niche protecti=\r\non help to mitigate the \nproblem \n&gt; &gt; via \n&gt; &gt; &gt; &gt; population size and main=\r\ntaining multiple species, but at a \n&gt; &gt; &gt; &gt; computational cost. But a hiera=\r\nrchical search methodology can \n&gt; &gt; apply \n&gt; &gt; &gt; &gt; the strengths of both GA=\r\n and local search, without having to \nuse \n&gt; &gt; &gt; &gt; computational power to c=\r\nover-up GA&#39;s weak points.\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; The remaining question as to whe=\r\nther to a) re-encode the \nfinal \n&gt; &gt; &gt; &gt; destination of the local search ba=\r\nck into the organism&#39;s \n&gt; &gt; topology, or \n&gt; &gt; &gt; &gt; b) simply take the final/=\r\nbest fitness derived by the local \nsearch \n&gt; &gt; &gt; &gt; (using the organism&#39;s or=\r\niginal topology as the start point), \nand \n&gt; &gt; &gt; &gt; associate it with the or=\r\nganism and it&#39;s original topology, is \nup \n&gt; &gt; for \n&gt; &gt; &gt; &gt; debate and/or p=\r\nersonal preference.\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; The moral of the story is that local s=\r\nearch does not have to \nbe \n&gt; &gt; &gt; &gt; constrained by organism topology, beyon=\r\nd that of providing \nthe \n&gt; &gt; start \n&gt; &gt; &gt; &gt; point. \n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; Real =\r\nworld problems are so complex in light of current \ncomputer \n&gt; &gt; &gt; &gt; speeds=\r\n and fitness computation requirements, to render GA \nalone \n&gt; &gt; to be \n&gt; &gt; =\r\n&gt; &gt; computationally unpractical in many instances, even with the \n&gt; &gt; obvio=\r\nus \n&gt; &gt; &gt; &gt; benefits that NEAT brings to the table.\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt;=\r\n &gt; \n&gt; &gt; &gt; &gt; --- In neat@yahoogroups.com, &quot;petar_chervenski&quot; \n&gt; &gt; &gt; &gt; &lt;petar=\r\n_chervenski@&gt; wrote:\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; Well actually speciation takes car=\r\ne of this. Species are \n&gt; &gt; allowed to \n&gt; &gt; &gt; &gt; &gt; exist until they stagnate=\r\n for too long time. If some new \n&gt; &gt; structure \n&gt; &gt; &gt; &gt; &gt; appears through m=\r\nutations, the mutated individuals are \n&gt; &gt; separated in \n&gt; &gt; &gt; &gt; &gt; another =\r\nspecies. Each species is a local protected \ncompetition \n&gt; &gt; &gt; &gt; among \n&gt; &gt;=\r\n &gt; &gt; &gt; individuals grouped by similarity. Consider it as a GA \n&gt; &gt; performe=\r\nd on \n&gt; &gt; &gt; &gt; &gt; near identical topologies. Then you can see NEAT as a \nalgo=\r\nrithm \n&gt; &gt; &gt; &gt; &gt; running multiple GAs. So this is actually what you mean by=\r\n \n&gt; &gt; dynamic \n&gt; &gt; &gt; &gt; &gt; programming.. or something. In fact this scheme is=\r\n far \nbetter \n&gt; &gt; than \n&gt; &gt; &gt; &gt; &gt; it. \n&gt; &gt; &gt; &gt; &gt; As for the idea of specula=\r\ntive structure, this is the core \nof \n&gt; &gt; NEAT \n&gt; &gt; &gt; &gt; &gt; and it is actuall=\r\ny Ken&#39;s idea :) \n&gt; &gt; &gt; &gt; &gt; Colin Green&#39;s idea is about phased searching, as=\r\n far as I \nknow. \n&gt; &gt; It \n&gt; &gt; &gt; &gt; is \n&gt; &gt; &gt; &gt; &gt; that after some structure i=\r\ns added through complexifying, a \n&gt; &gt; &gt; &gt; &gt; simplifying phase kicks in, rem=\r\noving any unnecessary \nstructure, \n&gt; &gt; &gt; &gt; thus \n&gt; &gt; &gt; &gt; &gt; returning the se=\r\narch down to a baseline low dimentional \nspace \n&gt; &gt; while \n&gt; &gt; &gt; &gt; &gt; retain=\r\ning the fitness (because of elitism). \n&gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; Peter\n&gt; &gt; &gt; &gt; &gt;=\r\n \n&gt; &gt; &gt; &gt; &gt; --- In neat@yahoogroups.com, c f &lt;christofer_fransson@&gt; \nwrote:=\r\n\n&gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; In dynamic programming the idea is to divide the\n&gt;=\r\n &gt; &gt; &gt; &gt; &gt; solution in steps and then for each step present a\n&gt; &gt; &gt; &gt; &gt; &gt; f=\r\nixed number of possible solutions.\n&gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; Collin Greens i=\r\ndea is that speculative nodes are added\n&gt; &gt; &gt; &gt; &gt; &gt; to the solutions but it=\r\n might take time/generations\n&gt; &gt; &gt; &gt; &gt; &gt; before an added node are shown to =\r\nbe useful. \n&gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; Is it possible to apply dynamic progra=\r\nmming approach\n&gt; &gt; &gt; &gt; &gt; &gt; to this area, to evolve NEAT driven networks?\n&gt; =\r\n&gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; To combine local optimization and dynamic programmin=\r\ng\n&gt; &gt; &gt; &gt; &gt; &gt; ideas?\n&gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; Br,\n&gt; &gt; &gt; &gt; &gt; &gt; Christofer\n&gt; =\r\n&gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; =\r\n\n&gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt;=\r\n &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; --- petar_chervenski &lt;petar_chervenski@&gt;\n&gt; &gt; &gt; &gt; &gt; &gt; wrote:=\r\n\n&gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; Given the simplest topology (a perceptron\n&gt; &gt; &gt;=\r\n &gt; &gt; &gt; &gt; structure), the local \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; minima is just one. Perceptro=\r\nns are always\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; guaranteed to converge on \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; correc=\r\nt weights. But increasing the dimentionality\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; of the solution =\r\n\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; increases the error surface&#39;s curvature as well. So\n&gt; &gt; &gt; &gt; =\r\n&gt; &gt; &gt; more dimentions \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; means more complex error surface. The =\r\ncoolest thing\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; in NEAT is that \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; when it increase=\r\ns the dimentionality of the\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; solution, the individuals \n&gt; &gt; &gt; =\r\n&gt; &gt; &gt; &gt; are already located in a promising area of the new\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; sp=\r\nace. In fact \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; those spaces are related to each other - you do=\r\nn&#39;t\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; know how the error \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; surface is going to loo=\r\nk like when you enter the new\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; space with more \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; =\r\ndimentions. There are unlimited possibilities. \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; So what local=\r\n gradient search will do in essence is\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; pushing the \n&gt; &gt; &gt; &gt; &gt;=\r\n &gt; &gt; weights towards the *local* minumim.. It is not\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; guarante=\r\ned that this \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; is the *solution*! It is simply because you don=\r\n&#39;t\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; know the solution&#39;s \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; dimentionality at first=\r\n. It may require 3 or\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; 21342532 dimentions. \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; Don=\r\n&#39;t forget that NEAT complexifies solutions\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; incrementaly. \n&gt; &gt;=\r\n &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; Peter\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; --- In neat@ya=\r\nhoogroups.com, &quot;afcarl2&quot; &lt;a.carl@&gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; wrote:\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt;=\r\n &gt; &gt; &gt; &gt; &gt; &gt; In fact, it may be that a substancial portion of\n&gt; &gt; &gt; &gt; &gt; &gt; &gt;=\r\n the value-added of \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; speciation and niche protection of inf=\r\nant\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; organisms, is associated \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; with providing =\r\nopportunity to accumulate\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; sufficient neighborhood \n&gt; &gt; &gt; &gt; &gt; =\r\n&gt; &gt; &gt; evaluations to &quot;discover&quot; the same local minimia\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; over m=\r\nultiple \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; generations, that a local search may discover in\n&gt;=\r\n &gt; &gt; &gt; &gt; &gt; &gt; one generation. \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; And \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; maintainin=\r\ng multiple species in hope that one of\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; the local minimia \n&gt; &gt;=\r\n &gt; &gt; &gt; &gt; &gt; &gt; will in fact also be the global minimia.\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; =\r\n&gt; &gt; &gt; &gt; &gt; &gt; --- In neat@yahoogroups.com, &quot;afcarl2&quot; &lt;a.carl@&gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; =\r\nwrote:\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; If &quot;most individuals in a specie=\r\ns represented by\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; a given \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; topology&quot; \n&gt; &gt; &gt; &gt; &gt; =\r\n&gt; &gt; &gt; &gt; ended up in &quot;the same local minimia&quot;, one could\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; argue=\r\n that the \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; subject specie&#39;s logical end point was the sam=\r\ne\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; local minimia, \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; and \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; that t=\r\nhe cost of maintaining more than one\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; organism was \n&gt; &gt; &gt; &gt; &gt; =\r\n&gt; &gt; &gt; &gt; computationally wasteful. Better to know sooner\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; and b=\r\nreed \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; additional \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; organisms of differing =\r\ntopology so as to\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; maintain the population \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; si=\r\nze \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; and maximize the population&#39;s &quot;effective&quot;\n&gt; &gt; &gt; &gt; &gt; &gt;=\r\n &gt; diversity.\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; Paying more for the same=\r\n answer does not make it\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; a better answer.\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; \n=\r\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; --- In neat@yahoogroups.com, &quot;petar_chervenski&quot; \n&gt; &gt; &gt; &gt; =\r\n&gt; &gt; &gt; &gt; &gt; &lt;petar_chervenski@&gt; wrote:\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; =\r\n&gt; Well I think that encoding the resulting\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; weights back to th=\r\ne \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; genome \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; would somehow hurt the popul=\r\nation weight\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; diversity, since most \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; indiv=\r\niduals in a species represented by a\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; given topology can \n&gt; &gt; =\r\n&gt; &gt; &gt; &gt; &gt; end \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; up \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; in the same local mi=\r\nnima, thus leaving out a\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; species with the \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; =\r\nnearly \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; same individuals, i.e. clones. \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;=\r\n &gt; &gt; This is why I think that backprop should be\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; applied occa=\r\nsionaly \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; after \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; long periods of stagn=\r\nation, for example the\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; cases where delta-\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; c=\r\noding \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; kicks in, when it focuses the search in the\n&gt; &gt; =\r\n&gt; &gt; &gt; &gt; &gt; most promising \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; areas \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; of \n&gt; &gt; &gt; &gt; =\r\n&gt; &gt; &gt; &gt; &gt; &gt; the search space. \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; I am still trying to re-=\r\nimplement RTRL myself,\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; though.. Then \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; I&#39;ll \n&gt; &gt;=\r\n &gt; &gt; &gt; &gt; &gt; &gt; &gt; see \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; if it is going to actually enhance\n=\r\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; performance. \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; Peter\n=\r\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; --- In neat@yahoogroups.com, &quot;Kenn=\r\neth Stanley&quot;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &lt;kstanley@&gt; \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; wrote:\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; =\r\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; Rafael, thank you for pointing out the\n&gt; &gt; &gt; =\r\n&gt; &gt; &gt; &gt; connection to memetic \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; algorithms.  That is g=\r\nood to point out that\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; such a \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; combination \n&gt; &gt; =\r\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; falls \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; under that category.\n&gt; &gt; &gt; &gt; &gt;=\r\n &gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; However, there are still those who woul=\r\nd\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; argue that the local \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; search \n&gt; &gt; &gt; &gt; &gt;=\r\n &gt; &gt; &gt; &gt; &gt; &gt; method should not be encoded back into the\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; genom=\r\ne, that is, \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; that \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; evolution should s=\r\nimply search for the best\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; starting point \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; from =\r\n\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; which \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; a local search would depa=\r\nrt.  Because of the\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; Baldwin Effect, \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; that \n&gt; =\r\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; may \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; even work better.\n&gt; &gt; &gt; &gt; &gt; &gt; =\r\n&gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; Personally, I do not know which approach\n&gt;=\r\n &gt; &gt; &gt; &gt; &gt; &gt; would work better \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; but \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; both \n=\r\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; are viable and it is probably domain\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; de=\r\npendent.\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; ken\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;=\r\n &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; --- In neat@yahoogroups.com, &quot;Rafael C.P.&quot;\n&gt; &gt; =\r\n&gt; &gt; &gt; &gt; &gt; &lt;kurama.youko.br@&gt; \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; wrote:\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; =\r\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; Ken, it doesn&#39;t fit pure evolution but it\n&gt;=\r\n &gt; &gt; &gt; &gt; &gt; &gt; fits memetic \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; algorithms, \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;=\r\n &gt; &gt; &gt; that\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; consists exactly of evolution alternate=\r\nd\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; with local search \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; methods \n&gt; &gt; &gt; &gt; &gt; &gt;=\r\n &gt; &gt; &gt; &gt; &gt; for fine\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; tunning (just few steps). NEAT+=\r\nBP may\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; become a good memetic \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; algorithm=\r\n for\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; neural networks.\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; =\r\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; On Mon, Mar 10, 2008 at 2:19 PM, Kenneth\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; =\r\nStanley &lt;kstanley@&gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; wrote:\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; =\r\n\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;   Peter, I believe that backprop can\n&gt; &gt; &gt; &gt; &gt; &gt;=\r\n &gt; potentially improve \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; the\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; accurac=\r\ny. It has been shown to work\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; effectively with \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; =\r\n&gt; &gt; &gt; neurevolution\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; in classification tasks in th=\r\ne past. So\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; in principle it \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; could\n&gt; &gt; &gt; &gt; &gt; &gt;=\r\n &gt; &gt; &gt; &gt; &gt; &gt; &gt; help. Of course, there is always the\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; chance th=\r\nat it will \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; not\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; enhance performance=\r\n as well.\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; One issue I w=\r\nould also consider is that\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; some people \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; disag=\r\nree \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; on\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; whether the changes to =\r\nweights from\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; backprop should be \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; encoded \n&gt;=\r\n &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; back\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; into the genome or not.=\r\n If it is\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; actually encoded back \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; into \n&gt; &gt; &gt; &gt; =\r\n&gt; &gt; &gt; &gt; &gt; the\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; genome, that is &quot;Lamarckian&quot; evolut=\r\nion\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; because in effect \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; what \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; =\r\n&gt; &gt; the\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; organism learned over its lifetime is\n&gt; &gt;=\r\n &gt; &gt; &gt; &gt; &gt; encoded into its own\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; =3D=3D=3D messag=\r\ne truncated =3D=3D=3D\n&gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt;   =\r\n    \n&gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; \n&gt; &gt; \n_____________________________________________=\r\n_________________________\n&gt; &gt; &gt; &gt; &gt; ______________\n&gt; &gt; &gt; &gt; &gt; &gt; Never miss a=\r\n thing.  Make Yahoo your home page. \n&gt; &gt; &gt; &gt; &gt; &gt; http://www.yahoo.com/r/hs\n=\r\n&gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt;\n&gt;\n\n\n\n"}}