{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":360532607,"authorName":"Sebastian Risi","from":"Sebastian Risi &lt;sebastian.risi@...&gt;","profile":"sebastian.risi","replyTo":"LIST","senderId":"Y9Gg5CAvLbpXmKz7cIrSBK9QtjyytbRbYC41IE51XNu1ratf4ngyPdqEb43CU4iDoiLeWn-HSR_Cfk9yogd_1S1WUQGkUBUQBWHoYysdwcs","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [neat] New paper: Automated Generation of Environments to Test the General Learning Capabilities of AI Agents","postDate":"1398878090","msgId":6286,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PENBSm42PWRyRU5yMnNoWURmYktaaTNYRWJDeDY0MHBRSDExZmFKdFhoQ1p5dnFKR3dMd0BtYWlsLmdtYWlsLmNvbT4=","inReplyToHeader":"PEY1MkE3MUQ3LURGMzUtNEEzMC1BM0ZDLTk3RjY0NjIwMzg0N0BnbWFpbC5jb20+","referencesHeader":"PENBK2R1aW1PMjRzYWtPWFNNVnVxYkVleDgremlCbVFIdmVjb1kza3dBZCt6QUI1Wmt3UUBtYWlsLmdtYWlsLmNvbT4JPENBTnRYaG12dUpHMkxkWXpSRGVXRldpU01HNW1iK3pmQkxWZ0VhQm10dHkyV0ZQaXhFd0BtYWlsLmdtYWlsLmNvbT4JPENBK2R1aW1ONCtZVTMtelN4ZnV1ek9OLVAtcnI4NVBTcHMrMDFDMW5Wa2twSkxjUmR0d0BtYWlsLmdtYWlsLmNvbT4JPENBTnRYaG10eG9oTzRSZmhVYzBCUzRhMmZXMTlKY2pEYmUtOHEwdzlNSDFXalhjbzh6UUBtYWlsLmdtYWlsLmNvbT4JPEY1MkE3MUQ3LURGMzUtNEEzMC1BM0ZDLTk3RjY0NjIwMzg0N0BnbWFpbC5jb20+"},"prevInTopic":6285,"nextInTopic":6288,"prevInTime":6285,"nextInTime":6287,"topicId":6279,"numMessagesInTopic":11,"msgSnippet":"Hi Oliver and Jeff, Very interesting paper! Good to see that more and more people are working on plastic ANNs! I also had one question about the CPPN outputs","rawEmail":"Return-Path: &lt;sebastian.risi@...&gt;\r\nX-Sender: sebastian.risi@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 36520 invoked by uid 102); 30 Apr 2014 17:14:50 -0000\r\nX-Received: from unknown (HELO mtaq5.grp.bf1.yahoo.com) (10.193.84.36)\n  by m5.grp.bf1.yahoo.com with SMTP; 30 Apr 2014 17:14:50 -0000\r\nX-Received: (qmail 32016 invoked from network); 30 Apr 2014 17:14:50 -0000\r\nX-Received: from unknown (HELO mail-qc0-f169.google.com) (209.85.216.169)\n  by mtaq5.grp.bf1.yahoo.com with SMTP; 30 Apr 2014 17:14:50 -0000\r\nX-Received: by mail-qc0-f169.google.com with SMTP id m20so2216488qcx.14\n        for &lt;neat@yahoogroups.com&gt;; Wed, 30 Apr 2014 10:14:50 -0700 (PDT)\r\nMIME-Version: 1.0\r\nX-Received: by 10.224.6.10 with SMTP id 10mr7150637qax.45.1398878090279; Wed,\n 30 Apr 2014 10:14:50 -0700 (PDT)\r\nX-Received: by 10.96.109.69 with HTTP; Wed, 30 Apr 2014 10:14:50 -0700 (PDT)\r\nIn-Reply-To: &lt;F52A71D7-DF35-4A30-A3FC-97F646203847@...&gt;\r\nReferences: &lt;CA+duimO24sakOXSMVuqbEex8+ziBmQHvecoY3kwAd+zAB5ZkwQ@...&gt;\n\t&lt;CANtXhmvuJG2LdYzRDeWFWiSMG5mb+zfBLVgEaBmtty2WFPixEw@...&gt;\n\t&lt;CA+duimN4+YU3-zSxfuuzON-P-rr85PSps+01C1nVkkpJLcRdtw@...&gt;\n\t&lt;CANtXhmtxohO4RfhUc0BS4a2fW19JcjDbe-8q0w9MH1WjXco8zQ@...&gt;\n\t&lt;F52A71D7-DF35-4A30-A3FC-97F646203847@...&gt;\r\nDate: Wed, 30 Apr 2014 19:14:50 +0200\r\nMessage-ID: &lt;CAJn6=drENr2shYDfbKZi3XEbCx640pQH11faJtXhCZyvqJGwLw@...&gt;\r\nTo: neat@yahoogroups.com\r\nContent-Type: multipart/alternative; boundary=001a11c2b994a7bf7c04f845afbd\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Sebastian Risi &lt;sebastian.risi@...&gt;\r\nSubject: Re: [neat] New paper: Automated Generation of Environments to Test\n the General Learning Capabilities of AI Agents\r\nX-Yahoo-Group-Post: member; u=360532607; y=u84s6fIG2N01UTJo58sk7r1IsFW2drYxpwKSXO-zMo_5wPGHTyL-z74\r\nX-Yahoo-Profile: sebastian.risi\r\n\r\n\r\n--001a11c2b994a7bf7c04f845afbd\r\nContent-Type: text/plain; charset=UTF-8\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\nHi Oliver and Jeff,\n\nVery interesting paper! Good to see that more and more=\r\n people are working\non plastic ANNs!\n\nI also had one question about the CPP=\r\nN outputs that determine the type of\nplasticity. In our original adaptive H=\r\nyperNEAT paper (Risi&Stanley, 2010)\nthe CPPN directly specifies the values =\r\nfor the parameters of Niv&#39;s\nequation.  Did you also try this approach or wh=\r\ny did you decide to limit\nthe number of possible rules to four? In your pap=\r\ner you\ncompare it to Ken&#39;s paper on adaptive NEAT and say that &quot;Two benefit=\r\ns of\nthis approach are that it reduces the overall search space...&quot;. But I =\r\nthink\nthere is a difference here when we are talking about a direct (NEAT) =\r\nor an\nindirect encoding (HyperNEAT).\n\nThis is the paper on Adaptive HyperNE=\r\nAT I was referring to:\nSebastian Risi and Kenneth O. Stanley (2010) Indirec=\r\ntly Encoding Neural\nPlasticity as a Pattern of Local Rules In: Proceedings =\r\nof the 11th\nInternational Conference on Simulation of Adaptive Behavior (SA=\r\nB 2010). New\nYork, NY: Springer.\nPDF: http://eplex.cs.ucf.edu/papers/risi_s=\r\nab10.pdf\n\nSebastian\n\n\n\n\nOn Wed, Apr 30, 2014 at 6:56 PM, Jeff Clune &lt;jclune=\r\n@...&gt; wrote:\n\n&gt;\n&gt;\n&gt;\n&gt; Yes, it is clear now. I guess one could also en=\r\ncode the mutation rates in\n&gt; the genotype, like in Evolution Strategies, an=\r\nd make these parameters\n&gt; self-adaptive.\n&gt;\n&gt;\n&gt;\n&gt; Self-adaptive mutation rat=\r\nes are a terrible idea! Please read this for an\n&gt; explanation:\n&gt;\n&gt; Clune J,=\r\n Misevic D, Ofria C, Lenski RE, Elena SF, and Sanju=C3=A1n R (2008)\n&gt; Natur=\r\nal selection fails to optimize mutation rates for long-term\n&gt; adaptation on=\r\n rugged fitness landscapes. PLoS Computational Biology 4(9):\n&gt; e1000187.\n&gt; =\r\npdf:\n&gt; http://jeffclune.com/publications/Clune-EvolvingMutationRates-PLoSCB=\r\n-2008.pdf\n&gt;\n&gt; I have spent years asking anyone I encounter who advocates se=\r\nlf-adaptive\n&gt; mutation rates for evidence that they work, or even an argume=\r\nnt as to why\n&gt; they could work, and all those conversations have come up em=\r\npty. Mostly I\n&gt; end up convincing them that self-adaptive mutation rates ar=\r\ne a bad idea, or\n&gt; they end up defending something other than self-adaptive=\r\n mutation rates\n&gt; (e.g. Rechenberg=E2=80=99s 1/5th rule, which is not an ex=\r\nample of a self adaptive\n&gt; mutation rate: it=E2=80=99s an externally contro=\r\nlled schedule).\n&gt;\n&gt; For some reason people believe they are a good idea, bu=\r\nt without evidence\n&gt; or intuition. I=E2=80=99m constantly surprised at how =\r\npersistent this errant belief\n&gt; is. I think it=E2=80=99s because ultimately=\r\n we want to believe that evolution is\n&gt; good at optimizing everything, and =\r\nwe don=E2=80=99t want to have to set parameters,\n&gt; so we feel like we shoul=\r\nd just turn them over to evolution. But we have\n&gt; lots of evidence of evolu=\r\ntion being short-sighted (e.g. it doesn=E2=80=99t evolve\n&gt; modularity when =\r\nit would help: http://goo.gl/2vzFv).\n&gt;\n&gt; Sorry to jump on your side comment=\r\n on this issue, but I=E2=80=99m trying to spread\n&gt; the word in the communit=\r\ny that self-adaptive mutation rates do not work.\n&gt;\n&gt;\n&gt;\n&gt; If I remember corr=\r\nectly when reading Soltoggio&#39;s paper, he used some\n&gt; constraints when evolv=\r\ning the parameters of the plasticity rule and\n&gt; specifically, A-D were in t=\r\nhe range [-1,1] and eta in the range [-100,100].\n&gt; Did you use any similar =\r\nconstraints?\n&gt;\n&gt; Just out of curiosity, what activation function did you us=\r\ne for these n+1\n&gt; outputs that correspond to the classes? Did you use a sof=\r\ntmax activation\n&gt; function to interpret the outputs as a probability distri=\r\nbution (and\n&gt; consequently selected the class probabilistically) or did you=\r\n just select\n&gt; the class based on the highest output among these neurons?\n&gt;=\r\n\n&gt;\n&gt;  The number of states is independent of the number of actions. Differe=\r\nnt\n&gt;&gt; actions in state A may all lead to state B but provide different rewa=\r\nrd\n&gt;&gt; values.\n&gt;&gt;\n&gt;\n&gt; So, how many states did you use for your simulations? =\r\nIs it 4 (like in\n&gt; Figure 1)? I might have missed that when reading the pap=\r\ner, this is why I\n&gt; asked whether the number of actions correspond to the n=\r\number of states.\n&gt;\n&gt;\n&gt;\n&gt;&gt;  3) On page 3 you say that &quot;the proportion of sta=\r\nte transitions that\n&gt;&gt;&gt; provide a reward value is 0.5&quot;. It is not clear to =\r\nme, however, what the\n&gt;&gt;&gt; reward values are. Do all transitions that have a=\r\n reward value have the\n&gt;&gt;&gt; *same* reward value (e.g. equal to 1), or does t=\r\nhis value vary?\n&gt;&gt;&gt;\n&gt;&gt;\n&gt;&gt; For transitions that provide a reward, the reward=\r\n is selected uniformly\n&gt;&gt; from the range [0, 1).\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; Also, regard=\r\ning the &quot;maximum possible reward maxRx&quot;, do you mean the\n&gt;&gt;&gt; &quot;return (sum o=\r\nf rewards) obtained by the optimal policy&quot;? If you have the\n&gt;&gt;&gt; *same* rewa=\r\nrd value on the transitions (as mentioned above) then it is easy\n&gt;&gt;&gt; to cal=\r\nculate maxRx; if the reward values vary then I guess you have to\n&gt;&gt;&gt; calcul=\r\nate maxRx using dynamic programming; the initial state and the trial\n&gt;&gt;&gt; le=\r\nngth matters, especially in the case where you have 16 actions (states?)\n&gt;&gt;=\r\n&gt; and trial length =3D 4.\n&gt;&gt;&gt;\n&gt;&gt;\n&gt;&gt; Because the length of trials is relativ=\r\nely small (and the MDPs\n&gt;&gt; deterministic) we calculate the maximum return v=\r\nia a simple brute force\n&gt;&gt; method that tries every possible sequence of act=\r\nions for the specific trial\n&gt;&gt; length in question.\n&gt;&gt;\n&gt;\n&gt; Ok, it&#39;s clear no=\r\nw.\n&gt;\n&gt;\n&gt;\n&gt;&gt; Yes, this is an interesting question in general, and certainly =\r\nprevious\n&gt;&gt; results on simple deceptive domains indicate that for delayed-r=\r\neward MDP\n&gt;&gt; environments like you describe (neuro)evolution will likely ge=\r\nt stuck if\n&gt;&gt; the search isn&#39;t aided by something like Novelty Search. In o=\r\nur paper we\n&gt;&gt; don&#39;t worry about this issue as we are comparing the perform=\r\nance of the\n&gt;&gt; different neural network models relative to each other and a=\r\nre not\n&gt;&gt; particularly interested in their performance relative to the maxi=\r\nmum\n&gt;&gt; possible (we do scale the results relative to the maximum possible b=\r\nut this\n&gt;&gt; is simply to make aggregation of results easier).\n&gt;&gt;\n&gt;\n&gt; One iss=\r\nue at a time :)\n&gt;\n&gt;\n&gt;\n&gt;  \n&gt;\n\n\n\n-- \nDr. Sebastian Risi\nAssistant Professor\nI=\r\nT University of Copenhagen, Room 5D08\nRued Langgaards Vej 7, 2300 Copenhage=\r\nn, Denmark\nemail: sebastian.risi@..., web: www.sebastianrisi.com\nmobi=\r\nle: +45-50250355, office: +45-7218-5127\n\r\n--001a11c2b994a7bf7c04f845afbd\r\nContent-Type: text/html; charset=UTF-8\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\n&lt;div dir=3D&quot;ltr&quot;&gt;&lt;div&gt;Hi Oliver and Jeff,&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Very int=\r\neresting paper! Good to see that more and more people are working on plasti=\r\nc ANNs!&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;I also had one question about the CPPN outputs =\r\nthat determine the type of plasticity. In our original adaptive HyperNEAT p=\r\naper (Risi&amp;Stanley, 2010) the CPPN directly specifies the values for th=\r\ne parameters of Niv&#39;s equation. =C2=A0Did you also try this approach or=\r\n why did you decide to limit the number of possible rules to four? In your =\r\npaper you&lt;div&gt;\ncompare it to Ken&#39;s paper on adaptive NEAT and say that =\r\n&quot;Two benefits of this approach are that it reduces the overall search =\r\nspace...&quot;. But I think there is a difference here when we are talking =\r\nabout a direct (NEAT) or an indirect encoding (HyperNEAT).&lt;/div&gt;\n&lt;div&gt;&lt;div&gt;=\r\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;This is the paper on Adaptive HyperNEAT I was referring=\r\n to:&lt;/div&gt;&lt;div&gt;&lt;div&gt;Sebastian Risi and Kenneth O. Stanley (2010) Indirectly=\r\n Encoding Neural Plasticity as a Pattern of Local Rules In: Proceedings of =\r\nthe 11th International Conference on Simulation of Adaptive Behavior (SAB 2=\r\n010). New York, NY: Springer.&lt;/div&gt;\n&lt;div&gt;PDF: &lt;a href=3D&quot;http://eplex.cs.uc=\r\nf.edu/papers/risi_sab10.pdf&quot;&gt;http://eplex.cs.ucf.edu/papers/risi_sab10.pdf&lt;=\r\n/a&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Sebastian&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;=\r\n/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;\n&lt;br&gt;&lt;br&gt;&lt;div class=3D&quot;gm=\r\nail_quote&quot;&gt;On Wed, Apr 30, 2014 at 6:56 PM, Jeff Clune &lt;span dir=3D&quot;ltr&quot;&gt;&l=\r\nt;&lt;a href=3D&quot;mailto:jclune@...&quot; target=3D&quot;_blank&quot;&gt;jclune@...&lt;/a=\r\n&gt;&gt;&lt;/span&gt; wrote:&lt;br&gt;&lt;blockquote class=3D&quot;gmail_quote&quot; style=3D&quot;margin:0 =\r\n0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex&quot;&gt;\n\n\n\n&lt;u&gt;&lt;/u&gt;\n\n\n\n\n\n\n\n\n\n=\r\n \n&lt;div style&gt;\n&lt;span&gt;=C2=A0&lt;/span&gt;\n\n\n&lt;div&gt;\n  &lt;div&gt;\n\n\n    &lt;div&gt;\n      \n      =\r\n\n      &lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;div&gt;&lt;div class=3D&quot;&quot;&gt;&lt;blockquote type=3D&quot;cite&quot;&gt;&lt;div style=\r\n&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div dir=3D&quot;ltr&quot;&gt;&lt;div&gt;&lt;div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;div =\r\nclass=3D&quot;gmail_quote&quot;&gt;&lt;div&gt;Yes, it is clear now. I guess one could also enc=\r\node the mutation rates in the genotype, like in Evolution Strategies, and m=\r\nake these parameters self-adaptive. &lt;/div&gt;\n&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/=\r\ndiv&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/blockquote&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;/div&gt;&lt;div=\r\n&gt;&lt;div style=3D&quot;font-family:Times-Roman&quot;&gt;Self-adaptive mutation rates are a =\r\nterrible idea! Please read this for an explanation:=C2=A0&lt;/div&gt;\n&lt;div style=\r\n=3D&quot;font-family:Times-Roman&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div style=3D&quot;font-family:Times-Roma=\r\nn&quot;&gt;Clune J, Misevic D, Ofria C, Lenski RE, Elena SF, and Sanju=C3=A1n R (20=\r\n08)=C2=A0&lt;br&gt;Natural selection fails to optimize mutation rates for long-te=\r\nrm adaptation on rugged fitness landscapes. PLoS Computational Biology 4(9)=\r\n: e1000187.=C2=A0&lt;/div&gt;\n&lt;div style=3D&quot;font-family:Times-Roman&quot;&gt;pdf:=C2=A0&lt;a=\r\n href=3D&quot;http://jeffclune.com/publications/Clune-EvolvingMutationRates-PLoS=\r\nCB-2008.pdf&quot; target=3D&quot;_blank&quot;&gt;http://jeffclune.com/publications/Clune-Evol=\r\nvingMutationRates-PLoSCB-2008.pdf&lt;/a&gt;&lt;/div&gt;\n&lt;div style=3D&quot;font-family:Times=\r\n-Roman&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div style=3D&quot;font-family:Times-Roman&quot;&gt;I have spent years=\r\n asking anyone I encounter who advocates self-adaptive mutation rates for e=\r\nvidence that they work, or even an argument as to why they could work, and =\r\nall those conversations have come up empty. Mostly I end up convincing them=\r\n that self-adaptive mutation rates are a bad idea, or they end up defending=\r\n something other than self-adaptive mutation rates (e.g. Rechenberg=E2=80=\r\n=99s 1/5th rule, which is not an example of a self adaptive mutation rate: =\r\nit=E2=80=99s an externally controlled schedule).=C2=A0&lt;/div&gt;\n&lt;div style=3D&quot;=\r\nfont-family:Times-Roman&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div style=3D&quot;font-family:Times-Roman&quot;&gt;F=\r\nor some reason people believe they are a good idea, but without evidence or=\r\n intuition. I=E2=80=99m constantly surprised at how persistent this errant =\r\nbelief is. I think it=E2=80=99s because ultimately we want to believe that =\r\nevolution is good at optimizing everything, and we don=E2=80=99t want to ha=\r\nve to set parameters, so we feel like we should just turn them over to evol=\r\nution. But we have lots of evidence of evolution being short-sighted (e.g. =\r\nit doesn=E2=80=99t evolve modularity when it would help:=C2=A0&lt;a href=3D&quot;ht=\r\ntp://goo.gl/2vzFv&quot; target=3D&quot;_blank&quot;&gt;http://goo.gl/2vzFv&lt;/a&gt;).=C2=A0&lt;/div&gt;\n=\r\n&lt;div style=3D&quot;font-family:Times-Roman&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div style=3D&quot;font-family:=\r\nTimes-Roman&quot;&gt;Sorry to jump on your side comment on this issue, but I=E2=80=\r\n=99m trying to spread the word in the community that self-adaptive mutation=\r\n rates do not work.=C2=A0&lt;/div&gt;\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=3D&quot;&quot;&gt;&lt;div&gt;&lt;=\r\nbr&gt;&lt;/div&gt;&lt;br&gt;&lt;blockquote type=3D&quot;cite&quot;&gt;&lt;div style&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div dir=\r\n=3D&quot;ltr&quot;&gt;&lt;div&gt;&lt;div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;div class=3D&quot;gmail_quote&quot;&gt;&lt;d=\r\niv&gt;If I remember correctly when reading Soltoggio&#39;s paper, he used some=\r\n constraints when evolving the parameters of the plasticity rule and specif=\r\nically, A-D were in the range [-1,1] and eta in the range [-100,100]. Did y=\r\nou use any similar constraints?=C2=A0&lt;/div&gt;\n\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Just out o=\r\nf curiosity, what activation function did you use for these n+1 outputs tha=\r\nt correspond to the classes? Did you use a softmax activation function to i=\r\nnterpret the outputs as a probability distribution (and consequently select=\r\ned the class probabilistically) or did you just select the class based on t=\r\nhe highest output among these neurons?=C2=A0&lt;/div&gt;\n\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;\n&lt;div&gt;&lt;b=\r\nr&gt;&lt;/div&gt;&lt;blockquote class=3D&quot;gmail_quote&quot; style=3D&quot;margin:0px 0px 0px 0.8ex=\r\n;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style=\r\n:solid&quot;&gt;&lt;div dir=3D&quot;ltr&quot;&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;\n&lt;div class=3D&quot;gmail_qu=\r\note&quot;&gt;\n&lt;div&gt;The number of states is independent of the number of actions. Di=\r\nfferent actions in state A may all lead to state B but provide different re=\r\nward values.&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/blockquote&gt;\n\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;So, =\r\nhow many states did you use for your simulations? Is it 4 (like in Figure 1=\r\n)? I might have missed that when reading the paper, this is why I asked whe=\r\nther the number of actions correspond to the number of states.&lt;/div&gt;\n\n\n&lt;div=\r\n&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;=C2=A0=C2=A0&lt;/div&gt;&lt;blockquote class=3D&quot;gmail_quote&quot; style=\r\n=3D&quot;margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(20=\r\n4,204,204);border-left-style:solid&quot;&gt;&lt;div dir=3D&quot;ltr&quot;&gt;\n&lt;div class=3D&quot;gmail_e=\r\nxtra&quot;&gt;&lt;div class=3D&quot;gmail_quote&quot;&gt;\n&lt;div&gt;&lt;blockquote class=3D&quot;gmail_quote&quot; st=\r\nyle=3D&quot;margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb=\r\n(204,204,204);border-left-style:solid&quot;&gt;&lt;div dir=3D&quot;ltr&quot;&gt;&lt;div&gt;&lt;/div&gt;&lt;div&gt;3) =\r\nOn page 3 you say that &quot;the proportion of state transitions that provi=\r\nde a reward value is 0.5&quot;. It is not clear to me, however, what the re=\r\nward values are. Do all transitions that have a reward value have the *same=\r\n* reward value (e.g. equal to 1), or does this value vary?&lt;/div&gt;\n\n\n\n\n&lt;/div&gt;=\r\n&lt;/blockquote&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;For transitions that provide a rewar=\r\nd, the reward is selected uniformly from the range [0, 1).&lt;/div&gt;&lt;div&gt;&lt;div&gt;=\r\n=C2=A0&lt;/div&gt;&lt;blockquote class=3D&quot;gmail_quote&quot; style=3D&quot;margin:0px 0px 0px 0=\r\n.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-s=\r\ntyle:solid&quot;&gt;\n\n\n\n\n&lt;div&gt;&lt;div dir=3D&quot;ltr&quot;&gt;\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Also, regarding=\r\n the &quot;maximum possible reward maxRx&quot;, do you mean the &quot;retur=\r\nn (sum of rewards) obtained by the optimal policy&quot;? If you have the *s=\r\name* reward value on the transitions (as mentioned above) then it is easy t=\r\no calculate=C2=A0maxRx; if the reward=C2=A0values vary then I guess you hav=\r\ne to calculate=C2=A0maxRx=C2=A0using dynamic programming; the initial state=\r\n and the trial length=C2=A0matters, especially in the case where you have 1=\r\n6 actions (states?) and trial length =3D 4.&lt;/div&gt;\n\n\n\n\n&lt;/div&gt;&lt;/div&gt;&lt;/blockqu=\r\note&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;Because the length of trials is relatively sm=\r\nall (and the MDPs deterministic) we calculate the maximum return via a simp=\r\nle brute force method that tries every possible sequence of actions for the=\r\n specific trial length in question.&lt;/div&gt;\n\n\n&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/blockquote&gt;=\r\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Ok, it&#39;s clear now.&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;=C2=A0=\r\n=C2=A0&lt;/div&gt;&lt;blockquote class=3D&quot;gmail_quote&quot; style=3D&quot;margin:0px 0px 0px 0=\r\n.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-s=\r\ntyle:solid&quot;&gt;\n\n\n&lt;div&gt;&lt;div dir=3D&quot;ltr&quot;&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;div class=\r\n=3D&quot;gmail_quote&quot;&gt;&lt;div&gt;Yes, this is an interesting question in general, and =\r\ncertainly previous results on simple deceptive domains indicate that for de=\r\nlayed-reward MDP environments like you describe (neuro)evolution will likel=\r\ny get stuck if the search isn&#39;t aided by something like Novelty Search.=\r\n In our paper we don&#39;t worry about this issue as we are comparing the p=\r\nerformance of the different neural network models relative to each other an=\r\nd are not particularly interested in their performance relative to the maxi=\r\nmum possible (we do scale the results relative to the maximum possible but =\r\nthis is simply to make aggregation of results easier).&lt;/div&gt;\n\n\n&lt;/div&gt;&lt;/div&gt;=\r\n&lt;/div&gt;&lt;/div&gt;&lt;/blockquote&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;One issue at a time :)&lt;/div&gt;&lt;d=\r\niv&gt;&lt;br&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;\n\n    &lt;/div&gt;\n    =\r\n \n\n    \n\n&lt;/div&gt;\n\n\n\n\n\n&lt;/div&gt;&lt;/div&gt;&lt;/blockquote&gt;&lt;/div&gt;&lt;/div&gt;&lt;br&gt;&lt;p&gt;&lt;/p&gt;\n\n    =\r\n&lt;/div&gt;\n     \n\n    \n    &lt;div style=3D&quot;color:#fff;min-height:0&quot;&gt;&lt;/div&gt;\n\n\n&lt;/di=\r\nv&gt;\n\n\n\n  \n\n\n\n\n\n\n&lt;/blockquote&gt;&lt;/div&gt;&lt;br&gt;&lt;br clear=3D&quot;all&quot;&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;-- &lt;=\r\nbr&gt;&lt;div dir=3D&quot;ltr&quot;&gt;Dr. Sebastian Risi&lt;br&gt;Assistant Professor=C2=A0&lt;br&gt;IT U=\r\nniversity of Copenhagen, Room 5D08&lt;br&gt;Rued Langgaards Vej 7, 2300 Copenhage=\r\nn, Denmark&lt;br&gt;email: &lt;a href=3D&quot;mailto:sebastian.risi@...&quot; target=3D&quot;=\r\n_blank&quot;&gt;sebastian.risi@...&lt;/a&gt;, web:=C2=A0&lt;a href=3D&quot;http://www.sebas=\r\ntianrisi.com&quot; target=3D&quot;_blank&quot;&gt;www.sebastianrisi.com&lt;/a&gt;&lt;div&gt;\nmobile: +45-=\r\n50250355, office: +45-7218-5127&lt;br&gt;&lt;/div&gt;&lt;/div&gt;\n&lt;/div&gt;\n\r\n--001a11c2b994a7bf7c04f845afbd--\r\n\n"}}