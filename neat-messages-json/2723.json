{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"oCAZ935NynroFuDsZ3AqH5gS2XsECA-jAS3gSDO_uwN-kB55GFOa1ZB-mBDacQsfKCs641r9-o_4070qGHVq8XzKWfz7k_XcqVpUIjZ7YSdI","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: NEAT and highly recurrent networks","postDate":"1157254944","msgId":2723,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGVkZGl2MCtyZmMyQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGVjdmZhZytqcWtmQGVHcm91cHMuY29tPg=="},"prevInTopic":2717,"nextInTopic":2750,"prevInTime":2722,"nextInTime":2724,"topicId":2711,"numMessagesInTopic":7,"msgSnippet":"NEAT really shouldn t be allowed to form recurrent connections to solve XOR.  It s not that there is anything wrong or cheating about it- it s just that","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 70516 invoked from network); 3 Sep 2006 03:42:25 -0000\r\nReceived: from unknown (66.218.66.172)\n  by m32.grp.scd.yahoo.com with QMQP; 3 Sep 2006 03:42:25 -0000\r\nReceived: from unknown (HELO n19a.bullet.scd.yahoo.com) (66.94.237.48)\n  by mta4.grp.scd.yahoo.com with SMTP; 3 Sep 2006 03:42:25 -0000\r\nReceived: from [66.218.66.59] by n19.bullet.scd.yahoo.com with NNFMP; 03 Sep 2006 03:42:24 -0000\r\nReceived: from [66.218.66.91] by t8.bullet.scd.yahoo.com with NNFMP; 03 Sep 2006 03:42:24 -0000\r\nDate: Sun, 03 Sep 2006 03:42:24 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;eddiv0+rfc2@...&gt;\r\nIn-Reply-To: &lt;ecvfag+jqkf@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: Re: NEAT and highly recurrent networks\r\nX-Yahoo-Group-Post: member; u=54567749; y=Buz0ahkJM2H3Tjj_gQCvsxWlx3v25oQ9lmoldLz9J8PV840DTZfV\r\nX-Yahoo-Profile: kenstanley01\r\n\r\nNEAT really shouldn&#39;t be allowed to form recurrent connections to \nsolve XO=\r\nR.  It&#39;s not that there is anything &quot;wrong&quot; or &quot;cheating&quot; \nabout it- it&#39;s j=\r\nust that the main point of XOR is to compare your \nimplementation with othe=\r\nr implementations, and since the standard \nbenchmark does not include recur=\r\nrent connections, you obfuscate the \ncomparison by allowing them.  It sound=\r\ns like your XOR is indeed \nmemorizing the order of presentation.  That is d=\r\nefinitely not the \nintent of the XOR problem, and means your NNs are solvin=\r\ng a very \ndifferent problem.  They are memorizing a sequence by using \nrecu=\r\nrrent connections.  That&#39;s interesting, but makes the comparison \nless mean=\r\ningful.\n\nTechnically, networks should be flushed (all nodes set to zero) \nb=\r\netween input presentations in XOR (or any other similar \nclassification pro=\r\nblem) since XOR is order-independent (the \ndefinition of the XOR problem sa=\r\nys nothing about order of \npresentation).  The flushing should preclude the=\r\n need to present the \ninstances in random order.  Even if you want to use r=\r\necurrent \nconnections, flushing should be part of the procedure.\n\nThe weigh=\r\nt cap of 5 sounds reasonable.  Up to 10 I think is \nreasonable.  My rule of=\r\n thumb for the power of mutations is that it \nshould take on average severa=\r\nl mutations to travel from a weight of \n0 to the max or min weight.  \n\nIf X=\r\nOR weights are tending to max out, that may be an artifact of \nthe XOR prob=\r\nlem- though I&#39;m not sure- there may be other reasons as \nwell.  But I would=\r\n not worry a lot about it until the XOR problem is \nbeing presented in a cu=\r\nstomary way.\n\n\n\n\n--- In neat@yahoogroups.com, &quot;mneylon01&quot; &lt;mneylon01@...&gt; w=\r\nrote:\n&gt;\n&gt; So I&#39;ve had a chance to do what Ken suggested below - start from =\r\n\nthe\n&gt; basic (2+1bias)-1-1 network for the static XOR problem, randomizing\n=\r\n&gt; the test data, and disabling any node or connection creation or\n&gt; togglin=\r\ng to see how my network worked.  I&#39;m using what I believe \nare\n&gt; the same p=\r\narameters in Ken&#39;s various papers: 80% chance of a \nchild&#39;s\n&gt; weights being=\r\n changed, with 90% chance of perturbation, 10% chance \nof\n&gt; mutation.  I us=\r\ned 150 species, and for the species calculation, I \nused\n&gt; 1.0 for the exce=\r\nss and disjoint coefficients, 0.4 for the common\n&gt; weight differences, and =\r\nset N to 1 (as noted by Ken below), with \nthe\n&gt; tolerance set to 3.0.\n&gt; \n&gt; =\r\nWhat was interesting is that I found there to be a VERY strong\n&gt; connection=\r\n between two parameters that I don&#39;t see mentioned much \nin\n&gt; the papers: t=\r\nhe weight cap, and the type and amount of \nperturbation or\n&gt; mutation done.=\r\n   (Note that I&#39;ve tried both the sigmoid steepness \nof\n&gt; 4.9 and 1, this s=\r\neems to have less of an impact that these other\n&gt; parameters).\n&gt; \n&gt; My orig=\r\ninal code capped the weight at 2, which I saw slow if any\n&gt; approach to a g=\r\nood network.  Increasing the cap to insane values \nlike\n&gt; 100 gave a great =\r\nspeed to evolution convergence, as this allowed \nfor a\n&gt; wider range of wei=\r\nght values to be explored by the NEAT method, but\n&gt; leads to some networks =\r\nthat one questions their ability to work.  \n&gt; Using a cap of 5 to 10 gave a=\r\n more reasonable set of networks while\n&gt; still converging quickly (about 50=\r\n generations to get the RMS under\n&gt; 0.1 on the 4 cases).  I&#39;ve seen the cap=\r\n of 5 tossed around here\n&gt; before, but does this seem to be a good practica=\r\nl number?\n&gt; \n&gt; Also, with my weight mutations, I found that using gaussian\n=\r\n&gt; distributions for both initial weights and mutated weights, and\n&gt; pertuba=\r\ntions in weights was better than straight random.  \n(Presently\n&gt; I&#39;m using =\r\nN rolls of the RNG and getting the average to simulate\n&gt; gaussian distribut=\r\nion, with larger N to reduce the standard\n&gt; deviation).  The quesiton I hav=\r\ne, since I&#39;m having trouble seeing \nhow\n&gt; it&#39;s done in either the C++ or C#=\r\n code, is what the good effective\n&gt; magnitude is.  The above cases, for the=\r\n mutated/initial weights, \nI&#39;m\n&gt; using a gaussian random number from -5 to =\r\n5, std deviation of \nabout 1,\n&gt; and for the perturbed weights, from -5 to 5=\r\n with a std deviation of\n&gt; about 0.1-0.5.    These cases seem to work, but =\r\nI can&#39;t seem how \nthey\n&gt; easily match up with the above code.  Even with th=\r\ne weight caps, I\n&gt; always seemed to have weights that wanted to hit the cap=\r\ns.  Any \ngood\n&gt; suggestions on what are good weight ranges and adjustments =\r\nto \nmake. \n&gt; Another thing I noticed is that if the weight cap is too low, =\r\nusing\n&gt; the values for the species tolerance calculation I give above, I\n&gt; =\r\nrarely got more than 1 or two species.\n&gt; \n&gt; Additionally, I find it interes=\r\nting in how one approaches the input\n&gt; data part.  I tried the standard XOR=\r\n with a fixed set of data, and \nit\n&gt; can converge quickly, but when you the=\r\nn make the data randomly\n&gt; presented (re-evaluating weights of genomes that=\r\n carry over to \naccount\n&gt; for repeated runs), the performance of the NEAT m=\r\nethod is poor.   \nThat\n&gt; is, when fixed, I almost always got a recurrent co=\r\nnnection (I don&#39;t\n&gt; prevent these from being formed), suggesting it&#39;s tryin=\r\ng to mimic \nthe\n&gt; input pattern order and not the actual data patterns.  I =\r\nfound \nthat if\n&gt; a random order was used, using a larger test pattern (in m=\r\ny case, \n50\n&gt; initially randomly generated XOR cases and then presented in =\r\na \nrandom\n&gt; order to the networks) seemed to help.  I still get some \nrecur=\r\nrancies,\n&gt; but the network size seemed to stay small and avoid new node\n&gt; f=\r\normation.  (Obviously, when I use recurrent data as like the \ndelayed\n&gt; XOR=\r\n problem, the data order will be fixed, but the size of the data\n&gt; set need=\r\ns to be large enough that it would be impractical for the\n&gt; genome to try t=\r\no learn the overall data set pattern as opposed to \nthe\n&gt; actual pattern th=\r\nat the dataset contains...)\n&gt; \n&gt; --- In neat@yahoogroups.com, &quot;Kenneth Stan=\r\nley&quot; &lt;kstanley@&gt; wrote:\n&gt; &gt;\n&gt; &gt; It sounds like the main issue before you ca=\r\nn really start \nlooking at \n&gt; &gt; delayed-XOR is to get XOR working more clos=\r\nely to my own NEAT\n&gt; &gt; (and others developed since).  Certainly the 500-100=\r\n0 \ngenerations \n&gt; &gt; it&#39;s taking you indicates somethign is likely wrong in =\r\nyour \n&gt; &gt; implementation.\n&gt; &gt; \n&gt; &gt; I would suggest the following experiment=\r\n:  Start evolution with  \na \n&gt; &gt; population of networks that already have t=\r\nhe correct topology \nfor a \n&gt; &gt; solution to XOR and turn off structural mut=\r\nations.  Then run \nyour \n&gt; &gt; version of NEAT as usual, except in this case,=\r\n it will only be \n&gt; &gt; searching over weight space.  If it takes forever, it=\r\n tells you \nthat \n&gt; &gt; the problem is in the way your weights are being muta=\r\nted, or the \nway \n&gt; &gt; they are being combined in crossover.  It may also in=\r\ndicate a \n&gt; &gt; problem in speciation (related to weight comparison).  In any=\r\n \ncase, \n&gt; &gt; it will greatly narrow down the problem.\n&gt; &gt; \n&gt; &gt; Once you get=\r\n it working on pure weight-evolution, then you can \nmove \n&gt; &gt; to the normal=\r\n topology evolution, and you will either see it \nwork \n&gt; &gt; right away, or f=\r\nind that there is a problem in adding structure \nas \n&gt; &gt; well.\n&gt; &gt; \n&gt; &gt; Abo=\r\nut compatibility testing for speciation, when you menion &quot;N,&quot; \ndo \n&gt; &gt; you =\r\nmean the normalization term in my papers?  Regrettably, many \n&gt; &gt; people mi=\r\nss that I did not use N (for normalization) in \npractice, \n&gt; &gt; that is, I s=\r\net N to 1 in all cases.  That may explain why you \nusing \n&gt; &gt; the same coef=\r\nficients as me does not work.  If you look at my \npapers \n&gt; &gt; closely, they=\r\n say that N can be set to 1 if genomes are not too \n&gt; &gt; large.  I have foun=\r\nd in practice that it always works fine with \nit \n&gt; &gt; set to 1, so that&#39;s w=\r\nhat I&#39;ve done.  This confusion is my fault \nand \n&gt; &gt; I apologize for it- th=\r\ne papers should be more clear- but I \nwanted \n&gt; &gt; people to be aware of N a=\r\nnd the option for normalization in case \nit \n&gt; &gt; indeed does come into play=\r\n with very large genomes.\n&gt; &gt; \n&gt; &gt; As for deviations from typical values, y=\r\nou can see all the \nvalues \n&gt; &gt; I&#39;ve used in the appendix to my dissertatio=\r\nn.  There is some \n&gt; &gt; explanation there too for why different values were =\r\nchosen.  One \n&gt; &gt; consideration might be whether very fine grained weight c=\r\nhanged \nare \n&gt; &gt; key or not in a particular problem.  If they are, you migh=\r\nt want \nthe \n&gt; &gt; coefficient of weight differences to be higher.\n&gt; &gt; \n&gt; &gt; O=\r\nnce you get XOR working, please let the group know how things \ngo \n&gt; &gt; with=\r\n the delayed XOR!  By the way, what language/platform did \nyou \n&gt; &gt; use for=\r\n your version of NEAT?\n&gt; &gt; \n&gt; &gt; ken\n&gt; &gt; \n&gt; &gt; --- In neat@yahoogroups.com, &quot;=\r\nmneylon01&quot; &lt;mneylon01@&gt; wrote:\n&gt; &gt; &gt;\n&gt; &gt; &gt; I&#39;ve been working on my own impl=\r\nimentation of NEAT (having to \nfit \n&gt; &gt; to a\n&gt; &gt; &gt; predescribed framework) =\r\nand while it&#39;s mostly working, I&#39;m \nlooking \n&gt; &gt; at\n&gt; &gt; &gt; a couple of quest=\r\nions.\n&gt; &gt; &gt; \n&gt; &gt; &gt; First, I know that the networks that NEAT generates can =\r\nhave\n&gt; &gt; &gt; recurrency (feedback), and this is likely why some of the \nrobot=\r\n \n&gt; &gt; game\n&gt; &gt; &gt; examples work well.  However, I&#39;m looking at trying to use=\r\n \nNEAT to\n&gt; &gt; &gt; find patterns in time series data, as one would use fully \n=\r\nrecurrent\n&gt; &gt; &gt; networks for (In these, also known as Elmen networks, all o=\r\nf \nthe\n&gt; &gt; &gt; hidden layer and output layer values are &#39;propagated&#39; into the=\r\n \nnext\n&gt; &gt; &gt; time step to give the network memory, with full connectivity \n=\r\n&gt; &gt; between\n&gt; &gt; &gt; all the input and previous nodes to the hidden/output nod=\r\nes).\n&gt; &gt; &gt; \n&gt; &gt; &gt; Such a network should be possible to be generated by NEAT=\r\n, \nthough I\n&gt; &gt; &gt; figure that not every recurrent type problem needs a full=\r\ny \n&gt; &gt; recurrent\n&gt; &gt; &gt; network.  So I&#39;m trying to use NEAT to generate such=\r\n, using the\n&gt; &gt; &gt; classic delayed-XOR problem (such that the result of xor =\r\nof \nthe two\n&gt; &gt; &gt; current inputs will be the actual output some time steps =\r\n\naway).  \n&gt; &gt; Fully\n&gt; &gt; &gt; recurrent networks can be trained to do this, but=\r\n I want to \n&gt; &gt; generate a\n&gt; &gt; &gt; NEAT network that, after running through t=\r\nhe fixed data series \na\n&gt; &gt; &gt; number of relaxation times, that the weights =\r\nhave already been \n&gt; &gt; trained\n&gt; &gt; &gt; through NEAT evolution such that I don=\r\n&#39;t have to perform \nadditional\n&gt; &gt; &gt; training on the network.  \n&gt; &gt; &gt; \n&gt; &gt; =\r\n&gt; Has anyone had any success directly in generating such \nrecurrent\n&gt; &gt; &gt; n=\r\networks?  I know my fitnesses improve with time, but it takes \na \n&gt; &gt; lot\n&gt;=\r\n &gt; &gt; of generations (1000+ with a 150 member population) to even see\n&gt; &gt; &gt; =\r\nsomething, and even then, it&#39;s not anywhere close to what \nsimple\n&gt; &gt; &gt; rec=\r\nurrent training can provide.  (This may be also related to \nmy\n&gt; &gt; &gt; second=\r\n question).\n&gt; &gt; &gt; \n&gt; &gt; &gt; The other question I had was about convergence tim=\r\nes.  I&#39;m \ntrying \n&gt; &gt; to\n&gt; &gt; &gt; test my network on the normal XOR problem (n=\r\non-recurrent mode) \nand\n&gt; &gt; &gt; find that it takes many more evolution genera=\r\ntions for the \nfitness \n&gt; &gt; to\n&gt; &gt; &gt; get to acceptable levels (based solely=\r\n on the distance of \nexpected \n&gt; &gt; vs\n&gt; &gt; &gt; observed output), exceptionaly =\r\nmore than listed in the NEAT \npapers\n&gt; &gt; &gt; (500-1000 evolution steps as opp=\r\nosed to 10-30 steps) even when \n&gt; &gt; using\n&gt; &gt; &gt; what I believe are the same=\r\n values described by Kenneth in his \n&gt; &gt; papers.\n&gt; &gt; &gt;  I&#39;ve tried nearly e=\r\nvery parameter, and the only one that I \nknow I\n&gt; &gt; &gt; want to keep low is t=\r\nhe new node probability to avoid \nexcessive \n&gt; &gt; growth\n&gt; &gt; &gt; of the networ=\r\nk.  Anyone have any pointers on what parameters \nare\n&gt; &gt; &gt; critical to help=\r\n with rapid convergence on the best network?  \nMind\n&gt; &gt; &gt; you, it could sti=\r\nll be something in my code which I&#39;ve been \n&gt; &gt; pounding\n&gt; &gt; &gt; through to t=\r\nry to find differences.\n&gt; &gt; &gt; \n&gt; &gt; &gt; Another related question is on the spe=\r\ncies comparison \nexpression \n&gt; &gt; and\n&gt; &gt; &gt; tolerance.  I tend to use N=3Dnu=\r\nmber of genes in largest species\n&gt; &gt; &gt; regardless of the case, and for that=\r\n I have to play with the \n&gt; &gt; tolerance\n&gt; &gt; &gt; as to get 5 or more species i=\r\nn a population of 150.  Is there \nan \n&gt; &gt; ideal\n&gt; &gt; &gt; average number of spe=\r\ncies that you want to carry through in the\n&gt; &gt; &gt; population in order to tak=\r\ne advantage of NEAT&#39;s use of \nspecies?  \n&gt; &gt; And\n&gt; &gt; &gt; when do people move =\r\naway from the &#39;typical&#39; values of the \n&gt; &gt; coefficients\n&gt; &gt; &gt; (1 and 1 for =\r\ndisjoint and excess elements, 0.4 for weight \n&gt; &gt; difference\n&gt; &gt; &gt; average)=\r\n\n&gt; &gt; &gt;\n&gt; &gt;\n&gt;\n\n\n\n\n\n"}}