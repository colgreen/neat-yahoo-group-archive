{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":7192225,"authorName":"Ian Badcoe","from":"Ian Badcoe &lt;ian_badcoe@...&gt;","profile":"ian_badcoe","replyTo":"LIST","senderId":"qQgKnhNX-B1GQQsXrYMED8DHFaKVj8mu3EubhigzF8HT0jG_fQfnpXpJuAX2OFKq2B3G5bq96CLc0Rz4IZF2IldcmxfjVJAaTRc","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [neat] Compactness of ANN&#39;s built from indirect encodings","postDate":"1134507505","msgId":2469,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDYuMi4zLjQuMC4yMDA1MTIxMzIwNDMzOS4wMjdiMzc5OEBwb3AubWFpbC55YWhvby5jby51az4=","inReplyToHeader":"PDQzOUIzQTQ1LjYwNTA1MDdAZHNsLnBpcGV4LmNvbT4=","referencesHeader":"PDQzOUIzQTQ1LjYwNTA1MDdAZHNsLnBpcGV4LmNvbT4="},"prevInTopic":2466,"nextInTopic":2472,"prevInTime":2468,"nextInTime":2470,"topicId":2466,"numMessagesInTopic":3,"msgSnippet":"... Yes and no (and you get into this a bit yourself).  A subroutine is (usually) entirely discrete, e.g. invocation#1 can have no influence on invocation#2. ","rawEmail":"Return-Path: &lt;ian_badcoe@...&gt;\r\nX-Sender: ian_badcoe@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 86631 invoked from network); 13 Dec 2005 20:55:57 -0000\r\nReceived: from unknown (66.218.66.167)\n  by m25.grp.scd.yahoo.com with QMQP; 13 Dec 2005 20:55:57 -0000\r\nReceived: from unknown (HELO smtp1.freeserve.com) (193.252.22.158)\n  by mta6.grp.scd.yahoo.com with SMTP; 13 Dec 2005 20:55:57 -0000\r\nReceived: from me-wanadoo.net (localhost [127.0.0.1])\n\tby mwinf3002.me.freeserve.com (SMTP Server) with ESMTP id 186AA1C0015B\n\tfor &lt;neat@yahoogroups.com&gt;; Tue, 13 Dec 2005 21:55:45 +0100 (CET)\r\nReceived: from giles.yahoo.co.uk (modem-1919.lion.dialup.pol.co.uk [217.135.167.127])\n\tby mwinf3002.me.freeserve.com (SMTP Server) with ESMTP id AC4C41C00158\n\tfor &lt;neat@yahoogroups.com&gt;; Tue, 13 Dec 2005 21:55:43 +0100 (CET)\r\nX-ME-UUID: 20051213205543705.AC4C41C00158@...\r\nMessage-Id: &lt;6.2.3.4.0.20051213204339.027b3798@...&gt;\r\nX-Mailer: QUALCOMM Windows Eudora Version 6.2.3.4\r\nDate: Tue, 13 Dec 2005 20:58:25 +0000\r\nTo: neat@yahoogroups.com\r\nIn-Reply-To: &lt;439B3A45.6050507@...&gt;\r\nReferences: &lt;439B3A45.6050507@...&gt;\r\nMime-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;us-ascii&quot;; format=flowed\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: Ian Badcoe &lt;ian_badcoe@...&gt;\r\nSubject: Re: [neat] Compactness of ANN&#39;s built from indirect encodings\r\nX-Yahoo-Group-Post: member; u=7192225; y=k9tURb-d1UTmcfsmiD3kMiQKipuEE24alQB2pq-RhP3qNJd0YQ\r\nX-Yahoo-Profile: ian_badcoe\r\n\r\nAt 20:27 10/12/2005, you wrote:\n&gt;Here&#39;s an idea I got while reading some GP papers.\n&gt;\n&gt;Often in GP work the use of ADF&#39;s (Automatically Defined Functions) is\n&gt;mentioned, as far as I can see this is just a fancy term for a\n&gt;sub-routine. In tree based GP there is usually a main program tree and\n&gt;optionally a seperate set of ADF&#39;s than can be invoked (from the main\n&gt;tree) with parameters, just as any other sub-routine. What this means is\n&gt;that each ADF is only defined once in memory, just as sub-routines (or\n&gt;the CPU instructions that make up a function) in C++, Java or whatever\n&gt;only exist once in computer memory regardless of how many times the\n&gt;function is called. Each invocation simply pushes its parameters onto\n&gt;the stack and jumps to the function&#39;s address in memory.\n&gt;\n&gt;Now consider, say, modular-NEAT. To me sub-networks or modules are the\n&gt;equivalent of sub-routines within GP, each instance of a module within\n&gt;an ANN (following decoding of the genome) is like a seperate invocation\n&gt;of a sub-routine with the parameters coming from the connections feeding\n&gt;signals into the module.\n\nYes and no (and you get into this a bit yourself).  A subroutine is \n(usually) entirely discrete, e.g. invocation#1 can have no influence \non invocation#2.\n\nYou can change this in some languages by adding static data to the \nsubroutine (in which case you have arguably turned it into simplistic \npure-static class).\n\nYou have the same choice in ANN, a sub-net can be shared.  In which \ncase you still have a load of questions to answer about how the \nwiring is done, but the nets that use it are to some extent sharing \ndata.  Or the sub-net can be uniquely cloned into each place where it \nis used, in which case it is completely insulated in each case.\n\nSo during instantiation, you have two separate choices to make:\n\n1) instantiate one definition into one or more sub-routines\n2) wire each subroutine into one or more places in the net\n\n&gt;Of course a fully decoded ANN would normally (I guess) contain\n&gt;duplicates of the modules, because each copy must maintain it&#39;s own set\n&gt;of neuron states (activation signal).\n\nOnly a recurrent net needs that, a non-recurrant net has the useful \nproperty that you could use the same copy of the sub-net in multiple \nplaces, and as long as you chase the sub-net&#39;s activation to \ncompletion in each context where it is used, you are not leaking data \nfrom one instance to another.  Now that&#39;s saving memory!\n\nNon-recurrent sub-nets also have the property that they are \ncompletely asynchronous from the net that uses them, so as long as \nyou wait until all the sub-nets inputs are available, you can then \nprocess it to completion as a discrete action, knowing it won&#39;t need \nany other input from the outer net.\n\nUNLESS, of course, the outer net _is_ recurrent, in which case what I \nsaid is still true but you might be concerned about the time-delay \nbetween setting sub-net inputs and the result working its way through...\n\n&gt;  This is distinct from\n&gt;(traditional) GP because GP trees (or graphs) traditionally describe\n&gt;control flow and so each invocation of a routine occurs in sequence,\n&gt;whereas ANN&#39;s describe data flow and all instances of an ANN module\n&gt;therefore have data flowing through them concurrently.\n&gt;\n&gt;  However this level of duplication is still potentially wasteful (of\n&gt;main memory) because only the neuron signals are unique to each copy,\n&gt;not the structure. The structure is equivalent to the CPU instructions\n&gt;of a normal sub-routine and as such doesn&#39;t actually need to be\n&gt;duplicated. Instead each module could be decoded into it&#39;s own sub-ANN\n&gt;that the main ANN connects to, but in addition to a normal connection we\n&gt;must specify the module &#39;instance&#39; we are connecting to, each neuron\n&gt;then contains an array (for example) of states and it operates on the\n&gt;relevant state data.\n&gt;\n&gt;Of course a sub-ANN could refer to other sub-ANN&#39;s, so overall the\n&gt;memory saving could potentially be very large. Such an ANN\n&gt;representation may not be as compact as the original genome where\n&gt;indirect encodings are in use, but it could help keep a lid on the\n&gt;memory usage of decoded ANN&#39;s. Of course functionaly the ANN&#39;s could\n&gt;still grow out of control - they are still the same ANN, just\n&gt;represented more compactly.\n\nIf you can wangle your network to fit a &quot;stateless&quot; or &quot;lightweight&quot; \npatterns, then potentially whole slews of neurones can refer to the \nsame object.  Of course it depends whether your indirect encoding \nproduces slews of identical neurones, but that&#39;s kinda the point...\n\nI have a prototype GP system that works this way, but I didn&#39;t touch \nit for a couple of years...\n\n         Ian\n\nIn 15 minutes everybody will be in the future.\n\n\n\n\n"}}