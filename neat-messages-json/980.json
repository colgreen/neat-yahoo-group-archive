{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"QbK3RzrpTOOg6mP-SaY6Ua_yljQKbowLaN1YVxneyg_zlO1A0ztLpgl-iGx-gXHqDrtjnvJNawEBNEKYQrOcj2HhyIe7ZXpVj7fgAj9EMYWP","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: Bloat","postDate":"1086294604","msgId":980,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGM5bzFvYytpNTYzQGVHcm91cHMuY29tPg==","inReplyToHeader":"PEJBWTItRjE1MTRvcXFVMnFqQWowMDAwZGQ4ZEBob3RtYWlsLmNvbT4="},"prevInTopic":977,"nextInTopic":981,"prevInTime":979,"nextInTime":981,"topicId":904,"numMessagesInTopic":68,"msgSnippet":"... the ... or ... the ... fitness ... When ... descending, and ... same fitness, ... places, it s ... But if you ... are close to one ... wins.  And ... ","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 58456 invoked from network); 3 Jun 2004 20:30:19 -0000\r\nReceived: from unknown (66.218.66.172)\n  by m25.grp.scd.yahoo.com with QMQP; 3 Jun 2004 20:30:19 -0000\r\nReceived: from unknown (HELO n36.grp.scd.yahoo.com) (66.218.66.104)\n  by mta4.grp.scd.yahoo.com with SMTP; 3 Jun 2004 20:30:19 -0000\r\nReceived: from [66.218.67.132] by n36.grp.scd.yahoo.com with NNFMP; 03 Jun 2004 20:30:05 -0000\r\nDate: Thu, 03 Jun 2004 20:30:04 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;c9o1oc+i563@...&gt;\r\nIn-Reply-To: &lt;BAY2-F1514oqqU2qjAj0000dd8d@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Length: 6682\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-eGroups-Remote-IP: 66.218.66.104\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: Re: Bloat\r\nX-Yahoo-Group-Post: member; u=54567749\r\nX-Yahoo-Profile: kenstanley01\r\n\r\n--- In neat@yahoogroups.com, &quot;John Arrowwood&quot; &lt;jarrowwx@h...&gt; wrote:\n&gt; &gt;From: Derek James &lt;djames@g...&gt;\n&gt; &gt;\n&gt; &gt;The point is, there is no direct bias against network size or\n&gt; &gt;efficiency.  So no, there is currently no bias *against* neutral\n&gt; &gt;mutations.  That&#39;s what I believe Colin and Ken are arguing for, \nthe\n&gt; &gt;allowance for a lot of structure that&#39;s neither directly helping \nor\n&gt; &gt;hurting fitness in the present, but that may be coopted later in \nthe\n&gt; &gt;run to increase fitness.\n&gt; &gt;\n&gt; &gt;If you want to see this in action, just run NEAT on a very simple\n&gt; &gt;problem and crank up the mutation rates.  It will still solve the\n&gt; &gt;problem, but you will have networks that are much larger than they\n&gt; &gt;really need to be.\n&gt; \n&gt; Ah...I see.  Well, isn&#39;t the solution to that to change the \nfitness \n&gt; function?\n&gt; \n&gt; Give each network both a &#39;fitness&#39; and a &#39;complexity&#39; ranking.  \nWhen \n&gt; deciding on the ranking of specimens, sort first on fitness, \ndescending, and \n&gt; then on complexity, ascending.  Thus, if two networks have the \nsame fitness, \n&gt; the smaller one wins.\n&gt; \n&gt; If the fitness function was very granular, like to 10 decimal \nplaces, it&#39;s \n&gt; unlikely that the complexity factor would ever come into play.  \nBut if you \n&gt; round the fitness (or scale it), then two networks that \nare &#39;close&#39; to one \n&gt; another would then be folded into &#39;the same&#39; and the smaller one \nwins.  And \n&gt; you could make that &#39;scaling&#39; be dynamic...like 10% of the range \nbetween \n&gt; average and best within a species.  If the average fitness is 10, \nthe best \n&gt; is 12, then the range between average and best is 2.  10% of that \nis 0.2.  \n&gt; So, any two that are within 0.2 of each other would (kinda) get \nthe same \n&gt; fitness score.  And that percentage could be a tunable parameter \nso we can \n&gt; find the best range for where an &#39;advantage&#39; is a CLEAR advantage \nand is \n&gt; more important than complexity.\n&gt; \n&gt; This does mean that if a mutation adds complexity without adding a \nclear \n&gt; advantage, that one is selected against.  However, that&#39;s not \nentirely fair. \n&gt;   So...instead of only keeping ONE &#39;elite&#39; member, have a minimum \nof 2 but \n&gt; calculated at 10% (or so) of the offspring for the species.  That \nallows \n&gt; those good but not clearly so networks to survive long enough to \ntry and get \n&gt; better before being pruned back out.\n&gt; \n&gt; What do you think?  Think that would do the trick?\n\nI have a feeling we are discussing how to fix something that isn&#39;t \nbroken.  Derek was only talking about what happens if you pump the \nmutation rate up too high.\n\nOne of my initial motivations for NEAT was to create an alternative \nto algorithms that consider complexity as part of their selection \ncriteria, i.e. those algorithms that figure complexity into \nfitness.  That way of doing things is ad hoc and opens up a whole \nnew can of worms in the way that now &quot;cheaters&quot; will exploit the \nfitness advantages of being small without really improving in \nperformance.   NEAT tends toward smaller networks by virtue of \nstarting small and complexifying; that way it avoids ad hoc fitness \ntampering.\n\nI also think &quot;bloat&quot; is a misnomer here.  Bloat is a term from \nGenetic Programming that referred to the accumulation of extraneous \nstructure over the *entire population* that didn&#39;t help improve \nfitness.  In general they were looking at a situaton where everyone \nwas getting bigger for no good reason.\n\nIn NEAT, genome expansion frequently happens during periods of \n*stagnation*.  In other words, it happens when fitness isn&#39;t \nimproving.  It is debatable whether that should be called bloat.  \nAfter all, it is a reasonable hypothesis that if you are stuck, more \nstructure needs to be added.  In fact, if you *didn&#39;t* add more \nstructure, that could just as easily be viewed as a weakness in the \nmethod.\n\nYet there is still the problem that allowing the entire population \nto grow in size is dangerous because it means *committing* to high-\ndimensional space without knowing it&#39;s where you need to be.  \nHowever, NEAT does not expand the entire population.  That&#39;s why I \nthink bloat is a misnomer.  NEAT expands *some species* but others \nstay small.  To see this, look at figure 5 at \n\nhttp://www-2.cs.cmu.edu/afs/cs/project/jair/pub/volume21/stanley04a-\nhtml/node6.html#SECTION00063100000000000000\n\nCheck out the upper and lower lines in both graphs; they represent \nmaxiumum and minimum complexity in a population that is stagnating.  \nSee what&#39;s going on?  The minimum stays small while the maximum goes \nway up.  In other words, NEAT expands the *range* of complexities.  \nThat is quite different than bloating the entire population.\n\nIn fact, expanding the range of complexity in a population is in my \nmind the most common sense approach to dealing with stagnation.  It \nsays this:  I don&#39;t want to give up on the chance of finding a \nsolution in low dimensional space since that&#39;s easier, but since \nthat&#39;s not working out, at the same time I want to look in \nincreasingly high-dimensional spaces in case that&#39;s the only place \nthe solution can be found.  \n\nNEAT is able to do this because of speciation: the smaller species \nare protected just as the larger ones are, keeping the minimum \ncomplexity level relatively intact.  This is a powerful dynamic you \ndon&#39;t see in typical &quot;bloat&quot; as it&#39;s usually referred to.\n\nSo I think this talk about whether we should be adding a little bit \nat a time is missing some of the point about stagnation: when you \nare stagnant, you have no idea how much to add.  If you have a \npolicy whereby adding &quot;too much&quot; structure needs to be stopped in \ngeneral, then you won&#39;t get the &quot;expanding the range&quot; dynamic which \nI believe is important.  Chunks of NN are not analogous to an \nappendix.  A single hidden-layer NN can approximate any continuous \nfunction.  An appendix is not going to approximate anything that \nisn&#39;t like an appendix in a very very long time.  In other words, \naddings more and more neurons during stagnation is creating space \nfor discovery, not tacking on an unalterable piece of junk.\n\nThe real principle should be this:  If you&#39;re doing well then stay \nsmall while you can, but also check out a few other complexity \nlevels as investments for the future.  If you&#39;re stuck, don&#39;t stick \nall your eggs in one basket: expand the *range* of complexities you \nare considering but don&#39;t give up on small solutions entirely.\n\nSpeciation in NEAT was *intended* to protect more complex networks \nthat didn&#39;t have better fitness.  If you really want to give less \nsupport to higher complexity, you could just change the fitness \nsharing equations so that they are less protective.  But I don&#39;t \nthink that&#39;s a good idea.\n\nken\n\n\n"}}