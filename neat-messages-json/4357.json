{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":306380549,"authorName":"Thomas Johnson","from":"&quot;Thomas Johnson&quot; &lt;thomas.j.johnson@...&gt;","profile":"thomasj9802","replyTo":"LIST","senderId":"qB-AgEo6mldyAvMqadzDVkD6AuYj0kvV1nXPmtuHKMIozH_NT2HdEZjtrKS2laeuQr2zWMnGJ8dCkwnqIbNcvlxu09cw24YUTRyGI7B0gLzGv4k_","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] Re: grammatical evolution","postDate":"1223758777","msgId":4357,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDEyODBjZjZhMDgxMDExMTM1OXIxYzdjNjg3NmoxYThkZmY1ZjExZmFmM2YwQG1haWwuZ21haWwuY29tPg==","inReplyToHeader":"PGdjcjFmNitnamNqQGVHcm91cHMuY29tPg==","referencesHeader":"PDIwMDgxMDA4LjIwMDYxMy44MzI0NDI4NS5tb3VyZXRAaXNpci5mcj4gPGdjcjFmNitnamNqQGVHcm91cHMuY29tPg=="},"prevInTopic":4356,"nextInTopic":4358,"prevInTime":4356,"nextInTime":4358,"topicId":4352,"numMessagesInTopic":11,"msgSnippet":"I m not sure that it s necessary to argue for GE from a purely encoding-based or search-space-based framework. It depends on what kind of attributes you value","rawEmail":"Return-Path: &lt;thomas.j.johnson@...&gt;\r\nX-Sender: thomas.j.johnson@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 89012 invoked from network); 11 Oct 2008 20:59:38 -0000\r\nX-Received: from unknown (66.218.67.95)\n  by m1.grp.re1.yahoo.com with QMQP; 11 Oct 2008 20:59:38 -0000\r\nX-Received: from unknown (HELO rv-out-0708.google.com) (209.85.198.247)\n  by mta16.grp.scd.yahoo.com with SMTP; 11 Oct 2008 20:59:38 -0000\r\nX-Received: by rv-out-0708.google.com with SMTP id c5so1114526rvf.24\n        for &lt;neat@yahoogroups.com&gt;; Sat, 11 Oct 2008 13:59:38 -0700 (PDT)\r\nX-Received: by 10.114.181.1 with SMTP id d1mr3504290waf.185.1223758778000;\n        Sat, 11 Oct 2008 13:59:38 -0700 (PDT)\r\nX-Received: by 10.115.60.16 with HTTP; Sat, 11 Oct 2008 13:59:37 -0700 (PDT)\r\nMessage-ID: &lt;1280cf6a0810111359r1c7c6876j1a8dff5f11faf3f0@...&gt;\r\nDate: Sat, 11 Oct 2008 15:59:37 -0500\r\nTo: neat@yahoogroups.com\r\nIn-Reply-To: &lt;gcr1f6+gjcj@...&gt;\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Transfer-Encoding: 7bit\r\nContent-Disposition: inline\r\nReferences: &lt;20081008.200613.83244285.mouret@...&gt; &lt;gcr1f6+gjcj@...&gt;\r\nFrom: &quot;Thomas Johnson&quot; &lt;thomas.j.johnson@...&gt;\r\nSubject: Re: [neat] Re: grammatical evolution\r\nX-Yahoo-Group-Post: member; u=306380549; y=BeSNeB4y3hX1XtrHhHcJsfHwyDf5a3lO55N3MOXbI_a4TMMuizo\r\nX-Yahoo-Profile: thomasj9802\r\n\r\nI&#39;m not sure that it&#39;s necessary to argue for GE from a purely\nencoding-based or search-space-based framework. It depends on what\nkind of attributes you value for your solutions. If you want a\nsolution that is &quot;merely&quot; a good mapping of future inputs to future\noutputs over some kind of input domain, then you desire a smooth\nsearch space which will maximize the probability of getting a solution\nlike that. But GE has the advantage that its solutions can be more\neasily understood by humans than NNs. So if I&#39;m concerned about the\nintuition behind the &quot;black box&quot; of the solution, or I want to analyze\nthe kinds of domains over which the solution might not perform well\n(e.g., modeling financial risk), GE could be a better methodology even\nif its searching power is somewhat inferior.\n\nOn Sat, Oct 11, 2008 at 3:14 PM, Kenneth Stanley &lt;kstanley@...&gt; wrote:\n&gt; I understand that &quot;hierarchical, modular, and formalized&quot; are\n&gt; sometimes viewed as advantages for Grammatical Encoding (a similar\n&gt; argument is also made for L-systems, which is also grammatical).\n&gt; While these are clearly properties you want in a programming language,\n&gt; are these really good things for a genetic encoding? The challenge\n&gt; with encodings is not only for them to theoretically express all\n&gt; possible procedures, but also to induce a nicely coupled (smooth)\n&gt; search space. It seems that the space induced by a strictly modular\n&gt; and hierarchical encodings is unlikely to be smooth, leading to\n&gt; brittleness. You can have things like a massive new module being\n&gt; doubled in one shot (which would usually be a mess, like being born\n&gt; with two heads) or a change at one level of the hierarchy breaking the\n&gt; rest of it entirely.\n&gt;\n&gt; I am not dismissing GE, but I would want to hear an argument\n&gt; supporting it more from the encoding and search spaces side rather\n&gt; than from the formal languages side.\n&gt;\n&gt; ken\n&gt;\n&gt; --- In neat@yahoogroups.com, Jean-Baptiste Mouret / Mandor\n&gt; &lt;mandor@...&gt; wrote:\n&gt;&gt;\n&gt;&gt; From: &quot;shaflidason&quot; &lt;styrmis@...&gt;\n&gt;\n&gt;&gt; &gt; &lt;shanemcdonaldryan@&gt; wrote:\n&gt;&gt; &gt; &gt;\n&gt;&gt; &gt; &gt; Has anyone ever compared the performance of grammatical evolution to\n&gt;&gt; &gt; &gt; neuro evolution? Perhaps on the standard problems\n&gt; pole-balancing, xor.\n&gt;&gt; &gt; &gt;\n&gt;&gt; &gt; &gt; It seems quite promising and I can post my results when they are\n&gt; done.\n&gt;&gt; &gt; &gt; But I wanted to see if anyone else has looked into it before I do.\n&gt;&gt; &gt; &gt; I&#39;ve invested quite a bit of enery in NE but GE looks very\n&gt; promising,\n&gt;&gt; &gt; &gt; and I like the fact that it produces actual code rather than a black\n&gt;&gt; &gt; &gt; box. Which makes it more appropriate for some domains. ie commercial\n&gt;&gt; &gt; &gt; video games.\n&gt;&gt;\n&gt;&gt; I did some comparaisons of NEAT with regards to our attribute grammar\n&gt;&gt; based encoding (this is not grammatical evolution but it&#39;s related). I\n&gt;&gt; used a pole-balancing task and a task in which a multi-dof robotic arm\n&gt;&gt; had to move to defined positions. Our system performed a bit better\n&gt;&gt; than NEAT on these tasks but NEAT could probably be tweaked\n&gt;&gt; more. While I&#39;m very happy with the theoritical features of the\n&gt;&gt; encodings (it&#39;s hierarchical, modular and formalized), more benchmarks\n&gt;&gt; will be needed to draw any final conclusion about the &quot;efficiency&quot;\n&gt;&gt; (btw, Ken previously highlighted the problems with benchmarks such as\n&gt;&gt; those).\n&gt;&gt;\n&gt;&gt; See : http://dx.doi.org/10.1007/s12065-008-0015-7\n&gt;&gt;\n&gt;&gt; Best regards,\n&gt;&gt; --\n&gt;&gt; Jean-Baptiste Mouret / Mandor\n&gt;&gt; http://animatlab.lip6.fr/~mouret\n&gt;&gt; tel : (+33) 6 28 35 10 49\n&gt;&gt;\n&gt;\n&gt; \n\n"}}