{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"sNgkFIt9Kh5NxFhFXE_Z3nC_0TbKYYSGgO4vqoTKt7XE6xFfbLvRGVA4jxEhDi2jfZdg6m1kSSKkohnEgJSGuwiy35Oq7wcRvBviAN-E4kln","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Yet another question on combining NEAT with backprop (or another non-genetic","postDate":"1205990600","msgId":3913,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGZyc3NjOCtoOTViQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGFiZGQwOTRlMDgwMzE3MDk1Nnk1ZTcwMjQzM2xhYmQxZDA0YzhlZGE3ZTdiQG1haWwuZ21haWwuY29tPg=="},"prevInTopic":3885,"nextInTopic":0,"prevInTime":3912,"nextInTime":3914,"topicId":3882,"numMessagesInTopic":3,"msgSnippet":"Eric, I believe that there was work done on this issue in the 90s with neuroevolution systems that came before NEAT.  I believe that if you refer to Xin Yao s","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 73841 invoked from network); 20 Mar 2008 05:23:22 -0000\r\nX-Received: from unknown (66.218.67.96)\n  by m48.grp.scd.yahoo.com with QMQP; 20 Mar 2008 05:23:22 -0000\r\nX-Received: from unknown (HELO n14b.bullet.sp1.yahoo.com) (69.147.64.116)\n  by mta17.grp.scd.yahoo.com with SMTP; 20 Mar 2008 05:23:22 -0000\r\nX-Received: from [216.252.122.219] by n14.bullet.sp1.yahoo.com with NNFMP; 20 Mar 2008 05:23:22 -0000\r\nX-Received: from [66.218.69.1] by t4.bullet.sp1.yahoo.com with NNFMP; 20 Mar 2008 05:23:22 -0000\r\nX-Received: from [66.218.66.76] by t1.bullet.scd.yahoo.com with NNFMP; 20 Mar 2008 05:23:22 -0000\r\nDate: Thu, 20 Mar 2008 05:23:20 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;frssc8+h95b@...&gt;\r\nIn-Reply-To: &lt;abdd094e0803170956y5e702433labd1d04c8eda7e7b@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: Re: Yet another question on combining NEAT with backprop (or another non-genetic\r\nX-Yahoo-Group-Post: member; u=54567749; y=_Eo4DLYGQvYwDDmGiyBLx4d4Xmec69wY-lr7uEjmX6g_4B-tBYe9\r\nX-Yahoo-Profile: kenstanley01\r\n\r\nEric,\n\nI believe that there was work done on this issue in the 90s with\nneu=\r\nroevolution systems that came before NEAT.  I believe that if you\nrefer to =\r\nXin Yao&#39;s big 1999 survery, &quot;Evolving Neural Networks,&quot; you\nwill find menti=\r\non of papers that explored this idea.  In fact, it\nmakes a lot of sense.  I=\r\n think there may have been a paper by Pujol\nand Poli on it, but I could be =\r\nremembering wrong.  In any case, Xin\nYao&#39;s survey likely has the right refe=\r\nrences.\n\nken\n\n--- In neat@yahoogroups.com, &quot;Eric Mohlenhoff&quot; &lt;eric.mohlenho=\r\nff@...&gt;\nwrote:\n&gt;\n&gt; Hey all,\n&gt; \n&gt; Long-time reader, first time writer here.\n=\r\n&gt; \n&gt; The recent discussion on using NEAT with backprop inspired me to ask\n&gt;=\r\n if anyone has done or knows of any research on a topic similar to the\n&gt; fo=\r\nllowing:\n&gt; \n&gt; The idea is to use NEAT for developing the topology of a give=\r\nn\n&gt; network, and at a later time complete the training with another\n&gt; non-g=\r\nenetic training algorithm such as gradient descent, etc. The\n&gt; &#39;NEAT&#39; phase=\r\n of the network &#39;training&#39; should try to develop a network\n&gt; topology that =\r\nis _conducive_ to being trained (ideally, quickly) using\n&gt; a non-genetic al=\r\ngorithm on a limited subset of the domain of a given\n&gt; class of problem. In=\r\n other words, has anyone ever tried integrating a\n&gt; backprop algorithm into=\r\n the fitness function of a given NEAT\n&gt; implementation, with the intention =\r\nof not merging the weight changes\n&gt; made by said backprop algorithm back in=\r\nto the population, but rather\n&gt; only factoring information relating to how =\r\nwell/quickly that network\n&gt; could be trained using backprop (against data s=\r\net(s) from the problem\n&gt; class, on the limited domain desired) into the fit=\r\nness score.\n&gt; \n&gt; If you need a reason for why this may be useful, consider =\r\nthe\n&gt; following scenario. An embedded system with limited memory (making GA=\r\n\n&gt; somewhat prohibitive), in which the network   (including topology) is\n&gt; =\r\nimplemented in hardware (making GA practically _impossible_), but can\n&gt; use=\r\n fast dedicated multipliers for algorithms such as gradient\n&gt; descent. One =\r\ndesires to fine-tune this network to data sets\n&gt; encountered by the system =\r\nin the field, hopefully by using a backprop\n&gt; algorithm that can be impleme=\r\nnted (mostly) in hardware. It would be\n&gt; desirable to run a GA in the lab p=\r\nrior to deployment to determine the\n&gt; best topology to implement in hardwar=\r\ne (if possible). The constraints\n&gt; of the data/problem domain are known pri=\r\nor to deployment in the field.\n&gt; \n&gt; If anyone knows of any research done on=\r\n this or similar, please let me\n&gt; know. I&#39;ve been doing some research and h=\r\nave found some information\n&gt; about applying gradient descent to NEAT-genera=\r\nted topologies, but\n&gt; never anything about developing a topology that was a=\r\nctually\n&gt; _conducive_ to being trained efficiently with a backprop algorith=\r\nm.\n&gt; \n&gt; Eric\n&gt;\n\n\n\n"}}