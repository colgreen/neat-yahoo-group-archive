{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":7192225,"authorName":"Ian Badcoe","from":"Ian Badcoe &lt;ian_badcoe@...&gt;","profile":"ian_badcoe","replyTo":"LIST","senderId":"VRT_VRvZmpyRiYCCYJ6L7YdkloePb4A88k7DpmfS3e1TJUqebG2h_Q-w7gGLfg3qorW5aoKL6_Rgu9VFrqSEI2a1rvvJH6SNoHQ","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] Paper on evolving modular neural networks","postDate":"1081260565","msgId":626,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDUuMi4xLjEuMC4yMDA0MDQwNjEyMjE0MS4wMWJmNmVlOEBwb3AubWFpbC55YWhvby5jby51az4=","inReplyToHeader":"PFBpbmUuTE5YLjQuNTguMDQwNDA1MTQwNzE1MC4zMjU2MkBvcmFuZ2UtcGVrb2UuY3MudXRleGFzLmUgZHU+","referencesHeader":"PDUuMi4xLjEuMC4yMDA0MDQwNTExMTkxMy4wMTgzNmVhOEBwb3AubWFpbC55YWhvby5jby51az4gPDUuMi4xLjEuMC4yMDA0MDQwNTExMTkxMy4wMTgzNmVhOEBwb3AubWFpbC55YWhvby5jby51az4="},"prevInTopic":613,"nextInTopic":640,"prevInTime":625,"nextInTime":627,"topicId":535,"numMessagesInTopic":47,"msgSnippet":"Hi Joseph, ... Really?  I would have thought it easier, just because of the which stones are mine angle. ... I don t see how it can distinguish its own","rawEmail":"Return-Path: &lt;ian_badcoe@...&gt;\r\nX-Sender: ian_badcoe@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 81566 invoked from network); 6 Apr 2004 14:09:30 -0000\r\nReceived: from unknown (66.218.66.172)\n  by m11.grp.scd.yahoo.com with QMQP; 6 Apr 2004 14:09:30 -0000\r\nReceived: from unknown (HELO smtp005.mail.ukl.yahoo.com) (217.12.11.36)\n  by mta4.grp.scd.yahoo.com with SMTP; 6 Apr 2004 14:09:30 -0000\r\nReceived: from unknown (HELO xp-410.yahoo.co.uk) (ian?badcoe@212.159.73.108 with login)\n  by smtp005.mail.ukl.yahoo.com with SMTP; 6 Apr 2004 14:09:27 -0000\r\nMessage-Id: &lt;5.2.1.1.0.20040406122141.01bf6ee8@...&gt;\r\nX-Sender: ian_badcoe@...\r\nX-Mailer: QUALCOMM Windows Eudora Version 5.2.1\r\nDate: Tue, 06 Apr 2004 15:09:25 +0100\r\nTo: neat@yahoogroups.com\r\nIn-Reply-To: &lt;Pine.LNX.4.58.0404051407150.32562@....e\n du&gt;\r\nReferences: &lt;5.2.1.1.0.20040405111913.01836ea8@...&gt;\n &lt;5.2.1.1.0.20040405111913.01836ea8@...&gt;\r\nMime-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;us-ascii&quot;; format=flowed\r\nX-eGroups-Remote-IP: 217.12.11.36\r\nFrom: Ian Badcoe &lt;ian_badcoe@...&gt;\r\nSubject: Re: [neat] Paper on evolving modular neural networks\r\nX-Yahoo-Group-Post: member; u=7192225\r\nX-Yahoo-Profile: ian_badcoe\r\n\r\nHi Joseph,\n\n&gt;I also tried the method you describe &quot;5 highest outputs&quot; which tends to\n&gt;speed up evaluation quite a bit, although it is a more difficult problem\n&gt;to learn.\n\nReally?  I would have thought it easier, just because of the &quot;which stones \nare mine&quot; angle.\n\n&gt; &gt;       Presumably placed stones appeared as 1&#39;s or -1&#39;s in the input just\n&gt; &gt; like the setup ones?  Since the latter can be used for scoring, but the\n&gt; &gt; former not, how did the networks distinguish between them?  (obviously\n&gt; &gt; this would only matter if the stones were placed one at a time)\n&gt;\n&gt;Yes, part of the task is the network learning to distinguish its own\n&gt;stones from other stones. This is compounded in the modular version of the\n&gt;game, since this depends on locality.\n\nI don&#39;t see how it can distinguish its own stones, because (i) initial \nstones may be anywhere, (ii) initial and own stones give same input, (iii) \nno memory?\n\n&gt;The mutation heuristics are given some sense of locality, i.e. if the\n&gt;problem has a square phenotype, then mutations moving (1,1)-&gt;(1,2) should\n&gt;occur equally as likely as (1,1)-&gt;(2,1).\n\nAnd both more likely than (1,1)-&gt;(4,5) ?\n\n&gt;One area of research I think\n&gt;would be interesting is to see how this kind of knowledge could be learned\n&gt;by the system, instead of added a priori. This would be analogous to\n&gt;learning the symmetries inherent in the problem, however, which turns out\n&gt;to be a difficult task.\n\nI&#39;m very interested in this.  Not so much from the ANN point of view, but \nif we step back and talk about general principles and what&#39;s known about \nbiological NN, then I&#39;ve never seen a theory for how such a &quot;symmetry&quot; \nwould be encoded/exploited.\n\nFor example, if I have learned the board geometry, then I can play any game \non the board and exploit the geometry in the same way.\n\nThus, in my head, the existing geometry/symmetry &quot;module&quot; (GSMod) must be \nable to communicate with an evolving module (NewMod) for a new game, and it \ndoesn&#39;t &quot;feel&quot; as if any learning is required for that.  e.g. I instantly \nunderstand that your toy-game is rotation and translation invariant (with \npossible special cases along the centre line).  It&#39;s not part of the \nlearning process, I already &quot;know&quot; that aspect.\n\nWhich to me suggests that somehow, the lessons learned in GSMod can be \napplied remotely to NewMod.\n\nI don&#39;t know any NN technique which can do that, vague ideas:\n\n1) GSMod iteratively preprocesses incoming game data into each possible \nrotation/translation, passes it to NewMod, and performs the reverse \ntransform on any positional output data\n\n2) GSMod expands input data into a representation which contains every \nsymmetric transform written out explicitly, NewMod learns from that\n\n3) GSMod expands input data into purely relative positions and NewMod \nlearns form those, thus:\n\nooo\noxy\nozo\n\n         might translate to:\n                 - there is an x\n                 - there is a y\n                 - there is a z\n                 - x adjoins y\n                 - x adjoins z\n                 - y does not adjoin z\n                 - y-x-z forms a right angle\n                 - ...\n\n         and probably as much further information about the empty space...\n\nAll of these have the weakness that they sort of imply the existance of \nmultiplexers and databuses in my head.  e.g. in order that GSMod can \nprocess data before NewMod sees it, first the data has to be routed to \nGSMod, then transferred into GSMod, then the results routed to NewMod, \ntransferred to it...\n\nAnd whilst one can build data paths from neurones, they&#39;re not general \nbuses, because they have to be designed to reflect the shape of the data \ncoming out of the source (e.g. correct number of &quot;channels&quot; and correct \nmapping between outputs and inputs).  And I can&#39;t see GSMod being much use \nif there&#39;s a choice in how its outputs map onto another module&#39;s inputs, as \nif the bus &quot;learned&quot; sometihng it would undo any existing learning in NewMod.\n\nI often speculate whether brain-modules have &quot;general purpose connectors&quot;, \nwho&#39;s only properties are that (i) they contain a lot of channels, and (ii) \nthat they contain the same channels (at the same &quot;positions&quot;) on every \nmodule.  Think of them as 100x100 pin plugs and sockets.  Initially, no pin \nhas any meaning, and modules use an arbitrary selection of pins for input \nand output.  When modules need to learn to work together, they learn \none-anothers input and output pin patterns.  If two modules are variations \non the same thing, then they frequently connect to the same other modules, \nand their pin patterns are forced to converge.  Thus a certain pattern of \npin usage becomes a de facto standard for a particular type of \ninformation.  I regard this as a strong example of extraction of semantics, \ncertain pins have come to &quot;mean&quot; certain types of data, and they only did \nthat because it was a type of data that modules needed to exchange...\n\nAlternatively, if biological NN&#39;s commonly encode information in the time \nor frequency domains, then it could less a &quot;pattern of pins&quot; and more a \nsingle pin, or even a part of the clock-cycle or frequency spectrum*.\n\n(*wild analogy and vague hand waving)\n\n&gt;Yes, there is no such mutation &quot;move module A right two spaces&quot; ... that\n&gt;seems to be the primary critique of the method, and I agree that it would\n&gt;be a promising research venue. However I&#39;m more interested in how this\n&gt;could be accomplished more generally, using a developmental system, then\n&gt;hand coded by the programmer.\n&gt;\n&gt;My main defense for allowing modules to be bound in different internal\n&gt;configurations is that it allows for the /possibility/ of discovering\n&gt;symmetries that aren&#39;t obvious to humans. I agree that this lack of\n&gt;constraint makes the search space enormous.\n\nIt&#39;s not that the space is enormous, I would not necessarily reduce it, but \nthe connectivity of the search-space does not put &quot;module-X at (1, 1)&quot; and \n&quot;module-X at (1, 3)&quot; close to one another.  Thus, there is a high barrier \nbetween creating a useful module and successfully deploying it somewhere \nelse.  If we had a system which could &quot;learn the symmetries&quot;, then \npresumably the main feature of the symmetry module would be some sort of \nability to &quot;translate&quot; other modules elsewhere.  But see my discussion \nabove, as I really have no idea how an ANN might do that -- in biological \nNN, it might even depend on some feature not implemented in ANN.\n\nI wouldn&#39;t call this a severe critique, but I do immediately wonder how \nmuch a &quot;move module on board&quot; mutation would change the results....\n\n&gt; &gt;       Did you consider letting the framework specify that one network&#39;s\n&gt; &gt; input be taken from another&#39;s outputs?\n&gt;\n&gt;Yeah. Actually my original design was much more flexible, allowing for\n&gt;module hierarchies, and more complex configurations. It seemed too\n&gt;arbitrary though, having to specify exactly how module interactions had to\n&gt;take place.\n\nI didn&#39;t mean you specifying the interactions, I meant letting it evolve as \npart of the framework.  E.g. the evolution of the frameworks would be very \nakin to NEAT evolution of a single network, with the exception that each \nconnection needs to specify which output(s) to connect to which input(s).\n\n         Ian Badcoe\n\n\n\nLiving@Home - Open Source Evolving Organisms - \nhttp://livingathome.sourceforge.net/\n\n\n\n\n"}}