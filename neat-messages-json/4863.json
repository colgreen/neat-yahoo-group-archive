{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":413744767,"authorName":"spoonsx21","from":"&quot;spoonsx21&quot; &lt;spoonsx21@...&gt;","profile":"spoonsx21","replyTo":"LIST","senderId":"yNoV07w0qBQQz_AQXFi7vpn0sXx4D4NXns4ZWpzS4cJD6yoTnWOvAFk7Z4tL2ig1vWJAfX4-Tz4MTELH4tzZo_8ae3QohfrC","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Substrate Evolution","postDate":"1253634149","msgId":4863,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGg5YXI5NSt0aXY3QGVHcm91cHMuY29tPg==","inReplyToHeader":"PGg5MThsNytiYWthQGVHcm91cHMuY29tPg=="},"prevInTopic":4854,"nextInTopic":4864,"prevInTime":4862,"nextInTime":4864,"topicId":4848,"numMessagesInTopic":5,"msgSnippet":"Hello Peter, I absolutely agree with you that Neural Darwinism isn t the ultimate answer, nor is it a complete picture. I thought of it as an affirmation of","rawEmail":"Return-Path: &lt;spoonsx21@...&gt;\r\nX-Sender: spoonsx21@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 88260 invoked from network); 22 Sep 2009 15:42:45 -0000\r\nX-Received: from unknown (98.137.34.45)\n  by m4.grp.re1.yahoo.com with QMQP; 22 Sep 2009 15:42:45 -0000\r\nX-Received: from unknown (HELO n44d.bullet.mail.sp1.yahoo.com) (66.163.169.158)\n  by mta2.grp.sp2.yahoo.com with SMTP; 22 Sep 2009 15:42:44 -0000\r\nX-Received: from [69.147.65.148] by n44.bullet.mail.sp1.yahoo.com with NNFMP; 22 Sep 2009 15:42:29 -0000\r\nX-Received: from [98.137.35.12] by t11.bullet.mail.sp1.yahoo.com with NNFMP; 22 Sep 2009 15:42:29 -0000\r\nDate: Tue, 22 Sep 2009 15:42:29 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;h9ar95+tiv7@...&gt;\r\nIn-Reply-To: &lt;h918l7+baka@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;spoonsx21&quot; &lt;spoonsx21@...&gt;\r\nSubject: Re: Substrate Evolution\r\nX-Yahoo-Group-Post: member; u=413744767; y=DBXCaCD1sicIZI850oDEKWkj51GzmcGWIDUmETnhT0HWTPeg\r\nX-Yahoo-Profile: spoonsx21\r\n\r\nHello Peter,\n\nI absolutely agree with you that Neural Darwinism isn&#39;t the u=\r\nltimate answer, nor is it a complete picture. I thought of it as an affirma=\r\ntion of the idea that life learning and brain structure are intertwined.\n\nI=\r\n do have to briefly disagree that the pruning of neurons is a biological pe=\r\nrformance issue. Your explanation accounts for why neurons make so many con=\r\nnections, but not for the fact that they make specific connections between =\r\nareas of the brain. It also leaves out an explanation for neuronal growth s=\r\npurts. Of course I can&#39;t point you to anything conclusive right now, so tha=\r\nt&#39;s better off left as my educated guess, not fact. \n\nBut that&#39;s neither he=\r\nre nor there. I think it is of critical importance that the substrate confi=\r\nguration isn&#39;t static for the entire experiment. It would seem that the que=\r\nstion of &quot;how many nodes and where&quot; is not yours to answer. In fact, if you=\r\n were to examine even the smallest of mammals, answering the question of ho=\r\nw many neurons and where is a computationally infeasible task. \n\nGoing back=\r\n to what Professor Stanley said, if you implement TNGS ideas within HyperNE=\r\nAT, it still lacks information on what topography you start with and how th=\r\ne rules are originally created that dictate which connections survive and w=\r\nhich are eliminated. But my argument would be that if CPPNs in HyperNEAT ar=\r\ne in some ways an abstraction of DNA-dictated brain structure, then couldn&#39;=\r\nt you have an abstraction of the rule-set that dictates neuronal placement =\r\nand movement? The CPPNS represent a pattern in 2n-D space (where n is the d=\r\nimension of your substrate, if I remember correctly). Then what&#39;s to stop t=\r\nhe use of a CPPN to describe the substrate? Picbreeder demonstrates the CPP=\r\nNs abilitiy to paint a picture, is there a downside to using it with the su=\r\nbstrate (other than increasing the dimension of the search space)?\n\nI still=\r\n think that there should be some mechanism for modifying the structure with=\r\nin the lifetime of the individual being evaluated, however, that adds anoth=\r\ner layer of complexity. \n\nPerhaps a better question might be, what have bee=\r\nn your thoughts on evolving the substrate? I&#39;m putting this out there as th=\r\ne equivalent of a thinking out loud kind of philosophy, but perhaps you hav=\r\ne come up with some suitable directions of your own. \n\nThanks for the reply=\r\n,\n-Paul\n\n\n--- In neat@yahoogroups.com, &quot;petar_chervenski&quot; &lt;petar_chervenski=\r\n@...&gt; wrote:\n&gt;\n&gt; Hi Paul, \n&gt; \n&gt; This is a second message, because the first=\r\n one was actually lost. My Yahoo e-mail was &quot;bouncing&quot; and stuff. So the gr=\r\noup automatically &quot;kicked&quot; me. Nevermind, \n&gt; \n&gt; I am researching the evolut=\r\nion of CPPNs, and in particular HyperNEAT substrate configurations, a lot o=\r\nf time. I still haven&#39;t found the solution to it. The problem is really kin=\r\nd of simple one: &quot;how many nodes, and where?&quot; \n&gt; \n&gt; It is usually a good id=\r\nea to see how biology does it, but the theory you point out is such that de=\r\nals with phenotypes during their lifetime. I mean, yes, the brain is born w=\r\nith lots of neurons and then some process gradually filters them out, makin=\r\ng more connections meanwhile. But I think this process is because of &quot;biolo=\r\ngical performance&quot; issues. Neural tissue requires lots of oxygen. More neur=\r\nons - more oxygen & food required. It is better to have few neurons computi=\r\nng lots of stuff, requiring less oxygen, as the whole body grows in size. S=\r\no this can kind of explain why neurons prefer to make many many connections=\r\n, no matter how much they are. \n&gt; \n&gt; But I can&#39;t see any connection to Hype=\r\nrNEAT at all. To evolve a substrate means to have all the nodes (count of n=\r\nodes and their placement) depend on evolutionary dynamics. Like for example=\r\n, if the fittest individual can evolve faster using a circular substrate co=\r\nnfiguration than such that displays a square config, the algorithm should m=\r\nake sure it has been selected for the next generation. And if the same circ=\r\nular-substrate individual performs good with 16 nodes and another one perfo=\r\nrms equally well with 64, the smaller one should be chosen as better. \n&gt; \n&gt;=\r\n The count of nodes is a big problem. Even the human brain, the chimp&#39;s bra=\r\nin, the rat&#39;s brain, all started from one single cell. So I think it is mor=\r\ne fundamental to understand biological development than the post-developmen=\r\nt of the human brain after it is born. We can actually think of the entire =\r\nlife cycle as one pattern in 4D space. CPPNs can create any pattern in any =\r\nspace. \n&gt; \n&gt; Peter\n&gt; \n&gt; --- In neat@yahoogroups.com, &quot;spoonsx21&quot; &lt;spoonsx21=\r\n@&gt; wrote:\n&gt; &gt;\n&gt; &gt; Hello everyone,\n&gt; &gt; \n&gt; &gt; I&#39;m new to the Neat group. My na=\r\nme is Paul, I&#39;m an undergraduate interested in evolutionary computation, an=\r\nd obviously HyperNEAT/ NEAT. I have been thinking a lot about substrate evo=\r\nlution, and its parallel in biology. One of the fundamental questions I&#39;ve =\r\nbeen trying to answer is, how did brain topology evolve? And it is clearly =\r\nfundamental to the problem of substrate evolution. Our sensory information =\r\nis pretty well segregated in the brain. For instance, visual areas are brok=\r\nen down into separate processing locations (V1-V5), each area dealing with =\r\ndifferent aspects of visualization like object movement, or pattern recogni=\r\ntion. There are plenty of studies and papers on current brain topology, but=\r\n it&#39;s difficult to find ideas on how brain topology evolved. \n&gt; &gt; \n&gt; &gt; Howe=\r\nver, I did stumble upon one theory I enjoyed. It was Gerald Edelman&#39;s theor=\r\ny on what he calls neural Darwinism, or the theory of neuronal group select=\r\nion (TNGS). And while I attempt to truly get my head around the theory, wha=\r\nt I have drawn from his theory seems applicable to HyperNEAT&#39;s extensions. =\r\nThe theory states that within the brain there is first a process of selecti=\r\non in creating the brain&#39;s anatomy, with small epigenetic changes occurring=\r\n in development (Here you can imagine that the anatomy in HyperNEAT is our =\r\nsubstrate). Then in the postnatal stage, there is a time of neuron selectio=\r\nn, where some synaptic connections are strengthened, and others simply disa=\r\nppear altogether through neuron death (something with little or no parallel=\r\n in HyperNEAT). He gives as an example a chicken, which is born with 20,000=\r\n neurons. At the adult stage, the chicken has 12,000 neurons, keeping only =\r\n60% of the original neurons. There is in fact much more to this theory, but=\r\n I am in no way able to communicate it effectively. I encourage you to read=\r\n any of his papers of books (or a quick Wikipedia scan). My interest was in=\r\n HyperNEAT&#39;s possible abstraction of the idea. \n&gt; &gt; \n&gt; &gt; Something HyperNEA=\r\nT has yet to incorporate is the idea of intra-life learning. I read a few o=\r\nf the other posts, and I think this might be in some ways related to the Hy=\r\nbrID conversation about irregularities. HybrID attempts to make up for this=\r\n lack of intra-life learning through the use of NEAT. At some point in the =\r\nalgorithm, HyperNEAT is stopped in favor of using NEAT to more accurately p=\r\ninpoint irregularities. But this is not really the &quot;job&quot; of evolution, rath=\r\ner this is an intralife task. Evolution can provide the framework (i.e. a s=\r\npecies), but the more fit individual is able to adapt to the irregularities=\r\n of life (i.e. through intralife learning). \n&gt; &gt; \n&gt; &gt; The point I&#39;m laborio=\r\nusly trying to bring you to is that I believe substrate evolution and intra=\r\nlife learning are related. And perhaps you could take out two birds with on=\r\ne stone using some ideas from neural Darwinism. My idea is a bit crude, and=\r\n the details aren&#39;t ironed out, but these were some thoughts. Speaking stri=\r\nctly about HyperNEAT, what I thought would be helpful would be to generate =\r\nmore points then necessary within the substrate. This would happen during m=\r\nutation/crossover. Perhaps duplicating inputs in more than one place on the=\r\n substrate (within a certain distance from each other), and adding addition=\r\nal layers in the hidden nodes (if they exist). Through the evaluation of th=\r\ne new population, essentially neuron&#39;s that fire together wire together (as=\r\n Edelman loves to say in his books) and weights can be modified slowly duri=\r\nng the evaluation, additionally allowing for the removal of less fit neuron=\r\ns. What&#39;s left is an individual whose substrate isn&#39;t strictly identical to=\r\n the original, and weight connections that might slightly differ from the o=\r\nriginal CPPN. \n&gt; &gt; \n&gt; &gt; Now problems. There are a host of them, and this is=\r\n what currently makes this idea a bit clunky and inelegant. At a very basic=\r\n level, I&#39;m still uncertain how one could reconcile the difference between =\r\nthe resulting substrate and the original. Also the resulting CPPN and the o=\r\nriginal. Also scaling issues, when trying to modify neuron connections, mak=\r\ning changes to 1 connection weight at a time makes this impossible when exa=\r\nmining a neural net of 9 million connections. This makes it difficult to co=\r\nnvince anyone that this is the direction that HyperNEAT should head in. Rat=\r\nher, it was my idea to ping some ideas off of you guys. And I do believe th=\r\nat the solutions to substrate evolution and intralife learning are linked, =\r\nwhether or not this is the best way to do it (most likely not). \n&gt; &gt; \n&gt; &gt; L=\r\net me know what you guys think,\n&gt; &gt; -Paul\n&gt; &gt;\n&gt;\n\n\n\n"}}