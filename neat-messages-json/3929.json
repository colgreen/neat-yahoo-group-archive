{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"VhgrCXLLbaSZiubPx5gS10HFW7AlElZvQnuFciOdX6S94h7xC_ikt4ogMhOI8W1hdA1CjdD0Obj3EtmPIb7EAcjghi3mRPnjChViKE5AII8y","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Combining evolution with evolution (request for references)","postDate":"1207177019","msgId":3929,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGZ0MTJ2citwNzNrQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDIzMGU0NjNlMDgwNDAyMTAxNWk3MmNmZTkyNWg1ZGVjMzRmMGFhNDhjZTFhQG1haWwuZ21haWwuY29tPg=="},"prevInTopic":3927,"nextInTopic":3930,"prevInTime":3928,"nextInTime":3930,"topicId":3922,"numMessagesInTopic":14,"msgSnippet":"Julian, here are a couple other questions/comments: I was curious why you chose to call it memetic?  Usually I think of memetic as meaning taking the result","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 34664 invoked from network); 2 Apr 2008 22:57:07 -0000\r\nX-Received: from unknown (66.218.67.95)\n  by m48.grp.scd.yahoo.com with QMQP; 2 Apr 2008 22:57:07 -0000\r\nX-Received: from unknown (HELO n50c.bullet.mail.sp1.yahoo.com) (66.163.168.184)\n  by mta16.grp.scd.yahoo.com with SMTP; 2 Apr 2008 22:57:00 -0000\r\nX-Received: from [216.252.122.216] by n50.bullet.mail.sp1.yahoo.com with NNFMP; 02 Apr 2008 22:57:00 -0000\r\nX-Received: from [209.73.164.86] by t1.bullet.sp1.yahoo.com with NNFMP; 02 Apr 2008 22:57:00 -0000\r\nX-Received: from [66.218.66.88] by t8.bullet.scd.yahoo.com with NNFMP; 02 Apr 2008 22:57:00 -0000\r\nDate: Wed, 02 Apr 2008 22:56:59 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;ft12vr+p73k@...&gt;\r\nIn-Reply-To: &lt;230e463e0804021015i72cfe925h5dec34f0aa48ce1a@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: Re: Combining evolution with evolution (request for references)\r\nX-Yahoo-Group-Post: member; u=54567749; y=ybxMF9qo6ADA8cKFfQ9JdjOtffpe0gTX1WetqxQ5HSQ-6GLL38ic\r\nX-Yahoo-Profile: kenstanley01\r\n\r\nJulian, here are a couple other questions/comments:\n\nI was curious why you =\r\nchose to call it &quot;memetic?&quot;  Usually I think \nof memetic as meaning taking =\r\nthe result of non-evolutionary learning \nand incorporating it back into the=\r\n genome for further evolution.  I \ncan see how you can conceptually think o=\r\nf the two time scales as \nfalling into such a frame work in an abstract sen=\r\nse, but do you \nworry that the term might be confusing since both timescale=\r\ns are in \nfact evolution?  Or perhaps one could say that neither are evolut=\r\nion \nsince neither really uses a population.\n\nAbout the topology search:  I=\r\n&#39;d be interested in whether the kinds \nof topologies that are possible affe=\r\nct the impact of evolving on \ndifferent timescales.  For example, your curr=\r\nent version chooses \nconnections inside a standard MLP, and there may be so=\r\nmething about \nthe MLP architecture that makes it necessary or perhaps bett=\r\ner to \nvary timescales (i.e. the MLP is a big summation of all its hidden \n=\r\nnode outputs).  Imagine for example a cascade architecture where one \nthing=\r\n depends entirely on another (as in cascade correlation).  I \nwonder if in =\r\nthat case the results would come out different?  Note \nthat I really have n=\r\no idea whether they would or not, but it is just \nsomething I am curious ab=\r\nout.\n\nIt&#39;s a thought-provoking work overall.\n\nken\n\n--- In neat@yahoogroups.=\r\ncom, &quot;Julian Togelius&quot; &lt;julian@...&gt; wrote:\n&gt;\n&gt; Thanks for your comments, JT=\r\n and Ken!\n&gt; \n&gt; JT: Yes, NEAT contains a similar idea, as part of a rather c=\r\nomplex\n&gt; algorithm. We&#39;re trying to boil it down to the bare bones though, =\r\n\nand\n&gt; analyze what effect using different time scales for weights and \ntop=\r\nologies\n&gt; (&quot;memetic learning&quot;) might have in itself. Some big differences t=\r\no \nNEAT are\n&gt; that the memetic climber has a &quot;population&quot; of one individual=\r\n, and \ncan be\n&gt; implemented in a few lines of code. NB that I am sure NEAT =\r\nis the \nbetter\n&gt; overall neuroevolution algorithm, right now we are not try=\r\ning to \ncompete in\n&gt; that respect...\n&gt; \n&gt; Ken: You&#39;re right that people don=\r\n&#39;t even bother testing hill-\nclimbers, which\n&gt; I think is a shame. They do =\r\nget stuck quite a lot, but for some \nproblems\n&gt; (e.g. evolving DFAs) a rand=\r\nom-restart hill-climber reaches the \noptimum\n&gt; faster than most population-=\r\nbased algorithms. (Of, course for \nproblems with\n&gt; many local optima popula=\r\ntion-based EAs are typically preferable.)\n&gt; \n&gt; The current topology represe=\r\nntation is simply an array of bits \nthat decide\n&gt; which of the connections =\r\nin a standard MLP are to be used, and \nwhich are to\n&gt; be ignored, when prop=\r\nagating activations. I&#39;d certainly like to \ntry the\n&gt; algorithm with a more=\r\n open-ended topology representation at some \npoint, but\n&gt; for now we&#39;re foc=\r\nused on keeping the algorithm as simple as \npossible, so as\n&gt; to best explo=\r\nre the basic idea of interleaving global and local \nsearch to\n&gt; alleviate t=\r\nhe destructiveness of topology mutations.\n&gt; \n&gt; There are some very simple e=\r\nxtensions of the basic principle to\n&gt; population-based search we are curren=\r\ntly trying out; they involve \njust\n&gt; replacing the hill-climbing in weight =\r\nspace with a population-based\n&gt; algorithm, such as an evolution strategy. A=\r\nctually, the local \nsearch\n&gt; algorithm could be almost any global continuou=\r\ns optimization \nalgorithm - not\n&gt; necessarily an evolutionary one. There se=\r\nems to be quite a few \npeople who\n&gt; have tried combining topology search wi=\r\nth backpropagation, but \noddly enough\n&gt; nobody seems to have tried any opti=\r\nmization algorithm, not even \nthe humble\n&gt; hill-climber...\n&gt; \n&gt; Julian\n&gt; \n&gt;=\r\n On 02/04/2008, Kenneth Stanley &lt;kstanley@...&gt; wrote:\n&gt; &gt;\n&gt; &gt;   Julian, tha=\r\nnks for sharing your recent work. I hadn&#39;t been \naware of\n&gt; &gt; that paper an=\r\nd it is indeed an interesting study.\n&gt; &gt;\n&gt; &gt; I&#39;m guessing that it probably =\r\nhas not been done before exactly\n&gt; &gt; because it is so simple. In particular=\r\n, running evolution with a\n&gt; &gt; population of one is not very popular these =\r\ndays, and so people\n&gt; &gt; probably don&#39;t even bother considering it.\n&gt; &gt;\n&gt; &gt; =\r\nOf course, as your paper discusses, it then leads to the \nquestion of\n&gt; &gt; h=\r\now you might expand it into a population-based approach, and a \nlot\n&gt; &gt; of =\r\nthe existing methods might embody ways that could be done to\n&gt; &gt; various de=\r\ngrees. NEAT seems to be a possible such expansion in \nthe\n&gt; &gt; sense that it=\r\n has multiple species of topologies and roughly \nevolves\n&gt; &gt; weights and to=\r\npologies on two different time scales with its \nusual\n&gt; &gt; parameter setting=\r\ns.\n&gt; &gt;\n&gt; &gt; In any case, your paper an elegant demonstration of how much you=\r\n \ncan\n&gt; &gt; learn from a simple concept.\n&gt; &gt;\n&gt; &gt; One question I have is what =\r\nis meant when you mention things\n&gt; &gt; like &quot;all connections on?&quot; It sounds l=\r\nike the topology is chosen\n&gt; &gt; from some finite constrained set of topologi=\r\nes. Is that true? Or\n&gt; &gt; is any potential topology possible? It makes me wo=\r\nnder whether \nthe\n&gt; &gt; connection set from which you are choosing somehow bi=\r\nases the \nresult\n&gt; &gt; to radically favor the different time scales, though I=\r\n&#39;m not sure\n&gt; &gt; how.\n&gt; &gt;\n&gt; &gt; In general, my intuitive explanation of the ph=\r\nenomenon is that if\n&gt; &gt; you stick with only a single topology it is likely =\r\nto be the \nwrong\n&gt; &gt; one, so you need to look at several to see if they hav=\r\ne \npotential.\n&gt; &gt; Then you ensure steady progress.\n&gt; &gt;\n&gt; &gt; ken\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; =\r\n--- In neat@yahoogroups.com &lt;neat%40yahoogroups.com&gt;, &quot;Julian \nTogelius&quot;\n&gt; =\r\n&gt; &lt;julian@&gt; wrote:\n&gt; &gt; &gt;\n&gt; &gt; &gt; Hi all,\n&gt; &gt; &gt;\n&gt; &gt; &gt; I&#39;d like to connect to t=\r\nhe recent discussion in this group on\n&gt; &gt; &gt; combining neuroevolution with b=\r\nackpropagation, and use it as a\n&gt; &gt; &gt; convenient excuse to expose you to a =\r\nrecent paper of mine. In\n&gt; &gt; &gt; particular, I&#39;d like to ask you if you know =\r\nwho did this \nbefore we\n&gt; &gt; &gt; (me, Faustino Gomez, Juergen Schmidhuber) did=\r\n. Because the\n&gt; &gt; algorithm\n&gt; &gt; &gt; we are presenting in this paper is so sim=\r\nple someone must have\n&gt; &gt; come up\n&gt; &gt; &gt; with it before, it&#39;s just that our =\r\nliterature search has been\n&gt; &gt; &gt; fruitless.\n&gt; &gt; &gt;\n&gt; &gt; &gt; The basic idea is t=\r\no combine hillclimbing (1+1 evolution\n&gt; &gt; strategy) of\n&gt; &gt; &gt; network topolo=\r\ngies, with hillclimbing of network weights, but \nat a\n&gt; &gt; &gt; faster timescal=\r\ne. So after each (&quot;global&quot;) topology mutation, \nwe\n&gt; &gt; &gt; search (&quot;local&quot;) w=\r\neight space for a number of steps (e.g. 50), \nand\n&gt; &gt; &gt; only accepts the gl=\r\nobal mutation if the fitness after local\n&gt; &gt; search is\n&gt; &gt; &gt; higher than be=\r\nfore the global mutation. The reason we do this \nis\n&gt; &gt; that\n&gt; &gt; &gt; global m=\r\nutations are so often disruptive. Yes, this is the same\n&gt; &gt; idea\n&gt; &gt; &gt; as &quot;=\r\ninnovation protection&quot; in NEAT, but here we&#39;re boiling it\n&gt; &gt; down a\n&gt; &gt; &gt; =\r\nbare minimum.\n&gt; &gt; &gt;\n&gt; &gt; &gt; We tried two versions of a moderately difficult c=\r\nontrol task: \none\n&gt; &gt; with\n&gt; &gt; &gt; a set of 7 carefully selected inputs to th=\r\ne neural network, and\n&gt; &gt; one\n&gt; &gt; &gt; with 15 not-so-carefully-selected input=\r\ns to the network. We \nfound\n&gt; &gt; &gt; that, when compared to simple hillclimbin=\r\ng in weight space, or \nto\n&gt; &gt; &gt; climbing weights and topologies at the same=\r\n time, performed\n&gt; &gt; slightly\n&gt; &gt; &gt; better on the small-input version of th=\r\ne problem problem. On \nthe\n&gt; &gt; &gt; big-input version, the difference was dram=\r\natic, in that using\n&gt; &gt; &gt; different timescales was really the only way to s=\r\nolve the \nproblem!\n&gt; &gt; &gt; (Incidentally, learning topology at the &quot;local&quot; le=\r\nvel and \nweights\n&gt; &gt; at\n&gt; &gt; &gt; the &quot;global&quot; level worked just as well.)\n&gt; &gt; =\r\n&gt;\n&gt; &gt; &gt; The paper is here:\n&gt; &gt; &gt; http://julian.togelius.com/Togelius2008Lea=\r\nrning.pdf\n&gt; &gt; &gt;\n&gt; &gt; &gt; Again, this is so obvious that someone must have done=\r\n this\n&gt; &gt; before. The\n&gt; &gt; &gt; literature search only pointed us to attempts t=\r\no combine \ntopology\n&gt; &gt; &gt; evolution with backpropagation, which is related =\r\nbut not the \nsame\n&gt; &gt; &gt; thing. We submitted the paper to WCCI (and had it a=\r\nccepted) in \nthe\n&gt; &gt; &gt; hope that the reviewers would know, but that they di=\r\ndn&#39;t. So \nnow I\n&gt; &gt; &gt; wonder if anyone of you might know. It would be good =\r\nto know \nabout\n&gt; &gt; &gt; this before go on with further experiments...\n&gt; &gt; &gt;\n&gt; =\r\n&gt; &gt; Grateful for any suggestions\n&gt; &gt; &gt; Julian\n&gt; &gt; &gt; --\n&gt; &gt; &gt; Julian Togeliu=\r\ns\n&gt; &gt; &gt; IDSIA\n&gt; &gt; &gt; Galleria 2\n&gt; &gt; &gt; 6928 Manno-Lugano\n&gt; &gt; &gt; Switzerland\n&gt; =\r\n&gt; &gt; julian@\n&gt; &gt; &gt; http://julian.togelius.com\n&gt; &gt; &gt; http://www.idsia.ch/~tog=\r\nelius &lt;http://www.idsia.ch/%7Etogelius&gt;\n&gt; &gt; &gt; +41-764-110679\n&gt; &gt; &gt; +46-705-=\r\n192088\n&gt; &gt; &gt;\n&gt; &gt;\n&gt; &gt;  \n&gt; &gt;\n&gt; \n&gt; \n&gt; \n&gt; -- \n&gt; Julian Togelius\n&gt; IDSIA\n&gt; Galle=\r\nria 2\n&gt; 6928 Manno-Lugano\n&gt; Switzerland\n&gt; julian@...\n&gt; http://julian.togeli=\r\nus.com\n&gt; http://www.idsia.ch/~togelius &lt;http://www.idsia.ch/%7Etogelius&gt;\n&gt; =\r\n+41-764-110679\n&gt; +46-705-192088\n&gt;\n\n\n\n"}}