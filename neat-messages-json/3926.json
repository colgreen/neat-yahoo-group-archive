{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":283334584,"authorName":"petar_chervenski","from":"&quot;petar_chervenski&quot; &lt;petar_chervenski@...&gt;","profile":"petar_chervenski","replyTo":"LIST","senderId":"7sK9iDOIkLy1eguFxu4G2zog7ciY68Bmig-cmMZLrDqNDnqofX1e0lSxoQUPN-JUfXcq1giyObTfS5QmEEu8N-ksSpa3QNrMLqaxcAerSduNRfvFgvk","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Combining evolution with evolution (request for references)","postDate":"1207140838","msgId":3926,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGZzdnZsNithcm1uQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGZzdWRoOStic2ZpQGVHcm91cHMuY29tPg=="},"prevInTopic":3925,"nextInTopic":3927,"prevInTime":3925,"nextInTime":3927,"topicId":3922,"numMessagesInTopic":14,"msgSnippet":"Well actually the NEAT parameters may be variable. For example stagnating species can increase their weight mutation rate (as a last chance to discover a good","rawEmail":"Return-Path: &lt;petar_chervenski@...&gt;\r\nX-Sender: petar_chervenski@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 29579 invoked from network); 2 Apr 2008 12:53:59 -0000\r\nX-Received: from unknown (66.218.67.94)\n  by m42.grp.scd.yahoo.com with QMQP; 2 Apr 2008 12:53:59 -0000\r\nX-Received: from unknown (HELO n19.bullet.mail.re1.yahoo.com) (69.147.102.102)\n  by mta15.grp.scd.yahoo.com with SMTP; 2 Apr 2008 12:53:59 -0000\r\nX-Received: from [68.142.237.87] by n19.bullet.mail.re1.yahoo.com with NNFMP; 02 Apr 2008 12:53:58 -0000\r\nX-Received: from [209.73.164.83] by t3.bullet.re3.yahoo.com with NNFMP; 02 Apr 2008 12:53:58 -0000\r\nX-Received: from [66.218.66.76] by t7.bullet.scd.yahoo.com with NNFMP; 02 Apr 2008 12:53:58 -0000\r\nDate: Wed, 02 Apr 2008 12:53:58 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;fsvvl6+armn@...&gt;\r\nIn-Reply-To: &lt;fsudh9+bsfi@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;petar_chervenski&quot; &lt;petar_chervenski@...&gt;\r\nSubject: Re: Combining evolution with evolution (request for references)\r\nX-Yahoo-Group-Post: member; u=283334584; y=7L49F0R-v6uKSNmVF3fvOp9YOIwLVGTUILirDWMR7Y5oFZGBcITl-J6mWA\r\nX-Yahoo-Profile: petar_chervenski\r\n\r\nWell actually the NEAT parameters may be variable. For example \nstagnating =\r\nspecies can increase their weight mutation rate (as a last \nchance to disco=\r\nver a good point in the search space, given the \ntopology). Or maybe when a=\r\n species is close to a solution, its weight \nmutation power can be lowered =\r\nso that it will search the local search \nspace better..\n\n--- In neat@yahoog=\r\nroups.com, &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt; wrote:\n&gt;\n&gt; Julian, thanks for s=\r\nharing your recent work.  I hadn&#39;t been aware \nof \n&gt; that paper and it is i=\r\nndeed an interesting study.\n&gt; \n&gt; I&#39;m guessing that it probably has not been=\r\n done before exactly \n&gt; because it is so simple.  In particular, running ev=\r\nolution with a \n&gt; population of one is not very popular these days, and so =\r\npeople \n&gt; probably don&#39;t even bother considering it.\n&gt; \n&gt; Of course, as you=\r\nr paper discusses, it then leads to the question \nof \n&gt; how you might expan=\r\nd it into a population-based approach, and a lot \n&gt; of the existing methods=\r\n might embody ways that could be done to \n&gt; various degrees.  NEAT seems to=\r\n be a possible such expansion in the \n&gt; sense that it has multiple species =\r\nof topologies and roughly \nevolves \n&gt; weights and topologies on two differe=\r\nnt time scales with its usual \n&gt; parameter settings.\n&gt; \n&gt; In any case, your=\r\n paper an elegant demonstration of how much you \ncan \n&gt; learn from a simple=\r\n concept.  \n&gt; \n&gt; One question I have is what is meant when you mention thin=\r\ngs \n&gt; like &quot;all connections on?&quot;  It sounds like the topology is chosen \n&gt; =\r\nfrom some finite constrained set of topologies.  Is that true?  Or \n&gt; is an=\r\ny potential topology possible?  It makes me wonder whether the \n&gt; connectio=\r\nn set from which you are choosing somehow biases the \nresult \n&gt; to radicall=\r\ny favor the different time scales, though I&#39;m not sure \n&gt; how.\n&gt; \n&gt; In gene=\r\nral, my intuitive explanation of the phenomenon is that if \n&gt; you stick wit=\r\nh only a single topology it is likely to be the wrong \n&gt; one, so you need t=\r\no look at several to see if they have potential.  \n&gt; Then you ensure steady=\r\n progress.\n&gt; \n&gt; ken\n&gt; \n&gt; \n&gt; --- In neat@yahoogroups.com, &quot;Julian Togelius&quot; =\r\n&lt;julian@&gt; wrote:\n&gt; &gt;\n&gt; &gt; Hi all,\n&gt; &gt; \n&gt; &gt;  I&#39;d like to connect to the recen=\r\nt discussion in this group on\n&gt; &gt;  combining neuroevolution with backpropag=\r\nation, and use it as a\n&gt; &gt;  convenient excuse to expose you to a recent pap=\r\ner of mine. In\n&gt; &gt;  particular, I&#39;d like to ask you if you know who did thi=\r\ns before \nwe\n&gt; &gt;  (me, Faustino Gomez, Juergen Schmidhuber) did. Because th=\r\ne \n&gt; algorithm\n&gt; &gt;  we are presenting in this paper is so simple someone mu=\r\nst have \n&gt; come up\n&gt; &gt;  with it before, it&#39;s just that our literature searc=\r\nh has been\n&gt; &gt;  fruitless.\n&gt; &gt; \n&gt; &gt;  The basic idea is to combine hillclimb=\r\ning (1+1 evolution \n&gt; strategy) of\n&gt; &gt;  network topologies, with hillclimbi=\r\nng of network weights, but at \na\n&gt; &gt;  faster timescale. So after each (&quot;glo=\r\nbal&quot;) topology mutation, we\n&gt; &gt;  search (&quot;local&quot;) weight space for a number=\r\n of steps (e.g. 50), \nand\n&gt; &gt;  only accepts the global mutation if the fitn=\r\ness after local \n&gt; search is\n&gt; &gt;  higher than before the global mutation. T=\r\nhe reason we do this is \n&gt; that\n&gt; &gt;  global mutations are so often disrupti=\r\nve. Yes, this is the same \n&gt; idea\n&gt; &gt;  as &quot;innovation protection&quot; in NEAT, =\r\nbut here we&#39;re boiling it \n&gt; down a\n&gt; &gt;  bare minimum.\n&gt; &gt; \n&gt; &gt;  We tried t=\r\nwo versions of a moderately difficult control task: \none \n&gt; with\n&gt; &gt;  a set=\r\n of 7 carefully selected inputs to the neural network, and \n&gt; one\n&gt; &gt;  with=\r\n 15 not-so-carefully-selected inputs to the network. We found\n&gt; &gt;  that, wh=\r\nen compared to simple hillclimbing in weight space, or to\n&gt; &gt;  climbing wei=\r\nghts and topologies at the same time, performed \n&gt; slightly\n&gt; &gt;  better on =\r\nthe small-input  version of the problem problem. On the\n&gt; &gt;  big-input vers=\r\nion, the difference was dramatic, in that using\n&gt; &gt;  different timescales w=\r\nas really the only way to solve the \nproblem!\n&gt; &gt;  (Incidentally, learning =\r\ntopology at the &quot;local&quot; level and \nweights \n&gt; at\n&gt; &gt;  the &quot;global&quot; level wo=\r\nrked just as well.)\n&gt; &gt; \n&gt; &gt;  The paper is here:\n&gt; &gt;  http://julian.togeliu=\r\ns.com/Togelius2008Learning.pdf\n&gt; &gt; \n&gt; &gt;  Again, this is so obvious that som=\r\neone must have done this \n&gt; before. The\n&gt; &gt;  literature search only pointed=\r\n us to attempts to combine topology\n&gt; &gt;  evolution with backpropagation, wh=\r\nich is related but not the same\n&gt; &gt;  thing. We submitted the paper to WCCI =\r\n(and had it accepted) in \nthe\n&gt; &gt;  hope that the reviewers would know, but =\r\nthat they didn&#39;t. So now \nI\n&gt; &gt;  wonder if anyone of you might know. It wou=\r\nld be good to know \nabout\n&gt; &gt;  this before go on with further experiments..=\r\n.\n&gt; &gt; \n&gt; &gt;  Grateful for any suggestions\n&gt; &gt;  Julian\n&gt; &gt; -- \n&gt; &gt; Julian Tog=\r\nelius\n&gt; &gt; IDSIA\n&gt; &gt; Galleria 2\n&gt; &gt; 6928 Manno-Lugano\n&gt; &gt; Switzerland\n&gt; &gt; ju=\r\nlian@\n&gt; &gt; http://julian.togelius.com\n&gt; &gt; http://www.idsia.ch/~togelius\n&gt; &gt; =\r\n+41-764-110679\n&gt; &gt; +46-705-192088\n&gt; &gt;\n&gt;\n\n\n\n"}}