{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"utcXUb8U37S2ECTTiIPklQgBtRFDZhjX6re4nLPCv4bkWcG4XjDw33pT3MzAm-kFgKaV9HHvHUUyqx2xZdtKJ75RuZt14Xz3eCNwqoYoMH-b","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: Fitness Question","postDate":"1093206927","msgId":1446,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGNnYjAyZitxcGFrQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGNnNWN0ZStlZXF0QGVHcm91cHMuY29tPg=="},"prevInTopic":1435,"nextInTopic":0,"prevInTime":1445,"nextInTime":1447,"topicId":1435,"numMessagesInTopic":2,"msgSnippet":"Hi Bladerp (I don t know your real name!), The technique you are looking for is called coevolution, which means evolution where individuals are directly","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 79761 invoked from network); 22 Aug 2004 20:36:27 -0000\r\nReceived: from unknown (66.218.66.218)\n  by m16.grp.scd.yahoo.com with QMQP; 22 Aug 2004 20:36:27 -0000\r\nReceived: from unknown (HELO n37.grp.scd.yahoo.com) (66.218.66.105)\n  by mta3.grp.scd.yahoo.com with SMTP; 22 Aug 2004 20:36:27 -0000\r\nReceived: from [66.218.67.177] by n37.grp.scd.yahoo.com with NNFMP; 22 Aug 2004 20:35:27 -0000\r\nDate: Sun, 22 Aug 2004 20:35:27 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;cgb02f+qpak@...&gt;\r\nIn-Reply-To: &lt;cg5cte+eeqt@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Length: 4595\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Remote-IP: 66.218.66.105\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: Re: Fitness Question\r\nX-Yahoo-Group-Post: member; u=54567749\r\nX-Yahoo-Profile: kenstanley01\r\n\r\nHi Bladerp (I don&#39;t know your real name!),\n\nThe technique you are looking for is called coevolution, which means \nevolution where individuals are directly evaluated against each \nother.  In other words, it&#39;s the situation you described where ANN&#39;s \nare evaluated relative to each other.  But you are absolutely \ncorrect that such a process is prone to getting stuck in local \noptima.\n\nHowever, the good news is that coevolution has been extenesively \nstudied recently and there is some understanding of how to keep \nthe &quot;arms race&quot; going, and the reasons it gets stuck.  I recommend \nreading our paper, &quot;Competitive Coevolution through Evolutionary \nComplexification,&quot; available at:\n\nhttp://nn.cs.utexas.edu/keyword?stanley:jair04\n\nIt will give you a general idea how coevolution is set up in \nneuroevolution (and with NEAT in particular), and also give you \nreferences to the classic papers in the coevolution literature such \nas Rosin and Belew&#39;s paper on methods for enhancing coevolution and \nlater work on Pareto coevolution.  The paper also shows how \ncomplexification, a process that NEAT implements, enhances \ncoevolution.\n\nThe issues here are deep and quite complicated, so I probably can&#39;t \ndo it justice in a single message, but that paper is a good place to \nstart, followed by citations within it.  Nevertheless, even with \neverything that&#39;s known about coevolution, it *still* is difficult \nto get the holy-grail type arms race going that just goes and goes \nforever.  \n\nSo it might help to combine coevolution with some precoded players, \nif you have on available.  For example, in Go GnuGo is publicly \navailable.  You could have individuals play eachother *and* GnuGo, \nwhich might lead to more balanced play.\n\nAlternatively, you could only play against GnuGo and forget \ncoevolution entirely.  I did this in our paper, &quot;Evolving a Roving \nEye for Go,&quot; available at:\n\nhttp://nn.cs.utexas.edu/keyword?stanley:gecco04\n\nThe problem of course is that then you only learn to defeat GnuGo \nand to exploit its particular idiosyncracies.  But it can get you \nstarted at least.\n\nThere are no easy answers in this area, and it is an open research \narea, so the best you can do for now is learn what is known, and \ncome up with your own formulation of what might work best based on \nyour own understanding.\n\nIf you really want to understand the dynamics of a competitive arms \nrace from a theoretical standpoint, I highly recommend the &quot;Pareto \nCoevolution&quot; stuff from Pollack&#39;s lab at Brandeis.  Here is a list \nof all their papers (includeing those not on coevolution):\n\nhttp://demo.cs.brandeis.edu/papers/year.html\n\nken\n\n--- In neat@yahoogroups.com, &quot;bladerp101&quot; &lt;bladerp@h...&gt; wrote:\n&gt; Hello, all.  I&#39;m new to this group.  I was in the process of \n&gt; designing an implementation of NEAT when I found this group.  It&#39;s \n&gt; exciting to see that there are so many others with similar \ninterests.\n&gt; \n&gt; My original interest was in using temporal difference learning to \n&gt; train neural networks (via backpropagation) to play games.  The \n&gt; problem I&#39;ve encountered is that learning tends to stop at a local \n&gt; optimum (the first good linear strategy).  I am considering \n&gt; NeuroEvolution as an approach to overcoming that problem that is  \n&gt; inherent in a gradient-based technique such as backpropagation. \n&gt; \n&gt; I&#39;ve noticed that several people have tried to apply ANNs to games \n&gt; such as Go.  My question is:  How would I calculate the fitness of \n&gt; an ANN for such cases?  \n&gt; \n&gt; Using TD learning, the error function is based upon whether the \nANN \n&gt; wins the game or not.  The ANN can play either a previously \ntrained \n&gt; ANN, or even itself.  When evolving ANNs, fitness could be \n&gt; calculated similarly -- by holding some kind of tournament among \nthe \n&gt; members of the population or by playing a previously trained ANN.  \n&gt; However, I expect that if fitness is always calculated relative to \n&gt; the other ANNs in the population, the search process would again \n&gt; become gradient-based and prone to getting stuck in local optima.\n&gt; \n&gt; Another way to calculate fitness might be to use a library of \n&gt; expertly evaluated moves (or games), and calculate the ANNs \nability \n&gt; to judge them.  However, this approach suffers from three \ndrawbacks:\n&gt; \n&gt;   1)  one needs a library of expertly evaluated moves\n&gt;   2)  those moves might not really be evaluated &quot;expertly&quot;\n&gt;   3)  the ANN may overfit the sample, resulting in poor\n&gt;       generalization\n&gt; \n&gt; All ideas and suggestions will be appreciated.  I hope that in the \n&gt; future I will be able to contribute answers in return.  \n&gt; \n&gt; Thanks.\n\n\n"}}