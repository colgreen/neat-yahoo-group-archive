{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":8147458,"authorName":"chhofchhof","from":"&quot;chhofchhof&quot; &lt;Christian.Hofmann@...&gt;","profile":"chhofchhof","replyTo":"LIST","senderId":"niXgtCn8LdVxtKlV7zNVCDdJH_9E-8SbVdnm3pahBS9aMKscp55UIWs6AZuqD8FOqsnFLesk0-GYYiN7BXSoPJd3d_kzjUZeDjC9oe9K","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: train 3 billion training data with 3000 inputs and 150 outputs?","postDate":"1262638082","msgId":5044,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGhodGs2MitrZTdiQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGhoOGFnOCs2OHYxQGVHcm91cHMuY29tPg=="},"prevInTopic":5042,"nextInTopic":5045,"prevInTime":5043,"nextInTime":5045,"topicId":5037,"numMessagesInTopic":9,"msgSnippet":"Thank you all for your replies! @William Sorry, I have problems to understand the ways how you have solved the things. You have used sensor neurons. So you","rawEmail":"Return-Path: &lt;Christian.Hofmann@...&gt;\r\nX-Sender: Christian.Hofmann@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 91330 invoked from network); 4 Jan 2010 20:48:28 -0000\r\nX-Received: from unknown (98.137.34.44)\n  by m14.grp.re1.yahoo.com with QMQP; 4 Jan 2010 20:48:28 -0000\r\nX-Received: from unknown (HELO n43d.bullet.mail.sp1.yahoo.com) (66.163.169.157)\n  by mta1.grp.sp2.yahoo.com with SMTP; 4 Jan 2010 20:48:28 -0000\r\nX-Received: from [69.147.65.173] by n43.bullet.mail.sp1.yahoo.com with NNFMP; 04 Jan 2010 20:48:03 -0000\r\nX-Received: from [98.137.35.12] by t15.bullet.mail.sp1.yahoo.com with NNFMP; 04 Jan 2010 20:48:03 -0000\r\nDate: Mon, 04 Jan 2010 20:48:02 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;hhtk62+ke7b@...&gt;\r\nIn-Reply-To: &lt;hh8ag8+68v1@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;chhofchhof&quot; &lt;Christian.Hofmann@...&gt;\r\nSubject: Re: train 3 billion training data with 3000 inputs and 150 outputs?\r\nX-Yahoo-Group-Post: member; u=8147458; y=v1lWy40emP_N-6qACEmrIrkZKp5uvDH06qSqNp9rn9wYxKH9Ng\r\nX-Yahoo-Profile: chhofchhof\r\n\r\nThank you all for your replies!\n\n@William\n\nSorry, I have problems to unders=\r\ntand the ways how you have solved the things. You have used sensor neurons.=\r\n So you would use for example 100 inputs instead of 3000. Then the output n=\r\neurons decide which neurons to use in the inputs?\n\nIs this correct? Do you =\r\nhave some documents or a link that describes that in detail?\n\n@Greg\n\nThe 34=\r\n7 days was for the whole population, but for one generation. The problem is=\r\n I have no cluster. Also in my experiments I need normally over millions of=\r\n generations till I reach the point where the fitness of my validation data=\r\n starts to get lower (so I need to stop training).\nSo even with 347 cluster=\r\n members I would need one million days. Without counting that the needed ca=\r\nlculation time is increasing from generation to generation.\n\n@torvhydda\n\nI =\r\nam currently using Hypersharp neat, but I will have a look, thank you.\n\n\n\n"}}