{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":7192225,"authorName":"Ian Badcoe","from":"Ian Badcoe &lt;ian_badcoe@...&gt;","profile":"ian_badcoe","replyTo":"LIST","senderId":"uZwwYmnN5jxb9rEQ8xJKIXicyp-WLl2ckeUtcxcVqDkgpnS5D0Pn6EYYP04JRSfvI2C_05qWAxwkmZRofgQpAugdYr2qfjxINvg","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] Vector Cross Product and Multiplication","postDate":"1092239660","msgId":1364,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDYuMS4yLjAuMC4yMDA0MDgxMTE2MjEwOC4wMjUyYWI5OEBwb3AubWFpbC55YWhvby5jby51az4=","inReplyToHeader":"PDQxMTU1MDcxLjkwNjAzMDdAZHNsLnBpcGV4LmNvbT4=","referencesHeader":"PDQxMTU1MDcxLjkwNjAzMDdAZHNsLnBpcGV4LmNvbT4="},"prevInTopic":1349,"nextInTopic":1372,"prevInTime":1363,"nextInTime":1365,"topicId":1342,"numMessagesInTopic":7,"msgSnippet":"Hi, This is a thought I have had for some time, but not got around to posting. w.r.t your test, I would use both types of neurone in the network and pick a","rawEmail":"Return-Path: &lt;ian_badcoe@...&gt;\r\nX-Sender: ian_badcoe@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 92489 invoked from network); 11 Aug 2004 15:55:09 -0000\r\nReceived: from unknown (66.218.66.167)\n  by m23.grp.scd.yahoo.com with QMQP; 11 Aug 2004 15:55:09 -0000\r\nReceived: from unknown (HELO smtp002.mail.ukl.yahoo.com) (217.12.11.33)\n  by mta6.grp.scd.yahoo.com with SMTP; 11 Aug 2004 15:55:09 -0000\r\nReceived: from unknown (HELO ian2k.yahoo.co.uk) (ian?badcoe@212.159.73.108 with login)\n  by smtp002.mail.ukl.yahoo.com with SMTP; 11 Aug 2004 15:55:07 -0000\r\nMessage-Id: &lt;6.1.2.0.0.20040811162108.0252ab98@...&gt;\r\nX-Sender: ian_badcoe@...\r\nX-Mailer: QUALCOMM Windows Eudora Version 6.1.2.0\r\nDate: Wed, 11 Aug 2004 16:54:20 +0100\r\nTo: neat@yahoogroups.com\r\nIn-Reply-To: &lt;41155071.9060307@...&gt;\r\nReferences: &lt;41155071.9060307@...&gt;\r\nMime-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;us-ascii&quot;; format=flowed\r\nX-eGroups-Remote-IP: 217.12.11.33\r\nFrom: Ian Badcoe &lt;ian_badcoe@...&gt;\r\nSubject: Re: [neat] Vector Cross Product and Multiplication\r\nX-Yahoo-Group-Post: member; u=7192225\r\nX-Yahoo-Profile: ian_badcoe\r\n\r\nHi,\n         This is a thought I have had for some time, but not got around to \nposting.\n\n         w.r.t your test, I would use both types of neurone in the network \nand pick a type for each one when it is first added.  Either that or some \nhow allow additive and multiplicative inputs.\n\n         I&#39;ve also had other, similar thoughts.  For example, in real \nneurones, if we make two synapses from neurone-A to neurone-B, do we know \nthat to be equivalent to one &quot;double strength&quot; synapse?  I suspect we \ndon&#39;t.  In at least one other area of biological cybernetics there&#39;s a \nsystem which uses multiplicity of signals to change the shape of the \nactivation curve.  When an enzyme is activated (or inhibited) by a \nsmall-molecule effector.  The effector works by binding to the enzyme and \nswitching it between active and inactive states.  So the enzyme population \nconsists of a mixture of molecules with and without the effector and has an \nactivity proportional to the subpopulation of those in the active state.\n\n         When one affector can bind an enzyme, then the curve of (overall) \nenzyme activity is a rectangular hyperbola.  However, if two activators \nbind to each enzyme, then the strength of the second&#39;s binding is often \neffected by the presence of the first.  In this case you get sigmoidicity \nin the curve.  the more effectors are involved, the more step-like the \ncurve can become.\n\n         Look here:\n\nhttp://www.uic.edu/depts/mcbc/curriculum/531/handouts/kineticsiii.pdf\n\n         ...page down to page 3 and look at the top.  I just found this \nquickly and he&#39;s making a slightly different point, but those are typical \nof the types of curve involved.  The top one is like when there&#39;s a single \nactivator molecule on each enzyme.  The lower is like when there are two \nactivators binding to each enzyme.\n\n         So...  Should multiple connections (W1, W2) from neurone A to B be \nexactly the same as one (W1+W2) or are we missing the opportunity to \nimplement some configurational choices for how A effects B.  I wonder what \nnature does (not that I slavishly mimic nature, you understand).\n\n         Similarly, if A and B have synapses to C then must their combined \naffect be simple addition?  ISRT in nature we find examples of synapses \nwhere the receiving neurone is never triggered by the stimulation, but it \nis rendered more or less sensitive to signals from other neurones.  This is \nlike having a choice of scaling or additive connections on each neurone (or \nequivalently, the scaling connection can adjust the steepness of the \nactivation function).\n\n         Getting back to types of neurone (and as an interesting aside, \nnatural systems do not distinguish &quot;excitation&quot; and &quot;inhibition&quot; at the \nsynapse level, its generally an entire neurone which is +ve or -ve) ISRT \none of the &quot;virtual creatures&quot; workers put a small network in each body \npart and used a mixture of mathematical and logical operators for the \nneurone types.  I think it was one of these:\n\nPeter Eggenberger\nJosh Bongard\nKunihiko Kaneko\n\n         But I&#39;ll dig the paper out if anybody wants to know.  Actually, \nall those people are worth looking at.\n\n         Ian Badcoe\n\nAt 22:58 07/08/2004, you wrote:\n&gt;I&#39;ve been experimenting with the cross product domain and I think my\n&gt;findings might be of interest to some of you who may not have been\n&gt;following the discussion...\n&gt;\n&gt;A 3D vector cross product can be calculated very simply with the formula\n&gt;Chad provided:\n&gt;\n&gt;To calculate the cross product of vectors a and b\n&gt;\n&gt;x = a1*b3 - a3*b2\n&gt;y = a3*b1 - a1*b3\n&gt;z = a1*b2 - a2*b1\n&gt;\n&gt;It&#39;s a simple formula that only contains subtraction and multiplication,\n&gt;but it seems to be quite difficult to find a network to represent this\n&gt;forumla. Neural nets of the knid we use are good at adding(and therfore\n&gt;subtracting) because each neuron adds it&#39;s inputs, but what about\n&gt;multiplication?\n&gt;\n&gt;I tried a simple multiplication experiment and discovered that although\n&gt;networks can be found, that they are not trivial. Typically they will\n&gt;have at least 4 hidden nodes to get the basic functionality and more\n&gt;nodes are required for extra accuracy. Now consider that for a network\n&gt;to calculate a cross product it must independently discover six\n&gt;multiplication sub-networks, connected up so that their result is\n&gt;combined appropriately. Hopefully this gives some idea of why the cross\n&gt;product domain is a non-trivial one.\n&gt;\n&gt;A quick google came up with this paper which investigates neurons that\n&gt;multiply their inputs instead of adding them:\n&gt;\n&gt;http://gs37.sp.cs.cmu.edu/papers/files/mult-NN-complexity2.pdf\n&gt;\n&gt;I havent read it yet but I tried out the concept and sure enough I can\n&gt;find a network that performs multiplication just by connecting the two\n&gt;input (operand) neurons directly to the output, all the NEAT search\n&gt;needs to do is find the appropriate two weights. Some accuracy is lost\n&gt;because the answer passes through the sigmoid function, but it&#39;s about\n&gt;as accurate as the networks found using adding neurons.\n&gt;\n&gt;I then tried the cross product domain again, this time with the\n&gt;multiplying neurons and a manually created seed genome with all the\n&gt;connections required to perform cross product. The result was failure,\n&gt;presumably because although the multiplication part is now being\n&gt;performed by the network, the additive (or subtraction) part of the\n&gt;formula is now more difficult to achieve with multiplying neurons. doh!\n&gt;So perhaps it would be an idea to mix adding and multiplying neurons? So\n&gt;now I&#39;m wondering what other domains might benefit from the inclusion of\n&gt;multiplying neurons into our various NEAT implementations? For starters\n&gt;John&#39;s image enlargment might benefit, as might Derek and Philips roving\n&gt;eye (with a zoom factor).\n&gt;\n&gt;Colin\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;Yahoo! Groups Links\n&gt;\n&gt;\n&gt;\n&gt;\n\n\n\nLiving@Home - Open Source Evolving Organisms - \nhttp://livingathome.sourceforge.net/\n\n\n\n\n"}}