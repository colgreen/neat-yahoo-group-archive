{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":234577593,"authorName":"Oliver Coleman","from":"Oliver Coleman &lt;oliver.coleman@...&gt;","profile":"olivercoleman04","replyTo":"LIST","senderId":"xATBTW8pr6uWaHGCWLYM-UBHZ2Z7K2wkiyvBf9d94KPd8aWKaFfd4J2zrcq0HproTrtJ4zQXKgd4sc0-Wjwl94lp6awz7AZ6vGkW_fINEK4","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [neat] New paper: Automated Generation of Environments to Test the General Learning Capabilities of AI Agents","postDate":"1398818235","msgId":6282,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PENBK2R1aW1ONCtZVTMtelN4ZnV1ek9OLVAtcnI4NVBTcHMrMDFDMW5Wa2twSkxjUmR0d0BtYWlsLmdtYWlsLmNvbT4=","inReplyToHeader":"PENBTnRYaG12dUpHMkxkWXpSRGVXRldpU01HNW1iK3pmQkxWZ0VhQm10dHkyV0ZQaXhFd0BtYWlsLmdtYWlsLmNvbT4=","referencesHeader":"PENBK2R1aW1PMjRzYWtPWFNNVnVxYkVleDgremlCbVFIdmVjb1kza3dBZCt6QUI1Wmt3UUBtYWlsLmdtYWlsLmNvbT4JPENBTnRYaG12dUpHMkxkWXpSRGVXRldpU01HNW1iK3pmQkxWZ0VhQm10dHkyV0ZQaXhFd0BtYWlsLmdtYWlsLmNvbT4="},"prevInTopic":6281,"nextInTopic":6284,"prevInTime":6281,"nextInTime":6283,"topicId":6279,"numMessagesInTopic":11,"msgSnippet":"Hi Vassilis, ... Thanks! ... Perhaps that sentence should have read (additions in bold): the genome *directly encodes* the parameter values for several","rawEmail":"Return-Path: &lt;oliver.coleman@...&gt;\r\nX-Sender: oliver.coleman@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 97500 invoked by uid 102); 30 Apr 2014 00:37:16 -0000\r\nX-Received: from unknown (HELO mtaq3.grp.bf1.yahoo.com) (10.193.84.142)\n  by m17.grp.bf1.yahoo.com with SMTP; 30 Apr 2014 00:37:16 -0000\r\nX-Received: (qmail 19630 invoked from network); 30 Apr 2014 00:37:16 -0000\r\nX-Received: from unknown (HELO mail-we0-f170.google.com) (74.125.82.170)\n  by mtaq3.grp.bf1.yahoo.com with SMTP; 30 Apr 2014 00:37:16 -0000\r\nX-Received: by mail-we0-f170.google.com with SMTP id w61so976472wes.15\n        for &lt;neat@yahoogroups.com&gt;; Tue, 29 Apr 2014 17:37:15 -0700 (PDT)\r\nMIME-Version: 1.0\r\nX-Received: by 10.180.94.226 with SMTP id df2mr823225wib.1.1398818235534; Tue,\n 29 Apr 2014 17:37:15 -0700 (PDT)\r\nX-Received: by 10.194.237.72 with HTTP; Tue, 29 Apr 2014 17:37:15 -0700 (PDT)\r\nIn-Reply-To: &lt;CANtXhmvuJG2LdYzRDeWFWiSMG5mb+zfBLVgEaBmtty2WFPixEw@...&gt;\r\nReferences: &lt;CA+duimO24sakOXSMVuqbEex8+ziBmQHvecoY3kwAd+zAB5ZkwQ@...&gt;\n\t&lt;CANtXhmvuJG2LdYzRDeWFWiSMG5mb+zfBLVgEaBmtty2WFPixEw@...&gt;\r\nDate: Wed, 30 Apr 2014 10:37:15 +1000\r\nMessage-ID: &lt;CA+duimN4+YU3-zSxfuuzON-P-rr85PSps+01C1nVkkpJLcRdtw@...&gt;\r\nTo: neat@yahoogroups.com\r\nContent-Type: multipart/alternative; boundary=f46d04447e6108d53304f837c0fe\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Oliver Coleman &lt;oliver.coleman@...&gt;\r\nSubject: Re: [neat] New paper: Automated Generation of Environments to Test\n the General Learning Capabilities of AI Agents\r\nX-Yahoo-Group-Post: member; u=234577593; y=BS7S6C-Dhf77YvOTn2sIy4IXzyQSP8klIJTN18jZ1I4nV2-f4mEBdcW0JS9y_0Rl11zr57NA_A\r\nX-Yahoo-Profile: olivercoleman04\r\n\r\n\r\n--f46d04447e6108d53304f837c0fe\r\nContent-Type: text/plain; charset=UTF-8\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\nHi Vassilis,\n\nOn 30 April 2014 02:50, Vassilis Vassiliades &lt;vassilisvas@gma=\r\nil.com&gt; wrote:\n\n&gt;\n&gt;\n&gt; Hi Oliver,\n&gt;\n&gt; First, congratulations to you and your=\r\n co-authors on this interesting\n&gt; paper.\n&gt;\n\nThanks!\n\n\n&gt; I am also intereste=\r\nd in adaptive neural networks and I have some questions\n&gt; and comments on y=\r\nour paper:\n&gt;\n&gt;\n&gt; 1) On page 3 you describe the generalized Hebbian rule, wh=\r\nich was\n&gt; introduced by Niv et al., but on page 5 you say that &quot;the genome =\r\ndefines\n&gt; the parameter values for several plasticity rules, or classes, an=\r\nd then\n&gt; encodes which rule each connection references&quot;... and later... the=\r\n CPPNs\n&gt; &quot;have n+1 outputs that specify the class for a connection&quot;.\n&gt;\n&gt; It=\r\n&#39;s not clear to me whether/how these classes relate to the generalized\n&gt; He=\r\nbbian rule, or which rule each output of the CPPN corresponds to.\n&gt;\n\nPerhap=\r\ns that sentence should have read (additions in bold): &quot;the\ngenome *directly=\r\n\nencodes* the parameter values for several plasticity rules, or\nclasses,  *=\r\n(based\non Niv&#39;s equation)* and then *indirectly encodes, via the CPPN,* whi=\r\nch rule\neach connection references.&quot; In HyperNEAT the genome is usually jus=\r\nt the\nCPPN, but we&#39;ve added some additional genes (floating-point numbers) =\r\nto\ndirectly encode parameter values for Niv&#39;s rule. Each genome encodes fou=\r\nr\nsets of parameter values for Niv&#39;s rule, resulting in four different\nspec=\r\nific weight update rules (all using Niv&#39;s rule with but different\nparameter=\r\n values). Then when we build the neural network with HyperNEAT we\nadd extra=\r\n outputs to the CPPN that determine which one of these plasticity\nrules is =\r\nto be used for each connection in the network. Does that make more\nsense?\n\n=\r\n\n&gt;\n&gt;\n&gt; 2) Some clarification: since the environments are deterministic, doe=\r\ns the\n&gt; number of actions determine the number of states of the MDP? For ex=\r\nample,\n&gt; in the configurations where the number of available actions is 16,=\r\n it is\n&gt; also implied that the number of states is 16. Is my interpretation=\r\n correct?\n&gt;\n\nThe number of states is independent of the number of actions. =\r\nDifferent\nactions in state A may all lead to state B but provide different =\r\nreward\nvalues.\n\n\n&gt;\n&gt;\n&gt; 3) On page 3 you say that &quot;the proportion of state t=\r\nransitions that\n&gt; provide a reward value is 0.5&quot;. It is not clear to me, ho=\r\nwever, what the\n&gt; reward values are. Do all transitions that have a reward =\r\nvalue have the\n&gt; *same* reward value (e.g. equal to 1), or does this value =\r\nvary?\n&gt;\n\nFor transitions that provide a reward, the reward is selected unif=\r\normly\nfrom the range [0, 1).\n\n\n&gt;\n&gt; Also, regarding the &quot;maximum possible re=\r\nward maxRx&quot;, do you mean the\n&gt; &quot;return (sum of rewards) obtained by the opt=\r\nimal policy&quot;? If you have the\n&gt; *same* reward value on the transitions (as =\r\nmentioned above) then it is easy\n&gt; to calculate maxRx; if the reward values=\r\n vary then I guess you have to\n&gt; calculate maxRx using dynamic programming;=\r\n the initial state and the trial\n&gt; length matters, especially in the case w=\r\nhere you have 16 actions (states?)\n&gt; and trial length =3D 4.\n&gt;\n\nBecause the=\r\n length of trials is relatively small (and the MDPs\ndeterministic) we calcu=\r\nlate the maximum return via a simple brute force\nmethod that tries every po=\r\nssible sequence of actions for the specific trial\nlength in question.\n\n\n&gt;\n&gt;=\r\n\n&gt; 4) Delayed reward: by appropriately setting the parameters of your rando=\r\nm\n&gt; MDP generator you could instantiate environments that have delayed rewa=\r\nrd.\n&gt; For example, consider the case where you have a large discrete state =\r\nspace\n&gt; and only one path towards a goal/absorbing state. This path might h=\r\nave a\n&gt; small negative reward on transitions along the way, e.g., -0.1, how=\r\never,\n&gt; the last transition towards the absorbing state has a huge reward, =\r\ne.g.,\n&gt; +100. Moreover, transitions from/to other states that are not on th=\r\nis\n&gt; optimal path, might have a reward value greater than or equal to -0.1,=\r\n but\n&gt; much smaller than 100, so that this path remains optimal.\n&gt;\n&gt; This p=\r\nroblem might be difficult for traditional neuroevolution especially\n&gt; as th=\r\ne length of the optimal path increases (another difficulty dimension\n&gt; perh=\r\naps?), however, I believe a novelty search approach is promising here\n&gt; bec=\r\nause what you really need now is better exploration.\n&gt;\n\nYes, this is an int=\r\neresting question in general, and certainly previous\nresults on simple dece=\r\nptive domains indicate that for delayed-reward MDP\nenvironments like you de=\r\nscribe (neuro)evolution will likely get stuck if\nthe search isn&#39;t aided by =\r\nsomething like Novelty Search. In our paper we\ndon&#39;t worry about this issue=\r\n as we are comparing the performance of the\ndifferent neural network models=\r\n relative to each other and are not\nparticularly interested in their perfor=\r\nmance relative to the maximum\npossible (we do scale the results relative to=\r\n the maximum possible but this\nis simply to make aggregation of results eas=\r\nier).\n\nLet me know if anything is still unclear or if you have further ques=\r\ntions\nor comments.\n\nCheers,\nOliver\n\n\n&gt;\n&gt;\n&gt; On Tue, Apr 29, 2014 at 4:28 AM,=\r\n Oliver Coleman &lt;oliver.coleman@...&gt;wrote:\n&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; Hi all,\n&gt;&gt;\n&gt;&gt; J=\r\neff Clune, Alan Blair and I are pleased to announce a new GECCO paper,\n&gt;&gt; &quot;=\r\nAutomated Generation of Environments to Test the General Learning\n&gt;&gt; Capabi=\r\nlities of AI Agents&quot;\n&gt;&gt;\n&gt;&gt; We&#39;d love to hear your comments and questions.\n&gt;=\r\n&gt;\n&gt;&gt; *Video summary:* http://youtu.be/itbKerV9g6s\n&gt;&gt;\n&gt;&gt; *PDF:* http://goo.g=\r\nl/t3tCp2\n&gt;&gt;\n&gt;&gt;  *Abstract*: Algorithms for evolving agents that learn durin=\r\ng their\n&gt;&gt; lifetime have typically been evaluated on only a handful of envi=\r\nronments.\n&gt;&gt; Designing such environments is labour intensive, potentially b=\r\niased, and\n&gt;&gt; provides only a small sample size that may prevent accurate g=\r\neneral\n&gt;&gt; conclusions from being drawn. In this paper we introduce a method=\r\n for\n&gt;&gt; automatically generating MDP environments which allows the difficul=\r\nty to be\n&gt;&gt; scaled in several ways. We present a case study in which enviro=\r\nnments are\n&gt;&gt; generated that vary along three key dimensions of difficulty:=\r\n the number of\n&gt;&gt; environment configurations, the number of available actio=\r\nns, and the length\n&gt;&gt; of each trial. The study reveals interesting differen=\r\nces between three\n&gt;&gt; neural network models -- Fixed-Weight, Plastic-Weight,=\r\n and Modulated\n&gt;&gt; Plasticity -- that would not have been obvious without sw=\r\neeping across\n&gt;&gt; these different dimensions. Our paper thus introduces a ne=\r\nw way of\n&gt;&gt; conducting reinforcement learning science: instead of manually =\r\ndesigning a\n&gt;&gt; few environments, researchers will be able to automatically =\r\ngenerate a\n&gt;&gt; range of environments across key dimensions of variation. Thi=\r\ns will allow\n&gt;&gt; scientists to more rigorously assess the general learning c=\r\napabilities of\n&gt;&gt; an algorithm, and may ultimately improve the rate at whic=\r\nh we discover how\n&gt;&gt; to create AI with general purpose learning.\n&gt;&gt;\n&gt;&gt; All =\r\ndata and software is available at http://goo.gl/n4D3w6\n&gt;&gt;\n&gt;&gt; T: +61 421 972=\r\n 953 | E: oliver.coleman@... | W:\n&gt;&gt; http://ojcoleman.com\n&gt;&gt;\n&gt;&gt;\n&gt;  \n&gt;=\r\n\n\r\n--f46d04447e6108d53304f837c0fe\r\nContent-Type: text/html; charset=UTF-8\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\n&lt;div dir=3D&quot;ltr&quot;&gt;Hi Vassilis,&lt;div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;div class=\r\n=3D&quot;gmail_quote&quot;&gt;On 30 April 2014 02:50, Vassilis Vassiliades &lt;span dir=3D&quot;=\r\nltr&quot;&gt;&lt;&lt;a href=3D&quot;mailto:vassilisvas@...&quot; target=3D&quot;_blank&quot;&gt;vassili=\r\nsvas@...&lt;/a&gt;&gt;&lt;/span&gt; wrote:&lt;br&gt;\n\n&lt;blockquote class=3D&quot;gmail_quote&quot;=\r\n style=3D&quot;margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:=\r\nrgb(204,204,204);border-left-style:solid;padding-left:1ex&quot;&gt;\n\n\n&lt;u&gt;&lt;/u&gt;\n\n\n\n\n\n=\r\n\n\n\n\n \n&lt;div&gt;\n&lt;span&gt;=C2=A0&lt;/span&gt;\n\n\n&lt;div&gt;\n  &lt;div&gt;\n\n\n    &lt;div&gt;\n      \n      \n =\r\n     &lt;p&gt;&lt;/p&gt;&lt;div dir=3D&quot;ltr&quot;&gt;Hi Oliver,&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;First, congratul=\r\nations to you and your co-authors on this interesting paper.&lt;/div&gt;&lt;/div&gt;&lt;/d=\r\niv&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/blockquote&gt;&lt;div&gt;=C2=A0&lt;/div&gt;&lt;div&gt;Thanks!&lt;/div&gt;&lt;div&gt;=\r\n=C2=A0&lt;/div&gt;\n\n&lt;blockquote class=3D&quot;gmail_quote&quot; style=3D&quot;margin:0px 0px 0px=\r\n 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left=\r\n-style:solid;padding-left:1ex&quot;&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div dir=3D&quot;ltr&quot;&gt;&lt;div&gt;I =\r\nam also interested in adaptive neural networks and I have some questions an=\r\nd comments on your paper:&lt;/div&gt;\n\n\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;1) On =\r\npage 3 you describe the generalized Hebbian rule, which was introduced by N=\r\niv et al., but on page 5 you say that &quot;the genome defines the paramete=\r\nr values for several plasticity rules, or classes, and then encodes which r=\r\nule each connection references&quot;... and later... the CPPNs &quot;have n=\r\n+1 outputs that specify the class for a connection&quot;.&lt;/div&gt;\n\n\n&lt;div&gt;&lt;br&gt;=\r\n&lt;/div&gt;&lt;div&gt;It&#39;s not clear to me whether/how these classes relate to the=\r\n generalized Hebbian rule, or which rule each output of the CPPN correspond=\r\ns to.&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/blockquote&gt;&lt;div&gt;&lt;br&gt;\n\n&lt;/div&gt;&lt;div=\r\n&gt;Perhaps that sentence should have read (additions in bold): &quot;the geno=\r\nme &lt;b&gt;directly encodes&lt;/b&gt; the parameter values for several plasticity rule=\r\ns, or classes, =C2=A0&lt;b&gt;(based on Niv&#39;s equation)&lt;/b&gt;=C2=A0and then &lt;b&gt;=\r\nindirectly encodes, via the CPPN,&lt;/b&gt;=C2=A0which rule each connection refer=\r\nences.&quot; In HyperNEAT the genome is usually just the CPPN, but we&#39;v=\r\ne added some additional genes (floating-point numbers) to directly encode p=\r\narameter values for Niv&#39;s rule. Each genome encodes four sets of parame=\r\nter values for Niv&#39;s rule, resulting in four different specific weight =\r\nupdate rules (all using Niv&#39;s rule with but different parameter values)=\r\n. Then when we build the neural network with HyperNEAT we add extra outputs=\r\n to the CPPN that determine which one of these plasticity rules is to be us=\r\ned for each connection in the network. Does that make more sense?&lt;/div&gt;\n\n&lt;d=\r\niv&gt;=C2=A0&lt;/div&gt;&lt;blockquote class=3D&quot;gmail_quote&quot; style=3D&quot;margin:0px 0px 0p=\r\nx 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-lef=\r\nt-style:solid;padding-left:1ex&quot;&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div dir=3D&quot;ltr&quot;&gt;&lt;div&gt;\n=\r\n&lt;br&gt;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;2) Some clarification: since the environments=\r\n are deterministic, does the number of actions determine the number of stat=\r\nes of the MDP? For example, in the configurations where the number of avail=\r\nable actions is 16, it is also implied that the number of states is 16. Is =\r\nmy interpretation correct?&lt;/div&gt;\n\n&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/blockquot=\r\ne&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;The number of states is independent of the number of =\r\nactions. Different actions in state A may all lead to state B but provide d=\r\nifferent reward values.&lt;/div&gt;\n\n&lt;div&gt;=C2=A0&lt;/div&gt;&lt;blockquote class=3D&quot;gmail_=\r\nquote&quot; style=3D&quot;margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-=\r\ncolor:rgb(204,204,204);border-left-style:solid;padding-left:1ex&quot;&gt;&lt;div&gt;&lt;div&gt;=\r\n&lt;div&gt;&lt;div&gt;&lt;div dir=3D&quot;ltr&quot;&gt;\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;3) On page 3=\r\n you say that &quot;the proportion of state transitions that provide a rewa=\r\nrd value is 0.5&quot;. It is not clear to me, however, what the reward valu=\r\nes are. Do all transitions that have a reward value have the *same* reward =\r\nvalue (e.g. equal to 1), or does this value vary?&lt;/div&gt;\n\n&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;=\r\n&lt;/div&gt;&lt;/div&gt;&lt;/blockquote&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;For transitions that provide a=\r\n reward, the reward is selected uniformly from the range [0, 1).&lt;/div&gt;&lt;div&gt;=\r\n=C2=A0&lt;/div&gt;&lt;blockquote class=3D&quot;gmail_quote&quot; style=3D&quot;margin:0px 0px 0px 0=\r\n.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-s=\r\ntyle:solid;padding-left:1ex&quot;&gt;\n\n&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div dir=3D&quot;ltr&quot;&gt;\n&lt;div&gt;&lt;=\r\nbr&gt;&lt;/div&gt;&lt;div&gt;Also, regarding the &quot;maximum possible reward maxRx&quot;=\r\n, do you mean the &quot;return (sum of rewards) obtained by the optimal pol=\r\nicy&quot;? If you have the *same* reward value on the transitions (as menti=\r\noned above) then it is easy to calculate=C2=A0maxRx; if the reward=C2=A0val=\r\nues vary then I guess you have to calculate=C2=A0maxRx=C2=A0using dynamic p=\r\nrogramming; the initial state and the trial length=C2=A0matters, especially=\r\n in the case where you have 16 actions (states?) and trial length =3D 4.&lt;/d=\r\niv&gt;\n\n&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/blockquote&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Because=\r\n the length of trials is relatively small (and the MDPs deterministic) we c=\r\nalculate the maximum return via a simple brute force method that tries ever=\r\ny possible sequence of actions for the specific trial length in question.&lt;/=\r\ndiv&gt;\n\n&lt;div&gt;=C2=A0&lt;/div&gt;&lt;blockquote class=3D&quot;gmail_quote&quot; style=3D&quot;margin:0p=\r\nx 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);bo=\r\nrder-left-style:solid;padding-left:1ex&quot;&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div dir=3D&quot;ltr=\r\n&quot;&gt;\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;4) Delayed reward: by appropriately s=\r\netting the parameters of your random MDP generator you could instantiate en=\r\nvironments that have delayed reward. For example, consider the case where y=\r\nou have a large discrete state space and only one path towards a goal/absor=\r\nbing state. This path might have a small negative reward on transitions alo=\r\nng the way, e.g., -0.1, however, the last transition towards the absorbing =\r\nstate has a huge reward, e.g., +100. Moreover, transitions from/to other st=\r\nates that are not on this optimal path, might have a reward value greater t=\r\nhan or equal to -0.1, but much smaller than 100, so that this path remains =\r\noptimal.&lt;/div&gt;\n\n\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;This problem might be difficult for tr=\r\naditional neuroevolution especially as the length of the optimal path incre=\r\nases (another difficulty dimension perhaps?), however, I believe a novelty =\r\nsearch approach is promising here because what you really need now is bette=\r\nr exploration.&lt;/div&gt;\n&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/blockquote&gt;&lt;div&gt;&lt;br&gt;&lt;/=\r\ndiv&gt;&lt;div&gt;Yes, this is an interesting question in general, and certainly pre=\r\nvious results on simple deceptive domains indicate that for delayed-reward =\r\nMDP environments like you describe (neuro)evolution will likely get stuck i=\r\nf the search isn&#39;t aided by something like Novelty Search. In our paper=\r\n we don&#39;t worry about this issue as we are comparing the performance of=\r\n the different neural network models relative to each other and are not par=\r\nticularly interested in their performance relative to the maximum possible =\r\n(we do scale the results relative to the maximum possible but this is simpl=\r\ny to make aggregation of results easier).&lt;/div&gt;\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Let me =\r\nknow if anything is still unclear or if you have further questions or comme=\r\nnts.&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Cheers,&lt;/div&gt;&lt;div&gt;Oliver&lt;/div&gt;&lt;div&gt;=C2=A0&lt;/di=\r\nv&gt;&lt;blockquote class=3D&quot;gmail_quote&quot; style=3D&quot;margin:0px 0px 0px 0.8ex;borde=\r\nr-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid=\r\n;padding-left:1ex&quot;&gt;\n&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div dir=3D&quot;ltr&quot;&gt;\n\n&lt;/div&gt;&lt;div&gt;&lt;div&gt;=\r\n&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;br&gt;&lt;div class=3D&quot;gmail_quote&quot;&gt;On Tue, Apr 2=\r\n9, 2014 at 4:28 AM, Oliver Coleman &lt;span dir=3D&quot;ltr&quot;&gt;&lt;&lt;a href=3D&quot;mailto:=\r\noliver.coleman@...&quot; target=3D&quot;_blank&quot;&gt;oliver.coleman@...&lt;/a&gt;&gt=\r\n;&lt;/span&gt; wrote:&lt;br&gt;\n\n\n&lt;blockquote class=3D&quot;gmail_quote&quot; style=3D&quot;border-lef=\r\nt-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid&quot;&gt;\n\n\n=\r\n&lt;u&gt;&lt;/u&gt;\n\n\n\n\n\n\n\n\n\n \n&lt;div&gt;\n&lt;span&gt;=C2=A0&lt;/span&gt;\n\n\n&lt;div&gt;\n  &lt;div&gt;\n\n\n    &lt;div&gt;\n  =\r\n    \n      \n      &lt;p&gt;&lt;/p&gt;&lt;div dir=3D&quot;ltr&quot;&gt;&lt;font face=3D&quot;georgia, serif&quot;&gt;Hi =\r\nall,&lt;/font&gt;&lt;div&gt;&lt;font face=3D&quot;georgia, serif&quot;&gt;&lt;br&gt;&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;font f=\r\nace=3D&quot;georgia, serif&quot;&gt;Jeff Clune, Alan Blair and I are pleased to announce=\r\n a new GECCO paper, &quot;Automated Generation of Environments to Test the =\r\nGeneral Learning Capabilities of AI Agents&quot;&lt;/font&gt;&lt;/div&gt;\n\n\n\n&lt;div&gt;&lt;font=\r\n face=3D&quot;georgia, serif&quot;&gt;&lt;br&gt;&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;font face=3D&quot;georgia, serif=\r\n&quot;&gt;We&#39;d love to hear your comments and questions.&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;font=\r\n face=3D&quot;georgia, serif&quot;&gt;&lt;br&gt;&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;font face=3D&quot;georgia, serif=\r\n&quot;&gt;&lt;b&gt;Video summary:&lt;/b&gt;=C2=A0&lt;a href=3D&quot;http://youtu.be/itbKerV9g6s&quot; target=\r\n=3D&quot;_blank&quot;&gt;http://youtu.be/itbKerV9g6s&lt;/a&gt;&lt;/font&gt;&lt;/div&gt;\n\n\n\n&lt;div&gt;&lt;font face=\r\n=3D&quot;georgia, serif&quot;&gt;&lt;br&gt;&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;div&gt;&lt;font face=3D&quot;georgia, serif=\r\n&quot;&gt;&lt;b&gt;PDF:&lt;/b&gt;=C2=A0&lt;a href=3D&quot;http://goo.gl/t3tCp2&quot; target=3D&quot;_blank&quot;&gt;http:=\r\n//goo.gl/t3tCp2&lt;/a&gt;&lt;/font&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;font face=3D&quot;georgia, serif&quot;&gt;&lt;b=\r\nr&gt;\n\n\n&lt;/font&gt;&lt;/div&gt;\n&lt;div&gt;&lt;div&gt;&lt;font face=3D&quot;georgia, serif&quot;&gt;&lt;b&gt;Abstract&lt;/b&gt;:=\r\n Algorithms for evolving agents that learn during their lifetime have typic=\r\nally been evaluated on only a handful of environments. Designing such envir=\r\nonments is labour intensive, potentially biased, and provides only a small =\r\nsample size that may prevent accurate general conclusions from being drawn.=\r\n In this paper we introduce a method for automatically generating MDP envir=\r\nonments which allows the difficulty to be scaled in several ways. We presen=\r\nt a case study in which environments are generated that vary along three ke=\r\ny dimensions of difficulty: the number of environment configurations, the n=\r\number of available actions, and the length of each trial. The study reveals=\r\n interesting differences between three neural network models -- Fixed-Weigh=\r\nt, Plastic-Weight, and Modulated Plasticity -- that would not have been obv=\r\nious without sweeping across these different dimensions. Our paper thus int=\r\nroduces a new way of conducting reinforcement learning science: instead of =\r\nmanually designing a few environments, researchers will be able to automati=\r\ncally generate a range of environments across key dimensions of variation. =\r\nThis will allow scientists to more rigorously assess the general learning c=\r\napabilities of an algorithm, and may ultimately improve the rate at which w=\r\ne discover how to create AI with general purpose learning.=C2=A0&lt;/font&gt;&lt;/di=\r\nv&gt;\n\n\n\n&lt;/div&gt;&lt;div&gt;&lt;font face=3D&quot;georgia, serif&quot;&gt;&lt;br&gt;&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;font =\r\nface=3D&quot;georgia, serif&quot;&gt;All data and software is available at=C2=A0&lt;a href=\r\n=3D&quot;http://goo.gl/n4D3w6&quot; target=3D&quot;_blank&quot;&gt;http://goo.gl/n4D3w6&lt;/a&gt;&lt;/font&gt;=\r\n&lt;/div&gt;&lt;div&gt;&lt;div&gt;\n\n\n&lt;div dir=3D&quot;ltr&quot;&gt;&lt;div&gt;\n&lt;font face=3D&quot;georgia, serif&quot;&gt;&lt;br=\r\n&gt;&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;font face=3D&quot;georgia, serif&quot;&gt;T: &lt;a href=3D&quot;tel:%2B61%20=\r\n421%20972%20953&quot; value=3D&quot;+61421972953&quot; target=3D&quot;_blank&quot;&gt;+61 421 972 953&lt;/=\r\na&gt; |=C2=A0E: &lt;a href=3D&quot;mailto:oliver.coleman@...&quot; target=3D&quot;_blank&quot;&gt;=\r\noliver.coleman@...&lt;/a&gt;=C2=A0|=C2=A0W:=C2=A0&lt;a href=3D&quot;http://ojcolema=\r\nn.com&quot; target=3D&quot;_blank&quot;&gt;http://ojcoleman.com&lt;/a&gt;&lt;br&gt;\n\n\n\n&lt;/font&gt;&lt;/div&gt;&lt;div&gt;=\r\n&lt;div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;\n&lt;/div&gt;&lt;/div&gt;\n&lt;p&gt;&lt;/p&gt;\n\n    &lt;/di=\r\nv&gt;\n     \n\n    \n    &lt;div style=3D&quot;color:rgb(255,255,255);min-height:0px&quot;&gt;&lt;/d=\r\niv&gt;\n\n\n&lt;/div&gt;\n\n\n\n  \n\n\n\n\n\n\n&lt;/div&gt;&lt;/div&gt;&lt;/blockquote&gt;&lt;/div&gt;&lt;br&gt;&lt;/div&gt;\n&lt;/div&gt;&lt;/=\r\ndiv&gt;&lt;p&gt;&lt;/p&gt;\n\n    &lt;/div&gt;\n     \n\n    \n    &lt;div style=3D&quot;color:rgb(255,255,255=\r\n);min-height:0px&quot;&gt;&lt;/div&gt;\n\n\n&lt;/div&gt;\n\n\n\n  \n\n\n\n\n\n\n&lt;/blockquote&gt;&lt;/div&gt;&lt;br&gt;&lt;/div&gt;=\r\n&lt;/div&gt;&lt;/div&gt;\n\r\n--f46d04447e6108d53304f837c0fe--\r\n\n"}}