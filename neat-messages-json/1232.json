{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":82117382,"authorName":"Jim O&#39;Flaherty, Jr.","from":"&quot;Jim O&#39;Flaherty, Jr.&quot; &lt;jim_oflaherty_jr@...&gt;","profile":"jim_oflaherty_jr","replyTo":"LIST","senderId":"KK3dOMoISfJ1n9VBnJusH2ZiYr9OyHnig_hPI0cwFpadw-QNmtin3IPzATi0LFXq79s70efH0YerqjL8O6KnOBov13a1H6PGyeVye2lfJZ1h8DYz6y6wFOI","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] Re: An ANN design question...","postDate":"1090612052","msgId":1232,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDIwMDQwNzIzMTk0NzMyLjMzOTkxLnFtYWlsQHdlYjUyODA5Lm1haWwueWFob28uY29tPg==","inReplyToHeader":"PGNkcmxwMys0YWw0QGVHcm91cHMuY29tPg=="},"prevInTopic":1231,"nextInTopic":1233,"prevInTime":1231,"nextInTime":1233,"topicId":1226,"numMessagesInTopic":19,"msgSnippet":"Ken, Thank you for the quick and brief response.  I really appreciate it. Good luck with your dissertation this weekend. Jim","rawEmail":"Return-Path: &lt;jim_oflaherty_jr@...&gt;\r\nX-Sender: jim_oflaherty_jr@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 88918 invoked from network); 23 Jul 2004 19:47:33 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m23.grp.scd.yahoo.com with QMQP; 23 Jul 2004 19:47:33 -0000\r\nReceived: from unknown (HELO web52809.mail.yahoo.com) (206.190.39.173)\n  by mta5.grp.scd.yahoo.com with SMTP; 23 Jul 2004 19:47:32 -0000\r\nMessage-ID: &lt;20040723194732.33991.qmail@...&gt;\r\nReceived: from [205.158.160.209] by web52809.mail.yahoo.com via HTTP; Fri, 23 Jul 2004 12:47:32 PDT\r\nDate: Fri, 23 Jul 2004 12:47:32 -0700 (PDT)\r\nTo: neat@yahoogroups.com\r\nIn-Reply-To: &lt;cdrlp3+4al4@...&gt;\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=us-ascii\r\nX-eGroups-Remote-IP: 206.190.39.173\r\nFrom: &quot;Jim O&#39;Flaherty, Jr.&quot; &lt;jim_oflaherty_jr@...&gt;\r\nSubject: Re: [neat] Re: An ANN design question...\r\nX-Yahoo-Group-Post: member; u=82117382\r\nX-Yahoo-Profile: jim_oflaherty_jr\r\n\r\nKen,\n\nThank you for the quick and brief response.  I really appreciate it.\n\nGood luck with your dissertation this weekend.\n\n\nJim\n\n\n--- Kenneth Stanley &lt;kstanley@...&gt; wrote:\n&gt; Jim, I&#39;ll get back to you on this in a couple days-my  dissertation is\n&gt; due this weekend!!! \n&gt; \n&gt; But a short answer is that at least in my implementation of NEAT the\n&gt; inputs don&#39;t go through any activation function.  Anyway, admittedly I\n&gt; skimmed what you wrote so I will read it in more detail after the\n&gt; dissertation is finished and reply in more detail.\n&gt; \n&gt; ken\n&gt; \n&gt; --- In neat@yahoogroups.com, &quot;Jim O&#39;Flaherty, Jr.&quot;\n&gt;  &lt;jim_oflaherty_jr@y...&gt; wrote:\n&gt; &gt; Ken,\n&gt; &gt; \n&gt; &gt; As I have written previously here, I am working on an optimized ANN\n&gt; implementation in Java which I\n&gt; &gt; have named SEMIANN (Sparsely Evaluated Matrix Interace Artificial\n&gt; Neural Network).\n&gt; &gt; \n&gt; &gt; In a meeting with Derek and Philip, we were reviewing my design and\n&gt; comparing it with the design\n&gt; &gt; they are currently using derived from your NEAT ANN design.  What\n&gt; showed up was a small difference\n&gt; &gt; in how I am handling the input data versus how it is being handled\n&gt; in their NEAT implementation.\n&gt; &gt; \n&gt; &gt; It is my understanding from the small number of ANN implementations\n&gt; I have seen (around 5)\n&gt; &gt; including that of David Fogel (author of book titled &quot;Blondie24&quot;\n&gt; from which I am duplicating\n&gt; &gt; experiments), the input data is placed directly into the input node.\n&gt;  The data is not bounded\n&gt; &gt; (other than the actual limits of a float or double).  And the input\n&gt; node does *not* have an\n&gt; &gt; activation function.  The unbounded data present in the input node\n&gt; is then used in the activation\n&gt; &gt; of the hidden nodes (simple 3 layer feed forward network).  Any sort\n&gt; of altering the input data is\n&gt; &gt; then handled by the weight attached to that input node.  The GA\n&gt; process will then drift the\n&gt; &gt; weights around such that inputs which are not so valuable are muted\n&gt; with smaller weight values. \n&gt; &gt; And inputs which are important are magnified with higher weight\n&gt; values.  And all of these weights\n&gt; &gt; will eventually form a function over which the input data is\n&gt; &quot;normalized&quot; based on each input&#39;s\n&gt; &gt; relative importance, a sort of first approximation of the input\n&gt; data&#39;s inter-relatedness.\n&gt; &gt; \n&gt; &gt; In contrast, Philip and Derek indicated an input data point entering\n&gt; their ANN implementation is\n&gt; &gt; actually being pushed through the input node&#39;s activation function.\n&gt;  Then the &quot;modified&quot; data\n&gt; &gt; point is now placed into the input node.  It is then used in the\n&gt; activation of the &quot;hidden&quot; nodes.\n&gt; &gt; \n&gt; &gt; In talking through the difference, we talked about how that might\n&gt; impact the efficacy of the\n&gt; &gt; evolving ANN.  In other words, by having the input data go through\n&gt; an activation function without\n&gt; &gt; their being a weight involved, it seems the input data is being\n&gt; skewed, meaningful data is lost \n&gt; &gt; with no opportunity for the GA to compensate prior to the data loss.\n&gt;  Essentially, some data is\n&gt; &gt; lost.  In pure mathematical terms, this implmentation provides a\n&gt; weight of 1.0 multiplied by the\n&gt; &gt; unbounded input value which is then submitted to the input node&#39;s\n&gt; activation function with the\n&gt; &gt; result of the function being placed into the input node.\n&gt; &gt; \n&gt; &gt; My immediate response was this: the skewing seems like it would make\n&gt; it more difficult for the ANN\n&gt; &gt; to generate associations to the inputs that range outside of the\n&gt; bounds of the activation\n&gt; &gt; function.  For example, in replicating Fogel&#39;s experiments, I am\n&gt; using the hyperbolic tangent \n&gt; &gt; bounded -1..1, and the following input values are used: a red\n&gt; checker has the value of 1.0, a\n&gt; &gt; black check has the value of -1.0, a red king has the value of 1.5,\n&gt; and a black king has the value\n&gt; &gt; of -1.5.\n&gt; &gt; \n&gt; &gt; Now, I know that Fogel was expecting the inputs to be related\n&gt; directly, as a ratio, as he\n&gt; &gt; discusses this at some length in his book.  He left it up to the\n&gt; GA/ANN to work out the optimal\n&gt; &gt; ratio relationship.  Additionally, the king&#39;s value was a GA\n&gt; parameter which was bounded between\n&gt; &gt; 1.0 and 3.0 and could randomly change by +/- 0.1 when a parent was\n&gt; generating a descendant.\n&gt; &gt; \n&gt; &gt; With the approach Philip and Derek have taken (and they said theirs\n&gt; is modeled after your design),\n&gt; &gt; it seems like the ratio gets perverted by the activation function on\n&gt; the input node.  So as input\n&gt; &gt; values fall further and further from the activation function bounds,\n&gt; relationships between inputs\n&gt; &gt; outside of the bounds are eventually lost due to\n&gt; approximation/rounding errors in the IEEE float\n&gt; &gt; or double.  Or so it seems to me.\n&gt; &gt; \n&gt; &gt; So my questions are this:\n&gt; &gt; A) What is the theoretical or mathematical explanation as to why the\n&gt; input values for NEAT are\n&gt; &gt; pushed through an input node&#39;s activation function as opposed to\n&gt; being used directly?\n&gt; &gt; B) Does some form of assumption exist in which to provide input to a\n&gt; NEAT ANN, the input data\n&gt; &gt; point for each input node must be scaled such that the data\n&gt; point&#39;s\n&gt; relevant range of values falls\n&gt; &gt; between the upper and lower bounds of the input node&#39;s\n&gt; activation\n&gt; function?\n&gt; &gt; C) What kinds of different types of activation functions on an input\n&gt; node might possibly handle\n&gt; &gt; this differently and/or more effectively?\n&gt; &gt; \n&gt; &gt; Sidenote: In SEMIANN, I do not allow a connection to have a\n&gt; destination of an input node.  So\n&gt; &gt; there is no need for an activation function at an input node.  And\n&gt; input node is treated as just\n&gt; &gt; an unbounded data point.  It was my understanding that if there was\n&gt; to be feedback to the &quot;input&quot;,\n&gt; &gt; it would occur as new nodes and connections around the hidden/output\n&gt; nodes from which the\n&gt; &gt; particular input node was connected.\n&gt; &gt; \n&gt; &gt; Well, that sort of took much longer to present than I initially\n&gt; thought.  Hmmmï¿½\n&gt; &gt; \n&gt; &gt; Thank you for any clarification(s) you can offer on this.\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; Jim O&#39;Flaherty, Jr.\n&gt; \n&gt; \n\n\n"}}