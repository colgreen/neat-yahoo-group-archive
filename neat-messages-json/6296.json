{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":234577593,"authorName":"Oliver Coleman","from":"Oliver Coleman &lt;oliver.coleman@...&gt;","profile":"olivercoleman04","replyTo":"LIST","senderId":"cqnzKQJEGT7Q-ybRWW6c0iN8rNBmv8iOAL7KeLgEW7VkRjXJ00Fc891u1LA3p6cjWVsKLljKe16nQdGNBhYJmvBq_O74HKroB4EMJ1D4SP4","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [neat] New paper: Automated Generation of Environments to Test the General Learning Capabilities of AI Agents","postDate":"1398983172","msgId":6296,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PENBK2R1aW1PSk1md1lvRVZwX1o9Rnc5bVF5ZmhndWYwRitHdE5DYVd6Tk80ZlNWTGM4UUBtYWlsLmdtYWlsLmNvbT4=","inReplyToHeader":"PENBTnRYaG1zdk5oK2Jpcz1zZUV1cG53eTRVaUZ0Z3FjTndoQ0I4VGkxYW50QURrQ2VWd0BtYWlsLmdtYWlsLmNvbT4=","referencesHeader":"PENBK2R1aW1PMjRzYWtPWFNNVnVxYkVleDgremlCbVFIdmVjb1kza3dBZCt6QUI1Wmt3UUBtYWlsLmdtYWlsLmNvbT4JPENBTnRYaG12dUpHMkxkWXpSRGVXRldpU01HNW1iK3pmQkxWZ0VhQm10dHkyV0ZQaXhFd0BtYWlsLmdtYWlsLmNvbT4JPENBK2R1aW1ONCtZVTMtelN4ZnV1ek9OLVAtcnI4NVBTcHMrMDFDMW5Wa2twSkxjUmR0d0BtYWlsLmdtYWlsLmNvbT4JPENBTnRYaG10eG9oTzRSZmhVYzBCUzRhMmZXMTlKY2pEYmUtOHEwdzlNSDFXalhjbzh6UUBtYWlsLmdtYWlsLmNvbT4JPEY1MkE3MUQ3LURGMzUtNEEzMC1BM0ZDLTk3RjY0NjIwMzg0N0BnbWFpbC5jb20+CTxDQUpuNj1kckVOcjJzaFlEZmJLWmkzWEViQ3g2NDBwUUgxMWZhSnRYaENaeXZxSkd3THdAbWFpbC5nbWFpbC5jb20+CTxDQStkdWltUENvVUZ5MVc1V0VkK0ZyaWdYWlpSdXJGUTlSR3hPNS1LeEFEWkdnVXBMQkFAbWFpbC5nbWFpbC5jb20+CTxDQU50WGhtc3ZOaCtiaXM9c2VFdXBud3k0VWlGdGdxY053aENCOFRpMWFudEFEa0NlVndAbWFpbC5nbWFpbC5jb20+"},"prevInTopic":6291,"nextInTopic":0,"prevInTime":6295,"nextInTime":6297,"topicId":6279,"numMessagesInTopic":11,"msgSnippet":"Oh, sorry! The number of states is 4 for all experiments. For these kinds of set-ups, where there are a fixed number of environment configurations (eg (double)","rawEmail":"Return-Path: &lt;oliver.coleman@...&gt;\r\nX-Sender: oliver.coleman@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 52019 invoked by uid 102); 1 May 2014 22:26:13 -0000\r\nX-Received: from unknown (HELO mtaq1.grp.bf1.yahoo.com) (10.193.84.32)\n  by m13.grp.bf1.yahoo.com with SMTP; 1 May 2014 22:26:13 -0000\r\nX-Received: (qmail 26430 invoked from network); 1 May 2014 22:26:13 -0000\r\nX-Received: from unknown (HELO mail-wi0-f170.google.com) (209.85.212.170)\n  by mtaq1.grp.bf1.yahoo.com with SMTP; 1 May 2014 22:26:13 -0000\r\nX-Received: by mail-wi0-f170.google.com with SMTP id f8so1417706wiw.1\n        for &lt;neat@yahoogroups.com&gt;; Thu, 01 May 2014 15:26:12 -0700 (PDT)\r\nMIME-Version: 1.0\r\nX-Received: by 10.194.189.201 with SMTP id gk9mr10830657wjc.13.1398983172761;\n Thu, 01 May 2014 15:26:12 -0700 (PDT)\r\nX-Received: by 10.194.237.72 with HTTP; Thu, 1 May 2014 15:26:12 -0700 (PDT)\r\nIn-Reply-To: &lt;CANtXhmsvNh+bis=seEupnwy4UiFtgqcNwhCB8Ti1antADkCeVw@...&gt;\r\nReferences: &lt;CA+duimO24sakOXSMVuqbEex8+ziBmQHvecoY3kwAd+zAB5ZkwQ@...&gt;\n\t&lt;CANtXhmvuJG2LdYzRDeWFWiSMG5mb+zfBLVgEaBmtty2WFPixEw@...&gt;\n\t&lt;CA+duimN4+YU3-zSxfuuzON-P-rr85PSps+01C1nVkkpJLcRdtw@...&gt;\n\t&lt;CANtXhmtxohO4RfhUc0BS4a2fW19JcjDbe-8q0w9MH1WjXco8zQ@...&gt;\n\t&lt;F52A71D7-DF35-4A30-A3FC-97F646203847@...&gt;\n\t&lt;CAJn6=drENr2shYDfbKZi3XEbCx640pQH11faJtXhCZyvqJGwLw@...&gt;\n\t&lt;CA+duimPCoUFy1W5WEd+FrigXZZRurFQ9RGxO5-KxADZGgUpLBA@...&gt;\n\t&lt;CANtXhmsvNh+bis=seEupnwy4UiFtgqcNwhCB8Ti1antADkCeVw@...&gt;\r\nDate: Fri, 2 May 2014 08:26:12 +1000\r\nMessage-ID: &lt;CA+duimOJMfwYoEVp_Z=Fw9mQyfhguf0F+GtNCaWzNO4fSVLc8Q@...&gt;\r\nTo: neat@yahoogroups.com\r\nContent-Type: multipart/alternative; boundary=047d7bb0398a0f2fac04f85e272d\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Oliver Coleman &lt;oliver.coleman@...&gt;\r\nSubject: Re: [neat] New paper: Automated Generation of Environments to Test\n the General Learning Capabilities of AI Agents\r\nX-Yahoo-Group-Post: member; u=234577593; y=GhcKbr5S3ONcW881vUQOx0Y4p8OXSnhaJ8b_RxjYqGAJ6VwCIEgs8p6eIfgeNr-KhBJW_CUW_w\r\nX-Yahoo-Profile: olivercoleman04\r\n\r\n\r\n--047d7bb0398a0f2fac04f85e272d\r\nContent-Type: text/plain; charset=UTF-8\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\nOh, sorry! The number of states is 4 for all experiments.\n\nFor these kinds =\r\nof set-ups, where there are a fixed number of environment\nconfigurations (e=\r\ng (double) T-maze) evolution produces neural networks that\ncan perform a fi=\r\nxed set of good/optimal behaviours (in this case sequence\nof actions), one =\r\nfor each configuration, and try them in turn, one per\ntrial, until the beha=\r\nviour that provides a good/optimal reward is found,\nand then remember to ke=\r\nep performing this behaviour until that behaviour\nstops providing a good/op=\r\ntimal reward. Thus the networks aren&#39;t behaving\nlike a regular RL algorithm=\r\n. The reason this approach was taken is because\n(from the paper): &quot;Switchin=\r\ng between environment configurations during the\nagent&#39;s lifetime, with each=\r\n configuration being presented for several\nconsecutive trials before the sw=\r\nitch, is similar in spirit to most\nenvironments from previous work on evolv=\r\ning plastic neural networks.&quot;\n\n\nT: 0421 972 953 | E: oliver.coleman@gmail.c=\r\nom | W: http://ojcoleman.com\n\n\n\nOn 2 May 2014 00:45, Vassilis Vassiliades &lt;=\r\nvassilisvas@...&gt; wrote:\n\n&gt;\n&gt;\n&gt; Hi Oliver,\n&gt;\n&gt; From the paper: &quot;Where =\r\nnot specified, the number of actions and the length\n&gt;&gt; of trials are set to=\r\n 4 and the number of environment configurations per run\n&gt;&gt; is 8.&quot;\n&gt;&gt;\n&gt;\n&gt; Ye=\r\ns, I read that part, but I was wondering about the number of *states*,\n&gt; no=\r\nt the number of actions or the length of the trials. :)\n&gt;\n&gt; The reason I am=\r\n asking is because there exist practically optimal\n&gt; algorithms in reinforc=\r\nement learning that have a sample complexity of ~\n&gt; O(SA), where S is the s=\r\nize of the state space and A the number of actions\n&gt; per state; however, th=\r\ney behave optimally in (1) stationary and (2) fully\n&gt; observable environmen=\r\nts. The environments you present are fully observable,\n&gt; but nonstationary,=\r\n so it is harder to perform optimally. Since you are\n&gt; generating *random* =\r\nMDPs per configuration, I believe that the length of\n&gt; trials must be *at l=\r\neast* equal to S*A, since you want to allow the\n&gt; networks to fully explore=\r\n the current environment configuration, before\n&gt; moving on to a new one.\n&gt;\n=\r\n&gt;\n&gt; Vassilis\n&gt;\n&gt;  \n&gt;\n\r\n--047d7bb0398a0f2fac04f85e272d\r\nContent-Type: text/html; charset=UTF-8\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\n&lt;div dir=3D&quot;ltr&quot;&gt;Oh, sorry! The number of states is 4 for all experiments.&lt;=\r\ndiv&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;For these kinds of set-ups, where there are a fixed numb=\r\ner of environment configurations (eg (double) T-maze) evolution produces ne=\r\nural networks that can perform a fixed set of good/optimal behaviours (in t=\r\nhis case sequence of actions), one for each configuration, and try them in =\r\nturn, one per trial, until the behaviour that provides a good/optimal rewar=\r\nd is found, and then remember to keep performing this behaviour until that =\r\nbehaviour stops providing a good/optimal reward. Thus the networks aren&#39=\r\n;t behaving like a regular RL algorithm. The reason this approach was taken=\r\n is because (from the paper): &quot;Switching between environment configura=\r\ntions during the agent&#39;s lifetime, with each configuration being presen=\r\nted for several consecutive trials before the switch, is similar in spirit =\r\nto most environments from previous work on evolving plastic neural networks=\r\n.&quot;&lt;/div&gt;\n&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br clear=3D&quot;all&quot;&gt;&lt;div&gt;&lt;div =\r\ndir=3D&quot;ltr&quot;&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;T: 0421 972 953 |=C2=A0E: &lt;a href=3D&quot;mailto=\r\n:oliver.coleman@...&quot; target=3D&quot;_blank&quot;&gt;oliver.coleman@...&lt;/a&gt;=\r\n=C2=A0|=C2=A0W:=C2=A0&lt;a href=3D&quot;http://ojcoleman.com&quot; target=3D&quot;_blank&quot;&gt;htt=\r\np://ojcoleman.com&lt;/a&gt;&lt;br&gt;\n&lt;/div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;=\r\n&lt;/div&gt;\n&lt;br&gt;&lt;br&gt;&lt;div class=3D&quot;gmail_quote&quot;&gt;On 2 May 2014 00:45, Vassilis Vas=\r\nsiliades &lt;span dir=3D&quot;ltr&quot;&gt;&lt;&lt;a href=3D&quot;mailto:vassilisvas@...&quot; tar=\r\nget=3D&quot;_blank&quot;&gt;vassilisvas@...&lt;/a&gt;&gt;&lt;/span&gt; wrote:&lt;br&gt;&lt;blockquote c=\r\nlass=3D&quot;gmail_quote&quot; style=3D&quot;margin:0 0 0 .8ex;border-left:1px #ccc solid;=\r\npadding-left:1ex&quot;&gt;\n\n\n\n&lt;u&gt;&lt;/u&gt;\n\n\n\n\n\n\n\n\n\n \n&lt;div style=3D&quot;background-color:#ff=\r\nf&quot;&gt;\n&lt;span&gt;=C2=A0&lt;/span&gt;\n\n\n&lt;div&gt;\n  &lt;div&gt;\n\n\n    &lt;div&gt;\n      \n      \n      &lt;p&gt;=\r\n&lt;/p&gt;&lt;div dir=3D&quot;ltr&quot;&gt;Hi Oliver,&lt;div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;div class=\r\n=3D&quot;gmail_quote&quot;&gt;&lt;div class=3D&quot;&quot;&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;blockquote class=3D&quot;gmail_=\r\nquote&quot; style=3D&quot;border-left:1px #ccc solid&quot;&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;\n&lt;div dir=\r\n=3D&quot;ltr&quot;&gt;&lt;div&gt;&lt;/div&gt;&lt;div&gt;From the paper: &quot;Where not specified, the num=\r\nber of actions and the length of trials are set to 4 and the number of envi=\r\nronment configurations per run is 8.&quot;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;\n\n&lt;/div&gt;&lt;=\r\n/div&gt;&lt;/blockquote&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;Yes, I read that part, but I wa=\r\ns wondering about the number of &lt;b&gt;states&lt;/b&gt;, not the number of actions or=\r\n the length of the trials. :)=C2=A0&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;The reason I a=\r\nm asking is because there exist practically optimal algorithms in reinforce=\r\nment learning that have a sample complexity of ~ O(SA), where S is the size=\r\n of the state space and A the number of actions per state; however, they be=\r\nhave optimally in (1) stationary and (2) fully observable environments. The=\r\n environments you present are fully observable, but nonstationary, so it is=\r\n harder to perform optimally. Since you are generating &lt;b&gt;random&lt;/b&gt; MDPs p=\r\ner configuration, I believe that the length of trials must be &lt;b&gt;at least&lt;/=\r\nb&gt; equal to S*A, since you want to allow the networks to fully explore the =\r\ncurrent environment configuration, before moving on to a new one.&lt;/div&gt;\n\n&lt;d=\r\niv&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Vassilis&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;\n&lt;p&gt;=\r\n&lt;/p&gt;\n\n    &lt;/div&gt;\n     \n\n    \n    &lt;div style=3D&quot;color:#fff;min-height:0&quot;&gt;&lt;/d=\r\niv&gt;\n\n\n&lt;/div&gt;\n\n\n\n  \n\n\n\n\n\n\n&lt;/blockquote&gt;&lt;/div&gt;&lt;br&gt;&lt;/div&gt;\n\r\n--047d7bb0398a0f2fac04f85e272d--\r\n\n"}}