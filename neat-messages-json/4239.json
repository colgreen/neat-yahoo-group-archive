{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":274910130,"authorName":"David D&#39;Ambrosio","from":"&quot;David D&#39;Ambrosio&quot; &lt;ddambro84@...&gt;","profile":"ddambroeplex","replyTo":"LIST","senderId":"Xlk4rOOCPtwy8Z8AMHio3LcIlkEp4wtJm9YJMIYkE4ZDDRVzSm4-KQi-Na6QYUwPtpEjy5QwoEp4Gd0F2Zc1IilDn-t5vNTPGjTOha8mQX52","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Questions regarding HyperSharpNeat and basic NEAT questions","postDate":"1217455862","msgId":4239,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGc2cW90bStkN3NmQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGc2cTd2ditxNGNyQGVHcm91cHMuY29tPg=="},"prevInTopic":4238,"nextInTopic":4240,"prevInTime":4238,"nextInTime":4240,"topicId":4238,"numMessagesInTopic":6,"msgSnippet":"Hi Christian, I m the one who converted SharpNEAT to HyperSharpNEAT so hopefully I ll be able to shed some light on the issues you re facing.  I would like to","rawEmail":"Return-Path: &lt;ddambro84@...&gt;\r\nX-Sender: ddambro84@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 77839 invoked from network); 30 Jul 2008 22:11:02 -0000\r\nX-Received: from unknown (66.218.67.97)\n  by m47.grp.scd.yahoo.com with QMQP; 30 Jul 2008 22:11:02 -0000\r\nX-Received: from unknown (HELO n24b.bullet.sp1.yahoo.com) (209.131.38.235)\n  by mta18.grp.scd.yahoo.com with SMTP; 30 Jul 2008 22:11:02 -0000\r\nX-Received: from [216.252.122.219] by n24.bullet.sp1.yahoo.com with NNFMP; 30 Jul 2008 22:11:02 -0000\r\nX-Received: from [66.218.69.5] by t4.bullet.sp1.yahoo.com with NNFMP; 30 Jul 2008 22:11:02 -0000\r\nX-Received: from [66.218.66.85] by t5.bullet.scd.yahoo.com with NNFMP; 30 Jul 2008 22:11:02 -0000\r\nDate: Wed, 30 Jul 2008 22:11:02 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;g6qotm+d7sf@...&gt;\r\nIn-Reply-To: &lt;g6q7vv+q4cr@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;David D&#39;Ambrosio&quot; &lt;ddambro84@...&gt;\r\nSubject: Re: Questions regarding HyperSharpNeat and basic NEAT questions\r\nX-Yahoo-Group-Post: member; u=274910130; y=t-_Y7CBZrs7AGYfPLqhGOLiA_bcNv3-ZUUdo79-XuxrKkRdM\r\nX-Yahoo-Profile: ddambroeplex\r\n\r\nHi Christian,\n\nI&#39;m the one who converted SharpNEAT to HyperSharpNEAT so hop=\r\nefully\nI&#39;ll be able to shed some light on the issues you&#39;re facing.  I woul=\r\nd\nlike to point out that your first question makes it seem like you may\nnot=\r\n be interested in HyperNEAT, but instead CPPNs.  CPPNs are\nessentially neur=\r\nal networks with different activation functions in the\nnodes.  HyperNEAT us=\r\nes CPPNs to create patterns of connections in\ntraditional neural networks (=\r\ni.e. only one activation function) that\nare laid out on a substrate to crea=\r\nte geometric relationships between\nthe nodes.  The answers I give are still=\r\n relevant either way, but\nthat&#39;s something to think about.\n\n1.  The experim=\r\nent stuff is mostly derived from Colin&#39;s SharpNEAT\n(http://sharpneat.source=\r\nforge.net/), and if you&#39;d like information\nabout that, it might be better t=\r\no look at his documentation as it is\nMUCH better than mine, of course I&#39;ll =\r\ntry and answer any questions you\nhave.  With that said, you do have the bas=\r\nic idea: each experiment\nneeds an experiment, a network evaluator and a pop=\r\nulation evaluator. \nThe basic substrate should work, but only for the simpl=\r\nest of\nsituations, you should inherit it and override the generateGenome\nfu=\r\nnction to represent your substrate.  Of course, if you are only\ninterested =\r\nin evolving CPPNs, you can completely ignore the substrate\nclass as well as=\r\n the the related parameters in the params.txt file,\nand the algorithm will =\r\nproduce just CPPNs.\n\n2.  The reason that MultipleSteps takes a variable num=\r\nber is because\nNEAT evolves networks of varying topologies which can includ=\r\ne\nrecurrent connections, making it impossible to determine the &quot;correct&quot;\nnu=\r\nmber of times to activate the network.  Colin did include\nRelaxNetwork func=\r\ntion which will run a network for some number of\nsteps or until the outputs=\r\n stop changing.\n\n3.  I don&#39;t use Hyperbolic Tanget for the simple reason th=\r\nat there\nwere several sigmoids included with SharpNEAT and they have about =\r\nthe\nsame functionality for what I use them for.  Colin did some cool stuff\n=\r\nto make it really easy to add your own activation functions though. \nSimply=\r\n make a class that implements IActivationFuction and put it in\nthe Activati=\r\nonFunctions folder and in the NeuralNetwork namespace.  It\ncan then be acce=\r\nssed through the factory.  To use it with my code,\njust add the name and pr=\r\nobability of that function occurring to the\nparams.txt file.\n\n4. I don&#39;t ha=\r\nve much experience with this issue, but my advice is that\nit depends on whi=\r\nch functions you have in your network.  If the\nfunctions in the network are=\r\n bipolar (-1,1) then your binary\nrepresentation should be as well.  Ken&#39;s o=\r\nriginal XOR experiment used\n&gt;.5 as 1 and &lt;.5 as 0, but that&#39;s because he us=\r\ned sigmoids that ranged\nfrom 0 to 1.\n\nHopefully that&#39;s answered everything,=\r\n if you have any further\nquestions you can email me at ddambro@...=\r\n and I&#39;m sure the\ngroup would be happy to address any general NEAT and Hype=\r\nrNEAT ideas\nor questions you might have.\n\nDavid D&#39;Ambrosio\n\n--- In neat@yah=\r\noogroups.com, &quot;chhofchhof&quot; &lt;Christian.Hofmann@...&gt; wrote:\n&gt;\n&gt; Hello,\n&gt; \n&gt; I=\r\n am new to this forum and stumbled lately upon the NEAT-idea. On the\n&gt; same=\r\n day I found HyperNeat. And as I am a c# developer I want to use\n&gt; HyperSha=\r\nrpNeat.\n&gt; \n&gt; Regarding this one I have some initial questions. Maybe somebo=\r\ndy has\n&gt; time to answer some of these questions.\n&gt; \n&gt; 1) To create a new ex=\r\nperiment I only need to implement classes with\n&gt; the interfaces IExperiment=\r\n, INetworkEvaluator,\n&gt; MultiThreadedPopulationEvaluator (here I can just us=\r\ne the base method\n&gt; as demonstrated in Skirmish experiment). Am I right?\n&gt; =\r\nCan I also use the base Substrate class or will I need to create my\n&gt; own? =\r\nI want to give my networks the possibilities to use different\n&gt; activation =\r\nfunctions. That&#39;s what the substrate class is for, am I\nright?\n&gt; \n&gt; 2) Call=\r\ning MultipleSteps(n). Let&#39;s say I have a network with 1 input,\n&gt; 1 output a=\r\nnd 2 hidden layers. Calling SingleStep / MultipleSteps(1) I\n&gt; asume, that t=\r\nhe values from the input layer will be propagated to the\n&gt; first hidden lay=\r\ner. The values from the first hidden layer to the\n&gt; second and the second h=\r\nidden layer values to the output layer. So I\n&gt; would need to call MultipleS=\r\nteps(3) / MultipleSteps(1+\n&gt; NumberOfHiddenLayers) till the input values re=\r\nached the output. Is\n&gt; this correct? Why is there a need to set this value.=\r\n Is it not\n&gt; possible to just do so many steps till the input reached the o=\r\nutput?\n&gt; Maybe I am also misunderstanding things here.\n&gt; \n&gt; 3) Activation f=\r\nunctions: I want to give the network all activation\n&gt; functions that are po=\r\nssible so it has more to choose from. Hyperbolic\n&gt; tangent is one mainly us=\r\ned activation function used in normal\n&gt; perception networks. Is there a rea=\r\nson that it is not available. Or is\n&gt; it obsolete due to an other function =\r\nthat is taking this place? Or\n&gt; shall I add it? \n&gt; \n&gt; 4) Maybe you can also=\r\n help me out on this, too. I have much binary\n&gt; inputs and outputs. Should =\r\nI just use (0 / 1) or convert it to (0 /\n&gt; 0.5) or\n&gt; (-0.5 / 0.5). Do you h=\r\nave experiences on here?\n&gt; \n&gt; Thank you very much for your answers in advan=\r\nce!\n&gt; \n&gt; Kind regards,\n&gt; \n&gt; Christian\n&gt;\n\n\n\n"}}