{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":234577593,"authorName":"Oliver Coleman","from":"Oliver Coleman &lt;oliver.coleman@...&gt;","profile":"olivercoleman04","replyTo":"LIST","senderId":"0emRmgeXWIrWd8LZQtw2EAsKtkHXsjkKjX22JTW0FfU8Zg6G1lSQuzI1yYQa8sIypSslIqGU8GKW2n4M9XVnVx8mcBrr5NVGTyfC5yQyBtk","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: Models of brains, what should we borrow from biology?","postDate":"1336692863","msgId":5804,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PENBK2R1aW1QWlp3MEJXbjg2Z3hXM2FoSmNVY18yVVhlUkNULWFxMjlpcHpXOWhrUExZQUBtYWlsLmdtYWlsLmNvbT4="},"prevInTopic":5802,"nextInTopic":5810,"prevInTime":5803,"nextInTime":5805,"topicId":5801,"numMessagesInTopic":16,"msgSnippet":"Hi Ken, Yes, I m pretty sure that not all of the phenomena I listed are important; and that a good starting point in general is to assume that they are not. I ","rawEmail":"Return-Path: &lt;oliver.coleman@...&gt;\r\nX-Sender: oliver.coleman@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 8302 invoked from network); 10 May 2012 23:34:24 -0000\r\nX-Received: from unknown (98.137.34.45)\n  by m7.grp.sp2.yahoo.com with QMQP; 10 May 2012 23:34:24 -0000\r\nX-Received: from unknown (HELO mail-yx0-f179.google.com) (209.85.213.179)\n  by mta2.grp.sp2.yahoo.com with SMTP; 10 May 2012 23:34:24 -0000\r\nX-Received: by yenr13 with SMTP id r13so778243yen.38\n        for &lt;neat@yahoogroups.com&gt;; Thu, 10 May 2012 16:34:23 -0700 (PDT)\r\nMIME-Version: 1.0\r\nX-Received: by 10.50.179.103 with SMTP id df7mr461237igc.35.1336692863068; Thu,\n 10 May 2012 16:34:23 -0700 (PDT)\r\nX-Received: by 10.231.137.195 with HTTP; Thu, 10 May 2012 16:34:23 -0700 (PDT)\r\nDate: Fri, 11 May 2012 09:34:23 +1000\r\nMessage-ID: &lt;CA+duimPZZw0BWn86gxW3ahJcUc_2UXeRCT-aq29ipzW9hkPLYA@...&gt;\r\nTo: neat@yahoogroups.com\r\nContent-Type: multipart/alternative; boundary=14dae934045146ec5a04bfb70f8a\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Oliver Coleman &lt;oliver.coleman@...&gt;\r\nSubject: Re: Models of brains, what should we borrow from biology?\r\nX-Yahoo-Group-Post: member; u=234577593; y=eduOi62FLPoXggup_JqCJ7JYIvfwBlPoS8lXFDKR4ojnwT0MH858UrOi64iBu-stdjgoBF0OSA\r\nX-Yahoo-Profile: olivercoleman04\r\n\r\n\r\n--14dae934045146ec5a04bfb70f8a\r\nContent-Type: text/plain; charset=ISO-8859-1\r\n\r\nHi Ken,\n\nYes, I&#39;m pretty sure that not all of the phenomena I listed are important;\nand that a good starting point in general is to assume that they are not. I\nalso agree with your argument that a lot of the low-level phenomena we see\nmay be a result of implementation with particular physical systems (and I\nwould add perhaps as a result of evolutionary happenstance). The CPPN is a\nparticularly compelling example of significant abstraction of developmental\nprocesses, producing many of the same features of the end result of\ndevelopmental processes. One thing it does abstract away, in the context of\nplastic networks, is the effect of external input on the developmental\nprocess (which may or may not be an issue depending on details of\nimplementation, problem domain, etc...).\n\nPerhaps we could also assume that, rather than some specific set of\nfunctions being the only workable set, what matters is having a workable\ncombination of functions, and that there are many possible combinations\nthat would work equally well. In this framework we could assume that\nbiological neural networks represent at least a reasonably good combination\nof low-level functions, and so we could use this combination as a guide\n(but of course this doesn&#39;t answer what functions in this combination are\nactually important, or what things can be abstracted away). Also, some\ncombinations may be workable, but are far harder to evolve solutions with,\nor require much larger networks, etc (eg evolving networks incorporating\nneuromodulation of synaptic plasticity can be much easier for some tasks\nthan for those without this type of neuromodulation).\n\nI&#39;m intending to run some experiments to explore these questions (which\nphenomena are important, acceptable level of abstraction, etc), but of\ncourse to try and thoroughly explore all of these functions in many\ncombinations would be a massive undertaking, and is not my main interest,\nso at some point I will have to pick a model and run with it after only a\nfew, hopefully well chosen, experiments... Perhaps one approach is to\ncreate flexible parameterised versions of these functions, and let\nevolution determine what combination is right (like your approach described\nin &quot;Evolving adaptive neural networks with and without adaptive synapses&quot;,\nbut perhaps more flexible and applied to more functions).\n\nDo you mind if I post/quote some/all of this discussion in the comments of\nmy blog post?\n\nCheers,\nOliver\n\r\n--14dae934045146ec5a04bfb70f8a\r\nContent-Type: text/html; charset=ISO-8859-1\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\nHi Ken,&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Yes, I&#39;m pretty sure that not all of the phe=\r\nnomena I listed are important; and that a good starting point in general is=\r\n to assume that they are not. I also agree with your argument that a lot of=\r\n the low-level phenomena we see may be a result of implementation with part=\r\nicular physical systems (and I would add perhaps as a result of evolutionar=\r\ny happenstance). The CPPN is a particularly compelling example of significa=\r\nnt abstraction of developmental processes, producing many of the same featu=\r\nres of the end result of developmental processes. One thing it does abstrac=\r\nt away, in the context of plastic networks, is the effect of external input=\r\n on the developmental process (which may or may not be an issue depending o=\r\nn details of implementation, problem domain, etc...).&lt;/div&gt;\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;=\r\n&lt;div&gt;Perhaps we could also assume that, rather than=A0some specific set of =\r\nfunctions being the only workable set,=A0what matters is having a workable =\r\ncombination of functions, and that there are many possible combinations tha=\r\nt would work equally well. In this framework we could assume that biologica=\r\nl neural networks represent at least a reasonably good combination of low-l=\r\nevel functions, and so we could use this combination as a guide (but of cou=\r\nrse this doesn&#39;t answer what functions in this combination are actually=\r\n important, or what things can be abstracted away). Also, some combinations=\r\n may be workable, but are far harder to evolve solutions with, or require m=\r\nuch larger networks, etc (eg evolving networks incorporating neuromodulatio=\r\nn of synaptic plasticity can be much easier for some tasks than for those w=\r\nithout this type of neuromodulation).&lt;/div&gt;\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;I&#39;m int=\r\nending to run some experiments to explore these questions (which phenomena =\r\nare important, acceptable level of abstraction, etc), but of course to try =\r\nand thoroughly explore all of these functions in many combinations would be=\r\n a massive undertaking, and is not my main interest, so at some point I wil=\r\nl have to pick a model and run with it after only a few, hopefully well cho=\r\nsen, experiments... Perhaps one approach is to create flexible parameterise=\r\nd versions of these functions, and let evolution determine what combination=\r\n is right (like your approach described in &quot;Evolving adaptive neural n=\r\networks with and without adaptive synapses&quot;, but perhaps more flexible=\r\n and applied to more functions).&lt;/div&gt;\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Do you mind if I=\r\n post/quote some/all of this discussion in the comments of my blog post?&lt;/d=\r\niv&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Cheers,&lt;/div&gt;&lt;div&gt;Oliver&lt;/div&gt;\n\r\n--14dae934045146ec5a04bfb70f8a--\r\n\n"}}