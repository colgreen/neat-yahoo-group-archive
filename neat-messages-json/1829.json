{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":7192225,"authorName":"Ian Badcoe","from":"Ian Badcoe &lt;ian_badcoe@...&gt;","profile":"ian_badcoe","replyTo":"LIST","senderId":"wCyiGN5yLcCILTM4y6N7qogjiAOOOlKD5FAjgXY2tgqnBZjS6Nv7xrW27YWyVUAvqNutxepmkEnU2YZ1RmMdzC1tC3Zq3WDMBzQ","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] Re: Symmetry, concepts and data buses in the brain","postDate":"1106154259","msgId":1829,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDYuMi4wLjE0LjAuMjAwNTAxMTgxMjEwNDkuMDNjMDllYThAcG9wLm1haWwueWFob28uY28udWs+","inReplyToHeader":"PGNzaXB0ayttc2RiQGVHcm91cHMuY29tPg==","referencesHeader":"PDYuMi4wLjE0LjAuMjAwNTAxMTQxNzE4MDIuMDI3ZDE5MzBAcG9wLm1haWwueWFob28uY28udWs+IDxjc2lwdGsrbXNkYkBlR3JvdXBzLmNvbT4="},"prevInTopic":1828,"nextInTopic":1830,"prevInTime":1828,"nextInTime":1830,"topicId":1698,"numMessagesInTopic":40,"msgSnippet":"... Hi! ... I m not familiar with the meaning of differentiate program , can you explain it some more? ... Hmm, certainly a non recurrent ANN is a function","rawEmail":"Return-Path: &lt;ian_badcoe@...&gt;\r\nX-Sender: ian_badcoe@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 49777 invoked from network); 19 Jan 2005 17:02:57 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m23.grp.scd.yahoo.com with QMQP; 19 Jan 2005 17:02:57 -0000\r\nReceived: from unknown (HELO smtp003.mail.ukl.yahoo.com) (217.12.11.34)\n  by mta5.grp.scd.yahoo.com with SMTP; 19 Jan 2005 17:02:57 -0000\r\nReceived: from unknown (HELO ian2k.yahoo.co.uk) (ian?badcoe@212.159.73.108 with login)\n  by smtp003.mail.ukl.yahoo.com with SMTP; 19 Jan 2005 17:02:56 -0000\r\nMessage-Id: &lt;6.2.0.14.0.20050118121049.03c09ea8@...&gt;\r\nX-Mailer: QUALCOMM Windows Eudora Version 6.2.0.14\r\nDate: Wed, 19 Jan 2005 17:04:19 +0000\r\nTo: neat@yahoogroups.com\r\nIn-Reply-To: &lt;csiptk+msdb@...&gt;\r\nReferences: &lt;6.2.0.14.0.20050114171802.027d1930@...&gt;\n &lt;csiptk+msdb@...&gt;\r\nMime-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;us-ascii&quot;; format=flowed\r\nX-eGroups-Remote-IP: 217.12.11.34\r\nFrom: Ian Badcoe &lt;ian_badcoe@...&gt;\r\nSubject: Re: [neat] Re: Symmetry, concepts and data buses in the brain\r\nX-Yahoo-Group-Post: member; u=7192225\r\nX-Yahoo-Profile: ian_badcoe\r\n\r\nAt 10:54 18/01/2005, you wrote:\n\n\n&gt;Hi Ian:\n\nHi!\n\n&gt;     Something that I asked in the past about Neural Networks evolution\n&gt;was the some you say at the final of the mail:  is it enough to use\n&gt;the actual kind of Neurons and structures to represent whatever class\n&gt;of response?.\n&gt;\n&gt;The answer I found in the mathematical approach of Neural Networks.\n&gt;They argued that a neural network is a &quot;differentiate program&quot; with\n\nI&#39;m not familiar with the meaning of &quot;differentiate program&quot;, can you \nexplain it some more?\n\n&gt;variables and conditional sentences like a hand-coded program. The\n&gt;difference is in the use of sigmoid functions that don&#39;t let the\n&gt;systems to memorize the variables for a long time.\n\nHmm, certainly a &quot;non recurrent&quot; ANN is a function which maps input to \noutput.  The classification of it as a program must depend a lot on what \nthat phrase &quot;differentiate program&quot; means, and also the definition of &quot;a \nlong time&quot;.  e.g. one could view a loop counter as a very short lived \nvariable, but creating a (recurrent) neural net which loops is a far from \ntrivial problem...\n\n&gt;To solve this and to let to neural networks to be a &quot;Turing machine&quot;,\n&gt;Schmidhuber have introduced the LSTM cells. (see\n&gt;http://www.idsia.ch/~juergen/rnn.html) .\n\nThe idea of ensuring some ANN system is a &quot;Turing machine&quot; is one question, \nbut it is hardly the whole of the problem.  e.g. every desktop computer is \na TM (OK, so the disk should technically be infinite...) but that doesn&#39;t \ngive them the properties I was discussing.  There may be systems which are \n&quot;Turing&quot; and also have those properties, but not every TM will.  Also, if \nwe get back to the example of me, I am only Turning-compliant to the total \nlength of tape I can memorize, which is maybe 7 cells (remember the cells \nchange all the time) and I don&#39;t do that by _being_ a TM, not even purely \nby simulating one, but rather by understanding them.\n\nSo TM&#39;s are interesting as a measure of power, but not necessarily a \nrequirement.\n\n[[Anybody else think &quot;simulating universal Turing machines&quot; should be an \nOlympic sport?]]\n\n&gt;  The use of  LSTM cells (Long Short-Term Memory cells) in a Neural\n&gt;Network let the network to use &quot;memories&quot;, &quot;conditional statements&quot;\n&gt;(&quot;routed events&quot; has more meaning) to extract or store that&#39;s memories\n&gt;depending of such conditional statements .\n&gt;\n&gt;The analysis of evolved LSTM-NEAT networks or the pure LSTM (with\n&gt;other learning algorithm) show that they map some events across time\n&gt;in the right memories (LSTM cells) giving some sort of &quot;mind state&quot;.\n&gt;This &quot;mind state&quot; can be as complex as the number of LSTM cells it\n&gt;has. This was evident in the amazing experiment of Hotchreiter (see\n&gt;http://www.ph.tn.tudelft.nl/PRInfo/reports/msg00684.html). Although,\n&gt;we have to point that it was &quot;amazing&quot;, too, the learning time: 5\n&gt;months!!!!&quot;.\n\nThat is interesting.  I always thought of LSTMs as just an attempt to \ncapture another traditional aspect of real neurones, but that paper sounds \nlike it&#39;s touching on evolving a &quot;module&quot;.  I&#39;ll read it in detail and get \nback... but I see why you are keen to merge the LSTMs with NEAT!\n\n&gt;So, It could be possible that  with some kind of evolution pressure\n&gt;we could guide Neural Networks to create a Neural Processor with a\n&gt;separation between Memories, buses, &quot;Conditional events&quot; and\n&gt;processing units. This would help us to understand a litle bit more\n&gt;the solutions (something that in today solutions,where such concepts\n&gt;are mixed, is realy difficult)\n\nI&#39;m not sure whether you mean we could adjust the selection deliberately to \ncreate modules like &quot;memory&quot;, &quot;bus&quot; etc, or whether you are suggesting \nthat, given a sufficiently challenging set of problems, and a _LOT_ of \ngenerations, a network might generate some modules all on its own.\n\nThe first case, would be an interesting proof of concept but has too much \nof our preconceptions built in to really demonstrate anything.\n\nThe latter is very interesting, but might need a indirect representation \nbefore there&#39;s any hope of keeping separately evolving sub-components \nsynchronized together.\n\n&gt;  Besides,Instead of let the systems to create whatever useful modules,\n&gt;such separation could help the systems to reduce searching space, and\n&gt;therefore to find more elaborated solutions.\n\nYes.  But probably only for problems which are complex enough.  Complex \nsolutions perform worse on simple problems.\n\n         Ian\n\n\n\nLiving@Home - Open Source Evolving Organisms - \nhttp://livingathome.sourceforge.net/\n\n\n\n\n"}}