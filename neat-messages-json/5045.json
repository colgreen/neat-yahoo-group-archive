{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":256638517,"authorName":"William Monti","from":"William Monti &lt;william.monti@...&gt;","profile":"william.monti","replyTo":"LIST","senderId":"Vfb51qRdRRVnoKb-_D0tYarIM451zMe8lO5pVTlBlKkm-VkmSPS4IowKLHPRYjHp0bVAt4W538hCwFgk_OMVfj7eBuupLuTzukHsRsr-","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [neat] Re: train 3 billion training data with 3000 inputs and 150 \toutputs?","postDate":"1262666250","msgId":5045,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDVlY2U1ODg3MTAwMTA0MjAzN2xjMDE1NWU5azQ2MTcwNTRhZjA0ZTg4NjlAbWFpbC5nbWFpbC5jb20+","inReplyToHeader":"PGhodGs2MitrZTdiQGVHcm91cHMuY29tPg==","referencesHeader":"PGhoOGFnOCs2OHYxQGVHcm91cHMuY29tPiA8aGh0azYyK2tlN2JAZUdyb3Vwcy5jb20+"},"prevInTopic":5044,"nextInTopic":5046,"prevInTime":5044,"nextInTime":5046,"topicId":5037,"numMessagesInTopic":9,"msgSnippet":"Yes, that is exactly how I did it, but it was an idea I had, I was doing a historical series analysis and I wanted the network to be able to look at any past","rawEmail":"Return-Path: &lt;william.monti@...&gt;\r\nX-Sender: william.monti@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 47679 invoked from network); 5 Jan 2010 04:37:32 -0000\r\nX-Received: from unknown (98.137.34.45)\n  by m4.grp.sp2.yahoo.com with QMQP; 5 Jan 2010 04:37:32 -0000\r\nX-Received: from unknown (HELO mail-bw0-f222.google.com) (209.85.218.222)\n  by mta2.grp.sp2.yahoo.com with SMTP; 5 Jan 2010 04:37:31 -0000\r\nX-Received: by bwz22 with SMTP id 22so12151501bwz.25\n        for &lt;neat@yahoogroups.com&gt;; Mon, 04 Jan 2010 20:37:30 -0800 (PST)\r\nMIME-Version: 1.0\r\nX-Received: by 10.204.5.205 with SMTP id 13mr313994bkw.63.1262666250813; Mon, 04 \n\tJan 2010 20:37:30 -0800 (PST)\r\nIn-Reply-To: &lt;hhtk62+ke7b@...&gt;\r\nReferences: &lt;hh8ag8+68v1@...&gt; &lt;hhtk62+ke7b@...&gt;\r\nDate: Mon, 4 Jan 2010 23:37:30 -0500\r\nMessage-ID: &lt;5ece58871001042037lc0155e9k4617054af04e8869@...&gt;\r\nTo: neat@yahoogroups.com\r\nContent-Type: multipart/alternative; boundary=001517588c92598020047c6365dd\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: William Monti &lt;william.monti@...&gt;\r\nSubject: Re: [neat] Re: train 3 billion training data with 3000 inputs and 150 \n\toutputs?\r\nX-Yahoo-Group-Post: member; u=256638517; y=Ffyw6fwzlK-cqjO-2k3yzyCPLtKZBhVg3V0YrrFROOixGML_mGvG1Q\r\nX-Yahoo-Profile: william.monti\r\n\r\n\r\n--001517588c92598020047c6365dd\r\nContent-Type: text/plain; charset=windows-1252\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\nYes, that is exactly how I did it, but it was an idea I had, I was doing a\n=\r\nhistorical series analysis and I wanted the network to be able to look at\na=\r\nny past data, and not just pre-defined ones, so I don&#39;t have any document\na=\r\nbout that... But I can help you if you have doubts :)\n\nThe idea is to inclu=\r\nde in the genetic code information about the &quot;sensors&quot;,\na simple one can co=\r\nntain just one variable (the input id it will read from),\nso when you are b=\r\nuilding the network, you read that data and create the\nsensors accordingly =\r\ninstead of creating every input neuron possible.\n\nOn Mon, Jan 4, 2010 at 3:=\r\n48 PM, chhofchhof &lt;Christian.Hofmann@...&gt; wrote:\n\n&gt;\n&gt;\n&gt; Thank you all fo=\r\nr your replies!\n&gt;\n&gt; @William\n&gt;\n&gt; Sorry, I have problems to understand the w=\r\nays how you have solved the\n&gt; things. You have used sensor neurons. So you =\r\nwould use for example 100\n&gt; inputs instead of 3000. Then the output neurons=\r\n decide which neurons to use\n&gt; in the inputs?\n&gt;\n&gt; Is this correct? Do you h=\r\nave some documents or a link that describes that\n&gt; in detail?\n&gt;\n&gt; @Greg\n&gt;\n&gt;=\r\n The 347 days was for the whole population, but for one generation. The\n&gt; p=\r\nroblem is I have no cluster. Also in my experiments I need normally over\n&gt; =\r\nmillions of generations till I reach the point where the fitness of my\n&gt; va=\r\nlidation data starts to get lower (so I need to stop training).\n&gt; So even w=\r\nith 347 cluster members I would need one million days. Without\n&gt; counting t=\r\nhat the needed calculation time is increasing from generation to\n&gt; generati=\r\non.\n&gt;\n&gt; @torvhydda\n&gt;\n&gt; I am currently using Hypersharp neat, but I will hav=\r\ne a look, thank you.\n&gt;\n&gt;  \n&gt;\n\r\n--001517588c92598020047c6365dd\r\nContent-Type: text/html; charset=windows-1252\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\nYes, that is exactly how I did it, but it was an idea I had, I was doing a =\r\nhistorical series analysis and I wanted the network to be able to look at a=\r\nny past data, and not just pre-defined ones, so I don&#39;t have any docume=\r\nnt about that... But I can help you if you have doubts :)&lt;div&gt;\n&lt;br&gt;&lt;/div&gt;&lt;d=\r\niv&gt;The idea is to include in the genetic code information about the &quot;s=\r\nensors&quot;, a simple one can contain just one variable (the input id it w=\r\nill read from), so when you are building the network, you read that data an=\r\nd create the sensors accordingly instead of creating every input neuron pos=\r\nsible.&lt;br&gt;\n&lt;br&gt;&lt;div class=3D&quot;gmail_quote&quot;&gt;On Mon, Jan 4, 2010 at 3:48 PM, c=\r\nhhofchhof &lt;span dir=3D&quot;ltr&quot;&gt;&lt;&lt;a href=3D&quot;mailto:Christian.Hofmann@...&quot;=\r\n&gt;Christian.Hofmann@...&lt;/a&gt;&gt;&lt;/span&gt; wrote:&lt;br&gt;&lt;blockquote class=3D&quot;gma=\r\nil_quote&quot; style=3D&quot;margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-lef=\r\nt:1ex;&quot;&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;div style=3D&quot;background-color:#fff&quot;&gt;\n&lt;span&gt;=A0&lt;/spa=\r\nn&gt;\n\n\n&lt;div&gt;\n  &lt;div&gt;\n\n\n    &lt;div&gt;\n      \n      \n      &lt;p&gt;Thank you all for you=\r\nr replies!&lt;br&gt;\n&lt;br&gt;\n@William&lt;br&gt;\n&lt;br&gt;\nSorry, I have problems to understand =\r\nthe ways how you have solved the things. You have used sensor neurons. So y=\r\nou would use for example 100 inputs instead of 3000. Then the output neuron=\r\ns decide which neurons to use in the inputs?&lt;br&gt;\n\n&lt;br&gt;\nIs this correct? Do =\r\nyou have some documents or a link that describes that in detail?&lt;br&gt;\n&lt;br&gt;\n@=\r\nGreg&lt;br&gt;\n&lt;br&gt;\nThe 347 days was for the whole population, but for one genera=\r\ntion. The problem is I have no cluster. Also in my experiments I need norma=\r\nlly over millions of generations till I reach the point where the fitness o=\r\nf my validation data starts to get lower (so I need to stop training).&lt;br&gt;\n=\r\n\nSo even with 347 cluster members I would need one million days. Without co=\r\nunting that the needed calculation time is increasing from generation to ge=\r\nneration.&lt;br&gt;\n&lt;br&gt;\n@torvhydda&lt;br&gt;\n&lt;br&gt;\nI am currently using Hypersharp neat=\r\n, but I will have a look, thank you.&lt;br&gt;\n&lt;br&gt;\n&lt;/p&gt;\n\n    &lt;/div&gt;\n     \n\n    \n=\r\n    &lt;div style=3D&quot;color:#fff;min-height:0&quot;&gt;&lt;/div&gt;\n\n\n&lt;/div&gt;\n\n\n\n  \n\n\n\n\n\n\n&lt;/bl=\r\nockquote&gt;&lt;/div&gt;&lt;br&gt;&lt;/div&gt;\n\r\n--001517588c92598020047c6365dd--\r\n\n"}}