{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":197999825,"authorName":"John Arrowwood","from":"John Arrowwood &lt;jarrowwx@...&gt;","profile":"jarrowwx","replyTo":"LIST","senderId":"3SxosDd7vB60xPThhb2Zx9oPyK-ZnQbP5vnFnNdz2JLaWLp8D07R_Vvu3iN1-lwBLYvb4NiSbc2o795Mywfh9Hg2fTEiGXfhUJg","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [neat] Re: Introduction---recurrency question","postDate":"1125703929","msgId":2238,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDUxN2ZhNmYxMDUwOTAyMTYzMjMyOGFlOWE0QG1haWwuZ21haWwuY29tPg==","inReplyToHeader":"PDIwMDUwOTAxMjI1NDIxLk03NDg3MkBkZWFyZG9yZmYuY29tPg==","referencesHeader":"PDUxN2ZhNmYxMDUwODMxMTcyNzc1OWU1Y2RjQG1haWwuZ21haWwuY29tPgkgPGRmN3JrMys3MTV0QGVHcm91cHMuY29tPiA8NTE3ZmE2ZjEwNTA5MDExNTI1MjNjNTY2YmZAbWFpbC5nbWFpbC5jb20+CSA8MjAwNTA5MDEyMjU0MjEuTTc0ODcyQGRlYXJkb3JmZi5jb20+"},"prevInTopic":2237,"nextInTopic":2239,"prevInTime":2237,"nextInTime":2239,"topicId":2209,"numMessagesInTopic":42,"msgSnippet":"... Oh, sorry, I made a cognitive leap that I didn t let you in on.  My bad... The assumption that I had was that the topology of the network was constantly","rawEmail":"Return-Path: &lt;jarrowwx@...&gt;\r\nX-Sender: jarrowwx@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 53614 invoked from network); 2 Sep 2005 23:32:09 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m27.grp.scd.yahoo.com with QMQP; 2 Sep 2005 23:32:09 -0000\r\nReceived: from unknown (HELO wproxy.gmail.com) (64.233.184.197)\n  by mta5.grp.scd.yahoo.com with SMTP; 2 Sep 2005 23:32:09 -0000\r\nReceived: by wproxy.gmail.com with SMTP id i13so615162wra\n        for &lt;neat@yahoogroups.com&gt;; Fri, 02 Sep 2005 16:32:09 -0700 (PDT)\r\nReceived: by 10.54.63.16 with SMTP id l16mr1065646wra;\n        Fri, 02 Sep 2005 16:32:09 -0700 (PDT)\r\nReceived: by 10.54.80.10 with HTTP; Fri, 2 Sep 2005 16:32:09 -0700 (PDT)\r\nMessage-ID: &lt;517fa6f10509021632328ae9a4@...&gt;\r\nDate: Fri, 2 Sep 2005 16:32:09 -0700\r\nTo: neat@yahoogroups.com\r\nIn-Reply-To: &lt;20050901225421.M74872@...&gt;\r\nMime-Version: 1.0\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Transfer-Encoding: quoted-printable\r\nContent-Disposition: inline\r\nReferences: &lt;517fa6f10508311727759e5cdc@...&gt;\n\t &lt;df7rk3+715t@...&gt; &lt;517fa6f1050901152523c566bf@...&gt;\n\t &lt;20050901225421.M74872@...&gt;\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: John Arrowwood &lt;jarrowwx@...&gt;\r\nReply-To: John@...\r\nSubject: Re: [neat] Re: Introduction---recurrency question\r\nX-Yahoo-Group-Post: member; u=197999825; y=_hQh2AJrjdGRHR7QSgkeuvsBY6aTtHOsrQQyPMYo8MIN_j4\r\nX-Yahoo-Profile: jarrowwx\r\n\r\nOn 9/1/05, Jeff Haynes &lt;jeff@...&gt; wrote:\n&gt; \n&gt; &gt; &gt; I am moving tow=\r\nards something much more complex.  I would like to\n&gt; &gt; &gt; try and implement =\r\nNEAT+Hawkins(On Intelligence) ideas on how the\n&gt; &gt; &gt; brain works.  This req=\r\nuires more complex structures that allow, for\n&gt; &gt; &gt; instance, for recurrent=\r\n connections back to *input* nodes.\n&gt; &gt;\n&gt; &gt; Yes, outputs are inputs.  It&#39;s =\r\nall a big loop.  But it&#39;s more than\n&gt; &gt; just that...  When the &#39;expected&#39; i=\r\nnput (which is an output of an\n&gt; &gt; &#39;expectation&#39; circuit) doesn&#39;t match the=\r\n input, you get one result.\n&gt; &gt; When it does, you get another.  But that ki=\r\nnd of thing is VERY\n&gt; &gt; different than NEAT.\n&gt; &gt;\n&gt; \n&gt; Well, NEAT doesn&#39;t ad=\r\ndress this specifically but I&#39;m not sure what you&#39;re\n&gt; saying here.  It&#39;s c=\r\nertainly &quot;handled&quot; by NEAT as well as anything else\n&gt; (seemingly better in =\r\nsome cases).\n\nOh, sorry, I made a cognitive leap that I didn&#39;t let you in o=\r\nn.  My bad...\n\nThe assumption that I had was that the topology of the netwo=\r\nrk was\nconstantly changing.  When the actual input matches expectation,\nsom=\r\nething happens THAT CHANGES THE NETWORK, reinforcing the path that\npredicte=\r\nd that input.  When the actual input does not match the\nexpected input, som=\r\nething else happens that destabalizes the pathway\nthat caused the predictio=\r\nn.  Thus, the topology or at least the\nweights morph over the life of the s=\r\ningle network, instead of changing\nbetween generations.\n\nNEAT doesn&#39;t build=\r\n dynamic networks.  Dynamic networks would be a\nmodification of NEAT.\n\n&gt; &gt; =\r\nI don&#39;t think NEAT is what you want.  I think you want a dynamic\n&gt; &gt; networ=\r\nk that grows during the life of the organism.  Which means you\n&gt; &gt; need a s=\r\net of rules governing that growth.  Evolution might best be\n&gt; &gt; applied to =\r\nthose rules, rather than to the structure itself.\n&gt; \n&gt; I disagree.  I think=\r\n this is exactly the sort of thing NEAT would excel at.\n\nNot really.  NEAT =\r\nbuilds fixed-weight topologies.  A fixed-weight\ntopology CAN&#39;T learn.  We l=\r\nearn.  Thus, inherently, NEAT and true\nintelligence don&#39;t go together very =\r\nwell...\n\n&gt; Any time you have recurrency, there are &quot;temporal aspects&quot; but i=\r\nt&#39;s just a\n&gt; higher order of topology.  In fact, NEAT is evolving rules for=\r\n building\n&gt; structure, just not at the same level you&#39;re suggesting here (a=\r\n la cellular\n&gt; automata).  Growth over the life of the organism is interest=\r\ning and something\n&gt; I&#39;m not experienced with.  When our brains &quot;grow&quot; is th=\r\nat out of necessity or\n&gt; opportunity?\n\nOpportunity.  The rules are, when ex=\r\npectation meets actual, it causes\na resonnant feedback loop, which causes t=\r\nhat circuit to get stronger. \nChemicals are released which solidify that co=\r\nnnectivity.\n\nWhen expectation and actual are dissonant, it sets up an inter=\r\nference\npattern in the firing which causes almost random firing, which shak=\r\nes\nup the network.  Seeking equilibrium, the network uses the energy of\nthe=\r\n firing pattern to adjust itself to a new configuration that causes\nits exp=\r\nectation to be in harmony with actual.\n\nFYI: That&#39;s my working theory of it=\r\n.  \n\n&gt; As for Bayesian networks, I believe it&#39;s just another spin on the sa=\r\nme\n&gt; problem.  Someone correct me if I&#39;m wrong, but I believe anything that=\r\n can be\n&gt; done by a bayesian network can in theory be done with a NN.\n\nI&#39;m =\r\nno expert, but my understanding is that the two are functionally\nidentical.=\r\n..  Bayesian uses experienced proportions to make\npredictions based on prob=\r\nability.  A standard NN uses weights.  Those\nweights are effectively the &#39;p=\r\nrobabilities&#39;.  If there&#39;s a difference,\nI don&#39;t know what it is.\n\n-- John\n=\r\n\n"}}