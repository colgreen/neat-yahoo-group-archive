{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":199382914,"authorName":"Mike Woodhouse","from":"&quot;Mike Woodhouse&quot; &lt;mikewoodhouse@...&gt;","profile":"mikewoodhouse","replyTo":"LIST","senderId":"MNqV2CyjKO3xB-XxLYKFLig6CnRAlc9QJi3BwH_58Al6XPj-spbFU3Y7kXFSH2dAb2A7Qadov9cIzRs86E7tmD9mHEE4_ZBbb32ezIxLJyTp","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: 378 days later...","postDate":"1142954763","msgId":2577,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGR2cDV1Yit1c25lQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDQ0MUREMDEwLjMwMjA3QGRzbC5waXBleC5jb20+"},"prevInTopic":2574,"nextInTopic":2580,"prevInTime":2576,"nextInTime":2578,"topicId":2571,"numMessagesInTopic":6,"msgSnippet":"... [much snipping to avoid ridiculous length ensuing] ... It seems pretty fully-featured - I just happended to come across it in an unrelated application.","rawEmail":"Return-Path: &lt;mikewoodhouse@...&gt;\r\nX-Sender: mikewoodhouse@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 83701 invoked from network); 21 Mar 2006 15:26:08 -0000\r\nReceived: from unknown (66.218.67.33)\n  by m35.grp.scd.yahoo.com with QMQP; 21 Mar 2006 15:26:08 -0000\r\nReceived: from unknown (HELO n7a.bullet.scd.yahoo.com) (66.94.237.41)\n  by mta7.grp.scd.yahoo.com with SMTP; 21 Mar 2006 15:26:08 -0000\r\nComment: DomainKeys? See http://antispam.yahoo.com/domainkeys\r\nReceived: from [66.218.69.6] by n7.bullet.scd.yahoo.com with NNFMP; 21 Mar 2006 15:26:03 -0000\r\nReceived: from [66.218.66.81] by t6.bullet.scd.yahoo.com with NNFMP; 21 Mar 2006 15:26:03 -0000\r\nDate: Tue, 21 Mar 2006 15:26:03 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;dvp5ub+usne@...&gt;\r\nIn-Reply-To: &lt;441DD010.30207@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: &quot;Mike Woodhouse&quot; &lt;mikewoodhouse@...&gt;\r\nSubject: Re: 378 days later...\r\nX-Yahoo-Group-Post: member; u=199382914; y=N2GGJLi47OQEscIa6D9PaAZRcm-5ssXBOtFEwqGTXcypT7HxLexa6A\r\nX-Yahoo-Profile: mikewoodhouse\r\n\r\n\n\n--- In neat@yahoogroups.com, Colin Green &lt;cgreen@...&gt; wrote:\n&gt;\n&gt; Hi Mike,=\r\n welcome back,\n\n\n[much snipping to avoid ridiculous length ensuing]\n\n&gt; &gt;Par=\r\nt of the exercise was to get myself back up to speed in the C#\n&gt; &gt;stakes: I=\r\n hadn&#39;t written any code in .NET for about 18 months. I\n&gt; &gt;replaced the pro=\r\ngress form graph controls with ZedGraph (from\n&gt; &gt;SourceForge) ones as an ex=\r\nercise.\n&gt;\n&gt; ahh ok, how did that work out? I realise my graphs are [intenti=\r\nonally]\n&gt; minimal/lightweight, I take it ZedGraph is a bit more developed?\n=\r\n\nIt seems pretty fully-featured - I just happended to come across it in\nan =\r\nunrelated application. I&#39;ve only used the simplest line graphs but\nthey loo=\r\nk nice. The stimulus came from wanting to see visually the\nrelationship bet=\r\nween evaluations per second (or generation evaluation\ntime) and complexity.=\r\n I threw in a lot of cheap resizing using the\nsplitter panels (which I thin=\r\nk were  new in VS2005). I&#39;ll upload a\nscreen grab and the code when I get b=\r\nack to my development machine.\n\n&gt; I don&#39;t think starting population has any=\r\n real affect on success, at\n&gt; least not in SharpNEAT in it&#39;s current form. =\r\nAs you say, minimal\n&gt; networks will evaluate quickly and rapidly acqcuire s=\r\ntructure. On the\n&gt; other hand, complex starting populations will eventually=\r\n be stripped\n&gt; down to a minimal structure during the first pruning phase. =\r\nSo there&#39;s\n&gt; probably not much in it. Some structure is good because it all=\r\nows\n&gt; differentiation into species without elevating the speciation\nthresho=\r\nld\n&gt; well out of it&#39;s normal range - thus potentially causing it to crash\na=\r\nt\n&gt; a later time, although there are mechanisms in place to alleviate this\n=\r\n&gt; problem.\n\nI&#39;m already a fan of pruning! I have also found myself rather c=\r\naptivated\nby the way the compatibility threshold is adjusted to keep the sp=\r\necies\ncount within specified bounds - enough that I have played with settin=\r\ng\ndeliberatley bad starting values, just to watch the balancing machanism\ni=\r\nn action...\n\n\n&gt; On the &#39;confidence&#39; output. It is important but does it nee=\r\nd to be a\n&gt; seperate output? E.g. if you have win and lose outputs - then i=\r\nf they\n&gt; are both low then that would effectively be the network giving you=\r\n a\n&gt; &#39;don&#39;t know&#39; or low confidence result - save your money for another\nda=\r\ny.\n\nThat makes sense: the code that deals with confidence and selection is =\r\na\nbit clumsy because of the the three values, and I&#39;ve been thinking about\n=\r\nthe whole predicted value thing the last day or so. I&#39;ll try this.\n\n&gt; The r=\r\natings data is an interesting idea. By reading and writing it on\n&gt; each gam=\r\ne (in chronological order) this also allows the ratings to\n&gt; adjust as the =\r\nteam&#39;s ratings change, e.g. if a key player is injured\n&gt; then you might exp=\r\nect the overall rating of the team to be reduced -\nthe\n&gt; point being that i=\r\nt&#39;s not a constant thing, always changing. I&#39;m not\n&gt; sure if this is the be=\r\nst way to do it without thinking about it some\n&gt; more, but the logic seems =\r\nsound.\n\nThe idea was to minimise the amount of externally-supplied data tha=\r\nt had\nto be prepared: as it stands, a &quot;production&quot; application using this\na=\r\npproach need only know what the last result was. All other previous\nhistory=\r\n is encapsulated in the &quot;memory block&quot;. I had been looking for a\nway to avo=\r\nid externally calculated ratings (that are biased/constrained\nby my own sho=\r\nrtcomings) for some time - this seems to deal with this\n&quot;input pollution&quot; i=\r\nssue, albeit at the expense of increased evolution\ntime (but CPU cycles are=\r\n cheap!)\n\n&gt; Have you tried a fixed number of activations? If not then it&#39;s =\r\nworth a\n&gt; try, just stick with 4 or 5 - it should be faster becasue relaxat=\r\nion\nhas\n&gt; the overhead of testing each neuron activation value over and ove=\r\nr, so\n&gt; if it&#39;s not helping then it&#39;s best avoided.\n\nI&#39;ve looked at the num=\r\nber of evaluations taken to converge to a 0.01\ndelta. With simple random ne=\r\ntworks it averages between 1 and 2, as able\nnetworks evolve it stretches to=\r\n 4 to 6. Looking at the networks that are\nemerging, there&#39;s considerable co=\r\nnnecting between the output nodes, so\nmulti-stepping looks appropriate. Res=\r\nults have been generally better\nthan single-step models (where the output l=\r\nayer connectivity doesn&#39;t\ntend to help much). And CPU cycles are cheap!\n\n&gt; =\r\n&gt;Results to date: The best network I&#39;ve evolved so far managed to take\n&gt; &gt;a=\r\nn initial stake of 1,000 and, subject to staking limits of a 50\n&gt; &gt;maximum,=\r\n generate a profit of about 10,000. I&#39;m rather pleased with\n&gt; &gt;that :-)\n&gt;\n&gt;=\r\n This suggests overfitting has taken place (see below). Is this over\n&gt; seve=\r\nral seasons, e.g. over 4 years that would be equivalent to a 77%\n&gt; return e=\r\nach year, which is nice :)\n\nI don&#39;t doubt that there&#39;s a degree of overfitt=\r\ning going on. I&#39;ve been\nrefining the model to simulate real world situation=\r\ns more closely (it\nnow understands that multiple events occur simultaneousl=\r\ny rather than as\na stream and deals with this) and I&#39;ve taken steps (via be=\r\nt selection\nalgorithm and fitness function tuning) to prevent it learning a=\r\n dozen\nlong-odds shots and punting the maximum on them. Not sure how well t=\r\nhat\nworks yet.\n\n\n&gt; &gt;Questions to be answered:\n&gt; &gt;1. Is the best network ove=\r\nrfitting/memorizing the profitable bets?\n&gt; &gt;Quite possibly - I need to sour=\r\nce some more recent data for testing.\n&gt; This is nearly always going to be a=\r\n problem. Having said that, without\n&gt; doing any complex maths we can intuit=\r\nively see that very simple\nnetworks\n&gt; simply won&#39;t have the capacity to mem=\r\norize large amounts of data (or\n&gt; overfit) the training data. Thus your sma=\r\nll champion networks almost\n&gt; certainly aren&#39;t overfitting. The litmus test=\r\n of course is to have a\n&gt; test set of data that wasn&#39;t trained against. If =\r\nresults against the\ntwo\n&gt; sets begin to diverge then you can assume that ov=\r\nerfitting is\noccuring.\n\nSadly my original data source dried up, although I =\r\nhave a couple of\nyears&#39; self-captured data. I have to spend a few hours on =\r\ntedious\ndata-cleansing before I have a test set. I&#39;ll be mightily pleased i=\r\nf\nthere isn&#39;t a significant drop-off in performance - every non-NEAT\napproa=\r\nch (GEP being the most sophisticated) I&#39;ve tried tested a lot\nworse than it=\r\n trained.\n\n&gt; &gt;2. How robust is the calibration/memory aspect? Would the net=\r\nwork\n&gt; &gt;arrive at the same result if, say, the first seasons&#39; data were\nexc=\r\nluded?\n&gt; err, not sure.\n\nI&#39;ve tried this - very similar results, both in pr=\r\nofitability and in\nfinal team &quot;memory block&quot; outputs. When I get the chance=\r\n I&#39;ll try\nseveral starting points - I also want to follow the evolution of =\r\nteam\nrating values over time against results to see if I can infer what the=\r\n\nnetwork is interested in.\n\n&gt; &gt;3. Will I continue to have time to build a f=\r\nront end for the &quot;final\n&gt; &gt;solution&quot; network and have it ready to test with=\r\n real money next\n&gt; &gt;season? (I&#39;m prepared to bankroll a prmising candidate =\r\nto the tune of\n&gt; &gt;100 quid or so, for the entertainment if nothing else).\n&gt;=\r\n\n&gt; A) Putting &quot;final solution&quot; in quotes made that sentence far more\n&gt; sini=\r\nster than it otherwise would have done :-|\n\n(evil laugh)\n\n&gt; B) I think it&#39;s=\r\n odds on that RealLife(TM) will scupper your plans\n&gt; somewhere along the li=\r\nne. That&#39;s not meant to be a reflection on you\n&gt; personally BTW, I&#39;ll take =\r\nthat bet on anyone any day! (it&#39;s in our DNA\n&gt; I&#39;m afraid).\n\nI&#39;m taking the=\r\n precaution of reworking the model to utilise a Manager\nclass that understa=\r\nnds working with a day&#39;s matches at a time. The front\nend could then be lit=\r\ntle more than a console app and some text files,\nwhich stands half a chance=\r\n...\n\n&gt; C) If you&#39;ve had builders in then what&#39;s another 100 quid eh? ;) I\nt=\r\nhink\n&gt; that&#39;s what Plumbers charger an hour these days! On a more serious\nn=\r\note,\n&gt; would that be an accumulative strategy whereby only one bet is place=\r\nd\nat\n&gt; a time, the proceeds being used for the next bet? And thus if the\nfi=\r\nrst\n&gt; few bets are bust then that&#39;s your 100 quid for this season gone? :(\n=\r\n\nThe BetManager takes care of this. It&#39;s given a bankroll and is subject\nto=\r\n several limits: 2% per match, and a maximum of 50 quid at any time.\nThe ba=\r\nnkroll at the end of each day is carried forward to the next match\nday. If =\r\na model goes bust, that&#39;s the end of the run. If the plumber\ngets called ou=\r\nt, that&#39;s the end of the bankroll.\n\n&gt; There have been much longer posts bel=\r\nieve me. I didn&#39;t really follow\n&gt; your explanation of the inputs and output=\r\ns I&#39;m afraid, maybe if I read\n&gt; it again tomorrow I&#39;ll absorb it. As a gene=\r\nral rule though you always\n&gt; need to use seperate training and test sets in=\r\n order to evaluate\n&gt; performance on data that wasn&#39;t trained against. You s=\r\nhould also try\nto\n&gt; use multiple test sets in order to geta better picture =\r\nof variability\nin\n&gt; performance, e.g. if it varies betweeen really bad and =\r\nreally good\nthen\n&gt; this probably is no use to you, consistently OK is bette=\r\nr than\n&gt; occasionally brilliant (and occasionally bad).\n\nThe test sets will=\r\n feed into the process once I&#39;ve done the tedious\ncleaning-up!\n\n&gt; If you ar=\r\ne using the approach of placing one bet after the other (lets\n&gt; call these =\r\nserial bets) then I would say that this is a poor way of\n&gt; evaluating netwo=\r\nrks in at search time. The reason being that bad luck\n&gt; early on in the bet=\r\nting sequence has a disproportionate affect on the\n&gt; overall success of a n=\r\network, you should look towards a fairer way of\n&gt; evaluating networks (e.g.=\r\n overall prediction success rate) - you can\n&gt; then use these networks to ex=\r\necute whatever strategy you like for\n&gt; placing real bets and/or evaluation =\r\nagainst the test data. E.g.\n&gt; evaulation using serial bets on your test dat=\r\na will give you an\n&gt; indication of how often your betting series will end u=\r\np in profit/loss\n&gt; in the real world.\n\nI think the combination of large sta=\r\nrting bank and heavily-constrained\nbets as a small proportion help to prote=\r\nct the models here - they can\nsustain an initial losing run without too muc=\r\nh long-term impact. I&#39;m\nthinking about ways to reward networks for other at=\r\ntributes such as good\nstatistical reliabilty - chi-squared or similar, indi=\r\ncating that the\nresults aren&#39;t just luck. Networks that learn to bet big on=\r\n a few\nlong-odds winners would be penalised undr this. I have already\nexper=\r\nimented with scaling fitness down for networks selecting verysmall\nnumbers =\r\nof  possible bets.\n\n&gt; P.S. If you make your fortune then mines a pint. ;)\n\n=\r\nI already owe you at least one for the code you&#39;ve written so far!\n\n\n\nMike\n=\r\n\n\n\n\n\n"}}