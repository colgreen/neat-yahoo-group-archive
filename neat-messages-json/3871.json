{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":283334584,"authorName":"petar_chervenski","from":"&quot;petar_chervenski&quot; &lt;petar_chervenski@...&gt;","profile":"petar_chervenski","replyTo":"LIST","senderId":"BGcsL62ibscFLgbjeuJvBsHZXrVsoqEm_ic21zNGyB7ySvx3AVY3ckzfM8yMFAHQKVqYjPOJ3VNeG29p-Msemkn81IaHX3X4w07bIuT_FfT4NNzGe7c","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Backpropagation and NEAT","postDate":"1205536288","msgId":3871,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGZyZjBuMCtmZWtuQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGZyZXU1NStuMzFiQGVHcm91cHMuY29tPg=="},"prevInTopic":3870,"nextInTopic":3872,"prevInTime":3870,"nextInTime":3872,"topicId":3846,"numMessagesInTopic":41,"msgSnippet":"Hi Andy, Well first of all, a local search in the sense of what I meant is performed on a network with fixed topology. That is, only weights are going do be","rawEmail":"Return-Path: &lt;petar_chervenski@...&gt;\r\nX-Sender: petar_chervenski@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 11285 invoked from network); 14 Mar 2008 23:11:29 -0000\r\nX-Received: from unknown (66.218.67.96)\n  by m53.grp.scd.yahoo.com with QMQP; 14 Mar 2008 23:11:29 -0000\r\nX-Received: from unknown (HELO n19a.bullet.scd.yahoo.com) (66.94.237.48)\n  by mta17.grp.scd.yahoo.com with SMTP; 14 Mar 2008 23:11:29 -0000\r\nX-Received: from [66.218.69.4] by n19.bullet.scd.yahoo.com with NNFMP; 14 Mar 2008 23:11:29 -0000\r\nX-Received: from [66.218.66.91] by t4.bullet.scd.yahoo.com with NNFMP; 14 Mar 2008 23:11:29 -0000\r\nDate: Fri, 14 Mar 2008 23:11:28 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;frf0n0+fekn@...&gt;\r\nIn-Reply-To: &lt;freu55+n31b@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;petar_chervenski&quot; &lt;petar_chervenski@...&gt;\r\nSubject: Re: Backpropagation and NEAT\r\nX-Yahoo-Group-Post: member; u=283334584; y=dL35G5NEJNQ5qYlHD3O1DDzhDhqKw2y4fS2OS-3P2BhhfiGyUGbzkjSisw\r\nX-Yahoo-Profile: petar_chervenski\r\n\r\nHi Andy, \n\nWell first of all, a local search in the sense of what I meant i=\r\ns \nperformed on a network with fixed topology. That is, only weights are \ng=\r\noing do be changed. We don&#39;t touch the topology of the network. So \nthis ca=\r\nn be done either by a local search or a GA (which in fact \nhappens in NEAT =\r\n- the species local niche). \n\nIt is obvious that GAs are not well suited to=\r\n problems requiring a \ngreat deal of computation time. It only depends of t=\r\nhe time required \nto evaluate a single individual. Sometimes this may be de=\r\npendant on \ntime itself, for example in a wall-avoiding robot task, the fit=\r\nness \nis going to be the time spent until hitting a wall. This is really \nd=\r\nomain dependant stuff. \n\nYou should also consider that in my case NEAT is b=\r\neing used in a \nsupervised learning domain. In my own hypothesis, NEAT shou=\r\nld be able \nto solve any problem, given enough time, and of course, if the =\r\n\nproblem *can* be solved at all. You can&#39;t expect to evolve a network \npred=\r\nicting a RNG, for example. \n\nI agree that real world problems are indeed ve=\r\nry complex, but you \nshould also consider that humans are developing more a=\r\nnd more complex \nsystems to solve them. Don&#39;t forget that neural networks a=\r\nre loose \nabstractions of living brains :) \n\nPeter\n\n--- In neat@yahoogroups=\r\n.com, &quot;afcarl2&quot; &lt;a.carl@...&gt; wrote:\n&gt;\n&gt; Peter,\n&gt; \n&gt; It appears that you are=\r\n implicitly assuming that that the only way \n&gt; to &quot;reach out and touch&quot; the=\r\n fitness evaluation is via a NEAT \n&gt; fabricated organism topology. Local se=\r\narch does not have to proceed \n&gt; via organism topology. \n&gt; \n&gt; Usage of NEAT=\r\n as a global search method, as part of a hierarchical \n&gt; methodology, surel=\r\ny uses organism topology as the local search \nstart \n&gt; point. And in the ca=\r\nse of your proposed backprop search on organism \n&gt; weight values, maintains=\r\n the same number and composition of nodes \nand \n&gt; associated connectivity. =\r\nBut a local search method could just as \n&gt; easily wander free in the local =\r\nfitness landscape, un-encumbered by \n&gt; organism topology issues, to find th=\r\ne local minima.\n&gt; \n&gt; The very strong point of GA can also be its greatest w=\r\neakness, in \n&gt; instances in which the computational resource requirements o=\r\nf the \n&gt; fitness evaluation, in light of the dimensionality and hyper volum=\r\ne \n&gt; size and complexity, become non-trivial.\n&gt; \n&gt; This is especially true =\r\nin the case of non-adaptive mutation \n&gt; parameters, as is currently the cas=\r\ne with NEAT. There are many \ntimes \n&gt; in which it is orders of magnitude qu=\r\nicker for a non-GA local \n&gt; gradient search method to find a local minima, =\r\nthan an equivalent \nGA \n&gt; to mutate through generations to find the same lo=\r\ncal minima. \n&gt; \n&gt; NEAT speciation and niche protection help to mitigate the=\r\n problem \nvia \n&gt; population size and maintaining multiple species, but at a=\r\n \n&gt; computational cost. But a hierarchical search methodology can apply \n&gt; =\r\nthe strengths of both GA and local search, without having to use \n&gt; computa=\r\ntional power to cover-up GA&#39;s weak points.\n&gt; \n&gt; The remaining question as t=\r\no whether to a) re-encode the final \n&gt; destination of the local search back=\r\n into the organism&#39;s topology, \nor \n&gt; b) simply take the final/best fitness=\r\n derived by the local search \n&gt; (using the organism&#39;s original topology as =\r\nthe start point), and \n&gt; associate it with the organism and it&#39;s original t=\r\nopology, is up \nfor \n&gt; debate and/or personal preference.\n&gt; \n&gt; The moral of=\r\n the story is that local search does not have to be \n&gt; constrained by organ=\r\nism topology, beyond that of providing the \nstart \n&gt; point. \n&gt; \n&gt; Real worl=\r\nd problems are so complex in light of current computer \n&gt; speeds and fitnes=\r\ns computation requirements, to render GA alone to \nbe \n&gt; computationally un=\r\npractical in many instances, even with the \nobvious \n&gt; benefits that NEAT b=\r\nrings to the table.\n&gt; \n&gt; \n&gt; \n&gt; --- In neat@yahoogroups.com, &quot;petar_chervens=\r\nki&quot; \n&gt; &lt;petar_chervenski@&gt; wrote:\n&gt; &gt;\n&gt; &gt; Well actually speciation takes ca=\r\nre of this. Species are allowed \nto \n&gt; &gt; exist until they stagnate for too =\r\nlong time. If some new \nstructure \n&gt; &gt; appears through mutations, the mutat=\r\ned individuals are separated \nin \n&gt; &gt; another species. Each species is a lo=\r\ncal protected competition \n&gt; among \n&gt; &gt; individuals grouped by similarity. =\r\nConsider it as a GA performed \non \n&gt; &gt; near identical topologies. Then you =\r\ncan see NEAT as a algorithm \n&gt; &gt; running multiple GAs. So this is actually =\r\nwhat you mean by \ndynamic \n&gt; &gt; programming.. or something. In fact this sch=\r\neme is far better \nthan \n&gt; &gt; it. \n&gt; &gt; As for the idea of speculative struct=\r\nure, this is the core of \nNEAT \n&gt; &gt; and it is actually Ken&#39;s idea :) \n&gt; &gt; C=\r\nolin Green&#39;s idea is about phased searching, as far as I know. \nIt \n&gt; is \n&gt;=\r\n &gt; that after some structure is added through complexifying, a \n&gt; &gt; simplif=\r\nying phase kicks in, removing any unnecessary structure, \n&gt; thus \n&gt; &gt; retur=\r\nning the search down to a baseline low dimentional space \nwhile \n&gt; &gt; retain=\r\ning the fitness (because of elitism). \n&gt; &gt; \n&gt; &gt; Peter\n&gt; &gt; \n&gt; &gt; --- In neat@=\r\nyahoogroups.com, c f &lt;christofer_fransson@&gt; wrote:\n&gt; &gt; &gt;\n&gt; &gt; &gt; In dynamic p=\r\nrogramming the idea is to divide the\n&gt; &gt; &gt; solution in steps and then for e=\r\nach step present a\n&gt; &gt; &gt; fixed number of possible solutions.\n&gt; &gt; &gt; \n&gt; &gt; &gt; C=\r\nollin Greens idea is that speculative nodes are added\n&gt; &gt; &gt; to the solution=\r\ns but it might take time/generations\n&gt; &gt; &gt; before an added node are shown t=\r\no be useful. \n&gt; &gt; &gt; \n&gt; &gt; &gt; Is it possible to apply dynamic programming appr=\r\noach\n&gt; &gt; &gt; to this area, to evolve NEAT driven networks?\n&gt; &gt; &gt; \n&gt; &gt; &gt; To co=\r\nmbine local optimization and dynamic programming\n&gt; &gt; &gt; ideas?\n&gt; &gt; &gt; \n&gt; &gt; &gt; =\r\nBr,\n&gt; &gt; &gt; Christofer\n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; &gt;=\r\n \n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; &gt; --- petar_chervenski &lt;petar_chervenski@=\r\n&gt;\n&gt; &gt; &gt; wrote:\n&gt; &gt; &gt; \n&gt; &gt; &gt; &gt; Given the simplest topology (a perceptron\n&gt; &gt;=\r\n &gt; &gt; structure), the local \n&gt; &gt; &gt; &gt; minima is just one. Perceptrons are alw=\r\nays\n&gt; &gt; &gt; &gt; guaranteed to converge on \n&gt; &gt; &gt; &gt; correct weights. But increas=\r\ning the dimentionality\n&gt; &gt; &gt; &gt; of the solution \n&gt; &gt; &gt; &gt; increases the error=\r\n surface&#39;s curvature as well. So\n&gt; &gt; &gt; &gt; more dimentions \n&gt; &gt; &gt; &gt; means mor=\r\ne complex error surface. The coolest thing\n&gt; &gt; &gt; &gt; in NEAT is that \n&gt; &gt; &gt; &gt;=\r\n when it increases the dimentionality of the\n&gt; &gt; &gt; &gt; solution, the individu=\r\nals \n&gt; &gt; &gt; &gt; are already located in a promising area of the new\n&gt; &gt; &gt; &gt; spa=\r\nce. In fact \n&gt; &gt; &gt; &gt; those spaces are related to each other - you don&#39;t\n&gt; &gt;=\r\n &gt; &gt; know how the error \n&gt; &gt; &gt; &gt; surface is going to look like when you ent=\r\ner the new\n&gt; &gt; &gt; &gt; space with more \n&gt; &gt; &gt; &gt; dimentions. There are unlimited=\r\n possibilities. \n&gt; &gt; &gt; &gt; So what local gradient search will do in essence i=\r\ns\n&gt; &gt; &gt; &gt; pushing the \n&gt; &gt; &gt; &gt; weights towards the *local* minumim.. It is =\r\nnot\n&gt; &gt; &gt; &gt; guaranteed that this \n&gt; &gt; &gt; &gt; is the *solution*! It is simply b=\r\necause you don&#39;t\n&gt; &gt; &gt; &gt; know the solution&#39;s \n&gt; &gt; &gt; &gt; dimentionality at fir=\r\nst. It may require 3 or\n&gt; &gt; &gt; &gt; 21342532 dimentions. \n&gt; &gt; &gt; &gt; Don&#39;t forget =\r\nthat NEAT complexifies solutions\n&gt; &gt; &gt; &gt; incrementaly. \n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; Pe=\r\nter\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; --- In neat@yahoogroups.com, &quot;afcarl2&quot; &lt;a.carl@&gt;\n&gt; &gt; &gt;=\r\n &gt; wrote:\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; In fact, it may be that a substancial portion=\r\n of\n&gt; &gt; &gt; &gt; the value-added of \n&gt; &gt; &gt; &gt; &gt; speciation and niche protection o=\r\nf infant\n&gt; &gt; &gt; &gt; organisms, is associated \n&gt; &gt; &gt; &gt; &gt; with providing opportu=\r\nnity to accumulate\n&gt; &gt; &gt; &gt; sufficient neighborhood \n&gt; &gt; &gt; &gt; &gt; evaluations t=\r\no &quot;discover&quot; the same local minimia\n&gt; &gt; &gt; &gt; over multiple \n&gt; &gt; &gt; &gt; &gt; genera=\r\ntions, that a local search may discover in\n&gt; &gt; &gt; &gt; one generation. \n&gt; &gt; &gt; &gt;=\r\n And \n&gt; &gt; &gt; &gt; &gt; maintaining multiple species in hope that one of\n&gt; &gt; &gt; &gt; th=\r\ne local minimia \n&gt; &gt; &gt; &gt; &gt; will in fact also be the global minimia.\n&gt; &gt; &gt; &gt;=\r\n &gt; \n&gt; &gt; &gt; &gt; &gt; --- In neat@yahoogroups.com, &quot;afcarl2&quot; &lt;a.carl@&gt;\n&gt; &gt; &gt; &gt; wrot=\r\ne:\n&gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; If &quot;most individuals in a species represented by=\r\n\n&gt; &gt; &gt; &gt; a given \n&gt; &gt; &gt; &gt; topology&quot; \n&gt; &gt; &gt; &gt; &gt; &gt; ended up in &quot;the same loca=\r\nl minimia&quot;, one could\n&gt; &gt; &gt; &gt; argue that the \n&gt; &gt; &gt; &gt; &gt; &gt; subject specie&#39;s =\r\nlogical end point was the same\n&gt; &gt; &gt; &gt; local minimia, \n&gt; &gt; &gt; &gt; and \n&gt; &gt; &gt; &gt;=\r\n &gt; &gt; that the cost of maintaining more than one\n&gt; &gt; &gt; &gt; organism was \n&gt; &gt; &gt;=\r\n &gt; &gt; &gt; computationally wasteful. Better to know sooner\n&gt; &gt; &gt; &gt; and breed \n&gt;=\r\n &gt; &gt; &gt; &gt; additional \n&gt; &gt; &gt; &gt; &gt; &gt; organisms of differing topology so as to\n&gt;=\r\n &gt; &gt; &gt; maintain the population \n&gt; &gt; &gt; &gt; &gt; size \n&gt; &gt; &gt; &gt; &gt; &gt; and maximize th=\r\ne population&#39;s &quot;effective&quot;\n&gt; &gt; &gt; &gt; diversity.\n&gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; Payi=\r\nng more for the same answer does not make it\n&gt; &gt; &gt; &gt; a better answer.\n&gt; &gt; &gt;=\r\n &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; --- In neat@yahoogroups.com, &quot;petar_chervenski&quot; \n&gt; &gt; &gt; =\r\n&gt; &gt; &gt; &lt;petar_chervenski@&gt; wrote:\n&gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; Well I think t=\r\nhat encoding the resulting\n&gt; &gt; &gt; &gt; weights back to the \n&gt; &gt; &gt; &gt; &gt; genome \n&gt;=\r\n &gt; &gt; &gt; &gt; &gt; &gt; would somehow hurt the population weight\n&gt; &gt; &gt; &gt; diversity, si=\r\nnce most \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; individuals in a species represented by a\n&gt; &gt; &gt; &gt; g=\r\niven topology can \n&gt; &gt; &gt; &gt; end \n&gt; &gt; &gt; &gt; &gt; up \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; in the same loc=\r\nal minima, thus leaving out a\n&gt; &gt; &gt; &gt; species with the \n&gt; &gt; &gt; &gt; &gt; &gt; nearly =\r\n\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; same individuals, i.e. clones. \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; This is why I =\r\nthink that backprop should be\n&gt; &gt; &gt; &gt; applied occasionaly \n&gt; &gt; &gt; &gt; &gt; &gt; afte=\r\nr \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; long periods of stagnation, for example the\n&gt; &gt; &gt; &gt; cases =\r\nwhere delta-\n&gt; &gt; &gt; &gt; &gt; &gt; coding \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; kicks in, when it focuses th=\r\ne search in the\n&gt; &gt; &gt; &gt; most promising \n&gt; &gt; &gt; &gt; areas \n&gt; &gt; &gt; &gt; &gt; of \n&gt; &gt; &gt; =\r\n&gt; &gt; &gt; &gt; the search space. \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; I am still trying to re-implement =\r\nRTRL myself,\n&gt; &gt; &gt; &gt; though.. Then \n&gt; &gt; &gt; &gt; I&#39;ll \n&gt; &gt; &gt; &gt; &gt; &gt; see \n&gt; &gt; &gt; &gt; =\r\n&gt; &gt; &gt; if it is going to actually enhance\n&gt; &gt; &gt; &gt; performance. \n&gt; &gt; &gt; &gt; &gt; &gt; =\r\n&gt; \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; Peter\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; --- In neat@yahoogroups=\r\n.com, &quot;Kenneth Stanley&quot;\n&gt; &gt; &gt; &gt; &lt;kstanley@&gt; \n&gt; &gt; &gt; &gt; wrote:\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;=\r\n\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; Rafael, thank you for pointing out the\n&gt; &gt; &gt; &gt; connection =\r\nto memetic \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; algorithms.  That is good to point out that\n&gt; &gt;=\r\n &gt; &gt; such a \n&gt; &gt; &gt; &gt; combination \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; falls \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; unde=\r\nr that category.\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; However, there are still =\r\nthose who would\n&gt; &gt; &gt; &gt; argue that the local \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; search \n&gt; &gt; &gt; &gt;=\r\n &gt; &gt; &gt; &gt; method should not be encoded back into the\n&gt; &gt; &gt; &gt; genome, that is=\r\n, \n&gt; &gt; &gt; &gt; &gt; that \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; evolution should simply search for the b=\r\nest\n&gt; &gt; &gt; &gt; starting point \n&gt; &gt; &gt; &gt; from \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; which \n&gt; &gt; &gt; &gt; &gt; &gt; =\r\n&gt; &gt; a local search would depart.  Because of the\n&gt; &gt; &gt; &gt; Baldwin Effect, \n&gt;=\r\n &gt; &gt; &gt; &gt; that \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; may \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; even work better.\n&gt; &gt; &gt; &gt;=\r\n &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; Personally, I do not know which approach\n&gt; &gt; &gt; &gt; =\r\nwould work better \n&gt; &gt; &gt; &gt; but \n&gt; &gt; &gt; &gt; &gt; &gt; both \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; are viabl=\r\ne and it is probably domain\n&gt; &gt; &gt; &gt; dependent.\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt;=\r\n &gt; &gt; ken\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; --- In neat@yahoogroups.com, &quot;Raf=\r\nael C.P.&quot;\n&gt; &gt; &gt; &gt; &lt;kurama.youko.br@&gt; \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; wrote:\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; =\r\n&gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; Ken, it doesn&#39;t fit pure evolution but it\n&gt; &gt; &gt; &gt; fit=\r\ns memetic \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; algorithms, \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; that\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; =\r\n&gt; consists exactly of evolution alternated\n&gt; &gt; &gt; &gt; with local search \n&gt; &gt; &gt;=\r\n &gt; &gt; &gt; &gt; methods \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; for fine\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; tunning (just =\r\nfew steps). NEAT+BP may\n&gt; &gt; &gt; &gt; become a good memetic \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; algo=\r\nrithm for\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; neural networks.\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt;=\r\n &gt; &gt; &gt; On Mon, Mar 10, 2008 at 2:19 PM, Kenneth\n&gt; &gt; &gt; &gt; Stanley &lt;kstanley@&gt;=\r\n\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; wrote:\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;   Peter, I=\r\n believe that backprop can\n&gt; &gt; &gt; &gt; potentially improve \n&gt; &gt; &gt; &gt; the\n&gt; &gt; &gt; &gt;=\r\n &gt; &gt; &gt; &gt; &gt; &gt; accuracy. It has been shown to work\n&gt; &gt; &gt; &gt; effectively with \n=\r\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; neurevolution\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; in classification tasks in =\r\nthe past. So\n&gt; &gt; &gt; &gt; in principle it \n&gt; &gt; &gt; &gt; &gt; could\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; h=\r\nelp. Of course, there is always the\n&gt; &gt; &gt; &gt; chance that it will \n&gt; &gt; &gt; &gt; no=\r\nt\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; enhance performance as well.\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; =\r\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; One issue I would also consider is that\n&gt; &gt; &gt; &gt; some people=\r\n \n&gt; &gt; &gt; &gt; &gt; disagree \n&gt; &gt; &gt; &gt; &gt; &gt; on\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; whether the change=\r\ns to weights from\n&gt; &gt; &gt; &gt; backprop should be \n&gt; &gt; &gt; &gt; &gt; &gt; encoded \n&gt; &gt; &gt; &gt; =\r\n&gt; &gt; &gt; &gt; back\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; into the genome or not. If it is\n&gt; &gt; &gt; &gt; a=\r\nctually encoded back \n&gt; &gt; &gt; &gt; into \n&gt; &gt; &gt; &gt; &gt; &gt; the\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; gen=\r\nome, that is &quot;Lamarckian&quot; evolution\n&gt; &gt; &gt; &gt; because in effect \n&gt; &gt; &gt; &gt; &gt; wh=\r\nat \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; the\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; organism learned over its lifetim=\r\ne is\n&gt; &gt; &gt; &gt; encoded into its own\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; =3D=3D=3D message truncate=\r\nd =3D=3D=3D\n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; &gt;       \n&gt; &gt; \n&gt; \n_____________________=\r\n_________________________________________________\n&gt; &gt; ______________\n&gt; &gt; &gt; =\r\nNever miss a thing.  Make Yahoo your home page. \n&gt; &gt; &gt; http://www.yahoo.com=\r\n/r/hs\n&gt; &gt; &gt;\n&gt; &gt;\n&gt;\n\n\n\n"}}