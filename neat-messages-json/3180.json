{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":7192225,"authorName":"Ian Badcoe","from":"Ian Badcoe &lt;ian_badcoe@...&gt;","profile":"ian_badcoe","replyTo":"LIST","senderId":"imzvxloO4cnhrfmZs3-o-RaODxKt1rbXS2r1uSnPH6imcHHsIhewD_vyknkOskb_302RrFaMto_aBht14AVxMg4DJbdYhpWcOxw","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] Re: Computation Time","postDate":"1177101617","msgId":3180,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDIwMzczMTA2NTQ5NDY2QG1haWwuaWRuZXQubmV0LnVrPg==","inReplyToHeader":"PGYwN2ZxaCtxZmZtQGVHcm91cHMuY29tPg==","referencesHeader":"PDIxNDUwOTQwODgxNDE4QG1haWwuaWRuZXQubmV0LnVrPiA8ZjA3ZnFoK3FmZm1AZUdyb3Vwcy5jb20+"},"prevInTopic":3179,"nextInTopic":3181,"prevInTime":3179,"nextInTime":3181,"topicId":845,"numMessagesInTopic":99,"msgSnippet":"... Well there s no reason to suppose that giving different simulations the same random seed is in any sense setting them on the same track.  e.g. there should","rawEmail":"Return-Path: &lt;ian_badcoe@...&gt;\r\nX-Sender: ian_badcoe@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 40235 invoked from network); 20 Apr 2007 20:37:27 -0000\r\nReceived: from unknown (66.218.66.71)\n  by m42.grp.scd.yahoo.com with QMQP; 20 Apr 2007 20:37:27 -0000\r\nReceived: from unknown (HELO mail.idnet.net.uk) (212.69.36.63)\n  by mta13.grp.scd.yahoo.com with SMTP; 20 Apr 2007 20:37:27 -0000\r\nReceived: from [212.69.55.164] by mail.idnet.net.uk (GMS\n 11.01.3365/NU3963.00.7ca42f0c) with ESMTP id dgexauaa for\n neat@yahoogroups.com; Fri, 20 Apr 2007 21:37:31 +0100\r\nX-Mailer: QUALCOMM Windows Eudora Version 7.1.0.9\r\nDate: Fri, 20 Apr 2007 21:40:17 +0100\r\nTo: neat@yahoogroups.com\r\nIn-Reply-To: &lt;f07fqh+qffm@...&gt;\r\nReferences: &lt;21450940881418@...&gt;\n &lt;f07fqh+qffm@...&gt;\r\nMime-Version: 1.0\r\nContent-Type: text/plain; charset=us-ascii; format=flowed; x-avg-checked=avg-ok-F9F7CDF\r\nMessage-Id: &lt;20373106549466@...&gt;\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Ian Badcoe &lt;ian_badcoe@...&gt;\r\nSubject: Re: [neat] Re: Computation Time\r\nX-Yahoo-Group-Post: member; u=7192225; y=TjmB68yYhCAE3qsajORTtzf6E-FVhwUS1GjPwRZOyUmWS7SCBA\r\nX-Yahoo-Profile: ian_badcoe\r\n\r\nAt 11:21 19/04/2007, you wrote:\n\n&gt;Hello Ian,\n&gt;\n&gt;Thanks yes I browsed through those threads. I am most interested in\n&gt;optimizing the sigmoid for now. I have implemented many of the other\n&gt;great optimizations mentioned. I saw the other sigmoid approximations\n&gt;and some of them were quite fast but this speed was offset by poor\n&gt;learning characteristics of the resulting programs.\n&gt;\n&gt;I ran my code with the standard sigmoid for a given number of\n&gt;generations and kept track of the random seed, the final fitness and\n&gt;the run time. I replaced the standard sigmoid with an approximated\n&gt;sigmoid. Then I evolved a net using the same seed and measured how\n&gt;long it took to arrive at the same fitness as the net with the\n&gt;standard sigmoid ( although when the approximated sigmoids when run\n&gt;in isolation they were much faster than the standard sigmoid). When\n&gt;trying to evolve a net with the same level of fitness in a given\n&gt;amount of time they actually negatively impacted performance! I think\n&gt;this is a more accurate way to measure real performance of the\n&gt;application. I think it doesn&#39;t really matter how fast the parts of\n&gt;the program run but how fast the program completes the task it was\n&gt;created for.\n\nWell there&#39;s no reason to suppose that giving different simulations \nthe same random seed is in any sense setting them on the same \ntrack.  e.g. there should be a chaotic (in the strict mathematical \nsense) relationship between all starting conditions (especially the \nrandom seed) and the path the system takes and which possible \nsolution it produces.\n\nOther than that I wonder whether you have something else going on \nhere which is damaging you results in with the other sigmoids?  What \ntype of problem were you running.  Bear in mind that there is AFAIK \nno theoretical justification for the use of the traditional sigmoid \nwhat-so-ever.  People just use it because it is traditional.  The \nonly reason it was introduced is because it is differentiable for the \npurposes of back-propagation, but since none of us use \nback-propagation we really don&#39;t _need_ to be traditional.  Like I \nsaid, I know no theory for the standard expensive sigmoid to be better.\n\n_BUT_ I&#39;d be very interested if anybody does have such a theory.  I \nguess it&#39;s trancendental?  So is that enabling the system to find a \ngreat range of power functions within it (e.g by focusing on parts of \nthe curve)?\n\nOtherwise I would expect either my inv-abs function or else the \npiece-wise function that... errr... if was either Mitchell or John \ncame up with.  BOth of those hugely out-performed any exponent-based \nmethod in various tests.\n\n&gt;Because of the problems I saw with the other approximations I decided\n&gt;to try and create my own sigmoid approximation with good learning\n&gt;characteristics and a fast runtime. To do this I created an\n&gt;approximation that closely follows the curve of the original sigmoid\n&gt;and that is relatively smooth and continuous. I think the reason why\n&gt;the other sigmoid approximations has such poor learning\n&gt;characteristics is because they were either to simple, or not smooth.\n&gt;I think if they are too simple you can&#39;t combine them in very may\n&gt;ways to arrive at a good approximation of a given function. This is\n&gt;why a straight line or a stepped function generally makes a poor\n&gt;activation function. If they aren&#39;t continuous small changes in the\n&gt;weights can cause big jumps or erratic jumps. Which can make it hard\n&gt;to converge on an optimal solution.\n\nI&#39;d be very worried about what the curve does outside the usual range \nof inputs, unless you are going to cap the absolute range of inputs \npermitted to a neuron?  Otherwise you risk a neuron with unusually \nlarge inputs producing even larger outputs and chaos (in the \nmathematical and other senses) ensues, especially in a recurrent network...\n\n&gt;I have only run my sigmoid approx on the problems I am interested in\n&gt;so my high regard for it is probably unfounded and naive. But it\n&gt;definitely seems to improve the rate of learning for the problems I\n&gt;am interested in right now.\n&gt;\n&gt;My next major optimization project is going to be implementing a\n&gt;GPGPU ANN implementation. I am hopefully hiring a chap in a few days\n&gt;to work on this project. It will be exciting to see if I can get the\n&gt;same performance gains (30 fold) from the GPGPU as others have in the\n&gt;papers I have read. Anyone have any experience with this?\n\nWhat is a GPGPGPGPGANN?  Oh, a graphics-processor-geometry-pipeline, \nyes good idea.\n\nDid any body see intel demonstrated (I think it was) a 80-core CPU \nwhich was effectively 80 floating point units on one chip?\n\n         Ian \n\n\n-- \nNo virus found in this outgoing message.\nChecked by AVG Free Edition. \nVersion: 7.5.463 / Virus Database: 269.5.5/769 - Release Date: 19/04/2007 17:56\n\n\n\n"}}