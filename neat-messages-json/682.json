{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":115403844,"authorName":"John Arrowwood","from":"&quot;John Arrowwood&quot; &lt;jarrowwx@...&gt;","profile":"jarrowwx","replyTo":"LIST","senderId":"FM_nklXeHckRww0U-LC-L8eLn3g1jr6POM394gjOMNUNtwPgLMkGaDLZRhL02wHMkb9hxblFL6E4X6bjYYCp-XoJ-mv0A8dKLv9qhEat","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] autonomous virtual humans project","postDate":"1082584514","msgId":682,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PEJBWTItRjUwNDhCc1R6QzZUbzAwMDAxZjNmN0Bob3RtYWlsLmNvbT4="},"prevInTopic":681,"nextInTopic":683,"prevInTime":681,"nextInTime":683,"topicId":657,"numMessagesInTopic":32,"msgSnippet":"... I can actually SEE my hands/arms in front of my face with my eyes closed. Where the arm is physically located, there is a darker visual appearance than","rawEmail":"Return-Path: &lt;jarrowwx@...&gt;\r\nX-Sender: jarrowwx@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 72229 invoked from network); 21 Apr 2004 21:55:16 -0000\r\nReceived: from unknown (66.218.66.217)\n  by m25.grp.scd.yahoo.com with QMQP; 21 Apr 2004 21:55:16 -0000\r\nReceived: from unknown (HELO hotmail.com) (65.54.247.50)\n  by mta2.grp.scd.yahoo.com with SMTP; 21 Apr 2004 21:55:16 -0000\r\nReceived: from mail pickup service by hotmail.com with Microsoft SMTPSVC;\n\t Wed, 21 Apr 2004 14:55:14 -0700\r\nReceived: from 64.122.44.102 by by2fd.bay2.hotmail.msn.com with HTTP;\n\tWed, 21 Apr 2004 21:55:14 GMT\r\nX-Originating-Email: [jarrowwx@...]\r\nX-Sender: jarrowwx@...\r\nTo: neat@yahoogroups.com\r\nBcc: \r\nDate: Wed, 21 Apr 2004 14:55:14 -0700\r\nMime-Version: 1.0\r\nContent-Type: text/plain; format=flowed\r\nMessage-ID: &lt;BAY2-F5048BsTzC6To00001f3f7@...&gt;\r\nX-OriginalArrivalTime: 21 Apr 2004 21:55:14.0918 (UTC) FILETIME=[53C12060:01C427EB]\r\nX-eGroups-Remote-IP: 65.54.247.50\r\nFrom: &quot;John Arrowwood&quot; &lt;jarrowwx@...&gt;\r\nReply-To: john@...\r\nSubject: Re: [neat] autonomous virtual humans project\r\nX-Yahoo-Group-Post: member; u=115403844\r\nX-Yahoo-Profile: jarrowwx\r\n\r\n&gt;From: Mitchell Timin &lt;zenguyuno@...&gt;\n\n&gt;It is well established that humans have a &quot;kinesthetic\n&gt;sense&quot; which let&#39;s them &quot;feel&quot; the position of their\n&gt;various body parts.  You can experience it.  Just\n&gt;close your eyes and move your arms into different\n&gt;positions, or better yet, relax them and have someone\n&gt;else put them in different positions.  Can you feel\n&gt;where your arms are?  (You should be able to.)\n\nI can actually SEE my hands/arms in front of my face with my eyes closed.  \nWhere the arm is physically located, there is a darker visual appearance \nthan right at the edges.  Almost like seeing an aura or something.  My \nconclusion is that it is a &#39;compound sense&#39; that is a result of many other \nsenses working together and creating an expectation of what will be seen.  \nWithin the brain, the expectation is merged with the reality such that if \nthe expectation is met, it creates a positive feedback loop, which \nreinforces the means by which the expectation was produced.  But since my \neyes are closed, there is nothing much to merge with.  So all that is seen \nis the expectation.  That merging of expectation creates a distortion of the \nreal visual input (nothing).  The distortion is weak, misty, faint, but \nvisible.  And perhaps my threshold of sensitivity is such that I can see \nwhat others can not...\n\nNevertheless, what is called a kinesthetic sense may not be a direct &#39;sense&#39; \nbut a compound sense.  Skin sensations, muscle tension sensations, and who \nknows how many other things, all mix together to form a compound \nunderstanding that results in a general awareness of limb position.  It only \nmakes sense when you consider other cognitive abilities that such a &#39;sense&#39; \nis accomplished in much the same way.\n\nNow, you can implement that however you want in the virtual human.  If you \ncan implement it programmatically instead of making the network synthesize \nit, you will simplify the network and speed up the search.  But if you want \nit to be more like humans, then you will want it to work more like humans.\n\nFor simplicity, what I would suggest you do it programmatically.  Here&#39;s \nwhat I&#39;d do...\n\nDefine a skeleton.  Impose directional movement limits, just like we have.  \nFor instance, the knee can only move within a certain range of motion.  Add \nsensors that raise when pressure is applied to the joint in an effort to \nmove it beyond its free range of motion.  It won&#39;t go beyond the available \nlimit, but the higher the pressure to do so, the higher the sensor is \nraised.  Each such sensor is an input.\n\nNow add muscles.  Each muscle does one thing...contracts.  Add enough \nmuscles that the skeleton can be moved in the full range.  Each muscle is an \noutput.  The higher the output, the stronger it tries to contract.  The \nmuscle is also an input.  The more resistance there is to its contraction, \nthe stronger the input.  There should also be a stretch sensor.  I suggest \nyou don&#39;t make it a direct input, but a translated input.  That is, don&#39;t \nreport the output as the input.  The amount of desire to contract (output) \nmay not correspond with the amount of actual contraction that has taken \nplace.\n\nNow, identify a point, probably between the eyes and slightly behind, or \nperhaps right in the center of the head, and make that the &#39;origin.&#39;  \nIdentify a plane that is parallel to the ground when the head is in normal \nposition.  Define the X coordinate as being from ear to ear, side to side.  \nDefine the Y coordinate as the front to back direction.  Define the Z \ncoordinate as the direction towards the ground.  However, define all this \nrelative to the shape of the head.  Thus, if the head tips down, the frame \nof reference shifts or rotates.\n\nNow, define every joint in the skeleton as a point.  Calculate its position \nas an X,Y,Z vector relative to the origin, adjusting for any repositioning \nof the head.  Include tips of fingers and toes, too.  Each such vector \nbecomes one input.  There&#39;s your kinesthetic sense.\n\nYou will probably also want a skin with an array of pressure sensors.  That \nway it can tell when it is touching something.\n\nWhen you add them up, that&#39;s a LOT of inputs.  You may want to do your \ninitial experiment with much simpler forms, to make sure you can get \nsomething that actually demonstrates some level of ability.  Or who knows, \nyou may want to include the body as part of the genome and see what kind of \nbody it develops...  Of course, then it&#39;s not a virtual human.  But once you \nhave gotten that far, you will have all the information you need to be able \nto extend the concept to an organism of the approximate structure of a \nhuman.\n\n-- John\n\n_________________________________________________________________\nIs your PC infected? Get a FREE online computer virus scan from McAfeeï¿½ \nSecurity. http://clinic.mcafee.com/clinic/ibuy/campaign.asp?cid=3963\n\n\n"}}