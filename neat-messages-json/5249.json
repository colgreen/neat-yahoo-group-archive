{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":37465196,"authorName":"Ken Lloyd","from":"&quot;Ken Lloyd&quot; &lt;kalloyd@...&gt;","profile":"kalloyd2","replyTo":"LIST","senderId":"Glnp5KLWgGpXP8E_5A2HEtrBm6jwAP6W0z7ayBi6vdZHlMPWLB9Yd9D5E6yWugj1OOqK-8WYedq1owc8GMF4EDfyeSv1UBs3","spamInfo":{"isSpam":false,"reason":"0"},"subject":"RE: [neat] New Activation Functions (Was Re: &#39;Boxes&#39; Visual Discrimination Task)","postDate":"1274963644","msgId":5249,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDMyQUFGNTdCOEIxMjQ1MzZCQkY2RkIyMDM0NEQ3NkI3QHdhdHRkZXYxPg==","inReplyToHeader":"PGh0a3JjcytjZTZtQGVHcm91cHMuY29tPg==","referencesHeader":"PEFBTkxrVGlsM3gwdXlRZjMwQ0g1cHUtdDZzOHNqMlFZYjJySHRCSWhNZnhiUkBtYWlsLmdtYWlsLmNvbT4gPGh0a3JjcytjZTZtQGVHcm91cHMuY29tPg=="},"prevInTopic":5247,"nextInTopic":0,"prevInTime":5248,"nextInTime":5250,"topicId":5237,"numMessagesInTopic":10,"msgSnippet":"All, Might I suggest that a more Category Theoretic approach be considered for activation and perturbation? For example, a node might represent functors, ","rawEmail":"Return-Path: &lt;kalloyd@...&gt;\r\nX-Sender: kalloyd@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 12936 invoked from network); 27 May 2010 12:34:26 -0000\r\nX-Received: from unknown (98.137.34.44)\n  by m3.grp.sp2.yahoo.com with QMQP; 27 May 2010 12:34:26 -0000\r\nX-Received: from unknown (HELO qmta15.emeryville.ca.mail.comcast.net) (76.96.27.228)\n  by mta1.grp.sp2.yahoo.com with SMTP; 27 May 2010 12:34:26 -0000\r\nX-Received: from omta20.emeryville.ca.mail.comcast.net ([76.96.30.87])\n\tby qmta15.emeryville.ca.mail.comcast.net with comcast\n\tid NcSg1e0081smiN4AFcaTiR; Thu, 27 May 2010 12:34:27 +0000\r\nX-Received: from wattdev1 ([174.56.66.94])\n\tby omta20.emeryville.ca.mail.comcast.net with comcast\n\tid NcaS1e005221HGW8gcaSjP; Thu, 27 May 2010 12:34:27 +0000\r\nTo: &lt;neat@yahoogroups.com&gt;\r\nReferences: &lt;AANLkTil3x0uyQf30CH5pu-t6s8sj2QYb2rHtBIhMfxbR@...&gt; &lt;htkrcs+ce6m@...&gt;\r\nDate: Thu, 27 May 2010 06:34:04 -0600\r\nMessage-ID: &lt;32AAF57B8B124536BBF6FB20344D76B7@wattdev1&gt;\r\nMIME-Version: 1.0\r\nContent-Type: multipart/alternative;\n\tboundary=&quot;----=_NextPart_000_05A9_01CAFD66.9A553F50&quot;\r\nX-Mailer: Microsoft Office Outlook 11\r\nThread-Index: Acr9Usi2sMCVjx0FR/mDiPT/jhOwJwARPMig\r\nX-MimeOLE: Produced By Microsoft MimeOLE V6.00.2900.5931\r\nIn-Reply-To: &lt;htkrcs+ce6m@...&gt;\r\nFrom: &quot;Ken Lloyd&quot; &lt;kalloyd@...&gt;\r\nSubject: RE: [neat] New Activation Functions (Was Re: &#39;Boxes&#39; Visual Discrimination Task)\r\nX-Yahoo-Group-Post: member; u=37465196; y=q7vuBgfpYj0Ld_AJ8vxXHc5IBZkhpLM5RXC3kmPF3X0x8J4\r\nX-Yahoo-Profile: kalloyd2\r\n\r\n\r\n------=_NextPart_000_05A9_01CAFD66.9A553F50\r\nContent-Type: text/plain;\n\tcharset=&quot;us-ascii&quot;\r\nContent-Transfer-Encoding: 7bit\r\n\r\nAll,\n \nMight I suggest that a more Category Theoretic approach be considered for\nactivation and perturbation? For example, a node might represent functors,\ncategory of functions - so-called compositions - of which groups, rings (and\nsemi-rings), and monads may be part.  The best fitting shape of the\nactivation can be discovered as well, in addition to certain distance\nmetrics discussed in other posting.  This makes programming the beast much\neasier.  For specific ideas, see Alex Stepanov and Paul McJones&#39; &quot;Elements\nof Programming&quot; (Addison-Wesley).\n \nKen Lloyd\n \n\n  _____  \n\nFrom: neat@yahoogroups.com [mailto:neat@yahoogroups.com] On Behalf Of Ken\nSent: Wednesday, May 26, 2010 10:11 PM\nTo: neat@yahoogroups.com\nSubject: [neat] New Activation Functions (Was Re: &#39;Boxes&#39; Visual\nDiscrimination Task)\n\n\n  \n\n\n\nColin, when you mentioned adding a log(x) it brought to my mind that it\nmight be interesting to add both log(x) and e^x to the set together. That\nway, some combinations of nodes could realize multiplications. \n\nI appreciate what you&#39;re saying about how combinations of nodes in our\nabstract &quot;neural networks&quot; are more realistic abstractions of neurons than\nsingle nodes. I&#39;ve never really felt that a node in a traditional ANN is a\ntrue correlate to a real neuron. Of course it goes beyond even activation\nfunctions; plasticity comes into play as well. \n\nBut at the same time just because real neurons do a lot of different kinds\nof information processing individually does not mean we need all that,\neither individually or collectively. The whole problem in our field is to\ndecide what from nature is essential and what is not, which is basically the\nproblem of abstraction. Much of the minutia of nature is tantalizing, but\nmay nevertheless not be essential. When it comes to neural activation\nfunctions, the question of what is essential is fuzzy. It&#39;s basically a\nquestion of bias and scale since we know sigmoids can theoretically\napproximate any function. Yet of course that does not mean sticking with\nonly sigmoids is superior. But neither does it tell us that we need\neverything either.\n\nken \n\n--- In neat@yahoogroups.com &lt;mailto:neat%40yahoogroups.com&gt; , Colin Green\n&lt;colin.green1@...&gt; wrote:\n&gt;\n&gt; On 25 May 2010 08:46, Ken &lt;kstanley@...&gt; wrote:\n&gt; &gt; I see where you&#39;re coming from. The multiplication examples are indeed\ninteresting. Have you\n&gt; &gt; tried running the inputs to neurons (after they run over their incoming\nconnections) through\n&gt; &gt; logarithms? I wonder what networks would do with logs at all their\ninputs. Maybe the nodes\n&gt; &gt; would then only need to be linear summations (because logs provide\nnonlinearities). Or maybe\n&gt; &gt; they could still run their usual activation function set.\n&gt; \n&gt; I think if we acknoweldge the possibility that there is some (possibly\n&gt; significant) functionality in dendritic trees then the traditional ANN\n&gt; model isn&#39;t as &#39;broken&#39; as that might suggest - so long as we accept\n&gt; that single ANN nodes aren&#39;t a simplification of single neurons but\n&gt; rather a representation of a fundamental unit of processing. That is,\n&gt; we let go of the one-to-one mapping of neuron==ANN node. From there\n&gt; we could e.g. add a log(x) activation function to our set of CPPN\n&gt; activation functions (as a first experiment).\n&gt; \n&gt; &gt; In general, there is definitely room for the neural model to be\nimproved. The main impediment\n&gt; &gt; to progress in this area is probably just that it is not very glorious\nwork to be fiddling with\n&gt; &gt; different function sets and network models and hoping to see a\nbreakthrough, but it is true\n&gt; &gt; there could be a payoff.\n&gt; \n&gt; Sure. The fact that Christof Koch wrote a whole book subtitled\n&gt; &quot;Information processing in single neurons&quot; suggests there might be\n&gt; more to neurons than a simple sigmoid input-response behaviour :)\n&gt; \n&gt; \n&gt; &gt; But I&#39;m not saying there isn&#39;t something better. I&#39;m just saying it&#39;s\nhard to tell if there is a problem or\n&gt; &gt; not that needs fixing. There may be and I would be very interested if\nsomething superior was\n&gt; &gt; demonstrated. Will the new SharpNEAT provide an ability to play with\nsuch ideas?\n&gt; \n&gt; Yes it&#39;s certainly more geared up for experimenting with different\n&gt; types of genome without having to hack about in the core code at all.\n&gt; In short I&#39;ve used generics (similar concept to C++ templates) to\n&gt; abstract away the core algorithms (evolution, speciation) from the\n&gt; genome implementation. So the idea is you devise whatever genome you\n&gt; want along with the &#39;phenome&#39; implementation and then plug in some\n&gt; code that &#39;decodes&#39; the genome to the phenome\n&gt; (IGenomeDecoder&lt;TGenome,TPhenome&gt; interface). The speciation\n&gt; abstraction works by defining the genomes position in an N dimensional\n&gt; space - that is, you have to translate the &#39;guts&#39; of the genome into a\n&gt; N-dimensional position and teh speciation works on those positions -\n&gt; it&#39;s completely abstracted away from the genome classes.\n&gt; \n&gt; I&#39;ll respond to one of your other points in a further post.\n&gt; \n&gt; Colin\n&gt;\n\n\n\n\n\r\n------=_NextPart_000_05A9_01CAFD66.9A553F50\r\nContent-Type: text/html;\n\tcharset=&quot;us-ascii&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\n&lt;!DOCTYPE HTML PUBLIC &quot;-//W3C//DTD HTML 4.01 Transitional//EN&quot; &quot;http://www.=\r\nw3c.org/TR/1999/REC-html401-19991224/loose.dtd&quot;&gt;\n&lt;HTML&gt;&lt;HEAD&gt;\n&lt;META http-eq=\r\nuiv=3DContent-Type content=3D&quot;text/html; charset=3Dus-ascii&quot;&gt;\n&lt;META content=\r\n=3D&quot;MSHTML 6.00.6000.17023&quot; name=3DGENERATOR&gt;&lt;/HEAD&gt;\n&lt;BODY style=3D&quot;BACKGRO=\r\nUND-COLOR: #fff&quot;&gt;\n&lt;DIV dir=3Dltr align=3Dleft&gt;&lt;SPAN class=3D546482512-27052=\r\n010&gt;&lt;FONT face=3DArial \ncolor=3D#0000ff size=3D2&gt;All,&lt;/FONT&gt;&lt;/SPAN&gt;&lt;/DIV&gt;\n&lt;=\r\nDIV dir=3Dltr align=3Dleft&gt;&lt;SPAN class=3D546482512-27052010&gt;&lt;FONT face=3DAr=\r\nial \ncolor=3D#0000ff size=3D2&gt;&lt;/FONT&gt;&lt;/SPAN&gt;&nbsp;&lt;/DIV&gt;\n&lt;DIV dir=3Dltr ali=\r\ngn=3Dleft&gt;&lt;SPAN class=3D546482512-27052010&gt;&lt;FONT face=3DArial \ncolor=3D#000=\r\n0ff size=3D2&gt;Might I suggest that a more Category Theoretic approach be \nco=\r\nnsidered for activation and perturbation? For example, a node might \nrepres=\r\nent&nbsp;functors,&nbsp;category of functions - so-called compositions - \no=\r\nf which groups, rings (and semi-rings), and monads may be part.&nbsp; The b=\r\nest \nfitting&nbsp;shape of the activation can be discovered as well, in add=\r\nition to \ncertain distance metrics discussed in other posting.&nbsp; This m=\r\nakes \nprogramming the beast much easier.&nbsp; For specific ideas, see Alex=\r\n Stepanov \nand Paul McJones&#39; &quot;Elements of Programming&quot; \n(Addison-Wesley).&lt;/=\r\nFONT&gt;&lt;/SPAN&gt;&lt;/DIV&gt;\n&lt;DIV dir=3Dltr align=3Dleft&gt;&lt;SPAN class=3D546482512-2705=\r\n2010&gt;&lt;FONT face=3DArial \ncolor=3D#0000ff size=3D2&gt;&lt;/FONT&gt;&lt;/SPAN&gt;&nbsp;&lt;/DIV=\r\n&gt;\n&lt;DIV dir=3Dltr align=3Dleft&gt;&lt;SPAN class=3D546482512-27052010&gt;&lt;FONT face=\r\n=3DArial \ncolor=3D#0000ff size=3D2&gt;Ken Lloyd&lt;/FONT&gt;&lt;/SPAN&gt;&lt;/DIV&gt;\n&lt;DIV dir=\r\n=3Dltr align=3Dleft&gt;&lt;SPAN class=3D546482512-27052010&gt;&lt;FONT face=3DArial \nco=\r\nlor=3D#0000ff size=3D2&gt;&lt;/FONT&gt;&lt;/SPAN&gt;&nbsp;&lt;/DIV&gt;&lt;BR&gt;\n&lt;DIV class=3DOutlookM=\r\nessageHeader lang=3Den-us dir=3Dltr align=3Dleft&gt;\n&lt;HR tabIndex=3D-1&gt;\n&lt;FONT =\r\nface=3DTahoma size=3D2&gt;&lt;B&gt;From:&lt;/B&gt; neat@yahoogroups.com \n[mailto:neat@yaho=\r\nogroups.com] &lt;B&gt;On Behalf Of &lt;/B&gt;Ken&lt;BR&gt;&lt;B&gt;Sent:&lt;/B&gt; Wednesday, \nMay 26, 20=\r\n10 10:11 PM&lt;BR&gt;&lt;B&gt;To:&lt;/B&gt; neat@yahoogroups.com&lt;BR&gt;&lt;B&gt;Subject:&lt;/B&gt; \n[neat] N=\r\new Activation Functions (Was Re: &#39;Boxes&#39; Visual Discrimination \nTask)&lt;BR&gt;&lt;/=\r\nFONT&gt;&lt;BR&gt;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;/DIV&gt;&lt;SPAN style=3D&quot;DISPLAY: none&quot;&gt;&nbsp;&lt;/SPAN&gt; \n&lt;DI=\r\nV id=3Dygrp-text&gt;\n&lt;P&gt;&lt;BR&gt;&lt;BR&gt;Colin, when you mentioned adding a log(x) it b=\r\nrought to my mind that \nit might be interesting to add both log(x) and e^x =\r\nto the set together. That \nway, some combinations of nodes could realize mu=\r\nltiplications. &lt;BR&gt;&lt;BR&gt;I \nappreciate what you&#39;re saying about how combinati=\r\nons of nodes in our abstract \n&quot;neural networks&quot; are more realistic abstract=\r\nions of neurons than single nodes. \nI&#39;ve never really felt that a node in a=\r\n traditional ANN is a true correlate to a \nreal neuron. Of course it goes b=\r\neyond even activation functions; plasticity \ncomes into play as well. &lt;BR&gt;&lt;=\r\nBR&gt;But at the same time just because real neurons \ndo a lot of different ki=\r\nnds of information processing individually does not mean \nwe need all that,=\r\n either individually or collectively. The whole problem in our \nfield is to=\r\n decide what from nature is essential and what is not, which is \nbasically =\r\nthe problem of abstraction. Much of the minutia of nature is \ntantalizing, =\r\nbut may nevertheless not be essential. When it comes to neural \nactivation =\r\nfunctions, the question of what is essential is fuzzy. It&#39;s basically \na qu=\r\nestion of bias and scale since we know sigmoids can theoretically \napproxim=\r\nate any function. Yet of course that does not mean sticking with only \nsigm=\r\noids is superior. But neither does it tell us that we need everything \neith=\r\ner.&lt;BR&gt;&lt;BR&gt;ken &lt;BR&gt;&lt;BR&gt;--- In &lt;A \nhref=3D&quot;mailto:neat%40yahoogroups.com&quot;&gt;ne=\r\nat@yahoogroups.com&lt;/A&gt;, Colin Green \n&lt;colin.green1@...&gt; wrote:&lt;BR&gt;&gt=\r\n;&lt;BR&gt;&gt; On 25 May 2010 08:46, Ken \n&lt;kstanley@...&gt; wrote:&lt;BR&gt;&gt; &g=\r\nt; I see where you&#39;re coming from. The \nmultiplication examples are indeed =\r\ninteresting. Have you&lt;BR&gt;&gt; &gt; tried \nrunning the inputs to neurons (af=\r\nter they run over their incoming connections) \nthrough&lt;BR&gt;&gt; &gt; logarit=\r\nhms? I wonder what networks would do with logs at \nall their inputs. Maybe =\r\nthe nodes&lt;BR&gt;&gt; &gt; would then only need to be linear \nsummations (becau=\r\nse logs provide nonlinearities). Or maybe&lt;BR&gt;&gt; &gt; they \ncould still ru=\r\nn their usual activation function set.&lt;BR&gt;&gt; &lt;BR&gt;&gt; I think if \nwe ackn=\r\noweldge the possibility that there is some (possibly&lt;BR&gt;&gt; significant) \n=\r\nfunctionality in dendritic trees then the traditional ANN&lt;BR&gt;&gt; model isn=\r\n&#39;t as \n&#39;broken&#39; as that might suggest - so long as we accept&lt;BR&gt;&gt; that s=\r\ningle ANN \nnodes aren&#39;t a simplification of single neurons but&lt;BR&gt;&gt; rath=\r\ner a \nrepresentation of a fundamental unit of processing. That is,&lt;BR&gt;&gt; =\r\nwe let go \nof the one-to-one mapping of neuron=3D=3DANN node. From there&lt;BR=\r\n&gt;&gt; we could e.g. \nadd a log(x) activation function to our set of CPPN&lt;BR=\r\n&gt;&gt; activation functions \n(as a first experiment).&lt;BR&gt;&gt; &lt;BR&gt;&gt; &gt; =\r\nIn general, there is definitely \nroom for the neural model to be improved. =\r\nThe main impediment&lt;BR&gt;&gt; &gt; to \nprogress in this area is probably just=\r\n that it is not very glorious work to be \nfiddling with&lt;BR&gt;&gt; &gt; differ=\r\nent function sets and network models and hoping \nto see a breakthrough, but=\r\n it is true&lt;BR&gt;&gt; &gt; there could be a \npayoff.&lt;BR&gt;&gt; &lt;BR&gt;&gt; Sure. T=\r\nhe fact that Christof Koch wrote a whole book \nsubtitled&lt;BR&gt;&gt; &quot;Informati=\r\non processing in single neurons&quot; suggests there \nmight be&lt;BR&gt;&gt; more to n=\r\neurons than a simple sigmoid input-response behaviour \n:)&lt;BR&gt;&gt; &lt;BR&gt;&gt; =\r\n&lt;BR&gt;&gt; &gt; But I&#39;m not saying there isn&#39;t something \nbetter. I&#39;m just sa=\r\nying it&#39;s hard to tell if there is a problem or&lt;BR&gt;&gt; &gt; \nnot that need=\r\ns fixing. There may be and I would be very interested if something \nsuperio=\r\nr was&lt;BR&gt;&gt; &gt; demonstrated. Will the new SharpNEAT provide an \nability=\r\n to play with such ideas?&lt;BR&gt;&gt; &lt;BR&gt;&gt; Yes it&#39;s certainly more geared \n=\r\nup for experimenting with different&lt;BR&gt;&gt; types of genome without having =\r\nto \nhack about in the core code at all.&lt;BR&gt;&gt; In short I&#39;ve used generics=\r\n (similar \nconcept to C++ templates) to&lt;BR&gt;&gt; abstract away the core algo=\r\nrithms \n(evolution, speciation) from the&lt;BR&gt;&gt; genome implementation. So =\r\nthe idea is \nyou devise whatever genome you&lt;BR&gt;&gt; want along with the &#39;ph=\r\nenome&#39; \nimplementation and then plug in some&lt;BR&gt;&gt; code that &#39;decodes&#39; th=\r\ne genome to \nthe phenome&lt;BR&gt;&gt; (IGenomeDecoder&lt;TGenome,TPhenome&gt; in=\r\nterface). The \nspeciation&lt;BR&gt;&gt; abstraction works by defining the genomes=\r\n position in an N \ndimensional&lt;BR&gt;&gt; space - that is, you have to transla=\r\nte the &#39;guts&#39; of the \ngenome into a&lt;BR&gt;&gt; N-dimensional position and teh =\r\nspeciation works on those \npositions -&lt;BR&gt;&gt; it&#39;s completely abstracted a=\r\nway from the genome \nclasses.&lt;BR&gt;&gt; &lt;BR&gt;&gt; I&#39;ll respond to one of your =\r\nother points in a further \npost.&lt;BR&gt;&gt; &lt;BR&gt;&gt; Colin&lt;BR&gt;&gt;&lt;BR&gt;&lt;BR&gt;&lt;/P&gt;=\r\n&lt;/DIV&gt;&lt;!-- end group email --&gt;&lt;/BODY&gt;&lt;/HTML&gt;\n\r\n------=_NextPart_000_05A9_01CAFD66.9A553F50--\r\n\n"}}