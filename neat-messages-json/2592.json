{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":199382914,"authorName":"Mike Woodhouse","from":"&quot;Mike Woodhouse&quot; &lt;mikewoodhouse@...&gt;","profile":"mikewoodhouse","replyTo":"LIST","senderId":"HJr936hjzWaEFeHQV6rVDhAFvgou1epKVkqIRqRQzt4M9hUwWXSZQE0Bo-S8yDANKyrh1yrXt9qM8Gl6vRd0ryeFVWL2zY4hR8sCnyWmsmqP","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: Alternative SharpNEAT Progress form","postDate":"1143193378","msgId":2592,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGUwMGV2MitrcWxoQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDQ0MjMyQkNCLjUwNjA4MDFAZHNsLnBpcGV4LmNvbT4="},"prevInTopic":2590,"nextInTopic":2596,"prevInTime":2591,"nextInTime":2593,"topicId":2586,"numMessagesInTopic":6,"msgSnippet":"... - ... two ... makes ... I d better confess that I couldn t find a 10-minute solution to getting two data sets using different scales to work properly!","rawEmail":"Return-Path: &lt;mikewoodhouse@...&gt;\r\nX-Sender: mikewoodhouse@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 5987 invoked from network); 24 Mar 2006 09:43:07 -0000\r\nReceived: from unknown (66.218.66.217)\n  by m35.grp.scd.yahoo.com with QMQP; 24 Mar 2006 09:43:07 -0000\r\nReceived: from unknown (HELO n9a.bullet.dcn.yahoo.com) (216.155.203.232)\n  by mta2.grp.scd.yahoo.com with SMTP; 24 Mar 2006 09:43:07 -0000\r\nComment: DomainKeys? See http://antispam.yahoo.com/domainkeys\r\nReceived: from [216.155.201.64] by n9.bullet.dcn.yahoo.com with NNFMP; 24 Mar 2006 09:42:59 -0000\r\nReceived: from [66.218.69.3] by t1.bullet.dcn.yahoo.com with NNFMP; 24 Mar 2006 09:42:58 -0000\r\nReceived: from [66.218.66.83] by t3.bullet.scd.yahoo.com with NNFMP; 24 Mar 2006 09:42:58 -0000\r\nDate: Fri, 24 Mar 2006 09:42:58 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;e00ev2+kqlh@...&gt;\r\nIn-Reply-To: &lt;44232BCB.5060801@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: &quot;Mike Woodhouse&quot; &lt;mikewoodhouse@...&gt;\r\nSubject: Re: Alternative SharpNEAT Progress form\r\nX-Yahoo-Group-Post: member; u=199382914; y=SZx6P961uGXRpgrHdgQoSgWWu9Rwx2jCImPZ3Y00GFHFIH6HfLnt_A\r\nX-Yahoo-Profile: mikewoodhouse\r\n\r\n\n--- In neat@yahoogroups.com, Colin Green &lt;cgreen@...&gt; wrote:\n&gt;\n&gt; Thanks Mi=\r\nke, this looks really nice. I see you attached some code as\n&gt; well so maybe=\r\n I&#39;ll take a look at integrating this into the main\n&gt; project, it should be=\r\n easy enough. Oh personally I quite like having\n&gt; more than one line on a g=\r\nraph where the two values are related somehow\n-\n&gt; it makes it easier to see=\r\n how the values affect each other and/or how\n&gt; they are related, e.g. # of =\r\nspecies and compat. threshold, or\n&gt; comlplexity and evals/sec. Having said =\r\nthat I see you have put those\ntwo\n&gt; particular pairings in graphs that are =\r\nvertically alligned, which\nmakes\n&gt; it pretty easy to see actually.\n\nI&#39;d bet=\r\nter confess that I couldn&#39;t find a 10-minute solution to getting\ntwo data s=\r\nets using different scales to work properly! You&#39;ve reminded\nme of another =\r\nentry for the to-do list.\n\n&gt; Some other stuff you may not have picked up on=\r\n. There was some work to\n&gt; break the experiment code into seperate binaries=\r\n and to attach them at\n&gt; run time with a config file - which in itself may =\r\nnot be that\n&gt; interesting to you, but there is also a mechanism for specify=\r\ning\n&gt; experimnent prameters such as data file paths within teh config file.=\r\n\nI\n&gt; can&#39;t remember if I got round to releasing this yet - anyway it might\n=\r\nbe\n&gt; useful for you.\n\nIt doesn&#39;t look like you&#39;ve uploaded that one to Sour=\r\nceForge (or made it\navailable or whatever the process is). It&#39;s a logical n=\r\next step, though,\nand I&#39;d definitely find it useful, but don&#39;t bust a gut -=\r\n I&#39;ve already\nhacked my experiment to take some degree of external configur=\r\nation, it\nwas a logical refactoring.\n\n&gt; On a more forward looking note, I a=\r\nlso started completely revising the\n&gt; whole code base and re-writing it to =\r\nmake it far more flexible to code\n&gt; against (e.g. plug-inable genome repres=\r\nentations and associated\ndecoded\n&gt; &#39;phenome&#39; classes). This is heavily usin=\r\ng templates (generics in\n&gt; .Net2/Java world) and was makign good progress, =\r\nright up to the point\nI\n&gt; got distracted :(\n\nOooh. I am slightly less terri=\r\nfied by templates/generics than I was,\nhaving made my first Dictionary, but=\r\n only slightly. I had to look at\nsome template-heavy C++ earlier this week =\r\nand I&#39;m still feeling the\neffects. I&#39;d be interested in having a look when =\r\nyou&#39;ve got something\nyou&#39;re happy with.\n\n[snipped stock model intro]\n\n&gt; My =\r\ninitial plan is to use both price and trade volume data. Although\nthe\n&gt; tra=\r\nde direction (buy vs sell) isn&#39;t officially available, most web\n&gt; financial=\r\n web sites determine this in real time by comparing against\nthe\n&gt; current b=\r\nid/offer spread, this can be horribly wrong some days but\nthat\n&gt; in itself =\r\nis a symptom of a specific situation where the spread is\nvery\n&gt; narrow. So =\r\nI think there is useful info in this data even in this\n&gt; slightly corrupted=\r\n form. Certainly I&#39;ve learned from my trading so far\n&gt; that the price by it=\r\nself is only a small part of the picture,\nespecially\n&gt; where trading is thr=\r\nough old fashioned market makers who set the price\n&gt; rather than the more m=\r\nodern automated SETS system - which system is in\n&gt; use for agiven share is =\r\nalso going to be pretty important as well I\nthink.\n\nYou&#39;re almost certainly=\r\n going to need to use derived data rather than\nraw prices: percentage chang=\r\ne from day to day is a good start. z-scores\nare good but I&#39;m always slightl=\r\ny concerned that they assume that the\nbounds of the range of source values =\r\nis known. There are a number of\nother derived values that can be calculated=\r\n from the technical analysis\n(TA) domain, from the simple (moving averages,=\r\n relative strength,\nmomentum) though trickier (volatility) to the downright=\r\n exotic\n(Bollinger bands, oscillators, fratal coefficients).\n\nIn (efficient=\r\n market) theory, everything that is known about the company\nis encapsulated=\r\n in the current price. In practice, that turns out not to\nbe the case, even=\r\n if it&#39;s just a latency effect. I would think that by\nsubmitting several da=\r\nys&#39; (months?) worth of basic data you could get a\nmodel to effectively do i=\r\nts own TA, at the expense of longer compute\ntime.\n\nI think that a Neat solu=\r\ntion might get some mileage out of the &quot;memory&quot;\nidea I&#39;ve been playing with=\r\n: each stock gets a set of values for input\nto the next evaluation that wer=\r\ne captured from the previous one.\n\n&gt; That&#39;s enough banter for now - this is=\r\n all vaguely to do with NEAT,\nhonest!\n\nI think I helped to pull it back the=\r\nre at the end....\n\n\nMike\n\n\n\n\n\n\n"}}