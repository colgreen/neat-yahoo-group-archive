{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":279942280,"authorName":"Michael Neylon","from":"Michael Neylon &lt;mneylon01@...&gt;","profile":"mneylon01","replyTo":"LIST","senderId":"jAlupvWL0Mn514K5oPhrwWhqKHa7aEZJlHNuD3Um6Go5E295QB5RP3YTGe1ConH-BD8wVUXHKziNlGxUf82tPJKfOKKUQrNljeUi","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] Re: NEAT and highly recurrent networks","postDate":"1158614975","msgId":2750,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDIwMDYwOTE4MjEyOTM2LjY3ODk0LnFtYWlsQHdlYjU3MTA5Lm1haWwucmUzLnlhaG9vLmNvbT4=","inReplyToHeader":"PGVkZGl2MCtyZmMyQGVHcm91cHMuY29tPg=="},"prevInTopic":2723,"nextInTopic":2755,"prevInTime":2749,"nextInTime":2751,"topicId":2711,"numMessagesInTopic":7,"msgSnippet":"I finally figured out where things were going south in my code and got it to work nicely for the various known tests (standard XOR, pole balancing) compariable","rawEmail":"Return-Path: &lt;mneylon01@...&gt;\r\nX-Sender: mneylon01@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 68299 invoked from network); 18 Sep 2006 21:30:00 -0000\r\nReceived: from unknown (66.218.67.36)\n  by m40.grp.scd.yahoo.com with QMQP; 18 Sep 2006 21:30:00 -0000\r\nReceived: from unknown (HELO web57109.mail.re3.yahoo.com) (216.252.111.122)\n  by mta10.grp.scd.yahoo.com with SMTP; 18 Sep 2006 21:29:59 -0000\r\nReceived: (qmail 67896 invoked by uid 60001); 18 Sep 2006 21:29:36 -0000\r\nMessage-ID: &lt;20060918212936.67894.qmail@...&gt;\r\nReceived: from [66.224.103.194] by web57109.mail.re3.yahoo.com via HTTP; Mon, 18 Sep 2006 14:29:35 PDT\r\nDate: Mon, 18 Sep 2006 14:29:35 -0700 (PDT)\r\nTo: neat@yahoogroups.com\r\nIn-Reply-To: &lt;eddiv0+rfc2@...&gt;\r\nMIME-Version: 1.0\r\nContent-Type: multipart/alternative; boundary=&quot;0-1939585008-1158614975=:23891&quot;\r\nContent-Transfer-Encoding: 8bit\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Michael Neylon &lt;mneylon01@...&gt;\r\nSubject: Re: [neat] Re: NEAT and highly recurrent networks\r\nX-Yahoo-Group-Post: member; u=279942280; y=5d9DcQBBjUKR7ll42r5oeO2X-JX_x5xZ3EumuzEGjzO0j0N0\r\nX-Yahoo-Profile: mneylon01\r\n\r\n\r\n--0-1939585008-1158614975=:23891\r\nContent-Type: text/plain; charset=iso-8859-1\r\nContent-Transfer-Encoding: 8bit\r\n\r\nI finally figured out where things were going south in my code and got it to work nicely for the various known tests (standard XOR, pole balancing) compariable to the papers out there.  The XOR was adding too many nodes as I didn&#39;t prevent feedback though once fixed to (optionally) only allow feedforward connections, that worked perfectly.\n\nI found that for highly recurrent networks like for the delayed XOR problem, the addition of training as a possible weight mutation (it technically is a mutation of the weights, but done in a logical/mathematical fashion) was necessary to boost the speed of the convergenace of the problem.   Training every network is not good (you&#39;d favor networks with too many nodes but that can match the pattern of data presented, and not the data itself), and you don&#39;t want to train the network all the way either.  I found that training about 5-10% of the time with a dataset about 1/20th of the full time series (in addition to weight/new node/new connection mutations) helped pushed the convergence along better, maybe abiyut 2 to 10 times fewer generations.  Also, pushing up the new connection likelihood and reducing the chance of connections being disabled through crossover and mutation helped to capture the recurrent nature - while the number of nodes may be a bit higher than needed,\n there was the right number of nodes connected in the highly recurrent fashion as expected to solve the problem.  I&#39;m still working to get the feel for how the training mutation parameters can affect the performance of the system, but it seems reasonable with what I have listed above for far.\n\nKenneth Stanley &lt;kstanley@...&gt; wrote:                                  NEAT really shouldn&#39;t be allowed to form recurrent connections to \n solve XOR.  It&#39;s not that there is anything &quot;wrong&quot; or &quot;cheating&quot; \n about it- it&#39;s just that the main point of XOR is to compare your \n implementation with other implementations, and since the standard \n benchmark does not include recurrent connections, you obfuscate the \n comparison by allowing them.  It sounds like your XOR is indeed \n memorizing the order of presentation.  That is definitely not the \n intent of the XOR problem, and means your NNs are solving a very \n different problem.  They are memorizing a sequence by using \n recurrent connections.  That&#39;s interesting, but makes the comparison \n less meaningful.\n \n Technically, networks should be flushed (all nodes set to zero) \n between input presentations in XOR (or any other similar \n classification problem) since XOR is order-independent (the \n definition of the XOR problem says nothing about order of \n presentation).  The flushing should preclude the need to present the \n instances in random order.  Even if you want to use recurrent \n connections, flushing should be part of the procedure.\n \n The weight cap of 5 sounds reasonable.  Up to 10 I think is \n reasonable.  My rule of thumb for the power of mutations is that it \n should take on average several mutations to travel from a weight of \n 0 to the max or min weight.  \n \n If XOR weights are tending to max out, that may be an artifact of \n the XOR problem- though I&#39;m not sure- there may be other reasons as \n well.  But I would not worry a lot about it until the XOR problem is \n being presented in a customary way.\n \n --- In neat@yahoogroups.com, &quot;mneylon01&quot; &lt;mneylon01@...&gt; wrote:\n &gt;\n &gt; So I&#39;ve had a chance to do what Ken suggested below - start from \n the\n &gt; basic (2+1bias)-1-1 network for the static XOR problem, randomizing\n &gt; the test data, and disabling any node or connection creation or\n &gt; toggling to see how my network worked.  I&#39;m using what I believe \n are\n &gt; the same parameters in Ken&#39;s various papers: 80% chance of a \n child&#39;s\n &gt; weights being changed, with 90% chance of perturbation, 10% chance \n of\n &gt; mutation.  I used 150 species, and for the species calculation, I \n used\n &gt; 1.0 for the excess and disjoint coefficients, 0.4 for the common\n &gt; weight differences, and set N to 1 (as noted by Ken below), with \n the\n &gt; tolerance set to 3.0.\n &gt; \n &gt; What was interesting is that I found there to be a VERY strong\n &gt; connection between two parameters that I don&#39;t see mentioned much \n in\n &gt; the papers: the weight cap, and the type and amount of \n perturbation or\n &gt; mutation done.   (Note that I&#39;ve tried both the sigmoid steepness \n of\n &gt; 4.9 and 1, this seems to have less of an impact that these other\n &gt; parameters).\n &gt; \n &gt; My original code capped the weight at 2, which I saw slow if any\n &gt; approach to a good network.  Increasing the cap to insane values \n like\n &gt; 100 gave a great speed to evolution convergence, as this allowed \n for a\n &gt; wider range of weight values to be explored by the NEAT method, but\n &gt; leads to some networks that one questions their ability to work.  \n &gt; Using a cap of 5 to 10 gave a more reasonable set of networks while\n &gt; still converging quickly (about 50 generations to get the RMS under\n &gt; 0.1 on the 4 cases).  I&#39;ve seen the cap of 5 tossed around here\n &gt; before, but does this seem to be a good practical number?\n &gt; \n &gt; Also, with my weight mutations, I found that using gaussian\n &gt; distributions for both initial weights and mutated weights, and\n &gt; pertubations in weights was better than straight random.  \n (Presently\n &gt; I&#39;m using N rolls of the RNG and getting the average to simulate\n &gt; gaussian distribution, with larger N to reduce the standard\n &gt; deviation).  The quesiton I have, since I&#39;m having trouble seeing \n how\n &gt; it&#39;s done in either the C++ or C# code, is what the good effective\n &gt; magnitude is.  The above cases, for the mutated/initial weights, \n I&#39;m\n &gt; using a gaussian random number from -5 to 5, std deviation of \n about 1,\n &gt; and for the perturbed weights, from -5 to 5 with a std deviation of\n &gt; about 0.1-0.5.    These cases seem to work, but I can&#39;t seem how \n they\n &gt; easily match up with the above code.  Even with the weight caps, I\n &gt; always seemed to have weights that wanted to hit the caps.  Any \n good\n &gt; suggestions on what are good weight ranges and adjustments to \n make. \n &gt; Another thing I noticed is that if the weight cap is too low, using\n &gt; the values for the species tolerance calculation I give above, I\n &gt; rarely got more than 1 or two species.\n &gt; \n &gt; Additionally, I find it interesting in how one approaches the input\n &gt; data part.  I tried the standard XOR with a fixed set of data, and \n it\n &gt; can converge quickly, but when you then make the data randomly\n &gt; presented (re-evaluating weights of genomes that carry over to \n account\n &gt; for repeated runs), the performance of the NEAT method is poor.   \n That\n &gt; is, when fixed, I almost always got a recurrent connection (I don&#39;t\n &gt; prevent these from being formed), suggesting it&#39;s trying to mimic \n the\n &gt; input pattern order and not the actual data patterns.  I found \n that if\n &gt; a random order was used, using a larger test pattern (in my case, \n 50\n &gt; initially randomly generated XOR cases and then presented in a \n random\n &gt; order to the networks) seemed to help.  I still get some \n recurrancies,\n &gt; but the network size seemed to stay small and avoid new node\n &gt; formation.  (Obviously, when I use recurrent data as like the \n delayed\n &gt; XOR problem, the data order will be fixed, but the size of the data\n &gt; set needs to be large enough that it would be impractical for the\n &gt; genome to try to learn the overall data set pattern as opposed to \n the\n &gt; actual pattern that the dataset contains...)\n &gt; \n &gt; --- In neat@yahoogroups.com, &quot;Kenneth Stanley&quot; &lt;kstanley@&gt; wrote:\n &gt; &gt;\n &gt; &gt; It sounds like the main issue before you can really start \n looking at \n &gt; &gt; delayed-XOR is to get XOR working more closely to my own NEAT\n &gt; &gt; (and others developed since).  Certainly the 500-1000 \n generations \n &gt; &gt; it&#39;s taking you indicates somethign is likely wrong in your \n &gt; &gt; implementation.\n &gt; &gt; \n &gt; &gt; I would suggest the following experiment:  Start evolution with  \n a \n &gt; &gt; population of networks that already have the correct topology \n for a \n &gt; &gt; solution to XOR and turn off structural mutations.  Then run \n your \n &gt; &gt; version of NEAT as usual, except in this case, it will only be \n &gt; &gt; searching over weight space.  If it takes forever, it tells you \n that \n &gt; &gt; the problem is in the way your weights are being mutated, or the \n way \n &gt; &gt; they are being combined in crossover.  It may also indicate a \n &gt; &gt; problem in speciation (related to weight comparison).  In any \n case, \n &gt; &gt; it will greatly narrow down the problem.\n &gt; &gt; \n &gt; &gt; Once you get it working on pure weight-evolution, then you can \n move \n &gt; &gt; to the normal topology evolution, and you will either see it \n work \n &gt; &gt; right away, or find that there is a problem in adding structure \n as \n &gt; &gt; well.\n &gt; &gt; \n &gt; &gt; About compatibility testing for speciation, when you menion &quot;N,&quot; \n do \n &gt; &gt; you mean the normalization term in my papers?  Regrettably, many \n &gt; &gt; people miss that I did not use N (for normalization) in \n practice, \n &gt; &gt; that is, I set N to 1 in all cases.  That may explain why you \n using \n &gt; &gt; the same coefficients as me does not work.  If you look at my \n papers \n &gt; &gt; closely, they say that N can be set to 1 if genomes are not too \n &gt; &gt; large.  I have found in practice that it always works fine with \n it \n &gt; &gt; set to 1, so that&#39;s what I&#39;ve done.  This confusion is my fault \n and \n &gt; &gt; I apologize for it- the papers should be more clear- but I \n wanted \n &gt; &gt; people to be aware of N and the option for normalization in case \n it \n &gt; &gt; indeed does come into play with very large genomes.\n &gt; &gt; \n &gt; &gt; As for deviations from typical values, you can see all the \n values \n &gt; &gt; I&#39;ve used in the appendix to my dissertation.  There is some \n &gt; &gt; explanation there too for why different values were chosen.  One \n &gt; &gt; consideration might be whether very fine grained weight changed \n are \n &gt; &gt; key or not in a particular problem.  If they are, you might want \n the \n &gt; &gt; coefficient of weight differences to be higher.\n &gt; &gt; \n &gt; &gt; Once you get XOR working, please let the group know how things \n go \n &gt; &gt; with the delayed XOR!  By the way, what language/platform did \n you \n &gt; &gt; use for your version of NEAT?\n &gt; &gt; \n &gt; &gt; ken\n &gt; &gt; \n &gt; &gt; --- In neat@yahoogroups.com, &quot;mneylon01&quot; &lt;mneylon01@&gt; wrote:\n &gt; &gt; &gt;\n &gt; &gt; &gt; I&#39;ve been working on my own implimentation of NEAT (having to \n fit \n &gt; &gt; to a\n &gt; &gt; &gt; predescribed framework) and while it&#39;s mostly working, I&#39;m \n looking \n &gt; &gt; at\n &gt; &gt; &gt; a couple of questions.\n &gt; &gt; &gt; \n &gt; &gt; &gt; First, I know that the networks that NEAT generates can have\n &gt; &gt; &gt; recurrency (feedback), and this is likely why some of the \n robot \n &gt; &gt; game\n &gt; &gt; &gt; examples work well.  However, I&#39;m looking at trying to use \n NEAT to\n &gt; &gt; &gt; find patterns in time series data, as one would use fully \n recurrent\n &gt; &gt; &gt; networks for (In these, also known as Elmen networks, all of \n the\n &gt; &gt; &gt; hidden layer and output layer values are &#39;propagated&#39; into the \n next\n &gt; &gt; &gt; time step to give the network memory, with full connectivity \n &gt; &gt; between\n &gt; &gt; &gt; all the input and previous nodes to the hidden/output nodes).\n &gt; &gt; &gt; \n &gt; &gt; &gt; Such a network should be possible to be generated by NEAT, \n though I\n &gt; &gt; &gt; figure that not every recurrent type problem needs a fully \n &gt; &gt; recurrent\n &gt; &gt; &gt; network.  So I&#39;m trying to use NEAT to generate such, using the\n &gt; &gt; &gt; classic delayed-XOR problem (such that the result of xor of \n the two\n &gt; &gt; &gt; current inputs will be the actual output some time steps \n away).  \n &gt; &gt; Fully\n &gt; &gt; &gt; recurrent networks can be trained to do this, but I want to \n &gt; &gt; generate a\n &gt; &gt; &gt; NEAT network that, after running through the fixed data series \n a\n &gt; &gt; &gt; number of relaxation times, that the weights have already been \n &gt; &gt; trained\n &gt; &gt; &gt; through NEAT evolution such that I don&#39;t have to perform \n additional\n &gt; &gt; &gt; training on the network.  \n &gt; &gt; &gt; \n &gt; &gt; &gt; Has anyone had any success directly in generating such \n recurrent\n &gt; &gt; &gt; networks?  I know my fitnesses improve with time, but it takes \n a \n &gt; &gt; lot\n &gt; &gt; &gt; of generations (1000+ with a 150 member population) to even see\n &gt; &gt; &gt; something, and even then, it&#39;s not anywhere close to what \n simple\n &gt; &gt; &gt; recurrent training can provide.  (This may be also related to \n my\n &gt; &gt; &gt; second question).\n &gt; &gt; &gt; \n &gt; &gt; &gt; The other question I had was about convergence times.  I&#39;m \n trying \n &gt; &gt; to\n &gt; &gt; &gt; test my network on the normal XOR problem (non-recurrent mode) \n and\n &gt; &gt; &gt; find that it takes many more evolution generations for the \n fitness \n &gt; &gt; to\n &gt; &gt; &gt; get to acceptable levels (based solely on the distance of \n expected \n &gt; &gt; vs\n &gt; &gt; &gt; observed output), exceptionaly more than listed in the NEAT \n papers\n &gt; &gt; &gt; (500-1000 evolution steps as opposed to 10-30 steps) even when \n &gt; &gt; using\n &gt; &gt; &gt; what I believe are the same values described by Kenneth in his \n &gt; &gt; papers.\n &gt; &gt; &gt;  I&#39;ve tried nearly every parameter, and the only one that I \n know I\n &gt; &gt; &gt; want to keep low is the new node probability to avoid \n excessive \n &gt; &gt; growth\n &gt; &gt; &gt; of the network.  Anyone have any pointers on what parameters \n are\n &gt; &gt; &gt; critical to help with rapid convergence on the best network?  \n Mind\n &gt; &gt; &gt; you, it could still be something in my code which I&#39;ve been \n &gt; &gt; pounding\n &gt; &gt; &gt; through to try to find differences.\n &gt; &gt; &gt; \n &gt; &gt; &gt; Another related question is on the species comparison \n expression \n &gt; &gt; and\n &gt; &gt; &gt; tolerance.  I tend to use N=number of genes in largest species\n &gt; &gt; &gt; regardless of the case, and for that I have to play with the \n &gt; &gt; tolerance\n &gt; &gt; &gt; as to get 5 or more species in a population of 150.  Is there \n an \n &gt; &gt; ideal\n &gt; &gt; &gt; average number of species that you want to carry through in the\n &gt; &gt; &gt; population in order to take advantage of NEAT&#39;s use of \n species?  \n &gt; &gt; And\n &gt; &gt; &gt; when do people move away from the &#39;typical&#39; values of the \n &gt; &gt; coefficients\n &gt; &gt; &gt; (1 and 1 for disjoint and excess elements, 0.4 for weight \n &gt; &gt; difference\n &gt; &gt; &gt; average)\n &gt; &gt; &gt;\n &gt; &gt;\n &gt;\n \n \n     \n                       \n\n \t\t\n---------------------------------\nTalk is cheap. Use Yahoo! Messenger to make PC-to-Phone calls.  Great rates starting at 1�/min.\r\n--0-1939585008-1158614975=:23891\r\nContent-Type: text/html; charset=iso-8859-1\r\nContent-Transfer-Encoding: 8bit\r\n\r\nI finally figured out where things were going south in my code and got it to work nicely for the various known tests (standard XOR, pole balancing) compariable to the papers out there.&nbsp; The XOR was adding too many nodes as I didn&#39;t prevent feedback though once fixed to (optionally) only allow feedforward connections, that worked perfectly.&lt;br&gt;&lt;br&gt;I found that for highly recurrent networks like for the delayed XOR problem, the addition of training as a possible weight mutation (it technically is a mutation of the weights, but done in a logical/mathematical fashion) was necessary to boost the speed of the convergenace of the problem.&nbsp;&nbsp; Training every network is not good (you&#39;d favor networks with too many nodes but that can match the pattern of data presented, and not the data itself), and you don&#39;t want to train the network all the way either.&nbsp; I found that training about 5-10% of the time with a dataset about 1/20th of the full time series (in addition\n to weight/new node/new connection mutations) helped pushed the convergence along better, maybe abiyut 2 to 10 times fewer generations.&nbsp; Also, pushing up the new connection likelihood and reducing the chance of connections being disabled through crossover and mutation helped to capture the recurrent nature - while the number of nodes may be a bit higher than needed, there was the right number of nodes connected in the highly recurrent fashion as expected to solve the problem.&nbsp; I&#39;m still working to get the feel for how the training mutation parameters can affect the performance of the system, but it seems reasonable with what I have listed above for far.&lt;br&gt;&lt;br&gt;&lt;b&gt;&lt;i&gt;Kenneth Stanley &lt;kstanley@...&gt;&lt;/i&gt;&lt;/b&gt; wrote:&lt;blockquote class=&quot;replbq&quot; style=&quot;border-left: 2px solid rgb(16, 16, 255); margin-left: 5px; padding-left: 5px;&quot;&gt;     &lt;!-- Network content --&gt;           &lt;div id=&quot;ygrp-text&quot;&gt;             &lt;div&gt;NEAT really shouldn&#39;t be allowed to form recurrent connections to &lt;br&gt; solve XOR.  It&#39;s not that there is anything &quot;wrong&quot; or &quot;cheating&quot; &lt;br&gt; about it- it&#39;s just that the main point of XOR is to compare your &lt;br&gt; implementation with other implementations, and since the standard &lt;br&gt; benchmark does not include recurrent connections, you obfuscate the &lt;br&gt; comparison by allowing them.  It sounds like your XOR is indeed &lt;br&gt; memorizing the order of presentation.  That is definitely not the &lt;br&gt; intent of the XOR problem, and means your NNs are solving a very &lt;br&gt; different problem.  They are memorizing a sequence by using &lt;br&gt; recurrent connections.  That&#39;s interesting, but makes the comparison &lt;br&gt; less meaningful.&lt;br&gt; &lt;br&gt; Technically, networks should be flushed (all nodes set to zero) &lt;br&gt;\n between input presentations in XOR (or any other similar &lt;br&gt; classification problem) since XOR is order-independent (the &lt;br&gt; definition of the XOR problem says nothing about order of &lt;br&gt; presentation)&lt;wbr&gt;.  The flushing should preclude the need to present the &lt;br&gt; instances in random order.  Even if you want to use recurrent &lt;br&gt; connections, flushing should be part of the procedure.&lt;br&gt; &lt;br&gt; The weight cap of 5 sounds reasonable.  Up to 10 I think is &lt;br&gt; reasonable.  My rule of thumb for the power of mutations is that it &lt;br&gt; should take on average several mutations to travel from a weight of &lt;br&gt; 0 to the max or min weight.  &lt;br&gt; &lt;br&gt; If XOR weights are tending to max out, that may be an artifact of &lt;br&gt; the XOR problem- though I&#39;m not sure- there may be other reasons as &lt;br&gt; well.  But I would not worry a lot about it until the XOR problem is &lt;br&gt; being presented in a customary way.&lt;br&gt; &lt;br&gt; --- In &lt;a\n href=&quot;mailto:neat%40yahoogroups.com&quot;&gt;neat@yahoogroups.&lt;wbr&gt;com&lt;/a&gt;, &quot;mneylon01&quot; &lt;mneylon01@.&lt;wbr&gt;..&gt; wrote:&lt;br&gt; &gt;&lt;br&gt; &gt; So I&#39;ve had a chance to do what Ken suggested below - start from &lt;br&gt; the&lt;br&gt; &gt; basic (2+1bias)-1-&lt;wbr&gt;1 network for the static XOR problem, randomizing&lt;br&gt; &gt; the test data, and disabling any node or connection creation or&lt;br&gt; &gt; toggling to see how my network worked.  I&#39;m using what I believe &lt;br&gt; are&lt;br&gt; &gt; the same parameters in Ken&#39;s various papers: 80% chance of a &lt;br&gt; child&#39;s&lt;br&gt; &gt; weights being changed, with 90% chance of perturbation, 10% chance &lt;br&gt; of&lt;br&gt; &gt; mutation.  I used 150 species, and for the species calculation, I &lt;br&gt; used&lt;br&gt; &gt; 1.0 for the excess and disjoint coefficients, 0.4 for the common&lt;br&gt; &gt; weight differences, and set N to 1 (as noted by Ken below), with &lt;br&gt; the&lt;br&gt; &gt; tolerance set to 3.0.&lt;br&gt; &gt; &lt;br&gt; &gt; What was interesting is that I found there to be a VERY strong&lt;br&gt; &gt; connection\n between two parameters that I don&#39;t see mentioned much &lt;br&gt; in&lt;br&gt; &gt; the papers: the weight cap, and the type and amount of &lt;br&gt; perturbation or&lt;br&gt; &gt; mutation done.   (Note that I&#39;ve tried both the sigmoid steepness &lt;br&gt; of&lt;br&gt; &gt; 4.9 and 1, this seems to have less of an impact that these other&lt;br&gt; &gt; parameters).&lt;br&gt; &gt; &lt;br&gt; &gt; My original code capped the weight at 2, which I saw slow if any&lt;br&gt; &gt; approach to a good network.  Increasing the cap to insane values &lt;br&gt; like&lt;br&gt; &gt; 100 gave a great speed to evolution convergence, as this allowed &lt;br&gt; for a&lt;br&gt; &gt; wider range of weight values to be explored by the NEAT method, but&lt;br&gt; &gt; leads to some networks that one questions their ability to work.  &lt;br&gt; &gt; Using a cap of 5 to 10 gave a more reasonable set of networks while&lt;br&gt; &gt; still converging quickly (about 50 generations to get the RMS under&lt;br&gt; &gt; 0.1 on the 4 cases).  I&#39;ve seen the cap of 5 tossed around here&lt;br&gt; &gt; before, but does\n this seem to be a good practical number?&lt;br&gt; &gt; &lt;br&gt; &gt; Also, with my weight mutations, I found that using gaussian&lt;br&gt; &gt; distributions for both initial weights and mutated weights, and&lt;br&gt; &gt; pertubations in weights was better than straight random.  &lt;br&gt; (Presently&lt;br&gt; &gt; I&#39;m using N rolls of the RNG and getting the average to simulate&lt;br&gt; &gt; gaussian distribution, with larger N to reduce the standard&lt;br&gt; &gt; deviation).  The quesiton I have, since I&#39;m having trouble seeing &lt;br&gt; how&lt;br&gt; &gt; it&#39;s done in either the C++ or C# code, is what the good effective&lt;br&gt; &gt; magnitude is.  The above cases, for the mutated/initial weights, &lt;br&gt; I&#39;m&lt;br&gt; &gt; using a gaussian random number from -5 to 5, std deviation of &lt;br&gt; about 1,&lt;br&gt; &gt; and for the perturbed weights, from -5 to 5 with a std deviation of&lt;br&gt; &gt; about 0.1-0.5.    These cases seem to work, but I can&#39;t seem how &lt;br&gt; they&lt;br&gt; &gt; easily match up with the above code.  Even with the weight caps, I&lt;br&gt;\n &gt; always seemed to have weights that wanted to hit the caps.  Any &lt;br&gt; good&lt;br&gt; &gt; suggestions on what are good weight ranges and adjustments to &lt;br&gt; make. &lt;br&gt; &gt; Another thing I noticed is that if the weight cap is too low, using&lt;br&gt; &gt; the values for the species tolerance calculation I give above, I&lt;br&gt; &gt; rarely got more than 1 or two species.&lt;br&gt; &gt; &lt;br&gt; &gt; Additionally, I find it interesting in how one approaches the input&lt;br&gt; &gt; data part.  I tried the standard XOR with a fixed set of data, and &lt;br&gt; it&lt;br&gt; &gt; can converge quickly, but when you then make the data randomly&lt;br&gt; &gt; presented (re-evaluating weights of genomes that carry over to &lt;br&gt; account&lt;br&gt; &gt; for repeated runs), the performance of the NEAT method is poor.   &lt;br&gt; That&lt;br&gt; &gt; is, when fixed, I almost always got a recurrent connection (I don&#39;t&lt;br&gt; &gt; prevent these from being formed), suggesting it&#39;s trying to mimic &lt;br&gt; the&lt;br&gt; &gt; input pattern order and not the actual data\n patterns.  I found &lt;br&gt; that if&lt;br&gt; &gt; a random order was used, using a larger test pattern (in my case, &lt;br&gt; 50&lt;br&gt; &gt; initially randomly generated XOR cases and then presented in a &lt;br&gt; random&lt;br&gt; &gt; order to the networks) seemed to help.  I still get some &lt;br&gt; recurrancies,&lt;br&gt; &gt; but the network size seemed to stay small and avoid new node&lt;br&gt; &gt; formation.  (Obviously, when I use recurrent data as like the &lt;br&gt; delayed&lt;br&gt; &gt; XOR problem, the data order will be fixed, but the size of the data&lt;br&gt; &gt; set needs to be large enough that it would be impractical for the&lt;br&gt; &gt; genome to try to learn the overall data set pattern as opposed to &lt;br&gt; the&lt;br&gt; &gt; actual pattern that the dataset contains...)&lt;br&gt; &gt; &lt;br&gt; &gt; --- In &lt;a href=&quot;mailto:neat%40yahoogroups.com&quot;&gt;neat@yahoogroups.&lt;wbr&gt;com&lt;/a&gt;, &quot;Kenneth Stanley&quot; &lt;kstanley@&gt; wrote:&lt;br&gt; &gt; &gt;&lt;br&gt; &gt; &gt; It sounds like the main issue before you can really start &lt;br&gt; looking at &lt;br&gt; &gt; &gt;\n delayed-XOR is to get XOR working more closely to my own NEAT&lt;br&gt; &gt; &gt; (and others developed since).  Certainly the 500-1000 &lt;br&gt; generations &lt;br&gt; &gt; &gt; it&#39;s taking you indicates somethign is likely wrong in your &lt;br&gt; &gt; &gt; implementation.&lt;br&gt; &gt; &gt; &lt;br&gt; &gt; &gt; I would suggest the following experiment:  Start evolution with  &lt;br&gt; a &lt;br&gt; &gt; &gt; population of networks that already have the correct topology &lt;br&gt; for a &lt;br&gt; &gt; &gt; solution to XOR and turn off structural mutations.  Then run &lt;br&gt; your &lt;br&gt; &gt; &gt; version of NEAT as usual, except in this case, it will only be &lt;br&gt; &gt; &gt; searching over weight space.  If it takes forever, it tells you &lt;br&gt; that &lt;br&gt; &gt; &gt; the problem is in the way your weights are being mutated, or the &lt;br&gt; way &lt;br&gt; &gt; &gt; they are being combined in crossover.  It may also indicate a &lt;br&gt; &gt; &gt; problem in speciation (related to weight comparison).  In any &lt;br&gt; case, &lt;br&gt; &gt; &gt; it will greatly narrow\n down the problem.&lt;br&gt; &gt; &gt; &lt;br&gt; &gt; &gt; Once you get it working on pure weight-evolution, then you can &lt;br&gt; move &lt;br&gt; &gt; &gt; to the normal topology evolution, and you will either see it &lt;br&gt; work &lt;br&gt; &gt; &gt; right away, or find that there is a problem in adding structure &lt;br&gt; as &lt;br&gt; &gt; &gt; well.&lt;br&gt; &gt; &gt; &lt;br&gt; &gt; &gt; About compatibility testing for speciation, when you menion &quot;N,&quot; &lt;br&gt; do &lt;br&gt; &gt; &gt; you mean the normalization term in my papers?  Regrettably, many &lt;br&gt; &gt; &gt; people miss that I did not use N (for normalization) in &lt;br&gt; practice, &lt;br&gt; &gt; &gt; that is, I set N to 1 in all cases.  That may explain why you &lt;br&gt; using &lt;br&gt; &gt; &gt; the same coefficients as me does not work.  If you look at my &lt;br&gt; papers &lt;br&gt; &gt; &gt; closely, they say that N can be set to 1 if genomes are not too &lt;br&gt; &gt; &gt; large.  I have found in practice that it always works fine with &lt;br&gt; it &lt;br&gt; &gt; &gt; set to 1, so that&#39;s what I&#39;ve done.  This\n confusion is my fault &lt;br&gt; and &lt;br&gt; &gt; &gt; I apologize for it- the papers should be more clear- but I &lt;br&gt; wanted &lt;br&gt; &gt; &gt; people to be aware of N and the option for normalization in case &lt;br&gt; it &lt;br&gt; &gt; &gt; indeed does come into play with very large genomes.&lt;br&gt; &gt; &gt; &lt;br&gt; &gt; &gt; As for deviations from typical values, you can see all the &lt;br&gt; values &lt;br&gt; &gt; &gt; I&#39;ve used in the appendix to my dissertation.  There is some &lt;br&gt; &gt; &gt; explanation there too for why different values were chosen.  One &lt;br&gt; &gt; &gt; consideration might be whether very fine grained weight changed &lt;br&gt; are &lt;br&gt; &gt; &gt; key or not in a particular problem.  If they are, you might want &lt;br&gt; the &lt;br&gt; &gt; &gt; coefficient of weight differences to be higher.&lt;br&gt; &gt; &gt; &lt;br&gt; &gt; &gt; Once you get XOR working, please let the group know how things &lt;br&gt; go &lt;br&gt; &gt; &gt; with the delayed XOR!  By the way, what language/platform did &lt;br&gt; you &lt;br&gt; &gt; &gt; use for your\n version of NEAT?&lt;br&gt; &gt; &gt; &lt;br&gt; &gt; &gt; ken&lt;br&gt; &gt; &gt; &lt;br&gt; &gt; &gt; --- In &lt;a href=&quot;mailto:neat%40yahoogroups.com&quot;&gt;neat@yahoogroups.&lt;wbr&gt;com&lt;/a&gt;, &quot;mneylon01&quot; &lt;mneylon01@&gt; wrote:&lt;br&gt; &gt; &gt; &gt;&lt;br&gt; &gt; &gt; &gt; I&#39;ve been working on my own implimentation of NEAT (having to &lt;br&gt; fit &lt;br&gt; &gt; &gt; to a&lt;br&gt; &gt; &gt; &gt; predescribed framework) and while it&#39;s mostly working, I&#39;m &lt;br&gt; looking &lt;br&gt; &gt; &gt; at&lt;br&gt; &gt; &gt; &gt; a couple of questions.&lt;br&gt; &gt; &gt; &gt; &lt;br&gt; &gt; &gt; &gt; First, I know that the networks that NEAT generates can have&lt;br&gt; &gt; &gt; &gt; recurrency (feedback), and this is likely why some of the &lt;br&gt; robot &lt;br&gt; &gt; &gt; game&lt;br&gt; &gt; &gt; &gt; examples work well.  However, I&#39;m looking at trying to use &lt;br&gt; NEAT to&lt;br&gt; &gt; &gt; &gt; find patterns in time series data, as one would use fully &lt;br&gt; recurrent&lt;br&gt; &gt; &gt; &gt; networks for (In these, also known as Elmen networks, all of &lt;br&gt; the&lt;br&gt; &gt; &gt; &gt; hidden layer\n and output layer values are &#39;propagated&#39; into the &lt;br&gt; next&lt;br&gt; &gt; &gt; &gt; time step to give the network memory, with full connectivity &lt;br&gt; &gt; &gt; between&lt;br&gt; &gt; &gt; &gt; all the input and previous nodes to the hidden/output nodes).&lt;br&gt; &gt; &gt; &gt; &lt;br&gt; &gt; &gt; &gt; Such a network should be possible to be generated by NEAT, &lt;br&gt; though I&lt;br&gt; &gt; &gt; &gt; figure that not every recurrent type problem needs a fully &lt;br&gt; &gt; &gt; recurrent&lt;br&gt; &gt; &gt; &gt; network.  So I&#39;m trying to use NEAT to generate such, using the&lt;br&gt; &gt; &gt; &gt; classic delayed-XOR problem (such that the result of xor of &lt;br&gt; the two&lt;br&gt; &gt; &gt; &gt; current inputs will be the actual output some time steps &lt;br&gt; away).  &lt;br&gt; &gt; &gt; Fully&lt;br&gt; &gt; &gt; &gt; recurrent networks can be trained to do this, but I want to &lt;br&gt; &gt; &gt; generate a&lt;br&gt; &gt; &gt; &gt; NEAT network that, after running through the fixed data series &lt;br&gt; a&lt;br&gt; &gt; &gt; &gt; number of relaxation times,\n that the weights have already been &lt;br&gt; &gt; &gt; trained&lt;br&gt; &gt; &gt; &gt; through NEAT evolution such that I don&#39;t have to perform &lt;br&gt; additional&lt;br&gt; &gt; &gt; &gt; training on the network.  &lt;br&gt; &gt; &gt; &gt; &lt;br&gt; &gt; &gt; &gt; Has anyone had any success directly in generating such &lt;br&gt; recurrent&lt;br&gt; &gt; &gt; &gt; networks?  I know my fitnesses improve with time, but it takes &lt;br&gt; a &lt;br&gt; &gt; &gt; lot&lt;br&gt; &gt; &gt; &gt; of generations (1000+ with a 150 member population) to even see&lt;br&gt; &gt; &gt; &gt; something, and even then, it&#39;s not anywhere close to what &lt;br&gt; simple&lt;br&gt; &gt; &gt; &gt; recurrent training can provide.  (This may be also related to &lt;br&gt; my&lt;br&gt; &gt; &gt; &gt; second question).&lt;br&gt; &gt; &gt; &gt; &lt;br&gt; &gt; &gt; &gt; The other question I had was about convergence times.  I&#39;m &lt;br&gt; trying &lt;br&gt; &gt; &gt; to&lt;br&gt; &gt; &gt; &gt; test my network on the normal XOR problem (non-recurrent mode) &lt;br&gt; and&lt;br&gt; &gt; &gt; &gt; find that it takes many more evolution\n generations for the &lt;br&gt; fitness &lt;br&gt; &gt; &gt; to&lt;br&gt; &gt; &gt; &gt; get to acceptable levels (based solely on the distance of &lt;br&gt; expected &lt;br&gt; &gt; &gt; vs&lt;br&gt; &gt; &gt; &gt; observed output), exceptionaly more than listed in the NEAT &lt;br&gt; papers&lt;br&gt; &gt; &gt; &gt; (500-1000 evolution steps as opposed to 10-30 steps) even when &lt;br&gt; &gt; &gt; using&lt;br&gt; &gt; &gt; &gt; what I believe are the same values described by Kenneth in his &lt;br&gt; &gt; &gt; papers.&lt;br&gt; &gt; &gt; &gt;  I&#39;ve tried nearly every parameter, and the only one that I &lt;br&gt; know I&lt;br&gt; &gt; &gt; &gt; want to keep low is the new node probability to avoid &lt;br&gt; excessive &lt;br&gt; &gt; &gt; growth&lt;br&gt; &gt; &gt; &gt; of the network.  Anyone have any pointers on what parameters &lt;br&gt; are&lt;br&gt; &gt; &gt; &gt; critical to help with rapid convergence on the best network?  &lt;br&gt; Mind&lt;br&gt; &gt; &gt; &gt; you, it could still be something in my code which I&#39;ve been &lt;br&gt; &gt; &gt; pounding&lt;br&gt; &gt; &gt; &gt; through to try to find\n differences.&lt;br&gt; &gt; &gt; &gt; &lt;br&gt; &gt; &gt; &gt; Another related question is on the species comparison &lt;br&gt; expression &lt;br&gt; &gt; &gt; and&lt;br&gt; &gt; &gt; &gt; tolerance.  I tend to use N=number of genes in largest species&lt;br&gt; &gt; &gt; &gt; regardless of the case, and for that I have to play with the &lt;br&gt; &gt; &gt; tolerance&lt;br&gt; &gt; &gt; &gt; as to get 5 or more species in a population of 150.  Is there &lt;br&gt; an &lt;br&gt; &gt; &gt; ideal&lt;br&gt; &gt; &gt; &gt; average number of species that you want to carry through in the&lt;br&gt; &gt; &gt; &gt; population in order to take advantage of NEAT&#39;s use of &lt;br&gt; species?  &lt;br&gt; &gt; &gt; And&lt;br&gt; &gt; &gt; &gt; when do people move away from the &#39;typical&#39; values of the &lt;br&gt; &gt; &gt; coefficients&lt;br&gt; &gt; &gt; &gt; (1 and 1 for disjoint and excess elements, 0.4 for weight &lt;br&gt; &gt; &gt; difference&lt;br&gt; &gt; &gt; &gt; average)&lt;br&gt; &gt; &gt; &gt;&lt;br&gt; &gt; &gt;&lt;br&gt; &gt;&lt;br&gt; &lt;br&gt; &lt;/div&gt;     &lt;/div&gt;          &lt;!--End group email --&gt;  &lt;/blockquote&gt;&lt;br&gt;&lt;p&gt;&#32;\n\t\t&lt;hr size=1&gt;Talk is cheap. Use Yahoo! Messenger to make PC-to-Phone calls. &lt;a href=&quot;http://us.rd.yahoo.com/mail_us/taglines/postman7/*http://us.rd.yahoo.com/evt=39666/*http://messenger.yahoo.com&quot;&gt; Great rates starting at 1�/min.\r\n--0-1939585008-1158614975=:23891--\r\n\n"}}