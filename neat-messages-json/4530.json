{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":283334584,"authorName":"petar_chervenski","from":"&quot;petar_chervenski&quot; &lt;petar_chervenski@...&gt;","profile":"petar_chervenski","replyTo":"LIST","senderId":"Nl4eGbv0E_b3KhJ8Q8QDataFgPEkkYRNUSN_Jd-Zt28qqdt3uMahQccJ0aE0mHFYKCoJdG9da7xqrPDiIA-2xDEtk_VMY_tn89Bf93wZe9UdPW4Jd-M","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: Growing Neural Gas and CPPNs","postDate":"1230455013","msgId":4530,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGdqN2ZkNStkN2RjQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGdqNzRhaSszNWNvQGVHcm91cHMuY29tPg=="},"prevInTopic":4529,"nextInTopic":4532,"prevInTime":4529,"nextInTime":4531,"topicId":4527,"numMessagesInTopic":7,"msgSnippet":"Hi Ken, I am happy that you find this thing thought-provoking. :) Well there are several reasons not to do it the deterministic way. It is hard to put a limit","rawEmail":"Return-Path: &lt;petar_chervenski@...&gt;\r\nX-Sender: petar_chervenski@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 32725 invoked from network); 28 Dec 2008 09:03:34 -0000\r\nX-Received: from unknown (66.218.67.96)\n  by m48.grp.scd.yahoo.com with QMQP; 28 Dec 2008 09:03:34 -0000\r\nX-Received: from unknown (HELO n13d.bullet.sp1.yahoo.com) (69.147.64.236)\n  by mta17.grp.scd.yahoo.com with SMTP; 28 Dec 2008 09:03:34 -0000\r\nX-Received: from [69.147.65.173] by n13.bullet.sp1.yahoo.com with NNFMP; 28 Dec 2008 09:03:34 -0000\r\nX-Received: from [66.218.66.77] by t15.bullet.mail.sp1.yahoo.com with NNFMP; 28 Dec 2008 09:03:34 -0000\r\nDate: Sun, 28 Dec 2008 09:03:33 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;gj7fd5+d7dc@...&gt;\r\nIn-Reply-To: &lt;gj74ai+35co@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: &quot;petar_chervenski&quot; &lt;petar_chervenski@...&gt;\r\nSubject: Re: Growing Neural Gas and CPPNs\r\nX-Yahoo-Group-Post: member; u=283334584; y=59paf-msoeWrh-GaBh3lM081Riuvg6PhzbcdO4oDB3BaftC_STnCFj2Qxw\r\nX-Yahoo-Profile: petar_chervenski\r\n\r\nHi Ken,\n\nI am happy that you find this thing thought-provoking. :) \n\nWell t=\r\nhere are several reasons not to do it the deterministic way. It \nis hard to=\r\n put a limit in a deterministic approach like the one you \nsuggest. If I ge=\r\nt a CPPN picture that is entirely white, that would \nmean adding too many n=\r\nodes (one at each iteration). And more, that \napproach would require a thre=\r\nshold value (the answer to &quot;should I add \na node given the intensity of the=\r\n neighborhood&quot;). \n\nCoordinates have to be grid-based and the grid density i=\r\ns another \nvariable coming into action. OK it can be proportional to the ma=\r\nximum \nnumber of nodes but there is something I don&#39;t really like in it - \n=\r\nthere is no freedom for the nodes to float freely in space, they would \nbe =\r\nall constrained to the grid. Curves are hard to approximate this \nway. \n\nHa=\r\nving a pattern that is very complex (like many cases of repetition \nwith va=\r\nriation) makes things even more complicated. The grid density \ncannot chang=\r\ne in most cases or if it changes, that would be removing \nthe old low-level=\r\n mesh and replacing it with a new one, which can be \nexpensive.\n\nA better a=\r\npproach is to let the mesh approximate the CPPN pattern as \nlong as it is r=\r\nequired. Give it enough time and nodes and it will \nalmost perfectly fit th=\r\ne pattern. What if you don&#39;t have much time or \ncomputational resources? De=\r\nvelopment can be canceled as well. It \nfollows the general idea of continuo=\r\nusly approximating a phenotype \nthat has no fixed, well defined coordinate =\r\ngranularity, but infinite. This provides two advantages:\n\n- You can stop th=\r\ne process at any time and still have a good \napproximation of the phenotype=\r\n that should do the job (unless you stop \ndevelopment too early).\n\n- You ca=\r\nn extend GNG to use CPPN data. For example, maximum number of \nnodes can be=\r\n made dependent of the overall CPPN complexity or the \ncomplexity of its en=\r\ntire pattern. In GNG nodes are added every N \ntimesteps. Introducing a prob=\r\nability value for adding a node (which \ncan be obtained by another CPPN pat=\r\ntern) should help the development \nto explore many more configurations that=\r\n would be hard to get if all \nnodes are just created automatically (I am ru=\r\nnning one experiment \nabout this idea just right now). \n\nHere are also some=\r\n more ideas about it:\n\n* if we have (x / distance), (y / distance), (distan=\r\nce) as input to \nthe CPPN instead of (x) and (y), the effect is beautiful r=\r\notational \npatterns! Spirals and all kind of stuff come up too. So this is =\r\nhow to \nget rotation! Just add these to the regular (x,y,1) inputs. \n\n* Pla=\r\nying around with the random distributions is a good idea. They \nactually co=\r\nntrol how the raw CPPN pattern is about to be interpreted. \n\n* HyperNEAT! (=\r\nyou know about what I mean :)) \n\nOK well there is more coming, I am just tw=\r\neaking parameters right now \nbut I will add the extensions soon and see wha=\r\nt happens. I am also \ngoing to try some experiments with HyperNEAT evolving=\r\n its own \nsubstrate configurations with GNG.\n\nCheers!\n\nPeter\n\n--- In neat@y=\r\nahoogroups.com, &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt; wrote:\n&gt;\n&gt; Peter, once aga=\r\nin you make another interesting contribution.  \nWatching\n&gt; the node pattern=\r\ns form certainly got my imagination going.  But let \nme\n&gt; ask you one funda=\r\nmental question about the idea:  Why is it \nnecessary\n&gt; to include a self-o=\r\nrganizing process like a growing neural gas?  Why\n&gt; can&#39;t you just devise a=\r\n deterministic rule about whether to\n&gt; instantiate a node at a certain posi=\r\ntion based on the pixel\n&gt; intensities from the CPPN output in its neighborh=\r\nood?  Then the node\n&gt; pattern would follow deterministically from any CPPN =\r\noutput, and it\n&gt; would work quite quickly.  Is there a reason not to do tha=\r\nt?\n&gt; \n&gt; ken\n&gt; \n&gt; --- In neat@yahoogroups.com, &quot;petar_chervenski&quot; &lt;petar_che=\r\nrvenski@&gt;\n&gt; wrote:\n&gt; &gt;\n&gt; &gt; Hi all, \n&gt; &gt; \n&gt; &gt; I was playing around with my l=\r\natest DCPPN code, and decided to go \n&gt; &gt; browse the web and find something =\r\nthat may help. And I found it, \nit \n&gt; &gt; was right in front of my eyes for s=\r\no long time and I didn&#39;t pay \n&gt; &gt; attention to it. \n&gt; &gt; \n&gt; &gt; I discussed so=\r\nme issues about evolution/development of neural \n&gt; &gt; substrates in one prev=\r\nious thread I started. Among other things, I \n&gt; &gt; stated that an approach c=\r\napable to map any CPPN pattern to a \nfinite-\n&gt; &gt; sized physics body or subs=\r\ntrate must be also capable of scaling \nwell. \n&gt; &gt; Because the CPPN pattern =\r\nhas constant complexity that can be \n&gt; &gt; approximated at any resolution, a =\r\ngood approach would be one that \ncan \n&gt; &gt; produce the same body derived fro=\r\nm the CPPN pattern, but on any \nlevel \n&gt; &gt; of detail. \n&gt; &gt; \n&gt; &gt; Here is one=\r\n such approach, which effectively interprets the plain \nCPPN \n&gt; &gt; output pi=\r\ncture as a probability map that further guides the \n&gt; &gt; development of a Gr=\r\nowing Neural Gas network. \n&gt; &gt; \n&gt; &gt; In GNG, the algorithm starts with 2 unc=\r\nonnected nodes and adds \n&gt; &gt; connections and new nodes as time progresses, =\r\nwhile in the same \ntime \n&gt; &gt; moving the nodes, resulting in closer and clos=\r\ner approximation of \nthe \n&gt; &gt; distribution of input vectors. \n&gt; &gt; \n&gt; &gt; How =\r\ncan one CPPN picture be turned into a random distribution of \ninput \n&gt; &gt; ve=\r\nctors, which still resembles the pattern? It turns out it is \n&gt; &gt; trivial. =\r\nSuppose we allocate an array of 100 000 input vectors \nthat \n&gt; &gt; will be us=\r\ned during the development of the GNG mesh. The task is \nto \n&gt; &gt; fill that a=\r\nrray. Please assume that the CPPN picture contains \npixels \n&gt; &gt; each in the=\r\n range of [0 .. 1]. To get a new (the next) input \nvector, \n&gt; &gt; first pick =\r\na random (uniform distribution) coordinate on the CPPN \n&gt; &gt; picture. Then, =\r\nif the pixel value at the picked coordinate is less \n(or \n&gt; &gt; higher, depen=\r\nds on choice) than random (0 .. 1), add the \ncoordinate to \n&gt; &gt; the array o=\r\nf input vectors. If not, then pick another coordinate \nand \n&gt; &gt; do this unt=\r\nil a new input vector is finally obtained. It is good \nto \n&gt; &gt; add a &quot;give =\r\nup&quot; mechanism to avoid entering infinite loop in case \nthe \n&gt; &gt; CPPN pictur=\r\ne is really weird one (all 0s). \n&gt; &gt; \n&gt; &gt; So this in effect turns the raw C=\r\nPPN output into a probability \nmap. \n&gt; &gt; Then the GNG algorithm can begin d=\r\neveloping the network. The \nanimation \n&gt; &gt; of a developing GNG network in t=\r\nhis way is strikingly natural. It \ncan \n&gt; &gt; be easily confused with real em=\r\nbryo development. But even though \nit is \n&gt; &gt; really entertaining to watch =\r\nthe mesh develops under the guide of \nthe \n&gt; &gt; CPPN in such a way, the appr=\r\noach does not solve all issues I \nraised \n&gt; &gt; before. \n&gt; &gt; \n&gt; &gt; First of al=\r\nl, I can still pick any coordinate, i.e. it is not \nreally a \n&gt; &gt; compact f=\r\ninite body. And second, the maximum density (maximum \nnumber \n&gt; &gt; of nodes)=\r\n is still a matter of choice, it has to be set by the \n&gt; &gt; experimenter. \n&gt;=\r\n &gt; \n&gt; &gt; The way I see it, this particular method can be used as a way to \n&gt;=\r\n &gt; evolve HyperNEAT substrate configurations. Don&#39;t confuse it with \n&gt; &gt; de=\r\nnsities - they are still predetermined at this point, or \n&gt; &gt; connectivity =\r\n- HyperNEAT has a connectivity concept as a pattern \nin 4D \n&gt; &gt; which doesn=\r\n&#39;t even care about the substrate configuration. \n&gt; &gt; \n&gt; &gt; But there is defi=\r\nnitely a merit in the approach. For example, the \n&gt; &gt; circular substrate in=\r\n the food gathering task can be represented \nwith \n&gt; &gt; 2 concentric circles=\r\n pattern, which appears often very early in \n&gt; &gt; evolution. Parallel placem=\r\nent schemes are created via repeating \n&gt; &gt; patterns across x or y. And ther=\r\ne are so many many possibilities. \nI \n&gt; &gt; think that now is worth to try ou=\r\nt the capability to evolve \nsubstrate \n&gt; &gt; configurations with connectivity=\r\n at the same time.\n&gt; &gt; \n&gt; &gt; Further exploration of the method will be targe=\r\nted towards \nextending \n&gt; &gt; the GNG model. For example, another pattern (se=\r\ncond output) can be \n&gt; &gt; interpreted as a probability map for adding new no=\r\ndes to the \nnetwork. \n&gt; &gt; And another pattern specifying neuron types (inpu=\r\nt/hidden/output). \n&gt; &gt; \n&gt; &gt; Peter\n&gt; &gt; \n&gt; &gt; P.S. I will upload a binary demo=\r\nnstration of the method to the \nFiles \n&gt; &gt; section after I complete this me=\r\nssage. It will be named \n&gt; &gt; &quot;DCPPN_GNG.exe&quot; It uses novelty search, where =\r\nthe behavior \n&gt; &gt; characterization is an array of N 2D vectors, where N is =\r\nthe \nmaximum \n&gt; &gt; number of nodes (in this setup - 100). The distance is th=\r\ne average \n&gt; &gt; distance between them. It exploits the fact that my particul=\r\nar \n&gt; &gt; implementation of GNG does not delete nodes - so we know which is \n=\r\n&gt; &gt; which by their order of appearance. Press C to view the underlying \n&gt; &gt;=\r\n CPPN probability picture. Press ESC to quit. And if you press Q, \n&gt; &gt; visu=\r\nalization if turned off (runs slightly faster).\n&gt; &gt;\n&gt;\n\n\n\n\n"}}