{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":203001720,"authorName":"tansey@vt.edu","from":"tansey@...","profile":"tansey4","replyTo":"LIST","senderId":"PxtPnpFlU2kSPu47rnI6adMpjEeXywc7jkQvUYy3TlMm843b5di7-WeDvyuZsp3Ew2pWBQ","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [neat] Evaluating population with large training data set","postDate":"1218064204","msgId":4254,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDEyMTgwNjQyMDQuNDg5YTJmNGM4NzJkYkB3ZWJtYWlsLnZ0LmVkdT4=","inReplyToHeader":"PGc3ZDYyMytiaXA0QGVHcm91cHMuY29tPg==","referencesHeader":"PGc3ZDYyMytiaXA0QGVHcm91cHMuY29tPg=="},"prevInTopic":4253,"nextInTopic":4255,"prevInTime":4253,"nextInTime":4255,"topicId":4253,"numMessagesInTopic":11,"msgSnippet":"Hi Christian, ... You could take the Pearson correlation of the data sets. The algorithm is fairly simple for calculating correlation for two variables, x and","rawEmail":"Return-Path: &lt;tansey@...&gt;\r\nX-Sender: tansey@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 42272 invoked from network); 6 Aug 2008 23:10:10 -0000\r\nX-Received: from unknown (66.218.67.97)\n  by m43.grp.scd.yahoo.com with QMQP; 6 Aug 2008 23:10:10 -0000\r\nX-Received: from unknown (HELO lennier.cc.vt.edu) (198.82.162.213)\n  by mta18.grp.scd.yahoo.com with SMTP; 6 Aug 2008 23:10:10 -0000\r\nX-Received: from vivi.cc.vt.edu (IDENT:mirapoint@... [10.1.1.12])\n\tby lennier.cc.vt.edu (8.12.11.20060308/8.12.11) with ESMTP id m76NA5X1030682;\n\tWed, 6 Aug 2008 19:10:05 -0400\r\nX-Received: from imp-b07.cc.vt.edu (imp.cc.vt.edu [198.82.161.55])\n\tby vivi.cc.vt.edu (MOS 3.8.6-GA)\n\twith ESMTP id JSB93732;\n\tWed, 6 Aug 2008 19:10:05 -0400 (EDT)\r\nX-Received: from imp-b07.cc.vt.edu (localhost.localdomain [127.0.0.1])\n\tby imp-b07.cc.vt.edu (8.13.1/8.13.1) with ESMTP id m76NA4Wm011734;\n\tWed, 6 Aug 2008 19:10:04 -0400\r\nX-Received: (from apache@localhost)\n\tby imp-b07.cc.vt.edu (8.13.1/8.13.1/Submit) id m76NA4Gf011733;\n\tWed, 6 Aug 2008 19:10:04 -0400\r\nX-Received: from ip65-44-200-130.z200-44-65.customer.algx.net (ip65-44-200-130.z200-44-65.customer.algx.net [65.44.200.130]) \n\tby webmail.vt.edu (IMP) with HTTP; Wed,  6 Aug 2008 19:10:04 -0400\r\nMessage-ID: &lt;1218064204.489a2f4c872db@...&gt;\r\nDate: Wed,  6 Aug 2008 19:10:04 -0400\r\nTo: neat@yahoogroups.com, Christian &lt;Christian.Hofmann@...&gt;\r\nCc: neat@yahoogroups.com\r\nReferences: &lt;g7d623+bip4@...&gt;\r\nIn-Reply-To: &lt;g7d623+bip4@...&gt;\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Transfer-Encoding: 8bit\r\nUser-Agent: Internet Messaging Program (IMP) 3.2.8\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: tansey@...\r\nSubject: Re: [neat] Evaluating population with large training data set\r\nX-Yahoo-Group-Post: member; u=203001720; y=UE2Moy1S5GpU_SmMI45jDcjOqo7gIpztDHwUaQUS_SFr_A\r\nX-Yahoo-Profile: tansey4\r\n\r\nHi Christian,\n\n&gt; 2) Instead of choosing just 10% if the data we could choose the\n&gt; training data that correlate like 0 or -1 with the other training\n&gt; data.  This way we train with data that is the most different. But\n&gt; how get this correlation? I don&#39;t know if there is an algorithm.\n&gt; Specialty if you have 100 input and 300 output neurons �\n\nYou could take the Pearson correlation of the data sets. The algorithm is fairly\nsimple for calculating correlation for two variables, x and y:\n\n/// &lt;summary&gt;\n/// Get correlation (C# Code)\n/// &lt;/summary&gt;\npublic static void PearsonCorrelation(double[] x, double[] y, ref double covXY,\nref double pearson)\n{\n    if (x.Length != y.Length)\n        throw new Exception(&quot;Length of sources is different (x=&quot; + x.Length + &quot;\ny=&quot; + y.Length + &quot;)&quot;);\n\n    double avgX = Average(x);\n    double stdevX = Stdev(x);\n    double avgY = Average(y);\n    double stdevY = Stdev(y);\n\n    int len = x.Length;\n\n    for (int i = 0; i &lt; len; i++)\n        covXY += (x[i] - avgX) * (y[i] - avgY);\n\n    covXY /= len - 1;\n\n    pearson = covXY / (stdevX * stdevY);\n}\n\nThere are ways of calculating correlation statistics when the number of\nvariables and data sets grow, but I&#39;m not familiar with them.\n\nWesley\n\n\nQuoting Christian &lt;Christian.Hofmann@...&gt;:\n\n&gt; Hello,\n&gt;\n&gt; I want to discuss with you about this topic. The main bottleneck of\n&gt; NEAT is the evaluation of every created network to get the fitness.\n&gt; But what if you have big networks and very much training data? I have\n&gt; over one million training datasets that I need to evaluate for every\n&gt; network. I need about one minute per network.\n&gt;\n&gt; I have thought about some solutions regarding this problem. Maybe you\n&gt; have already tried one of these or you think that I cannot do some of\n&gt; them or should favorite one of them.\n&gt; First I need to say that I calculate the fitness based on fitness\n&gt; values divided by the number of used sample data. So it is the same\n&gt; fitness level regarding of using 10% or 100% of the training data.\n&gt;\n&gt; 1) You could evaluate only (for example) 10% of the training data. And\n&gt; calculate the overall fitness based on this data.  Then if the fitness\n&gt; is higher than the previous highest  10% fitness data, calculate the\n&gt; remaining 90% and return the value for all 100% trainingdata. If the\n&gt; fitness is lower than the previous highest 10% fitness data you can\n&gt; return the fitness data for these 10%.\n&gt;\n&gt; I don&#39;t know if it is better to choose just the first 10% of training\n&gt; data (so every network is getting the same data) or just picking the\n&gt; 10% training data by random from the complete training data set. I\n&gt; don&#39;t know if 10% is the right number or if you should use some\n&gt; additional steps (10%, 25%,50%, 75%, 100%). I really don&#39;t know.\n&gt;\n&gt; 2) Instead of choosing just 10% if the data we could choose the\n&gt; training data that correlate like 0 or -1 with the other training\n&gt; data.  This way we train with data that is the most different. But\n&gt; how get this correlation? I don&#39;t know if there is an algorithm.\n&gt; Specialty if you have 100 input and 300 output neurons �\n&gt;\n&gt; 3) Just pick 10% training data randomly from the complete training\n&gt; data set without calculating the other 90% ever. This way every\n&gt; training dataset will get chosen one time.\n&gt;\n&gt; Do you have some experience in this area? Maybe there are some other\n&gt; ways. I would really appreciate to hear them :-)\n&gt;\n&gt; Kind regards,\n&gt;\n&gt; Christian\n&gt;\n&gt;\n&gt;\n\n\n\n"}}