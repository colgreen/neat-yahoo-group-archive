{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":115403844,"authorName":"John Arrowwood","from":"&quot;John Arrowwood&quot; &lt;jarrowwx@...&gt;","profile":"jarrowwx","replyTo":"LIST","senderId":"ocHofxx_wJayZZFAWbhLNwLnVviZc596sd6kh32WHJFP5mRu623LF9E_oUsq6ahJ0eZsSb6YzRzjGfLxn7FjTjcWo_bRN9nFkcgNhNtG","spamInfo":{"isSpam":false,"reason":"0"},"subject":"RE: [neat] Bloat","postDate":"1086308406","msgId":985,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PEJBWTItRjEyNDMwa2taSWhUckIwMDA0MjI2N0Bob3RtYWlsLmNvbT4="},"prevInTopic":984,"nextInTopic":986,"prevInTime":984,"nextInTime":986,"topicId":904,"numMessagesInTopic":68,"msgSnippet":"... The way I described it, I wouldn t think that could happen.  Complexity isn t taken into account unless trying to decide between two networks that are","rawEmail":"Return-Path: &lt;jarrowwx@...&gt;\r\nX-Sender: jarrowwx@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 27688 invoked from network); 4 Jun 2004 00:20:07 -0000\r\nReceived: from unknown (66.218.66.172)\n  by m20.grp.scd.yahoo.com with QMQP; 4 Jun 2004 00:20:07 -0000\r\nReceived: from unknown (HELO hotmail.com) (65.54.247.124)\n  by mta4.grp.scd.yahoo.com with SMTP; 4 Jun 2004 00:20:07 -0000\r\nReceived: from mail pickup service by hotmail.com with Microsoft SMTPSVC;\n\t Thu, 3 Jun 2004 17:20:07 -0700\r\nReceived: from 64.122.44.102 by by2fd.bay2.hotmail.msn.com with HTTP;\n\tFri, 04 Jun 2004 00:20:06 GMT\r\nX-Originating-Email: [jarrowwx@...]\r\nX-Sender: jarrowwx@...\r\nTo: neat@yahoogroups.com\r\nBcc: \r\nDate: Thu, 03 Jun 2004 17:20:06 -0700\r\nMime-Version: 1.0\r\nContent-Type: text/plain; format=flowed\r\nMessage-ID: &lt;BAY2-F12430kkZIhTrB00042267@...&gt;\r\nX-OriginalArrivalTime: 04 Jun 2004 00:20:07.0241 (UTC) FILETIME=[B08B9B90:01C449C9]\r\nX-eGroups-Remote-IP: 65.54.247.124\r\nFrom: &quot;John Arrowwood&quot; &lt;jarrowwx@...&gt;\r\nReply-To: john@...\r\nSubject: RE: [neat] Bloat\r\nX-Yahoo-Group-Post: member; u=115403844\r\nX-Yahoo-Profile: jarrowwx\r\n\r\n&gt;From: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\n\n&gt;One of my initial motivations for NEAT was to create an alternative\n&gt;to algorithms that consider complexity as part of their selection\n&gt;criteria, i.e. those algorithms that figure complexity into\n&gt;fitness.\n\n&gt;That way of doing things is ad hoc and opens up a whole\n&gt;new can of worms in the way that now &quot;cheaters&quot; will exploit the\n&gt;fitness advantages of being small without really improving in\n&gt;performance.\n\nThe way I described it, I wouldn&#39;t think that could happen.  Complexity \nisn&#39;t taken into account unless trying to decide between two networks that \nare extremely close in fitness.  And then only if they are in the same \nspecies.  As the larger one gets bigger, that size will not penalize the \nnetwork and lower its fitness, per se.  Thus, a less fit network can not \nrate higher than it, just because it is smaller/simpler.  It just means that \nwhen you have to choose between two networks that are otherwise virtually \nidentical in fitness, choose the smaller one.\n\nBut the more I think about it, the less beneficial I think it would be.  It \nmight impact the choice of ONE breeder per generation (per species), since \nit wouldn&#39;t come into play except at the boundary between who breeds and who \ndoesn&#39;t.  And then only if there just happens to be a near-tie for those \nfighting for that Nth slot.  So, to heck with it, it&#39;s not worth doing!!!\n\n&gt;So I think this talk about whether we should be adding a little bit\n&gt;at a time is missing some of the point about stagnation: when you\n&gt;are stagnant, you have no idea how much to add.  If you have a\n&gt;policy whereby adding &quot;too much&quot; structure needs to be stopped in\n&gt;general, then you won&#39;t get the &quot;expanding the range&quot; dynamic which\n&gt;I believe is important.\n\nI agree with that.  If you don&#39;t add, you&#39;re stuck.  If you delete it as \nsoon as you add it, you don&#39;t get un-stuck.\n\n&gt;If you really want to give less\n&gt;support to higher complexity, you could just change the fitness\n&gt;sharing equations so that they are less protective.  But I don&#39;t\n&gt;think that&#39;s a good idea.\n\nWell, the point was to &#39;speed&#39; the search by not wasting time calculating \nthings that didn&#39;t need to be.   And while the actual network activations \nare the bulk of the cost, trying to keep those to a minimum has other \nrepercussions.  If you refuse to let complexity develop, you hinder the \nsearch.  Then, you end up having to have more generations.  Ultimately, you \nend up with a more complex solution anyway, if it is a difficult problem.  \nAND it took you longer to get there.\n\nI think the reason why the higher mutation rates gives you more complicated \nsolutions to simple problems may be in part because the wider search found a \nsolution faster, it just happened to be a non-optimal one.  There are, after \nall, nearly infinite possible solutions to any one problem, right?  So the \nfact that you ended up with a more complicated network is actually testimony \nto the fact that adding complexity DOES add to the ability to find *A* \nsolution, though not necessarily an optimal one.\n\nI suppose it really depends on your goal.  If you want an optimal solution, \nthen mutate slowly.  If you just need any solution, then mutating faster \nwill help you get NEAR a solution faster.  So perhaps the best thing to do \nis a mix of the two?  If fitness is low or stagnant, increase the mutation \nrate.  If it is high and improving, mutate slower so you can spend more time \nlooking around near where you are currently searching.\n\nThe truth is, because these are not hand-designed solutions, they will \nalways be less than &#39;perfect&#39; unless you are willing to spend an inordinate \namount of time on them.  &quot;junk&quot; will exist in the final product, unless you \ninclude a &#39;pruning&#39; phase at the end.  Which is great if you intend to use \nthe end result in a real-world product.\n\nAs for dealing with that junk during the course of the experiment...you will \nreap far more rewards by optimizing the execution of the network activation \nthan you will by tweaking the algorithm to add less junk.  For example, add \ncode that, before evaluating a network, identifies any nodes that are not \ninfluenced by the input nodes (if it is even possible to have it happen...), \nand thus never have a non-zero value.  Drop those from the phenotype before \nevaluating it.  But leave them in the genome so that it could, at some \nfuture time, prove useful.  Likewise, any other analysis you could think of \ndoing to identify non-contributing nodes, go for it.  But the REAL benefit \nwill come with a change of how you activate.  If you have a 10 layer \nnetwork, you have to activate it 10 times in order to get a final solution, \nunless I misunderstood the code.  If you initially analyze the network and \noptimize it so you can evaluate it in a single pass, you will be able to \nactivate it in 1/10th the time.  Further optimize that code, and you could \nget it even faster.  THAT is where you are going to see performance gains, \nlong before you find any benefit from tweaking the evolutionary algorithm to \nreduce &#39;bloat.&#39;\n\nBut that&#39;s just my opinion.  If you have have experience to show otherwise, \nthen I stand corrected. :)\n\n\n\n"}}