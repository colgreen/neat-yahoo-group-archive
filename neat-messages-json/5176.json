{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":200957992,"authorName":"jgmath2000","from":"&quot;jgmath2000&quot; &lt;jgmath2000@...&gt;","profile":"jgmath2000","replyTo":"LIST","senderId":"2faK5XiuZ5S3X_9R9m8d6Tv4uZ3nez2fcKdilpBI5vWoRz4Y-rIPgDcdxfMhkI-Hi4xugdCW0C3V3LjfIDRO9Hyh-xGy1Gks3H4","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Weird substrate output","postDate":"1269348569","msgId":5176,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGhvYWRjcCtiOTRxQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGhvYTZqZSthbzRjQGVHcm91cHMuY29tPg=="},"prevInTopic":5175,"nextInTopic":0,"prevInTime":5175,"nextInTime":5177,"topicId":5171,"numMessagesInTopic":4,"msgSnippet":"Hey Andrei, If you are using the FastBiasNetwork, then each node should have a bias forcing its official weight.  If the biases are high, or if you have biases","rawEmail":"Return-Path: &lt;jgmath2000@...&gt;\r\nX-Sender: jgmath2000@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 11664 invoked from network); 23 Mar 2010 12:51:55 -0000\r\nX-Received: from unknown (98.137.34.45)\n  by m10.grp.re1.yahoo.com with QMQP; 23 Mar 2010 12:51:55 -0000\r\nX-Received: from unknown (HELO n40b.bullet.mail.sp1.yahoo.com) (66.163.168.154)\n  by mta2.grp.sp2.yahoo.com with SMTP; 23 Mar 2010 12:51:55 -0000\r\nX-Received: from [69.147.65.148] by n40.bullet.mail.sp1.yahoo.com with NNFMP; 23 Mar 2010 12:49:29 -0000\r\nX-Received: from [98.137.35.12] by t11.bullet.mail.sp1.yahoo.com with NNFMP; 23 Mar 2010 12:49:29 -0000\r\nDate: Tue, 23 Mar 2010 12:49:29 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;hoadcp+b94q@...&gt;\r\nIn-Reply-To: &lt;hoa6je+ao4c@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;jgmath2000&quot; &lt;jgmath2000@...&gt;\r\nSubject: Re: Weird substrate output\r\nX-Yahoo-Group-Post: member; u=200957992; y=PEOimlXOtFZJP8MWnLfChyZQbiUCJp1v0rLjX0lP_X1klr2o6g\r\nX-Yahoo-Profile: jgmath2000\r\n\r\nHey Andrei,\n\nIf you are using the FastBiasNetwork, then each node should ha=\r\nve a bias forcing its official weight.  If the biases are high, or if you h=\r\nave biases on each node in the CPPN, this could cause the bias to overpower=\r\n everything else in the network, forcing all of your weights in the substra=\r\nte to be the same value. Typically CPPNs are implemented using the FastNetw=\r\nork which does not have a separate bias for each node.\n\nAs far as the tanh,=\r\n both of the equations you mentioned look to produce identical output.  I p=\r\nlugged it into python and here&#39;s what I got:\n\n&gt;&gt;&gt; a =3D range(0,21)\n&gt;&gt;&gt; for=\r\n index in xrange(0,21): a[index] =3D (1 - 2 / (1 + math.exp(2 * (index-10.0=\r\n\n)/10.0)))\n...\n&gt;&gt;&gt; print a\n[-0.76159415595576463, -0.71629787019902458, -0.=\r\n66403677026784891, -0.6043677771\n1716339, -0.53704956699803508, -0.46211715=\r\n726000979, -0.37994896225522501, -0.29\n131261245159079, -0.1973753202249040=\r\n1, -0.099667994624955902, 0.0, 0.09966799462\n495568, 0.19737532022490401, 0=\r\n.2913126124515909, 0.3799489622552249, 0.462117157\n26000979, 0.537049566998=\r\n0353, 0.6043677771171635, 0.66403677026784891, 0.7162978\n7019902447, 0.7615=\r\n9415595576485]\n&gt;&gt;&gt; for index in xrange(0,21): a[index] =3D 2 / (1 + math.ex=\r\np(-2 * (index-10.0)/10\n.0)) - 1\n...\n&gt;&gt;&gt; print a\n[-0.76159415595576485, -0.7=\r\n1629787019902447, -0.66403677026784891, -0.6043677771\n171635, -0.5370495669=\r\n980353, -0.46211715726000979, -0.3799489622552249, -0.29131\n26124515909, -0=\r\n.19737532022490401, -0.09966799462495568, 0.0, 0.0996679946249559\n02, 0.197=\r\n37532022490401, 0.29131261245159079, 0.37994896225522501, 0.46211715726\n000=\r\n979, 0.53704956699803508, 0.60436777711716339, 0.66403677026784891, 0.71629=\r\n78\n7019902458, 0.76159415595576463]\n\n--- In neat@yahoogroups.com, &quot;Andrei&quot; =\r\n&lt;andrei.rusu@...&gt; wrote:\n&gt;\n&gt; I have set all inputs and biases to 0 and I am=\r\n getting all 0s in all 3 layers. Which makes sense, because I have &quot;SignedA=\r\nctivation 1.0&quot; in the NEAT config.dat file. If I set &quot;SignedActivation 0.0&quot;=\r\n, layer A is all 0s, and layers B & C are all (almost) 1.0. I looked into t=\r\nhe FastBiaseNetwork.cpp NEAT code to see how the outputs are being computed=\r\n, and it seems my code needs more debugging. \n&gt; \n&gt; One more thing. Would yo=\r\nu recommend using tanh sigmoids? From the way they are computed in the NEAT=\r\n code, they seem to be just a different rescaling of the 1/(1+exp(- a*x)) s=\r\nigmoid. \n&gt; \n&gt; Shouldn&#39;t that be:\n&gt; \n&gt; retVal =3D (1 - 2 / (1 + exp(2 * tmpV=\r\nal));  // tanh \n&gt; \n&gt; // instead of: (signed sigmoid case) \n&gt; retVal =3D 2 /=\r\n (1 + exp(-2 * tmpVal)) - 1; // a=3D2 sigmoid\n&gt; \n&gt; \n&gt; \n&gt; Thanks for your in=\r\nput!\n&gt; Andrei\n&gt; \n&gt; --- In neat@yahoogroups.com, &quot;Ken&quot; &lt;kstanley@&gt; wrote:\n&gt; =\r\n&gt;\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; It may be useful to do a little debugging to see what&#39;s goi=\r\nng on:\n&gt; &gt; \n&gt; &gt; If you input all zeros to the substrate input layer, what h=\r\nappens at the hidden layer?  If there are no bias values, and the hidden la=\r\nyer computes unsigned sigmoids (i.e. sigmoids that range between 0 and 1), =\r\nthen every sigmoid in the hidden layer should output 0.5 (since they all ha=\r\nve zero activation coming on).  If that is not the case, then there is some=\r\nthing else at play.\n&gt; &gt; \n&gt; &gt; Then you can check what the output nodes are g=\r\nenerating from there.  For example, look at a single output node and the 81=\r\n or so inputs that go into it, and compute for yourself the correct output =\r\nbased on the weights and the activations going over them (which would all b=\r\ne 0.5).\n&gt; &gt; \n&gt; &gt; You may then find out why it is saturating.  Or you may fi=\r\nnd a bug.   But it can help to isolate the problem.\n&gt; &gt; \n&gt; &gt; You can also t=\r\nry manipulating the substrate weights by hand to isolate the issue.  For ex=\r\nample, you can set all of the hidden-layer-to-output weights to zero except=\r\n those going into a single one of the output nodes.  In that case, only tha=\r\nt node should have a unique output value.  If that does not happen, then yo=\r\nu may isolate what went wrong.\n&gt; &gt; \n&gt; &gt; ken\n&gt; &gt; \n&gt; &gt; --- In neat@yahoogroup=\r\ns.com, &quot;Andrei&quot; &lt;andrei.rusu@&gt; wrote:\n&gt; &gt; &gt;\n&gt; &gt; &gt; I am using a 3 layered sa=\r\nndwich substrate with biases, each layer 9x9 nodes, generated by a CPPN wit=\r\nh sigmoid outputs. The weights are computed in exactly the same fashion as =\r\nin the checkers experiment,\n&gt; &gt; &gt; with a formula like: (w-0.2)/0.8 * 3.0 fo=\r\nr positive weights w &gt; 0.2 (etc) \n&gt; &gt; &gt; \n&gt; &gt; &gt; The output is all 1&#39;s, and t=\r\nhe substrate does indeed seem to have more positive weighted links than neg=\r\native ones. I am facing the same &#39;issue&#39; in later generations. \n&gt; &gt; &gt; \n&gt; &gt; =\r\n&gt; The substrate inputs are all between -1 and 1, but I do not normalize the=\r\n 9x9 input matrix. Might this be the issue. \n&gt; &gt; &gt; \n&gt; &gt; &gt; I changed the for=\r\nmulas to something like: (w-0.2)/0.8 * 0.01 and now I get values around 0.4=\r\n4 on all output nodes.\n&gt; &gt; &gt; \n&gt; &gt; &gt; Does this &#39;issue&#39; suggest any misuse of=\r\n (Hyper)NEAT to anyone?\n&gt; &gt; &gt; \n&gt; &gt; &gt; Thanks, \n&gt; &gt; &gt; Andrei\n&gt; &gt; &gt;\n&gt; &gt;\n&gt;\n\n\n\n"}}