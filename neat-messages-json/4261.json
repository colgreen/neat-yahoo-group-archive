{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":8147458,"authorName":"Christian","from":"&quot;Christian&quot; &lt;Christian.Hofmann@...&gt;","profile":"chhofchhof","replyTo":"LIST","senderId":"o9zU2OXxEFeA85CgQTwJurx5v2ebYew4BRK4SeaPIRx3YQkqtgb75F6FRarLitCN6SWQ8CF9nytpuINxOLFGDxPWqsc7RnendKbUMVc","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Evaluating population with large training data set","postDate":"1218239506","msgId":4261,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGc3aW02aSttZzQ1QGVHcm91cHMuY29tPg==","inReplyToHeader":"PGE3ODdiNjA2MDgwODA4MTYyM2o0MDVmY2VmM3Q4MmY5YTY5MzE3MjhkNmIwQG1haWwuZ21haWwuY29tPg=="},"prevInTopic":4260,"nextInTopic":4262,"prevInTime":4260,"nextInTime":4262,"topicId":4253,"numMessagesInTopic":11,"msgSnippet":"Thank you very much, your pseudo code and formula helped me a lot (Now I understand the mathematics) I will try it. It looks very promising! It will speed up","rawEmail":"Return-Path: &lt;Christian.Hofmann@...&gt;\r\nX-Sender: Christian.Hofmann@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 63495 invoked from network); 8 Aug 2008 23:51:47 -0000\r\nX-Received: from unknown (66.218.67.95)\n  by m42.grp.scd.yahoo.com with QMQP; 8 Aug 2008 23:51:47 -0000\r\nX-Received: from unknown (HELO n28b.bullet.scd.yahoo.com) (66.94.237.30)\n  by mta16.grp.scd.yahoo.com with SMTP; 8 Aug 2008 23:51:47 -0000\r\nX-Received: from [66.218.69.4] by n28.bullet.scd.yahoo.com with NNFMP; 08 Aug 2008 23:51:47 -0000\r\nX-Received: from [66.218.66.88] by t4.bullet.scd.yahoo.com with NNFMP; 08 Aug 2008 23:51:47 -0000\r\nDate: Fri, 08 Aug 2008 23:51:46 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;g7im6i+mg45@...&gt;\r\nIn-Reply-To: &lt;a787b6060808081623j405fcef3t82f9a6931728d6b0@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;Christian&quot; &lt;Christian.Hofmann@...&gt;\r\nSubject: Re: Evaluating population with large training data set\r\nX-Yahoo-Group-Post: member; u=8147458; y=EFvkzJUxalwxMamCD-y4zMdO8ISpoH_rcluTI2qXe7krA8WzRA\r\nX-Yahoo-Profile: chhofchhof\r\n\r\nThank you very much, your pseudo code and formula helped me a lot (Now\nI un=\r\nderstand the mathematics)\n\nI will try it. It looks very promising!\n\nIt will=\r\n speed up the training a lot as I need less training data to\nevaluate :)\n\n-=\r\n-- In neat@yahoogroups.com, &quot;Robert DeLisle&quot; &lt;rkdelisle@...&gt; wrote:\n&gt;\n&gt; I f=\r\norgot to mention that during the calculations of distance, you\ncould omit\n&gt;=\r\n taking the square root.  If you have a very large dataset, the\nsquare root=\r\n\n&gt; function will consume a lot of extra time.  You&#39;ll have to adjust your\n&gt;=\r\n distance cutoff value, obviously, but the end result is the same.\n&gt; \n&gt; \n&gt; =\r\n\n&gt; \n&gt; \n&gt; On Fri, Aug 8, 2008 at 5:19 PM, Robert DeLisle &lt;rkdelisle@...&gt; wro=\r\nte:\n&gt; \n&gt; &gt; I would suggest ignoring the output at this point and looking on=\r\nly\nat the\n&gt; &gt; input variables.  Here&#39;s a quick bit of pseudocode that may h=\r\nelp:\n&gt; &gt;\n&gt; &gt; 1)  Pick a single point in your dataset -put into the Training=\r\n Set.\n&gt; &gt; 2)  For all other points in the dataset, compute the Euclidean\ndi=\r\nstance\n&gt; &gt; based upon the input variables.\n&gt; &gt;      2a)  If the distance to=\r\n another point is less than the cutoff\nthat you\n&gt; &gt; specify, place that oth=\r\ner point into the Test or Validation Set. \nIf the\n&gt; &gt; distance is greater t=\r\nhan the cutoff, leave it in the original dataset.\n&gt; &gt; 3)  If you still have=\r\n points in the original dataset, return to\nStep 1.\n&gt; &gt;\n&gt; &gt; For the Euclidea=\r\nn distance, that&#39;s very easy.  In two dimensions,\nor if you\n&gt; &gt; only had tw=\r\no inputs, you would have this:\n&gt; &gt;\n&gt; &gt; distance =3D square_root( (x1-x2)^2 =\r\n+ (y1-y2)^2) )  where (x1,y1) is\nthe data\n&gt; &gt; for point #1, and (x2,y2) is =\r\nthe data for point number 2.\n&gt; &gt;\n&gt; &gt; In three dimensions, this:\n&gt; &gt;\n&gt; &gt; dis=\r\ntance =3D square_root( (x1-x2)^2 + (y1-y2)^2) + (z1-z2)^2 )  where\n&gt; &gt; (x1,=\r\ny1,z1) is the data for point #1, and (x2,y2,z2) is the data\nfor point\n&gt; &gt; n=\r\number 2.\n&gt; &gt;\n&gt; &gt; As you get into more and more variables/dimensions/inputs,=\r\n you\njust keep\n&gt; &gt; adding in the square of the differences for those dimens=\r\nions. \nWhat you are\n&gt; &gt; doing is taking the square root of the sum of the s=\r\nquared\ndifferences of all\n&gt; &gt; your input variables.\n&gt; &gt;\n&gt; &gt; This technique =\r\nis described in this paper:\n&gt; &gt;\n&gt; &gt; Golbraikh, A. and A. Tropsha (2002). &quot;P=\r\nredictive QSAR modeling\nbased on\n&gt; &gt; diversity sampling of experimental dat=\r\nasets for the training and\ntest set\n&gt; &gt; selection.&quot; J Comput Aided Mol Des =\r\n16(5-6): 357-69.\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; But if you can grasp what I described above yo=\r\nu won&#39;t need to read the\n&gt; &gt; paper at all.\n&gt; &gt;\n&gt; &gt; -Kirk\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; =\r\n&gt;\n&gt; &gt;\n&gt; &gt; On Fri, Aug 8, 2008 at 5:00 PM, Christian\n&lt;Christian.Hofmann@...&gt;=\r\nwrote:\n&gt; &gt;\n&gt; &gt;&gt;   Thank you Robert,\n&gt; &gt;&gt;\n&gt; &gt;&gt; that sounds exactly like what=\r\n i am looking for!\n&gt; &gt;&gt;\n&gt; &gt;&gt; I also understand the theory, but have problem=\r\ns with the mathematics\n&gt; &gt;&gt; behind you theory.\n&gt; &gt;&gt;\n&gt; &gt;&gt; So I need to do th=\r\ne following steps:\n&gt; &gt;&gt;\n&gt; &gt;&gt; 1) Choose one random dataset.\n&gt; &gt;&gt; 2) calculat=\r\ne a coordinate for this dataset in a x dimensional room.\n&gt; &gt;&gt; But how can I=\r\n do this? Shall I take the whole input and output\nneurons\n&gt; &gt;&gt; as an n-dime=\r\nnsional vector (n =3D # input + # output)?\n&gt; &gt;&gt; 3) I choose an other random=\r\n dataset and calculate the coordinates for\n&gt; &gt;&gt; this one.\n&gt; &gt;&gt; 4) calculate=\r\n the the distance between both points. But how can i\ndo this?\n&gt; &gt;&gt; ...\n&gt; &gt;&gt;=\r\n The other things are easy :)\n&gt; &gt;&gt;\n&gt; &gt;&gt; I have looked for &quot;Spherical Exclus=\r\nion&quot;, but haven&#39;t found anything.\n&gt; &gt;&gt; is there an other keyword I can sear=\r\nch for?\n&gt; &gt;&gt;\n&gt; &gt;&gt; Thanks,\n&gt; &gt;&gt;\n&gt; &gt;&gt; Christian\n&gt; &gt;&gt;\n&gt; &gt;&gt; --- In neat@yahoogr=\r\noups.com &lt;neat%40yahoogroups.com&gt;, &quot;Robert\nDeLisle&quot;\n&gt; &gt;&gt; &lt;rkdelisle@&gt; wrote=\r\n:\n&gt; &gt;&gt; &gt;\n&gt; &gt;&gt; &gt; Another possible option would be to use a technique called\n=\r\nSpherical\n&gt; &gt;&gt; &gt; Exclusion to partition your data. The premise is that you =\r\nchose\na data\n&gt; &gt;&gt; &gt; point at random and remove all those data points that a=\r\nre within a\n&gt; &gt;&gt; &gt; particular distance from it, in other words, you remove =\r\nall those\n&gt; &gt;&gt; within a\n&gt; &gt;&gt; &gt; sphere around that point. Pick another point=\r\n in the remaining set,\n&gt; &gt;&gt; remove\n&gt; &gt;&gt; &gt; the sphere around it, etc. The cr=\r\nitical piece is defining the size\n&gt; &gt;&gt; of the\n&gt; &gt;&gt; &gt; sphere. You can easily=\r\n use Euclidean distance and most likely\ndetermine\n&gt; &gt;&gt; &gt; what size sphere g=\r\nives you a certain percentage of the original\ndata for\n&gt; &gt;&gt; &gt; your training=\r\n set. This has the appeal that the chosen points\nroughly\n&gt; &gt;&gt; &gt; represent c=\r\nentroid in your data space.\n&gt; &gt;&gt; &gt;\n&gt; &gt;&gt; &gt;\n&gt; &gt;&gt; &gt;\n&gt; &gt;&gt; &gt;\n&gt; &gt;&gt; &gt; On Fri, Aug =\r\n8, 2008 at 3:44 PM, Christian &lt;Christian.Hofmann@&gt;\n&gt; &gt;&gt; wrote:\n&gt; &gt;&gt; &gt;\n&gt; &gt;&gt; =\r\n&gt; &gt; Hello Sandor,\n&gt; &gt;&gt; &gt; &gt;\n&gt; &gt;&gt; &gt; &gt; thank you very much for your detailed p=\r\nost.\n&gt; &gt;&gt; &gt; &gt;\n&gt; &gt;&gt; &gt; &gt; Testing the younger generations with less training d=\r\nata\nsounds very\n&gt; &gt;&gt; &gt; &gt; good. But currently I am using (Hyper)SharpNeat an=\r\nd there is\nno way to\n&gt; &gt;&gt; &gt; &gt; get the genome age. I only have the network i=\r\ntself and not\nthe genom.\n&gt; &gt;&gt; &gt; &gt; But I think I could implement it.\n&gt; &gt;&gt; &gt; =\r\n&gt;\n&gt; &gt;&gt; &gt; &gt; You write that you don&#39;t want to use completely new random\ndata =\r\n(out\n&gt; &gt;&gt; &gt; &gt; of training set) as you think every genome should have the sa=\r\nme\n&gt; &gt;&gt; &gt; &gt; chance. I don&#39;t think so. When you take one random training\ndat=\r\na set\n&gt; &gt;&gt; &gt; &gt; and per random you have bad luck and some bad samples, every=\r\n\n&gt; &gt;&gt; &gt; &gt; generation is trained wrong. But when you take a completely\nnew r=\r\nandom\n&gt; &gt;&gt; &gt; &gt; training data set then bad genomes will get erased soon. Als=\r\no\nyou have\n&gt; &gt;&gt; &gt; &gt; a much bigger deviation.\n&gt; &gt;&gt; &gt; &gt;\n&gt; &gt;&gt; &gt; &gt; So my next q=\r\nuestion is how big the training part of all\nsample data\n&gt; &gt;&gt; &gt; &gt; should be =\r\nso I should have a good average.\n&gt; &gt;&gt; &gt; &gt;\n&gt; &gt;&gt; &gt; &gt; If I have one million tr=\r\naining data, mabye 5 percent is\nenough. When I\n&gt; &gt;&gt; &gt; &gt; only have 1000, I w=\r\nill need them all.\n&gt; &gt;&gt; &gt; &gt;\n&gt; &gt;&gt; &gt; &gt; By choosing better samples I could dec=\r\nrement the needed value\nrate\n&gt; &gt;&gt; &gt; &gt; even further.\n&gt; &gt;&gt; &gt; &gt;\n&gt; &gt;&gt; &gt; &gt; Your =\r\nadvice about logging the fitness variance sounds very\ngood. But\n&gt; &gt;&gt; &gt; &gt; wh=\r\nat is a high variance? Let&#39;s say my fitness range is from 0\nto 1.\n&gt; &gt;&gt; &gt; &gt; =\r\nIs, for example, 5% std. deviation a level you raise or lower\nsample\n&gt; &gt;&gt; &gt;=\r\n &gt; data amount?\n&gt; &gt;&gt; &gt; &gt;\n&gt; &gt;&gt; &gt; &gt; I am looking for something to give every =\r\ntraining data set\nbased on\n&gt; &gt;&gt; &gt; &gt; inputs and outputs a special number. So=\r\n I can choose the\nsamples that\n&gt; &gt;&gt; &gt; &gt; are most critical for the test.\n&gt; &gt;=\r\n&gt; &gt; &gt;\n&gt; &gt;&gt; &gt; &gt; What&#39;s about that:\n&gt; &gt;&gt; &gt; &gt;\n&gt; &gt;&gt; &gt; &gt; I just sum up all input=\r\n and output values. After that I can\nchoose the\n&gt; &gt;&gt; &gt; &gt; sample data with t=\r\nhe most different sums. Maybe that will\nwork. But\n&gt; &gt;&gt; &gt; &gt; when input 1 has=\r\n the same value as input 2 in a different\ntest data\n&gt; &gt;&gt; &gt; &gt; (and vice vers=\r\na with some other inputs) they will get the\nsame sum.\n&gt; &gt;&gt; &gt; &gt;\n&gt; &gt;&gt; &gt; &gt; So =\r\nthey are different but would seen as identical. So this\napoach is\n&gt; &gt;&gt; &gt; &gt; =\r\nnot good.\n&gt; &gt;&gt; &gt; &gt;\n&gt; &gt;&gt; &gt; &gt; Maybe someone has an additional one?\n&gt; &gt;&gt; &gt; &gt;\n&gt;=\r\n &gt;&gt; &gt; &gt; Kind regards,\n&gt; &gt;&gt; &gt; &gt;\n&gt; &gt;&gt; &gt; &gt; Christian\n&gt; &gt;&gt; &gt; &gt;\n&gt; &gt;&gt;\n&gt; &gt;&gt;  \n&gt; &gt;&gt;=\r\n\n&gt; &gt;\n&gt; &gt;\n&gt;\n\n\n\n"}}