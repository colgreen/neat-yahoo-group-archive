{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Ken","from":"&quot;Ken&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"dIfTEA5Sffo2VwcEIGkgeeNox8B4Jjwg6Se7tjOW0VzoV_mNGYTRiMOOlHaQWsWgXc4lBEUk_P8vsVCOBD5LRmCBOAVw","spamInfo":{"isSpam":false,"reason":"6"},"subject":"New Activation Functions (Was Re: &#39;Boxes&#39; Visual Discrimination Task)","postDate":"1274933468","msgId":5247,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGh0a3JjcytjZTZtQGVHcm91cHMuY29tPg==","inReplyToHeader":"PEFBTkxrVGlsM3gwdXlRZjMwQ0g1cHUtdDZzOHNqMlFZYjJySHRCSWhNZnhiUkBtYWlsLmdtYWlsLmNvbT4="},"prevInTopic":5245,"nextInTopic":5249,"prevInTime":5246,"nextInTime":5248,"topicId":5237,"numMessagesInTopic":10,"msgSnippet":"Colin, when you mentioned adding a log(x) it brought to my mind that it might be interesting to add both log(x) and e^x to the set together.  That way, some","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 46635 invoked from network); 27 May 2010 04:11:56 -0000\r\nX-Received: from unknown (98.137.34.45)\n  by m14.grp.re1.yahoo.com with QMQP; 27 May 2010 04:11:56 -0000\r\nX-Received: from unknown (HELO n41b.bullet.mail.sp1.yahoo.com) (66.163.168.155)\n  by mta2.grp.sp2.yahoo.com with SMTP; 27 May 2010 04:11:56 -0000\r\nX-Received: from [69.147.65.173] by n41.bullet.mail.sp1.yahoo.com with NNFMP; 27 May 2010 04:11:09 -0000\r\nX-Received: from [98.137.34.33] by t15.bullet.mail.sp1.yahoo.com with NNFMP; 27 May 2010 04:11:09 -0000\r\nDate: Thu, 27 May 2010 04:11:08 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;htkrcs+ce6m@...&gt;\r\nIn-Reply-To: &lt;AANLkTil3x0uyQf30CH5pu-t6s8sj2QYb2rHtBIhMfxbR@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;Ken&quot; &lt;kstanley@...&gt;\r\nSubject: New Activation Functions (Was Re: &#39;Boxes&#39; Visual Discrimination Task)\r\nX-Yahoo-Group-Post: member; u=54567749; y=yphRNdjGOo4w_YYUO6wGw2kP6dDQpKsXZ9D_MoDCiWdEXGyc24JP\r\nX-Yahoo-Profile: kenstanley01\r\n\r\n\n\nColin, when you mentioned adding a log(x) it brought to my mind that it m=\r\night be interesting to add both log(x) and e^x to the set together.  That w=\r\nay, some combinations of nodes could realize multiplications.  \n\nI apprecia=\r\nte what you&#39;re saying about how combinations of nodes in our abstract &quot;neur=\r\nal networks&quot; are more realistic abstractions of neurons than single nodes. =\r\n I&#39;ve never really felt that a node in a traditional ANN is a true correlat=\r\ne to a real neuron.  Of course it goes beyond even activation functions; pl=\r\nasticity comes into play as well.  \n\nBut at the same time just because real=\r\n neurons do a lot of different kinds of information processing individually=\r\n does not mean we need all that, either individually or collectively.  The =\r\nwhole problem in our field is to decide what from nature is essential and w=\r\nhat is not, which is basically the problem of abstraction.  Much of the min=\r\nutia of nature is tantalizing, but may nevertheless not be essential.  When=\r\n it comes to neural activation functions, the question of what is essential=\r\n is fuzzy.  It&#39;s basically a question of bias and scale since we know sigmo=\r\nids can theoretically approximate any function.  Yet of course that does no=\r\nt mean sticking with only sigmoids is superior.  But neither does it tell u=\r\ns that we need everything either.\n\nken \n\n--- In neat@yahoogroups.com, Colin=\r\n Green &lt;colin.green1@...&gt; wrote:\n&gt;\n&gt; On 25 May 2010 08:46, Ken &lt;kstanley@..=\r\n.&gt; wrote:\n&gt; &gt; I see where you&#39;re coming from. The multiplication examples a=\r\nre indeed interesting. Have you\n&gt; &gt; tried running the inputs to neurons (af=\r\nter they run over their incoming connections) through\n&gt; &gt; logarithms? I won=\r\nder what networks would do with logs at all their inputs. Maybe the nodes\n&gt;=\r\n &gt; would then only need to be linear summations (because logs provide nonli=\r\nnearities). Or maybe\n&gt; &gt; they could still run their usual activation functi=\r\non set.\n&gt; \n&gt; I think if we acknoweldge the possibility that there is some (=\r\npossibly\n&gt; significant) functionality in dendritic trees then the tradition=\r\nal ANN\n&gt; model isn&#39;t as &#39;broken&#39; as that might suggest - so long as we acce=\r\npt\n&gt; that single ANN nodes aren&#39;t a simplification of single neurons but\n&gt; =\r\nrather a representation of a fundamental unit of processing. That is,\n&gt; we =\r\nlet go of the one-to-one mapping of neuron=3D=3DANN node.  From there\n&gt; we =\r\ncould e.g. add a log(x) activation function to our set of CPPN\n&gt; activation=\r\n functions (as a first experiment).\n&gt; \n&gt; &gt; In general, there is definitely =\r\nroom for the neural model to be improved. The main impediment\n&gt; &gt; to progre=\r\nss in this area is probably just that it is not very glorious work to be fi=\r\nddling with\n&gt; &gt; different function sets and network models and hoping to se=\r\ne a breakthrough, but it is true\n&gt; &gt; there could be a payoff.\n&gt; \n&gt; Sure. Th=\r\ne fact that Christof Koch wrote a whole book subtitled\n&gt; &quot;Information proce=\r\nssing in single neurons&quot; suggests there might be\n&gt; more to neurons than a s=\r\nimple sigmoid input-response behaviour :)\n&gt; \n&gt; \n&gt; &gt; But I&#39;m not saying ther=\r\ne isn&#39;t something better. I&#39;m just saying it&#39;s hard to tell if there is a p=\r\nroblem or\n&gt; &gt; not that needs fixing. There may be and I would be very inter=\r\nested if something superior was\n&gt; &gt; demonstrated. Will the new SharpNEAT pr=\r\novide an ability to play with such ideas?\n&gt; \n&gt; Yes it&#39;s certainly more gear=\r\ned up for experimenting with different\n&gt; types of genome without having to =\r\nhack about in the core code at all.\n&gt; In short I&#39;ve used generics (similar =\r\nconcept to C++ templates) to\n&gt; abstract away the core algorithms (evolution=\r\n, speciation) from the\n&gt; genome implementation. So the idea is you devise w=\r\nhatever genome you\n&gt; want along with the &#39;phenome&#39; implementation and then =\r\nplug in some\n&gt; code that &#39;decodes&#39; the genome to the phenome\n&gt; (IGenomeDeco=\r\nder&lt;TGenome,TPhenome&gt; interface). The speciation\n&gt; abstraction works by def=\r\nining the genomes position in an N dimensional\n&gt; space - that is, you have =\r\nto translate the &#39;guts&#39; of the genome into a\n&gt; N-dimensional position and t=\r\neh speciation works on those positions -\n&gt; it&#39;s completely abstracted away =\r\nfrom the genome classes.\n&gt; \n&gt; I&#39;ll respond to one of your other points in a=\r\n further post.\n&gt; \n&gt; Colin\n&gt;\n\n\n\n"}}