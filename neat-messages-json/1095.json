{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":7192225,"authorName":"Ian Badcoe","from":"Ian Badcoe &lt;ian_badcoe@...&gt;","profile":"ian_badcoe","replyTo":"LIST","senderId":"Ejtr8CMHtbHtBDvTGBbnbmCwu1_vqx5JKaJbcrVlohC8VcCyVJLSHmy5Y9v7RDSwg70H5pSKZE4TJSlcck95OuQAgkGWMBNDHSo","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] Computation Time","postDate":"1087567415","msgId":1095,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDYuMS4wLjYuMC4yMDA0MDYxODE0MzIyOC4wMjUxOWIwMEBwb3AubWFpbC55YWhvby5jby51az4=","inReplyToHeader":"PEJBWTItRjkyTFlMZlg5V1NCZFAwMDA2N2MwYkBob3RtYWlsLmNvbT4=","referencesHeader":"PEJBWTItRjkyTFlMZlg5V1NCZFAwMDA2N2MwYkBob3RtYWlsLmNvbT4="},"prevInTopic":1093,"nextInTopic":1097,"prevInTime":1094,"nextInTime":1096,"topicId":845,"numMessagesInTopic":99,"msgSnippet":"Hi, ... Oh yes, sorry, I did see that, but forgot again before I came to reply. Your test sample is far too small to see the memory effects I was trying to ","rawEmail":"Return-Path: &lt;ian_badcoe@...&gt;\r\nX-Sender: ian_badcoe@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 42527 invoked from network); 18 Jun 2004 14:00:31 -0000\r\nReceived: from unknown (66.218.66.172)\n  by m25.grp.scd.yahoo.com with QMQP; 18 Jun 2004 14:00:31 -0000\r\nReceived: from unknown (HELO smtp003.mail.ukl.yahoo.com) (217.12.11.34)\n  by mta4.grp.scd.yahoo.com with SMTP; 18 Jun 2004 14:00:30 -0000\r\nReceived: from unknown (HELO ian2k.yahoo.co.uk) (ian?badcoe@212.159.73.108 with login)\n  by smtp003.mail.ukl.yahoo.com with SMTP; 18 Jun 2004 14:00:26 -0000\r\nMessage-Id: &lt;6.1.0.6.0.20040618143228.02519b00@...&gt;\r\nX-Sender: ian_badcoe@...\r\nX-Mailer: QUALCOMM Windows Eudora Version 6.1.0.6\r\nDate: Fri, 18 Jun 2004 15:03:35 +0100\r\nTo: neat@yahoogroups.com\r\nIn-Reply-To: &lt;BAY2-F92LYLfX9WSBdP00067c0b@...&gt;\r\nReferences: &lt;BAY2-F92LYLfX9WSBdP00067c0b@...&gt;\r\nMime-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;us-ascii&quot;; format=flowed\r\nX-eGroups-Remote-IP: 217.12.11.34\r\nFrom: Ian Badcoe &lt;ian_badcoe@...&gt;\r\nSubject: Re: [neat] Computation Time\r\nX-Yahoo-Group-Post: member; u=7192225\r\nX-Yahoo-Profile: ian_badcoe\r\n\r\nHi,\n\nAt 18:05 17/06/2004, you wrote:\n&gt; &gt;From: Ian Badcoe &lt;ian_badcoe@...&gt;\n&gt; &gt;\n&gt; &gt;Hi,\n&gt; &gt;          I can&#39;t quite get the details of your 4 approaches from these\n&gt; &gt;brief descriptions, can you post some pseudo-code.\n&gt;\n&gt;I actually posted the real code to the group files section.\n&gt;\n&gt;http://groups.yahoo.com/group/neat/files/compare.c\n\nOh yes, sorry, I did see that, but forgot again before I came to reply.\n\nYour test sample is far too small to see the memory effects I was trying to \nexplain.\n\nBasically the idea is this:\n\nFor small amounts of data, they have to be read into the cache once, but \nonce they are in the cache they are fast.\n\nIn that case speed will depend mostly on the detail of the cpu instructions \nand their order.\n\nFor large amounts of data (meaning bigger than the cache but particularly \nwhen _much_ bigger than the cache) the order in which data is accessed \nbecomes more important than instruction order.\n\nSince you said &quot;billions&quot; of accesses, memory speed will be critical to \nyour program, so that is what I concentrated on.\n\nProcessing a small amount of data 1 billion times is not the same as \nprocessing a lot of data once.\n\nThe reason memory access patters matter is because (i) uncached memory \naccess is slower than CPU processing (ii) the cache loads an entire &quot;line&quot; \nof contiguous memory in one &quot;burst&quot; -- this is 32 bytes (8 floats, 4 doubles).\n\nThus this:\n\nint a[1000000000];\nint i;\n\nfor(i = 0; i &lt; 1000000000; i += 100) {\n   for(j = 0; j &lt; 100; j++) {\n     a[i+j] = 1;\n   }\n}\n\nWill be far faster than this:\n\nfor(j = 0; j &lt; 100; j++) {\n   for(i = 0; i &lt; 1000000000; i += 100) {\n     a[i+j] = 1;\n   }\n}\n\nBecause the former accesses memory in a coherent fashion, thus the first \nload (at address a + 0 + 0) brings in 8 ints, which are then all used in \nthe next 8 cycles of the inner loop.  The whole line can then be written \nback to memory once.\n\nThe latter, in contrast, loads 8 ints but only uses one of them, then jumps \n400 bytes off through memory and reads different 8.  Because the first 8 \nloaded have no chance of still being in the cache when the inner-loop \nexits, they will definitely need loading again on the next pass of the \nouter loop.  The second example will do 8 times as much memory access as \nthe first, and be pretty close to 8 fold slower.\n\nThat&#39;s what I was getting at, but all your examples will fit in the cache, \nso none of this will apply.\n\nBig-data programs are different to small-data programs.\n\n--\n\nNow, you can rightly say that if you process one network at a time, it will \nall fit in the cache and you need not worry, but I was thinking that (i) \nyour input data (the image) is quite large, and (ii) if you are storing it, \nthen your output data is also quite large, and as they get accessed during \nnetwork evaluations, I was considering that your network might have trouble \nstaying in the cache.  Hence my large-scale rearrangement.\n\n--\n\nThere is an issue which I never got onto with my brief explanation before, \nwhich is that obviously the approach I outlined (a set of loops each \nprocessing two arrays) does do more work than the more linear approach.  So \nthere is a compromise between gains in accessing memory faster; and losses \nin making more passes.\n\nI would expect that breaking the problem into a set of loops, each \naccessing some small number of arrays (like 2, 7, or 23) would represent \nthe best compromise between these approaches.  But only measuring it would \ntell us.\n\n--\n\nAnd finally, I deliberately neglected before to mention the other cost, \nwhich is that any process which wants to exploit coherent memory access \npatterns will have to pay the price of rearranging its data into coherent \norder.  If you can arrange to do that rearrangement rarely, then coherent \naccess will save you time, but otherwise you&#39;ll spend longer getting ready \nthan doing the work.\n\nYou really cannot second-guess these things too much.  There&#39;s certainly no \nsubstitute for timing programs with real data, and you cannot extrapolate \nfrom small data to large (from large to very-large you can do better), but \n100-item arrays will not be like 100000-itme arrays.\n\n         Ian Badcoe\n\np.s. you do know those numeric constants you wrote are far too high \nprecision?  About 12 sig-figs in the limit of double.\n\n\n&gt; &gt;Also networks was\n&gt; &gt;small, were you evaluating it over and over with different data?\n&gt;\n&gt;Yes, I call it over and over again with random numbers for each of the 10\n&gt;inputs, for a configurable amount of time, but usually 5 minutes.  So, for\n&gt;example, the fastest one would have activated it almost 136 million times.\n&gt;\n&gt;As you&#39;ll see from the code, I also dynamically calculate\n&gt;activations-per-second as I go so that I can do the activations in a tight\n&gt;loop, and only probe the clock once per second.  So the overhead for the\n&gt;timing mechanism is minimal.\n&gt;\n&gt;I could have used a larger network, but the purpose of the experiment was to\n&gt;see what effect different ways of structuring the activation code might\n&gt;have.  And this network size was more than adequate to this task.\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;Yahoo! Groups Links\n&gt;\n&gt;\n&gt;\n&gt;\n\n\n\nLiving@Home - Open Source Evolving Organisms - \nhttp://livingathome.sourceforge.net/\n\n\n\n"}}