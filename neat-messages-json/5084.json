{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Ken","from":"&quot;Ken&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"Z_W-kGvkAvosy6WAbeILUUBybkmhKva73gYWyBw8Gks6Q3S7dBldPgdFXVn2P_OyBT2BQqwSe9qA8sKMOg5YLTlUQu72","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: HyperNEAT Tutorial?","postDate":"1264226899","msgId":5084,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGhqZTNvays2Y3JrQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGhmbWRwNCtlaXFpQGVHcm91cHMuY29tPg=="},"prevInTopic":4985,"nextInTopic":5085,"prevInTime":5083,"nextInTime":5085,"topicId":4884,"numMessagesInTopic":21,"msgSnippet":"I wanted to follow up a little on the HyperNEAT tutorial issue.  We have started to update the HyperNEAT Users Page FAQ section with tutorial-like questions,","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 7893 invoked from network); 23 Jan 2010 06:08:33 -0000\r\nX-Received: from unknown (66.196.94.107)\n  by m10.grp.re1.yahoo.com with QMQP; 23 Jan 2010 06:08:33 -0000\r\nX-Received: from unknown (HELO n6-vm6.bullet.mail.sp2.yahoo.com) (67.195.135.102)\n  by mta3.grp.re1.yahoo.com with SMTP; 23 Jan 2010 06:08:33 -0000\r\nX-Received: from [67.195.134.49] by n6.bullet.mail.sp2.yahoo.com with NNFMP; 23 Jan 2010 06:08:20 -0000\r\nX-Received: from [69.147.65.151] by t2.bullet.mail.sp2.yahoo.com with NNFMP; 23 Jan 2010 06:08:20 -0000\r\nX-Received: from [98.137.34.184] by t5.bullet.mail.sp1.yahoo.com with NNFMP; 23 Jan 2010 06:08:20 -0000\r\nDate: Sat, 23 Jan 2010 06:08:19 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;hje3ok+6crk@...&gt;\r\nIn-Reply-To: &lt;hfmdp4+eiqi@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;Ken&quot; &lt;kstanley@...&gt;\r\nSubject: Re: HyperNEAT Tutorial?\r\nX-Yahoo-Group-Post: member; u=54567749; y=1HTqVuDAWovzG6R3l5_kSUKzRpnOh3eLyqOaI1tPufTDSoj9htpU\r\nX-Yahoo-Profile: kenstanley01\r\n\r\n\n\nI wanted to follow up a little on the HyperNEAT tutorial issue.  We have =\r\nstarted to update the HyperNEAT Users Page FAQ section with tutorial-like q=\r\nuestions, so it is slowly starting to fulfill some of the hopes of such a r=\r\neference.  The page is:\n\nhttp://eplex.cs.ucf.edu/hyperNEATpage/HyperNEAT.ht=\r\nml\n\nOn the left-hand side, click on &quot;HyperNEAT Methodology FAQ.&quot;\n\nHopefully=\r\n some of the answers there address common questions for beginners (or even =\r\nnot beginners) with HyperNEAT.  We do plan to expand the list over time.\n\nk=\r\nen\n\n--- In neat@yahoogroups.com, &quot;Ken&quot; &lt;kstanley@...&gt; wrote:\n&gt;\n&gt; \n&gt; \n&gt; We&#39;r=\r\ne still thinking about whether to provide a wiki or try to provide more som=\r\nething more static.  The wiki presents several possible challenges.  For ex=\r\nample, it could turn out sparse or die out if not enough people add to it. =\r\n On the other hand, it could fill up with inaccurate information or even sp=\r\nam, which would require policing.  At the moment we are considering more st=\r\natic tutorial-like introductions but a wiki is a possibility.  If people ha=\r\nve thoughts on the Wiki idea I am happy to hear them. \n&gt; \n&gt; ken\n&gt; \n&gt; --- In=\r\n neat@yahoogroups.com, Jeff Clune &lt;jclune@&gt; wrote:\n&gt; &gt;\n&gt; &gt; Hello all-\n&gt; &gt; \n=\r\n&gt; &gt; I like the idea of a tutorial, especially with respect to the many Hype=\r\nrNEAT\n&gt; &gt; (NEAT) parameters. For example, I just had a question internally =\r\nabout what\n&gt; &gt; the SpeciesSizeTarget would be, and had to search around for=\r\n a while to find\n&gt; &gt; the thread on the NEAT list where I had previously ask=\r\ned about this\n&gt; &gt; parameter (and Ken provided a helpful answer, pasted belo=\r\nw).\n&gt; &gt; \n&gt; &gt; It would be great to capture this knowledge in a wiki, instead=\r\n of just in\n&gt; &gt; this forum, and let people add their own thoughts. That way=\r\n, a researcher\n&gt; &gt; could read a paragraph or three about the parameter they=\r\n are wondering\n&gt; &gt; about, which can help them understand it and possibly op=\r\ntimize it for their\n&gt; &gt; problem. It would probably also highlight which par=\r\nameters we have not yet\n&gt; &gt; had discussions about, so people can fill in th=\r\neir own rules of thumb for\n&gt; &gt; that parameter. \n&gt; &gt; \n&gt; &gt; Here was Ken&#39;s rep=\r\nly about SpeciesSizeTarget:\n&gt; &gt; &gt;&gt;&gt; I haven&#39;t seen any explicit studies on =\r\npopulation size in NEAT\n&gt; &gt; &gt;&gt;&gt; specifically, but there have been such stud=\r\nies for genetic algorithms\n&gt; &gt; &gt;&gt;&gt; in general. I think Kenneth De Jong&#39;s te=\r\nxtbook, &quot;Evolutionary\n&gt; &gt; &gt;&gt;&gt; Computation: A Unified Perspective&quot; examines =\r\nit.\n&gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt; However, of course as people have pointed out in NEAT i=\r\nt&#39;s not just a\n&gt; &gt; &gt;&gt;&gt; question of a healthy population but of healthy spec=\r\nies, since each\n&gt; &gt; &gt;&gt;&gt; species is like its own little population. In other=\r\n words, the\n&gt; &gt; &gt;&gt;&gt; species need enough internal diversity to drive their o=\r\nwn\n&gt; &gt; &gt;&gt;&gt; explorations. My own rule of thumb has been a minimum of 15 memb=\r\ners\n&gt; &gt; &gt;&gt;&gt; per species (on average) to keep them healthy. I&#39;m sure 30 is e=\r\nven\n&gt; &gt; &gt;&gt;&gt; safer.\n&gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt; Another thing to note is that the popula=\r\ntion size you need can vary\n&gt; &gt; &gt;&gt;&gt; wildly by problem and it can be surpris=\r\ning. For pole balancing, it\n&gt; &gt; &gt;&gt;&gt; turns out that you can solve it fastest=\r\n with a minuscule population of\n&gt; &gt; &gt;&gt;&gt; size under 20 total (that&#39;s for the=\r\n whole population!). That is\n&gt; &gt; &gt;&gt;&gt; because smaller populations are greedi=\r\ner (they concentrate resources\n&gt; &gt; &gt;&gt;&gt; on a small part of the search space)=\r\n and apparently pole balancing\n&gt; &gt; &gt;&gt;&gt; benefits greatly from greedy search.=\r\n However, that is a specific\n&gt; &gt; &gt;&gt;&gt; property of pole balancing and I would=\r\n not expect it to carry over to\n&gt; &gt; &gt;&gt;&gt; more significant domains. Surely in=\r\n some domains having a population\n&gt; &gt; &gt;&gt;&gt; over 1000 would lead to the most =\r\ninteresting results, if only you can\n&gt; &gt; &gt;&gt;&gt; afford the luxury of the added=\r\n computational cost of a large population.\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; Cheers,\n=\r\n&gt; &gt; Jeff Clune\n&gt; &gt; \n&gt; &gt; Digital Evolution Lab, Michigan State University\n&gt; =\r\n&gt; jclune@\n&gt; &gt; www.msu.edu/~jclune\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; &gt; From: Ken &lt;ksta=\r\nnley@&gt;\n&gt; &gt; &gt; Reply-To: &quot;neat@yahoogroups.com&quot; &lt;neat@yahoogroups.com&gt;\n&gt; &gt; &gt; =\r\nDate: Sat, 28 Nov 2009 22:00:25 -0000\n&gt; &gt; &gt; To: &quot;neat@yahoogroups.com&quot; &lt;nea=\r\nt@yahoogroups.com&gt;\n&gt; &gt; &gt; Subject: [neat] Re: HyperNEAT Tutorial?\n&gt; &gt; &gt; \n&gt; &gt;=\r\n &gt; \n&gt; &gt; &gt; \n&gt; &gt; &gt; Anthony,\n&gt; &gt; &gt; \n&gt; &gt; &gt; We are hoping to make the HyperNEAT =\r\nUsers Page at\n&gt; &gt; &gt; http://eplex.cs.ucf.edu/hyperNEATpage/HyperNEAT.html in=\r\nto a place where people\n&gt; &gt; &gt; can have questions like yours answered.  The =\r\n&quot;Introduction / What is\n&gt; &gt; &gt; HyperNEAT?&quot; section on that page is intended =\r\nto provide some general answers\n&gt; &gt; &gt; to beginners without having to read a=\r\n research paper.  Did that section help\n&gt; &gt; &gt; you at all?  I&#39;d like to make=\r\n the page as useful as possible and we will\n&gt; &gt; &gt; continue to improve it.\n&gt;=\r\n &gt; &gt; \n&gt; &gt; &gt; To answer your question, HyperNEAT is a significant step beyond=\r\n NEAT so it\n&gt; &gt; &gt; involves a lot of new ideas that aren&#39;t part of the origi=\r\nnal NEAT.  Some of\n&gt; &gt; &gt; those concepts can theoretically be applied on top=\r\n of non-NEAT methods.  For\n&gt; &gt; &gt; example, in the following paper, NEAT is s=\r\nubstituted with GP to create a\n&gt; &gt; &gt; &quot;HyperGP,&quot; in which GP evolves the CPP=\r\nN:\n&gt; &gt; &gt; \n&gt; &gt; &gt; Buk Z., Koutn=EDk J., =8Anorek M., NEAT in HyperNEAT Substi=\r\ntuted with Genetic\n&gt; &gt; &gt; Programming, In: ICANNGA 2009.\n&gt; &gt; &gt; http://cig.fe=\r\nlk.cvut.cz/research/publications/hypergp.pdf\n&gt; &gt; &gt; \n&gt; &gt; &gt; That said, HyperN=\r\nEAT addresses a limitation of NEAT, which is a limitation of\n&gt; &gt; &gt; all dire=\r\nct encodings:  In NEAT, there is one gene for every connection in the\n&gt; &gt; &gt;=\r\n network.  Even with complexification, that kind of representation cannot h=\r\nope\n&gt; &gt; &gt; to scale to networks with millions or more connections, because s=\r\nuch networks\n&gt; &gt; &gt; would have millions or more genes, which is an astronomi=\r\ncal search space.\n&gt; &gt; &gt; \n&gt; &gt; &gt; However, there are in fact 100 trillion conn=\r\nections in the human brain, which\n&gt; &gt; &gt; means that in principle it is possi=\r\nble to evolve such structures.  Yet there\n&gt; &gt; &gt; are only about 30,000 genes=\r\n in the human genome, which suggests that any\n&gt; &gt; &gt; evolutionary approach t=\r\no evolving large-scale neural networks must encode the\n&gt; &gt; &gt; connection wei=\r\nghts in a compressed description, which is called an indirect\n&gt; &gt; &gt; encodin=\r\ng.\n&gt; &gt; &gt; \n&gt; &gt; &gt; In HyperNEAT, the indirect encoding is the CPPN, which enco=\r\ndes the\n&gt; &gt; &gt; connectivity of a network as a pattern across its geometry.  =\r\nHyperNEAT\n&gt; &gt; &gt; combines the idea of indirect encoding with a strong notion=\r\n of geometry and\n&gt; &gt; &gt; builds on our understanding of encoding patterns to =\r\nproduce an algorithm that\n&gt; &gt; &gt; encodes large-scale topographies (i.e. conn=\r\nection weights across a geometry).\n&gt; &gt; &gt; Thus it extends NEAT by giving it =\r\nthe power of indirect encoding, thereby\n&gt; &gt; &gt; greatly expanding the scope o=\r\nf networks it can evolve.\n&gt; &gt; &gt; \n&gt; &gt; &gt; Of course NEAT is still there under =\r\nthe hood.  NEAT is evolving the CPPNs,\n&gt; &gt; &gt; which in turn encode neural ne=\r\ntworks (called substrates in HyperNEAT).  The\n&gt; &gt; &gt; CPPNs themselves are st=\r\nill complexifying.  However, that complexification is\n&gt; &gt; &gt; no longer liter=\r\nally adding one connection at a time to a neural network.\n&gt; &gt; &gt; Rather it i=\r\ns adding *information* to the encoding, so that it can encode more\n&gt; &gt; &gt; co=\r\nmplex *holistic* connectivity patterns.  In other words, HyperNEAT means\n&gt; =\r\n&gt; &gt; that evolution is no longer limited by the dimensionality of the inputs=\r\n and\n&gt; &gt; &gt; outputs but rather can search for the correct implicit problem c=\r\nomplexity,\n&gt; &gt; &gt; whatever that may be, inside the CPPN.  The substrate (whi=\r\nch the CPPN encodes)\n&gt; &gt; &gt; will then have as many connections as it needs, =\r\nin principle up to millions or\n&gt; &gt; &gt; even trillions (with enough CPU power)=\r\n.\n&gt; &gt; &gt; \n&gt; &gt; &gt; It is true that geometry may be vague or difficult to decide=\r\n in some problems.\n&gt; &gt; &gt; Those may be more difficult for users to approach =\r\nwith HyperNEAT.  Yet I think\n&gt; &gt; &gt; most if not all problems can ultimately =\r\nbe posed within some geometry, even if\n&gt; &gt; &gt; it is abstract or conceptual. =\r\n As Jeff Clune has shown\n&gt; &gt; &gt; (https://www.msu.edu/~jclune/webfiles/public=\r\nations/Clune-HyperNEATSensitivityT\n&gt; &gt; &gt; oGeometry.pdf), geometry does not =\r\nneed to be perfect, or necessarily even\n&gt; &gt; &gt; close to perfect, for HyperNE=\r\nAT to still find some regularities to exploit, so\n&gt; &gt; &gt; while it may be an =\r\nimperfect art, geometry is still ultimately a useful tool\n&gt; &gt; &gt; for conveyi=\r\nng exploitable information about a domain.\n&gt; &gt; &gt; \n&gt; &gt; &gt; I hope that provide=\r\ns some insight,\n&gt; &gt; &gt; \n&gt; &gt; &gt; ken\n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; &gt; --- In neat@yahoogroup=\r\ns.com, &quot;Anthony Ison&quot; &lt;anthony.ison@&gt; wrote:\n&gt; &gt; &gt;&gt; \n&gt; &gt; &gt;&gt; I&#39;d like to add=\r\n my vote to some kind of non-research style tutorial -\n&gt; &gt; &gt;&gt; especially to=\r\nwards the HyperNEAT methodology.\n&gt; &gt; &gt;&gt; \n&gt; &gt; &gt;&gt;  \n&gt; &gt; &gt;&gt; \n&gt; &gt; &gt;&gt; Is there s=\r\nomewhere I can get an overview of how the CPPN is actually\n&gt; &gt; &gt;&gt; used and =\r\nwhat it does?  I&#39;ve looked through a number of papers on the\n&gt; &gt; &gt;&gt; main Hy=\r\nperNEAT site, but I feel like I&#39;m missing something.  I have read\n&gt; &gt; &gt;&gt; th=\r\nat HyperNEAT is the future of NEAT - does this apply to problems where\n&gt; &gt; =\r\n&gt;&gt; there is no useful input geometry?  I understand how NEAT grows a\n&gt; &gt; &gt;&gt;=\r\n network by adding nodes and connections and overall I love the concept.\n&gt; =\r\n&gt; &gt;&gt; It makes sense that a learning network can adjust itself to improve it=\r\ns\n&gt; &gt; &gt;&gt; performance.  What I don&#39;t really understand is how a CPPN is invo=\r\nlved\n&gt; &gt; &gt;&gt; in this process.\n&gt; &gt; &gt;&gt; \n&gt; &gt; &gt;&gt;  \n&gt; &gt; &gt;&gt; \n&gt; &gt; &gt;&gt; Is anyone able=\r\n to give a short overview on what HyperNEAT offers over\n&gt; &gt; &gt;&gt; NEAT?\n&gt; &gt; &gt;&gt;=\r\n \n&gt; &gt; &gt;&gt;  \n&gt; &gt; &gt;&gt; \n&gt; &gt; &gt;&gt; Cheers,\n&gt; &gt; &gt;&gt; \n&gt; &gt; &gt;&gt; Anthony\n&gt; &gt; &gt;&gt; \n&gt; &gt; &gt;&gt;  \n&gt;=\r\n &gt; &gt;&gt; \n&gt; &gt; &gt;&gt;  \n&gt; &gt; &gt;&gt; \n&gt; &gt; &gt;&gt; From: dkuppitz [mailto:daniel_kuppitz@]\n&gt; &gt; =\r\n&gt;&gt; Sent: Monday, 23 November 2009 9:08 AM\n&gt; &gt; &gt;&gt; To: neat@yahoogroups.com\n&gt;=\r\n &gt; &gt;&gt; Subject: [neat] Re: HyperNEAT Tutorial?\n&gt; &gt; &gt;&gt; \n&gt; &gt; &gt;&gt;  \n&gt; &gt; &gt;&gt; \n&gt; &gt; =\r\n&gt;&gt;   \n&gt; &gt; &gt;&gt; \n&gt; &gt; &gt;&gt; Hello Ken,\n&gt; &gt; &gt;&gt; \n&gt; &gt; &gt;&gt; here&#39;s just another vote for=\r\n a tutorial, with the difference that I\n&gt; &gt; &gt;&gt; would prefer it for HyperSha=\r\nrpNEAT.\n&gt; &gt; &gt;&gt; \n&gt; &gt; &gt;&gt; It would be great to see something like a HOL (Hands=\r\n on Labs) where a\n&gt; &gt; &gt;&gt; new experiment is created from the scratch. Parame=\r\nters should be\n&gt; &gt; &gt;&gt; explained in detail, for example: Which impact has th=\r\ne parameter\n&gt; &gt; &gt;&gt; Treshold, which impact has WeightRange, etc.? How are th=\r\ne values for\n&gt; &gt; &gt;&gt; each parameter determined, what is taken into account w=\r\nhen you set the\n&gt; &gt; &gt;&gt; values? There are so many unanswered questions for t=\r\nhose who are new to\n&gt; &gt; &gt;&gt; HyperNEAT and I think most people (including me)=\r\n have a really great\n&gt; &gt; &gt;&gt; interest in this topic, but not the time to rea=\r\nd (and understand) all\n&gt; &gt; &gt;&gt; the technical papers. So any tutorial should =\r\ntarget beginners and\n&gt; &gt; &gt;&gt; explain things that have become self-evident fo=\r\nr intermediates.\n&gt; &gt; &gt;&gt; \n&gt; &gt; &gt;&gt; I think one such &quot;official&quot; tutorial that e=\r\nxplains every step in detail\n&gt; &gt; &gt;&gt; should be enough, more will surely foll=\r\now from the growing community.\n&gt; &gt; &gt;&gt; \n&gt; &gt; &gt;&gt; Cheers,\n&gt; &gt; &gt;&gt; Daniel\n&gt; &gt; &gt;&gt; =\r\n\n&gt; &gt; &gt;&gt; --- In neat@yahoogroups.com &lt;mailto:neat%40yahoogroups.com&gt; , &quot;Ken&quot;=\r\n\n&gt; &gt; &gt;&gt; &lt;kstanley@&gt; wrote:\n&gt; &gt; &gt;&gt;&gt; \n&gt; &gt; &gt;&gt;&gt; \n&gt; &gt; &gt;&gt;&gt; \n&gt; &gt; &gt;&gt;&gt; Andrei, which=\r\n version of HyperNEAT are you interested in and what\n&gt; &gt; &gt;&gt; references have=\r\n you looked at so far? We can potentially improve the\n&gt; &gt; &gt;&gt; documentation =\r\nbased on your comments (and a tutorial is a good idea),\n&gt; &gt; &gt;&gt; but first I =\r\nwant to understand which &quot;comment-less examples&quot; you are\n&gt; &gt; &gt;&gt; referring t=\r\no.\n&gt; &gt; &gt;&gt;&gt; \n&gt; &gt; &gt;&gt;&gt; Note that several experiments with complete source code=\r\n are available\n&gt; &gt; &gt;&gt; in two existing HyperNEAT releases of which I am awar=\r\ne. These and a\n&gt; &gt; &gt;&gt; variety of publications from several groups are linke=\r\nd from the\n&gt; &gt; &gt;&gt; HyperNEAT Users Page, which also provides a brief introdu=\r\nction:\n&gt; &gt; &gt;&gt;&gt; \n&gt; &gt; &gt;&gt;&gt; http://eplex.cs.ucf.edu/hyperNEATpage/HyperNEAT.htm=\r\nl\n&gt; &gt; &gt;&gt;&gt; \n&gt; &gt; &gt;&gt;&gt; I understand you may have already been through this site=\r\n and its\n&gt; &gt; &gt;&gt; associated software and papers, but I wanted to point it ou=\r\nt in case you\n&gt; &gt; &gt;&gt; had not been aware of it.\n&gt; &gt; &gt;&gt;&gt; \n&gt; &gt; &gt;&gt;&gt; We want to =\r\nmake the algorithm as accessible as possible so your\n&gt; &gt; &gt;&gt; comments are ap=\r\npreciated.\n&gt; &gt; &gt;&gt;&gt; \n&gt; &gt; &gt;&gt;&gt; ken\n&gt; &gt; &gt;&gt;&gt; \n&gt; &gt; &gt;&gt;&gt; --- In neat@...=\r\nm &lt;mailto:neat%40yahoogroups.com&gt; , &quot;Andrei&quot;\n&gt; &gt; &gt;&gt; &lt;andrei.rusu@&gt; wrote:\n&gt;=\r\n &gt; &gt;&gt;&gt;&gt; \n&gt; &gt; &gt;&gt;&gt;&gt; Can anyone please recommend some HyperNEAT documentation,=\r\n a\n&gt; &gt; &gt;&gt; tutorial, diagram, some clue, or anything that does not mean reve=\r\nrse\n&gt; &gt; &gt;&gt; engineering the comment-less examples ?\n&gt; &gt; &gt;&gt;&gt;&gt; \n&gt; &gt; &gt;&gt;&gt;&gt; Thank=\r\ns! Andrei.\n&gt; &gt; &gt;&gt;&gt;&gt; \n&gt; &gt; &gt;&gt;&gt; \n&gt; &gt; &gt;&gt; \n&gt; &gt; &gt; \n&gt; &gt; &gt;\n&gt; &gt;\n&gt;\n\n\n\n"}}