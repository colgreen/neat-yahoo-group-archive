{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"SyN8hxWgeAY_HlGTbkvcfmp8Un1F8e5_bVGI7neRX8PtH4G82EPN2ZC82swZk8hSgDSS9ZXUDdUpHDnGNSDnfotAVu3jsoN7HUXgcb6jkBfX","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Machine Learning and the Long View of AI","postDate":"1209489214","msgId":4010,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGZ2N2t2ditxZHU4QGVHcm91cHMuY29tPg==","inReplyToHeader":"PGViNDRhMmQ3MDgwNDI4MTY1N3IzMDJmNWZiajliODljNjVmNTMzYTQ1MGFAbWFpbC5nbWFpbC5jb20+"},"prevInTopic":4009,"nextInTopic":4011,"prevInTime":4009,"nextInTime":4011,"topicId":3955,"numMessagesInTopic":49,"msgSnippet":"Daniel, I absolutely agree with your analysis.  The fitness function can indeed interfere with the discovery of overarching architectures that would be useful","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 35642 invoked from network); 29 Apr 2008 17:13:35 -0000\r\nX-Received: from unknown (66.218.67.97)\n  by m55.grp.scd.yahoo.com with QMQP; 29 Apr 2008 17:13:35 -0000\r\nX-Received: from unknown (HELO n31c.bullet.scd.yahoo.com) (66.94.237.8)\n  by mta18.grp.scd.yahoo.com with SMTP; 29 Apr 2008 17:13:35 -0000\r\nX-Received: from [66.218.69.4] by n31.bullet.scd.yahoo.com with NNFMP; 29 Apr 2008 17:13:35 -0000\r\nX-Received: from [66.218.66.82] by t4.bullet.scd.yahoo.com with NNFMP; 29 Apr 2008 17:13:35 -0000\r\nDate: Tue, 29 Apr 2008 17:13:34 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;fv7kvv+qdu8@...&gt;\r\nIn-Reply-To: &lt;eb44a2d70804281657r302f5fbj9b89c65f533a450a@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: Re: Machine Learning and the Long View of AI\r\nX-Yahoo-Group-Post: member; u=54567749; y=zFREU9a-3xRUS1hDmEm5Mfb7FvU1sN1zoG1vwuGOUuJU6WkNe9SG\r\nX-Yahoo-Profile: kenstanley01\r\n\r\nDaniel, I absolutely agree with your analysis.  The fitness function\ncan in=\r\ndeed interfere with the discovery of overarching architectures\nthat would b=\r\ne useful but only some time after they are found and\nexploited.  Still, som=\r\netimes they will be picked up and exploited\nthrough good fortune, but good =\r\nfortune is not a systematic or reliable\navenue to progress.  So I agree tha=\r\nt there is a serious problem there.\n We have an accepted paper that address=\r\nes this problem that we will be\nmaking available soon.\n\nOn your second ques=\r\ntion about complexification in the substrate:  I am\nnot sure if we are maki=\r\nng any particular assumption about how\nsubstrate configurations would be di=\r\nscovered since there does not yet\nexist a theory on the best way to evolve =\r\nthe placement of nodes in the\nsubstrate, so the question remains open.  How=\r\never, the potential to\ngradually complexify does seem like an appealing opt=\r\nion.\n\nHere is one interesting question:  If there is a problem that I can\ne=\r\nasily solve with 1,000 nodes in the substrate, would it be any harder\nfor H=\r\nyperNEAT to solve it given 10,000 nodes?  Interestingly, it may\nnot be hard=\r\ner in some cases because HyperNEAT need only discover a\ndescription of a pa=\r\nttern and that pattern can be zoomed to any\ndensity.  Yet if it is not hard=\r\ner, then there would seem to be no\nselective force favoring a smaller subst=\r\nrate.  In nature, one\nselective force that is missing from EC is resource c=\r\nonstraints, i.e.\nmatter and energy.  Is that kind of constraint one reason =\r\nour brain\ndoes not have a trillion trillion neurons?  Or is there another r=\r\neason\nthat is more computational in nature?\n\nken\n\n\n--- In neat@yahoogroups.=\r\ncom, &quot;Daniel Tuohy&quot; &lt;danielr2e@...&gt; wrote:\n&gt;\n&gt; &gt;\n&gt; &gt; Ideally, we would like=\r\n to see the substrate itself become\n&gt; &gt; increasingly complex/dense over evo=\r\nlutionary time in response to\n&gt; &gt; evolutionary pressures. But if that is go=\r\ning to happen, ideally it\n&gt; &gt; should work elegantly and seamlessly, rather =\r\nthan being ad hoc.\n&gt; \n&gt; \n&gt; So, do you suppose that complex substrates may b=\r\ne discovered without\nmaking\n&gt; explicit effort to identify them?  It seems l=\r\nike the sort of thing that\n&gt; could be really difficult if left to evolution=\r\n alone, because the\n&gt; &quot;evolutionary pressures&quot; exert influence so, and I&#39;m =\r\nnot sure this\nis the\n&gt; correct word, indirectly.  Does that make sense?  In=\r\n other words, the\n&gt; overall fitness of the individual may dominate any prog=\r\nress that\nindividuals\n&gt; may be making towards discovering useful substrates=\r\n, and thus\ndifficult to\n&gt; maintain in the population until the substrate ha=\r\ns matured.\n&gt; \n&gt; As an example, it has been found surprisingly difficult for=\r\n a GA to\nlearn\n&gt; gene linkages in parallel with evolution without extra ana=\r\nlyses. \nThat is,\n&gt; if you encode each individual with it&#39;s own linkage to u=\r\nse during\ncrossover,\n&gt; you would suspect that individuals with linkage that=\r\n accurately reflects\n&gt; epistatic interactions would be more profitable proc=\r\nreators.  This\nis true\n&gt; to a certain extent, but overall it&#39;s easy for sel=\r\nection pressure to be\n&gt; dominated by the fitness function and linkage learn=\r\ning to be stifled.\n&gt; Substrates, it seems to me, would be significantly mor=\r\ne complex than\n&gt; linkage, and perhaps unable to influence fitness strongly =\r\nenough to be\n&gt; optimized by evolution.\n&gt; \n&gt; The obvious rebuttal to what my=\r\n concern is that natural evolution\ndiscovered\n&gt; substrates just fine.  Howe=\r\nver, as you said, we don&#39;t want to wait 3\nbillion\n&gt; years, and perhaps thes=\r\ne substrates are something that we may well\nhave to\n&gt; keep an eye on during=\r\n evolution in order to preserve them and\nnurture them.\n&gt; \n&gt; And just for my=\r\n own edification, we are assuming that the substrates we\n&gt; could discover a=\r\nre only those which could be built through gradual\n&gt; complexification, righ=\r\nt?  In other words, not substrates that must\nbe fully\n&gt; formed in order to =\r\nbe useful.\n&gt; \n&gt; \n&gt; \n&gt; On Mon, Apr 28, 2008 at 7:08 PM, Jeff Clune &lt;jclune@.=\r\n..&gt; wrote:\n&gt; \n&gt; &gt;   &gt; However, I do not believe that the mutations in Hyper=\r\nNEAT are\nactually\n&gt; &gt; &gt; more destructive than those in NEAT. It is true tha=\r\nt HyperNEAT\n&gt; &gt; &gt; mutations have holistic effects, but those holistic effec=\r\nts are\n&gt; &gt; &gt; orderly, that is, it is not the equivalent of randomizing the\n=\r\nweights\n&gt; &gt; &gt; of all the connections involved. Rather, it is a warping of t=\r\nhe\n&gt; &gt; &gt; weight distribution along a dimension of regularity that was\nselec=\r\nted\n&gt; &gt; &gt; by evolution. I think there is no a priori reason to believe that=\r\n\n&gt; &gt; &gt; such changes are more or less destructive than single-weight\n&gt; &gt; &gt; m=\r\nutations, as long as the magnitude of the overall change is kept\n&gt; &gt; &gt; with=\r\nin a reasonable limit, just as with any kind of mutation.\n&gt; &gt;\n&gt; &gt; This woul=\r\nd be a very interesting study to see. Hornby did similar\n&gt; &gt; comparisons wh=\r\nen he used L-systems to evolve tables, creatures and\nneural\n&gt; &gt; nets. For t=\r\nhose of you interested in this subject, I recommend\nlooking at\n&gt; &gt; his anal=\r\nyses. That said, I know we would all be interested to see\nsimilar\n&gt; &gt; inves=\r\ntigations comparing HyperNEAT to P-NEAT, and on a few different\n&gt; &gt; problem=\r\ns to see how that affects things.\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt; ken\n&gt; &gt; &gt;\n&gt; &gt; &gt; --- I=\r\nn neat@yahoogroups.com &lt;neat%40yahoogroups.com&gt;,\n&quot;petar_chervenski&quot;\n&gt; &gt; &lt;pe=\r\ntar_chervenski@&gt;\n&gt; &gt; &gt; wrote:\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt; Great post, Ken! I really enjoy=\r\ned reading it. It is just all true.\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt; There is really a differe=\r\nnce between building a brain and evolving\n&gt; &gt; &gt;&gt; one. No matter if the brai=\r\nn learns in its lifetime or not. In the\n&gt; &gt; &gt;&gt; case you build a brain yours=\r\nelf that doesn&#39;t learn, it is just\n&gt; &gt; &gt;&gt; conventional programming at all.\n=\r\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt; I think the constraints for evolution should be very sharp, s=\r\no to\n&gt; &gt; &gt;&gt; say, because in EC in general the fitness function is the most\n=\r\n&gt; &gt; &gt;&gt; important thing as well as the representation/mapping. You\ncan&#39;t jus=\r\nt\n&gt; &gt; &gt;&gt; say &quot;be smart!&quot; to an EC algorithm. You have to model\n&gt; &gt; &gt;&gt; its &quot;=\r\nenvironment&quot; as well, and the process of evaluation usually\n&gt; &gt; &gt;&gt; takes a =\r\nlot of computation time for the most interesting problems.\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt; Th=\r\nere is a kind of.. Hm I guess I can&#39;t express myself in english\n&gt; &gt; &gt;&gt; well=\r\n. The more complex the task is, the more computation time is\n&gt; &gt; &gt;&gt; require=\r\nd for a proper evaluation.\n&gt; &gt; &gt;&gt; I am maybe not saying anything new to you=\r\n, Ken, but I just\nmention I\n&gt; &gt; &gt;&gt; understand it.\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt; HyperNEAT a=\r\nnd CPPNs in general opened up an entire new field of\n&gt; &gt; &gt;&gt; research, that =\r\nis, the evolution of mathematical compositions\n&gt; &gt; &gt;&gt; describing phenotypes=\r\n of any kind. What I think about it is, that\n&gt; &gt; &gt;&gt; mutations are mostly de=\r\nstructive to the networks, while in a\nrobotics\n&gt; &gt; &gt;&gt; experiments with dire=\r\nct representation, one weight change is\nnot that\n&gt; &gt; &gt;&gt; bad, so to say. But=\r\n change one weight of a CPPN and you get a\ntotally\n&gt; &gt; &gt;&gt; different thing. =\r\nIn HyperNEAT this is not just a minor change,\nbut a\n&gt; &gt; &gt;&gt; total change of =\r\nthe network&#39;s behaviour. So there is a great\ndeal of\n&gt; &gt; &gt;&gt; computation tim=\r\ne required to discover some concepts. In fact the\n&gt; &gt; &gt;&gt; fitness landscape =\r\nin CPPN-based evolution is totally different than\n&gt; &gt; &gt;&gt; other approaches t=\r\no the same problem.\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt; Another thing is that the geometry itself=\r\n does not provide\n&gt; &gt; &gt;&gt; information about the phenotype complexity at all.=\r\n I mean that\neven a\n&gt; &gt; &gt;&gt; network of 1000000000 connections can be generat=\r\ned by a connective\n&gt; &gt; &gt;&gt; CPPN but the bias is usually towards minimal solu=\r\ntions. I know that\n&gt; &gt; &gt;&gt; complexification is a property of the genotype sp=\r\nace, but why to\n&gt; &gt; &gt;&gt; waste computation time evaluating individuals with m=\r\nillions of\n&gt; &gt; &gt;&gt; connections that actually are bad solutions?\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;=\r\n&gt; So, you may provide the geometry to the search, but you still can&#39;t\n&gt; &gt; &gt;=\r\n&gt; provide the complexity. You need a priori that the complexity\nof the\n&gt; &gt; =\r\n&gt;&gt; substrate is big enough.\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt; That 0.2 treshold is like a hard-=\r\ncoded hack to me. It may be\nable to\n&gt; &gt; &gt;&gt; represent any kind of connectivi=\r\nty, but I think the effort for\n&gt; &gt; &gt;&gt; discovering it is bigger than discove=\r\nring the actual\nregularities at\n&gt; &gt; &gt;&gt; all.\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt; There should be a=\r\n way to map complexity of the genotype to the\n&gt; &gt; &gt;&gt; phenotype, but not in =\r\nsuch a constrained way. It should be\n&gt; &gt; &gt;&gt; increasing. Did you ever see an=\r\n animal as simple as a worm but\nbig as\n&gt; &gt; &gt;&gt; a whale? OK size doesn&#39;t matt=\r\ner. :) This comparison was not a good\n&gt; &gt; &gt;&gt; one.\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt; I know ther=\r\ne is an option that HyperNEAT can evolve the\nsubstrate by\n&gt; &gt; &gt;&gt; itself, bu=\r\nt how to control it? The dynamics of the neural networks\n&gt; &gt; &gt;&gt; has to be t=\r\naken into account.\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt; Sorry about my scattered around thoughts. =\r\nThat was just a stream of\n&gt; &gt; &gt;&gt; conciosness.\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt; Peter\n&gt; &gt; &gt;&gt;\n&gt; =\r\n&gt; &gt;&gt; --- In neat@yahoogroups.com &lt;neat%40yahoogroups.com&gt;, &quot;Kenneth\nStanley=\r\n&quot;\n&gt; &gt; &lt;kstanley@&gt; wrote:\n&gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt; --- In neat@yahoogroups.com &lt;neat%=\r\n40yahoogroups.com&gt;, &quot;Derek\nJames&quot;\n&gt; &gt; &lt;djames@&gt; wrote:\n&gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt;&gt;\n&gt; &gt;=\r\n &gt;&gt;&gt;&gt;&gt; In RL, in contrast, the long view is almost the opposite: They\n&gt; &gt; &gt;=\r\n&gt;&gt; want to\n&gt; &gt; &gt;&gt;&gt;&gt;&gt; remove all constraints and still learn nevertheless.\n&gt;=\r\n &gt; &gt;&gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt;&gt; I&#39;m not sure what you mean by this, Ken. Could you elabora=\r\nte a\n&gt; &gt; &gt;&gt; little?\n&gt; &gt; &gt;&gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt; Sure. I think the problem is t=\r\nhat I can&#39;t find a way to explain my\n&gt; &gt; &gt;&gt;&gt; point concisely. As I try to e=\r\nxplain it, it starts taking up too\n&gt; &gt; &gt;&gt; much\n&gt; &gt; &gt;&gt;&gt; text so I shorten it=\r\n and then it loses its meaning. Let me give it\n&gt; &gt; &gt;&gt; a\n&gt; &gt; &gt;&gt;&gt; try again..=\r\n.\n&gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt; I think the difference between the goals of RL and NE is =\r\nan\n&gt; &gt; &gt;&gt;&gt; interesting topic because they are almost always conflated, as i=\r\nf\n&gt; &gt; &gt;&gt; they\n&gt; &gt; &gt;&gt;&gt; are trying to solve the same problem.\n&gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt;=\r\n The RL community (e.g. value-function approaches) is trying to\nbuild\n&gt; &gt; &gt;=\r\n&gt;&gt; something that learns like a natural brain. They are saying,\n&gt; &gt; &gt;&gt; thro=\r\nugh\n&gt; &gt; &gt;&gt;&gt; analytic means we can deduce how a brain can learn from sparse\n=\r\n&gt; &gt; &gt;&gt;&gt; reinforcement and formalize that process in an algorithm. The\n&gt; &gt; &gt;=\r\n&gt; hope, I\n&gt; &gt; &gt;&gt;&gt; would think, is to eventually build the &quot;general intellig=\r\nence&quot;\nthat\n&gt; &gt; &gt;&gt;&gt; aligns with the holy grail of AI. So each step along the=\r\n way is an\n&gt; &gt; &gt;&gt;&gt; improvement in that general ability.\n&gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt; So =\r\nif that is your goal, then the benchmarks you choose have to be\n&gt; &gt; &gt;&gt;&gt; des=\r\nigned to measure progress to that goal. So what they need to do\n&gt; &gt; &gt;&gt; is\n&gt;=\r\n &gt; &gt;&gt;&gt; show that their designed intelligence can work largely\nindependently=\r\n\n&gt; &gt; &gt;&gt;&gt; of a priori &quot;cheats&quot; that provide the meat of the solution.\n&gt; &gt; &gt;&gt;=\r\n Because,\n&gt; &gt; &gt;&gt;&gt; after all, how can it be a general intelligence if it nee=\r\nds you to\n&gt; &gt; &gt;&gt;&gt; tell it something that it is supposed to be able to figur=\r\ne out?\n&gt; &gt; &gt;&gt; This\n&gt; &gt; &gt;&gt;&gt; perspective, I believe, is aligned with Jeff&#39;s v=\r\niew.\n&gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt; However, NE as a long-term pursuit is involved in some=\r\nthing\n&gt; &gt; &gt;&gt; different,\n&gt; &gt; &gt;&gt;&gt; even though it can be applied to the same p=\r\nroblems. NE is not an\n&gt; &gt; &gt;&gt;&gt; attempt to formalize how people learn with sp=\r\narse reinforcement.\n&gt; &gt; &gt;&gt;&gt; Rather, it is an attempt to formalize how evolu=\r\ntion can build a\n&gt; &gt; &gt;&gt; brain.\n&gt; &gt; &gt;&gt;&gt; So RL is formalizing the brain itsel=\r\nf and NE is formalizing how\n&gt; &gt; &gt;&gt;&gt; evolution succeeds in creating a brain.=\r\n NE is therefore one step\n&gt; &gt; &gt;&gt; removed.\n&gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt; This difference i=\r\ns ultimately a philosophical difference on\nthe best\n&gt; &gt; &gt;&gt;&gt; approach to cre=\r\nating a full-blown AI. The instrumental issue is\n&gt; &gt; &gt;&gt;&gt; whether you think =\r\nit&#39;s easier to build it yourself or to design an\n&gt; &gt; &gt;&gt;&gt; algorithm that can=\r\n build it. The confusion and hence conflation of\n&gt; &gt; &gt;&gt;&gt; the two approaches=\r\n arises in part because they do indeed both\naim at\n&gt; &gt; &gt;&gt;&gt; the same long vi=\r\new goal: a general AI. But they are coming at it\n&gt; &gt; &gt;&gt; from\n&gt; &gt; &gt;&gt;&gt; very d=\r\nifferent angles.\n&gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt; And because of this stark difference, the =\r\n*metric* of progress\n&gt; &gt; &gt;&gt; should\n&gt; &gt; &gt;&gt;&gt; be quite different. We cannot me=\r\nasure our progress in building a\n&gt; &gt; &gt;&gt;&gt; general intelligence directly in t=\r\nhe same way that we measure our\n&gt; &gt; &gt;&gt;&gt; progress in creating an evolutionar=\r\ny algorithm that itself will\n&gt; &gt; &gt;&gt;&gt; someday output one.\n&gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt; Th=\r\nis distinction is potentially subtle and confusing so let me try\n&gt; &gt; &gt;&gt; to\n=\r\n&gt; &gt; &gt;&gt;&gt; make it clearer: Human brains aren&#39;t designed to build yet more\n&gt; &gt;=\r\n &gt;&gt; human\n&gt; &gt; &gt;&gt;&gt; brains. We are good at a lot of things, and we learn\ngene=\r\nrally, but\n&gt; &gt; &gt;&gt;&gt; we do not build 100-trillion part devices that are more\n=\r\ncomplex than\n&gt; &gt; &gt;&gt;&gt; any known object in the universe. I&#39;m not saying we wo=\r\nn&#39;t ever be\n&gt; &gt; &gt;&gt;&gt; able to do it, but if you want to simulate a human brai=\r\nn, your\nfirst\n&gt; &gt; &gt;&gt;&gt; thought would not be that it needs to be capable of d=\r\nesigning yet\n&gt; &gt; &gt;&gt;&gt; another brain by itself. Your first thought is about t=\r\nhings like\n&gt; &gt; &gt;&gt;&gt; object recognition or pursuit and evasion.\n&gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt;=\r\n&gt;&gt; In contrast, building brains is exactly what natural evolution\ndid,\n&gt; &gt; =\r\n&gt;&gt;&gt; and it did it quite well. Natural evolution does not perform\nobject\n&gt; &gt;=\r\n &gt;&gt;&gt; recognition; it does not communicate with language; it does\nnot run\n&gt; =\r\n&gt; &gt;&gt;&gt; away from predators or hunt for prey. Yet it does build brains\nthat\n&gt;=\r\n &gt; &gt;&gt;&gt; themselves do those things. And that is the aspect of it we\nwish to\n=\r\n&gt; &gt; &gt;&gt;&gt; harness- a very specific niche kind of skill (though radically\n&gt; &gt; =\r\n&gt;&gt;&gt; impressive)- not a general skill.\n&gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt; So the two pursuits a=\r\nre really quite different. And therefore they\n&gt; &gt; &gt;&gt;&gt; deserve different met=\r\nrics to judge their progress with respect to\n&gt; &gt; &gt;&gt; the\n&gt; &gt; &gt;&gt;&gt; long term g=\r\noal. That is, unless we conflate them to be the same\n&gt; &gt; &gt;&gt;&gt; thing, which w=\r\ne often do without thinking about it.\n&gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt; For example, we could=\r\n just say, well, both NE and RL are learning\n&gt; &gt; &gt;&gt;&gt; techniques, and after =\r\nall, we can apply them to the same problems,\n&gt; &gt; &gt;&gt; so\n&gt; &gt; &gt;&gt;&gt; why make a b=\r\nig distinction in how we judge them? Let&#39;s just\ncompare\n&gt; &gt; &gt;&gt;&gt; them direct=\r\nly on the same benchmarks and get on with it.\n&gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt; That&#39;s fine f=\r\nor the short-term view, i.e. let&#39;s just improve our\n&gt; &gt; &gt;&gt;&gt; ability to tack=\r\nle practical problems, but for the long view, they\n&gt; &gt; &gt;&gt;&gt; cannot be judged=\r\n in the same way. If I improve at my ability to\n&gt; &gt; &gt;&gt;&gt; balance on one foot=\r\n is that a sign that I will be able to build a\n&gt; &gt; &gt;&gt;&gt; brain someday? If ev=\r\nolution evolves a brain that plays\ncheckers, is\n&gt; &gt; &gt;&gt;&gt; that a sign that ev=\r\nolution *itself* is on the road to performing\n&gt; &gt; &gt;&gt;&gt; object recognition? T=\r\nhese are totally different pursuits.\n&gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt; So in that context, ho=\r\nw should they be judged with respect to long\n&gt; &gt; &gt;&gt;&gt; term goals? Well, I th=\r\nink RL deserves to be judged based on its\n&gt; &gt; &gt;&gt;&gt; increasing ability to lea=\r\nrn more generally. And in that sense,\n&gt; &gt; &gt;&gt;&gt; exactly Jeff&#39;s criteria shoul=\r\nd apply to it: We should be\ninterested\n&gt; &gt; &gt;&gt; in\n&gt; &gt; &gt;&gt;&gt; whether it &quot;needs&quot;=\r\n a priori information to learn. In other words,\n&gt; &gt; &gt;&gt; the\n&gt; &gt; &gt;&gt;&gt; less we =\r\nneed to constrain the problem for the learner, the more\n&gt; &gt; &gt;&gt;&gt; impressed w=\r\ne deserve to be. That shows progress towards more and\n&gt; &gt; &gt;&gt; more\n&gt; &gt; &gt;&gt;&gt; g=\r\neneral AI and ML.\n&gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt; But if evolution is not *itself* supposed=\r\n to be a general learner\n&gt; &gt; &gt;&gt;&gt; (rather, we just want it to concentrate on=\r\n one very specific\nskill:\n&gt; &gt; &gt;&gt;&gt; brain building), then those consideration=\r\ns are orthogonal to its\n&gt; &gt; &gt;&gt;&gt; greatest promise. Its promise is to evolve =\r\na brain itself, and as\n&gt; &gt; &gt;&gt;&gt; such, neuroevolutionary algorithms deserve t=\r\no be judged on our\n&gt; &gt; &gt;&gt; ability\n&gt; &gt; &gt;&gt;&gt; to *constrain* the problem so tha=\r\nt they can accomplish exactly\n&gt; &gt; &gt;&gt; that.\n&gt; &gt; &gt;&gt;&gt; In other words, the prob=\r\nlem NE *algorithms* face is leaps and\nbounds\n&gt; &gt; &gt;&gt;&gt; beyond what RL algorit=\r\nhms face. RL algorithms just need to be able\n&gt; &gt; &gt;&gt; to\n&gt; &gt; &gt;&gt;&gt; do as well a=\r\ns brains; NE has to be able to discover brains\n&gt; &gt; &gt;&gt; themselves.\n&gt; &gt; &gt;&gt;&gt; T=\r\nherefore, progress is NE should in part be measured with respect\n&gt; &gt; &gt;&gt; to\n=\r\n&gt; &gt; &gt;&gt;&gt; progress in constraining the problem to make such a discovery more\n=\r\n&gt; &gt; &gt;&gt;&gt; likely. When an NE algorithm is improved to allow us to tell it\n&gt; &gt;=\r\n &gt;&gt; more\n&gt; &gt; &gt;&gt;&gt; about the world in which its output will be situated, that=\r\n is good\n&gt; &gt; &gt;&gt;&gt; news for the long view. In short, we don&#39;t care at all how=\r\n NE\n&gt; &gt; &gt;&gt;&gt; produced a brain as long as it really does. Will anyone complai=\r\nn\n&gt; &gt; &gt;&gt; if a\n&gt; &gt; &gt;&gt;&gt; human brain pops out of a system that was a priori gi=\r\nven the\nconcept\n&gt; &gt; &gt;&gt;&gt; of symmetry? Rather, we should be glad that such a =\r\npriori context\n&gt; &gt; &gt;&gt; was\n&gt; &gt; &gt;&gt;&gt; possible to provide in the first place, b=\r\necause it may have saved\n&gt; &gt; &gt;&gt; us a\n&gt; &gt; &gt;&gt;&gt; year of wasted computation in =\r\nfiguring it out needlessly.\n&gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt; This distinction is almost comp=\r\nletely ignored when NE and RL are\n&gt; &gt; &gt;&gt;&gt; compared directly. Therefore, the=\r\n implications of any such\n&gt; &gt; &gt;&gt; comparison\n&gt; &gt; &gt;&gt;&gt; are fuzzy and lacking c=\r\nontext with respect to the long view. I am\n&gt; &gt; &gt;&gt; not\n&gt; &gt; &gt;&gt;&gt; sure if I sho=\r\nuld care or not if RL solves something better\nthan NE,\n&gt; &gt; &gt;&gt; or\n&gt; &gt; &gt;&gt;&gt; vi=\r\nce versa, because the author doesn&#39;t explain how the result\naligns\n&gt; &gt; &gt;&gt;&gt; =\r\nwith the long-term goals of the fields. Long term goals seem like\n&gt; &gt; &gt;&gt;&gt; u=\r\nnwelcome guests these days in AI, which is why I probably\nwon&#39;t be\n&gt; &gt; &gt;&gt;&gt; =\r\nwriting about any of this in a publication any time soon.\n&gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt; .=\r\n..\n&gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt; So Derek what you are saying about NE being good at &quot;har=\r\nd-wired&quot;\n&gt; &gt; &gt;&gt;&gt; solutions and RL being appropriate for ontogenetic lifetim=\r\ne\n&gt; &gt; &gt;&gt; learning,\n&gt; &gt; &gt;&gt;&gt; while true, is not what I think of as the primar=\r\ny long-view issue.\n&gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt; In the long view, NE will be used to evo=\r\nlve structures that do\nlearn\n&gt; &gt; &gt;&gt;&gt; over their lifetime, i.e. not hardwire=\r\nd at all. The only reason\n&gt; &gt; &gt;&gt; that\n&gt; &gt; &gt;&gt;&gt; it tends to be used to evolve=\r\n hardwired solutions today is because\n&gt; &gt; &gt;&gt; we\n&gt; &gt; &gt;&gt;&gt; are trying to get a=\r\n foothold on how to evolve certain types of\n&gt; &gt; &gt;&gt; complex\n&gt; &gt; &gt;&gt;&gt; structur=\r\nes. Once we get very good at it, focus will naturally\n&gt; &gt; &gt;&gt; shift\n&gt; &gt; &gt;&gt;&gt; =\r\nto evolving dynamic brains (and of course there is already work\n&gt; &gt; &gt;&gt; alon=\r\ng\n&gt; &gt; &gt;&gt;&gt; these lines today, much from Floreano). I do not even think\nthat =\r\nwe\n&gt; &gt; &gt;&gt;&gt; will need to include stock learning algorithms like Hebbian\n&gt; &gt; =\r\n&gt;&gt; learning.\n&gt; &gt; &gt;&gt;&gt; When we achieve our long-term goals, those *themselves=\r\n* will be\n&gt; &gt; &gt;&gt; left\n&gt; &gt; &gt;&gt;&gt; up to evolution because after all there may b=\r\ne something even\n&gt; &gt; &gt;&gt; better.\n&gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt;&gt;&gt; My aim is to design an\n&gt; =\r\n&gt; &gt;&gt;&gt;&gt;&gt; algorithm that will output a brain, not to design the brain\n&gt; &gt; &gt;&gt; =\r\nitself.\n&gt; &gt; &gt;&gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt;&gt; But what kind of brain are you wanting to output?=\r\n\n&gt; &gt; &gt;&gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt; Note that I&#39;m speaking purely about the long view=\r\n for these\n&gt; &gt; &gt;&gt; different\n&gt; &gt; &gt;&gt;&gt; fields here. Of course on a day-to-day =\r\nbasis I am not solely\n&gt; &gt; &gt;&gt; focused\n&gt; &gt; &gt;&gt;&gt; on what will happen 100 years =\r\nfrom now. On a practical day-to-day\n&gt; &gt; &gt;&gt;&gt; basis, of course I want to make=\r\n NE better capable to tackle\nproblems\n&gt; &gt; &gt;&gt;&gt; that e.g. RL tackles. So in t=\r\nhe short-term context, I just want to\n&gt; &gt; &gt;&gt;&gt; output something that works f=\r\nor the problem at hand.\n&gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt; But in the long view, which we were=\r\n talking about, I think the\n&gt; &gt; &gt;&gt;&gt; ultimate goal would be to output a full=\r\n-fledged adaptive\nsystem with\n&gt; &gt; &gt;&gt;&gt; astronomical complexity and the power=\r\n and subtlety of human\n&gt; &gt; &gt;&gt; reasoning.\n&gt; &gt; &gt;&gt;&gt; On that path, constraint i=\r\ns the only hope, unless you want to wait\n&gt; &gt; &gt;&gt;&gt; three billion years and ju=\r\nst hope in the meantime that the initial\n&gt; &gt; &gt;&gt;&gt; conditions were set up cor=\r\nrectly. Therefore, demonstrations of the\n&gt; &gt; &gt;&gt;&gt; power of constraint deserv=\r\ne to be judged as evidence of the\npromise\n&gt; &gt; &gt;&gt; of\n&gt; &gt; &gt;&gt;&gt; and progress to=\r\nwards the long term goal in NE.\n&gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt; ken\n&gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;\n&gt; =\r\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;  \n&gt; &gt;\n&gt; \n&gt; \n&gt; \n&gt; -- \n&gt; Daniel Tuohy (danielr2e@...)\n&gt; Artificia=\r\nl Intelligence Software Engineer\n&gt; Stottler Henke\n&gt; 404-314-8467\n&gt;\n\n\n\n"}}