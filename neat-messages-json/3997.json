{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"tcqh67J4SSrq8KfQNCxx8XGF1PTKTviSsRE8iztwIHJJ_7v0JWTDeJ54kP9M73yiak303w1MGgExqhAc-gy-PkXKKQRaCpo-Fyflr1ygSZpu","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Machine Learning and the Long View of AI","postDate":"1209255280","msgId":3997,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGZ2MGdoaCtsb3BjQGVHcm91cHMuY29tPg==","inReplyToHeader":"PEM0MzkxMEI0LjIyNzUxJWpjbHVuZUBtc3UuZWR1Pg=="},"prevInTopic":3996,"nextInTopic":3998,"prevInTime":3996,"nextInTime":3998,"topicId":3955,"numMessagesInTopic":49,"msgSnippet":"Jeff, I figure it will be interesting to respond more generally to your comments in the context of the long view argument, which you ... It sounds like you","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 81591 invoked from network); 27 Apr 2008 00:14:43 -0000\r\nX-Received: from unknown (66.218.67.95)\n  by m54.grp.scd.yahoo.com with QMQP; 27 Apr 2008 00:14:43 -0000\r\nX-Received: from unknown (HELO n38a.bullet.mail.sp1.yahoo.com) (66.163.168.132)\n  by mta16.grp.scd.yahoo.com with SMTP; 27 Apr 2008 00:14:43 -0000\r\nX-Received: from [216.252.122.219] by n38.bullet.mail.sp1.yahoo.com with NNFMP; 27 Apr 2008 00:14:43 -0000\r\nX-Received: from [66.218.69.4] by t4.bullet.sp1.yahoo.com with NNFMP; 27 Apr 2008 00:14:43 -0000\r\nX-Received: from [66.218.67.197] by t4.bullet.scd.yahoo.com with NNFMP; 27 Apr 2008 00:14:43 -0000\r\nDate: Sun, 27 Apr 2008 00:14:40 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;fv0ghh+lopc@...&gt;\r\nIn-Reply-To: &lt;C43910B4.22751%jclune@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: Re: Machine Learning and the Long View of AI\r\nX-Yahoo-Group-Post: member; u=54567749; y=8PixzOIKjRXD6ER0HLJjsJfggWyp5OhIMUMT-RbdaPWTQCTgNAPe\r\nX-Yahoo-Profile: kenstanley01\r\n\r\nJeff, I figure it will be interesting to respond more generally to\nyour com=\r\nments in the context of the &quot;long view&quot; argument, which you\nput this way:\n\n=\r\n&gt; But if I was taking the long view,\n&gt; I might be interested in seeing whet=\r\nher we could automate the process \n&gt; of it discovering such heuristics with=\r\nout them being manually input. I \n&gt; think most will agree that every time w=\r\ne figure out these deep issues \n&gt; of how to get the system to do the learni=\r\nng on its own, we take a \n&gt; necessary step towards general AI. I guess that=\r\n is why I think these \n&gt; hard problems are part of the deep work in AI rese=\r\narch.\n\nIt sounds like you see the long view as a process in which we\ngradua=\r\nlly figure out how to make the AI more and more general.\n\nBut is that a mis=\r\nmatch to evolutionary computation?  I feel like\nthat&#39;s more the long view o=\r\nf reinforcement learning (RL) than\nneuroevolution.\n\nIn NE, our long view se=\r\nems more to be about harnessing the power of\nevolution, and evolutionary &quot;l=\r\nearning&quot; is quite different and quite a\nbit less general than human learnin=\r\ng.  \n\nThe place they intersect is at the idea that we might someday evolve =\r\na\nbrain that itself learns.  But that long view would require a\ndifferent s=\r\net of steps and focuses.  In particular, the evolutionary\nlong view is pred=\r\nicated most on constraining/biasing the search.  In\nother words, we want to=\r\n constrain evolution sufficiently so that it\ncan find a brain.\n\nIn RL, in c=\r\nontrast, the long view is almost the opposite: They want to\nremove all cons=\r\ntraints and still learn nevertheless.  \n\nBut for us in NE, we are all about=\r\n constraint because the purity of\nour result is in its output: a brain.  Th=\r\nus a &quot;hack&quot; or shortcut has\nquite different meanings in the two contexts.  =\r\nFor us, what should\nalways be pure is that a neural network with no other c=\r\nheats comes out\nat the end that does what we want.  Someday it may be a gen=\r\neral-AI\nbrain.  But whatever information came in the front end of evolution=\r\n to\npop out that brain at the end isn&#39;t important (that is, in the long\nvie=\r\nw; short-term goals may be different and more practical in nature).\n\nSo per=\r\nhaps some conflation is happening, as if we want a little of one\npath and a=\r\n little of the other at the same time: &quot;Show me how humans\nlearn but make e=\r\nvolution work better using the lessons.&quot;  I believe\nthere is indeed some ro=\r\nom for such serendipity (such as in lessons on\nrepresentation), but at some=\r\n point it starts to fracture with the\ndivergence between the two divergent =\r\naims.  My aim is to design an\nalgorithm that will output a brain, not to de=\r\nsign the brain itself.\n\nken\n\n\n--- In neat@yahoogroups.com, Jeff Clune &lt;jclu=\r\nne@...&gt; wrote:\n&gt;\n&gt; Ken, one issue is what the goal of the research is. If o=\r\nne is trying\nto get\n&gt; the best controller for the fleet of ships in the nea=\r\nr future, all\nshortcuts\n&gt; are fine. But if one is trying to test whether a =\r\nnew algorithm is\ngood at\n&gt; discovering and exploiting regularities in the e=\r\nnvironment, then it\nmight be\n&gt; interesting to present the algorithm with a =\r\nproblem that is easily\ndivided\n&gt; and see whether it figures that out. If it=\r\n can&#39;t, we can then move on to\n&gt; trying to improve it. This sort of deep re=\r\nsearch will then yield payoffs\n&gt; when the algorithm comes across regulariti=\r\nes we did not envision and\ntell it\n&gt; how to exploit. In checkers, for examp=\r\nle, there are all sorts of\n&gt; regularities in the game we *hope* HyperNEAT i=\r\ns exploiting. But the fact\n&gt; that Dave&#39;s non-r(x) treatments did so poorly =\r\nmight suggest that it\nis not,\n&gt; in fact, doing a good job of exploiting man=\r\ny of those regularities.\n&gt; \n&gt; If I had a decent go player, and I wanted it =\r\nto be as good as\npossible for a\n&gt; match next week, I&#39;d probably hack in cer=\r\ntain heuristics that it\ndoes not\n&gt; seem to be able to figure out on its own=\r\n. But if I was taking the\nlong view,\n&gt; I might be interested in seeing whet=\r\nher we could automate the\nprocess of it\n&gt; discovering such heuristics witho=\r\nut them being manually input. I\nthink most\n&gt; will agree that every time we =\r\nfigure out these deep issues of how to\nget the\n&gt; system to do the learning =\r\non its own, we take a necessary step towards\n&gt; general AI. I guess that is =\r\nwhy I think these hard problems are part\nof the\n&gt; deep work in AI research.=\r\n\n&gt; \n&gt; I do agree that it is also important to be able to inject knowledge\na=\r\nnd have\n&gt; the algorithm use it. So, I am in no way discounting the approach=\r\n you\n&gt; champion. However, I also think there is plenty of merit in trying t=\r\no\n&gt; improve the ability of these algorithms to exploit regularities in the\n=\r\n&gt; environment on their own. To me it seems disconcerting that they cannot\n&gt;=\r\n discover such regularities easily, and it would be worthwhile to see\nif we=\r\n\n&gt; can improve them in this regard.  I guess I also don&#39;t see the\nproblem a=\r\ns so\n&gt; daunting that we can&#39;t currently address it. For example, isn&#39;t your=\r\n\nfitness\n&gt; sharing diversity technique, which you call &#39;speciation&#39;, one wa=\r\ny of\n&gt; facilitating modifications to underlying bauplans? Such approaches\nm=\r\nake it\n&gt; easier for a better upstream division of the problem to survive,\nb=\r\necause it\n&gt; allows some time for that improved bauplan to realize the fitne=\r\nss\nbenefits\n&gt; associated with that more intelligent subdivision of the prob=\r\nlem.\n&gt; \n&gt; \n&gt; \n&gt; Cheers,\n&gt; Jeff Clune\n&gt; \n&gt; Digital Evolution Lab, Michigan S=\r\ntate University\n&gt; \n&gt; jclune@...\n&gt; \n&gt; \n&gt; \n&gt; \n&gt; &gt; From: Kenneth Stanley &lt;ksta=\r\nnley@...&gt;\n&gt; &gt; Reply-To: &quot;neat@yahoogroups.com&quot; &lt;neat@yahoogroups.com&gt;\n&gt; &gt; D=\r\nate: Thu, 24 Apr 2008 19:50:23 -0000\n&gt; &gt; To: &quot;neat@yahoogroups.com&quot; &lt;neat@y=\r\nahoogroups.com&gt;\n&gt; &gt; Subject: [neat] Re: Another New Paper:  Multiagent Hype=\r\nrNEAT\n&gt; &gt; \n&gt; &gt; Jeff, perhaps the issue is partly a matter of degree: I agre=\r\ne that we\n&gt; &gt; want the algorithm to discover on its own the regularities in=\r\n the\n&gt; &gt; geometry (and HyperNEAT is designed to be able to do so), but at s=\r\nome\n&gt; &gt; point, it becomes almost pedantic to withhold such information when=\r\n it\n&gt; &gt; is at the most very basic core of the problem description.  For\n&gt; &gt;=\r\n example, it does not seem rational to me to pose a multiagent problem\n&gt; &gt; =\r\nto a learning algorithm and ask that the learner *figure out* that the\n&gt; &gt; =\r\nproblem is multiagent.  In effect, that is how I would interpret\n&gt; &gt; starti=\r\nng HyperNEAT without the r(x) repeating frame.\n&gt; &gt; \n&gt; &gt; It&#39;s like saying, f=\r\nirst you have to discover what kind of problem this\n&gt; &gt; is, then you can go=\r\n on and figure out how to solve it.  Or, it is like\n&gt; &gt; giving a person a k=\r\neyboard to control of a fleet of ships, but not\n&gt; &gt; telling them that in fa=\r\nct different keys on the keyboard are assigned\n&gt; &gt; to different ships, or e=\r\nven that that there is more than one ship in\n&gt; &gt; the first place! (All the =\r\nperson finds out is who gets killed in the\n&gt; &gt; end and how badly.)  I&#39;m not=\r\n sure even a great general intelligence\n&gt; &gt; should be expected to solve suc=\r\nh an unreasonable problem in any nice\n&gt; &gt; amount of time?\n&gt; &gt; \n&gt; &gt; I do see=\r\n that you can always argue that we should want our algorithm\n&gt; &gt; to be able=\r\n to do everything, including that.  I think that&#39;s actually\n&gt; &gt; a more inte=\r\nresting subject, because it&#39;s about the grand goals of AI:\n&gt; &gt; Are we tryin=\r\ng to create a general intelligence that precludes us from\n&gt; &gt; needing to pr=\r\novide any prior information?\n&gt; &gt; \n&gt; &gt; And I think the answer is yes.  So I =\r\nagree with you.  So then, you\n&gt; &gt; ask, how can I believe that and then be a=\r\ndvocating systems where we\n&gt; &gt; provide prior information?\n&gt; &gt; \n&gt; &gt; The answ=\r\ner to that is question is that I think you are seeing progress\n&gt; &gt; moving i=\r\nn a different order than I see it moving.  I believe that the\n&gt; &gt; most ambi=\r\ntious general intelligence is far far away, and is basically\n&gt; &gt; something =\r\nlike human intelligence: Not the kind of thing you just get\n&gt; &gt; by improvin=\r\ng your encoding or diversity.  It&#39;s more like an\n&gt; &gt; astronomical paradigm =\r\nshift from what we have now.\n&gt; &gt; \n&gt; &gt; Yet I am still genuinely trying to ma=\r\nke steps down that road.  And I\n&gt; &gt; believe it will only be possible to cre=\r\nate that general intelligence\n&gt; &gt; with an algorithm that *itself* can lever=\r\nage every bit of prior\n&gt; &gt; information that could possibly be useful.  In o=\r\nther words, I am\n&gt; &gt; working with my students on the algorithm that will cr=\r\neate the AI\n&gt; &gt; someday.  In contrast, you are seeing our algorithm as a ca=\r\nndidate for\n&gt; &gt; that AI itself.  That is, HyperNEAT is not an embryonic for=\r\nm of\n&gt; &gt; general AI; rather, it is an embryonic form of an algorithm that w=\r\nill\n&gt; &gt; eventually generate a general AI.\n&gt; &gt; \n&gt; &gt; For example, if I could =\r\ncreate a world champion Go player, it would\n&gt; &gt; not matter how much informa=\r\ntion I stuffed in at the start.  The point\n&gt; &gt; of the exercise would be to =\r\nachieve my ultimate goal, not to satisfy a\n&gt; &gt; particular pedagogical crite=\r\nrion for purist learning.  In the same way\n&gt; &gt; , if I can create a human-le=\r\nvel intelligence (which itself is a\n&gt; &gt; general learner), it does not matte=\r\nr either whether I stuff in some\n&gt; &gt; regularities from the start or not.  T=\r\nhe more I give myself the\n&gt; &gt; capability to do that, the better.  Does anyo=\r\nne really believe that\n&gt; &gt; general AI can be most effectively evolved by *l=\r\nimiting* the amount of\n&gt; &gt; prior information?\n&gt; &gt; \n&gt; &gt; So the ability to pr=\r\novide a priori context is in my view directly on\n&gt; &gt; the track to the ultim=\r\nate goal.  In fact, I believe the universe\n&gt; &gt; provides this type of inform=\r\nation implicitly, which is one reason\n&gt; &gt; evolution did succeed as it did. =\r\n For example, the brain *does* exist\n&gt; &gt; in physical geometry that is the s=\r\name geometry as that which is\n&gt; &gt; outside the brain, which thereby provided=\r\n a strong bias to the kinds\n&gt; &gt; of structures it contains being correlated =\r\nto the real world.  In this\n&gt; &gt; way, the universe is not without its biases=\r\n, and we hardly fault it\n&gt; &gt; for cheating to reach its grandest achievement=\r\ns.\n&gt; &gt; \n&gt; &gt; In any case, on the road the general AI, any shortcut is fair g=\r\name.\n&gt; &gt; The goal should be to find ways to provide all kinds of dramatic\n&gt;=\r\n &gt; shortcuts, not to weed them out.\n&gt; &gt; \n&gt; &gt; ken\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; --- In neat@=\r\nyahoogroups.com, Jeff Clune &lt;jclune@&gt; wrote:\n&gt; &gt;&gt; \n&gt; &gt;&gt;&gt; Perhaps one way to=\r\n explain why we thought about that first is to\n&gt; &gt;&gt;&gt; consider that one impo=\r\nrtant philosophical motivation behind\nHyperNEAT\n&gt; &gt;&gt;&gt; is that machine learn=\r\ning needs a way for humans to convey to the\n&gt; &gt;&gt;&gt; learner a priori known do=\r\nmain geometry.  In effect, we are running\n&gt; &gt;&gt;&gt; away from the black box of =\r\nNo Free Lunch (which is a nasty trap) by\n&gt; &gt;&gt;&gt; finding new ways to convey c=\r\nritical a priori domain information.\n&gt; &gt;&gt;&gt; While arguments can be made that=\r\n because certain techniques\nalign with\n&gt; &gt;&gt;&gt; certain problem classes we sho=\r\nuld not pay too much heed to NFL, why\n&gt; &gt;&gt;&gt; would we purposefully move *tow=\r\nards* the black box when we don&#39;t\nhave\n&gt; &gt;&gt;&gt; to?  The real excitement, I th=\r\nink, is to find very general\ntechniques\n&gt; &gt;&gt;&gt; for conveying to the learner =\r\nstandard kinds of a priori practical\n&gt; &gt;&gt;&gt; information (or bias), e.g. geom=\r\netry.\n&gt; &gt;&gt; \n&gt; &gt;&gt; Hi Ken. I think it is indeed cool that you demonstrate an =\r\neasy way\n&gt; &gt; to inject\n&gt; &gt;&gt; user knowledge in a way that appropriately bias=\r\nes the algorithm\ntoward\n&gt; &gt;&gt; better solutions. However, in my opinion, the =\r\nreason we are\n&gt; &gt; interested in\n&gt; &gt;&gt; machine learning is because it can sol=\r\nve the problems we *don&#39;t*\n&gt; &gt; know how to\n&gt; &gt;&gt; solve. The reason we use si=\r\nmple toy problems is because we know\nwhat the\n&gt; &gt;&gt; expected solutions shoul=\r\nd look like, and we want to demonstrate\nthat our\n&gt; &gt;&gt; algorithms can find t=\r\nhem. That gives us some confidence that, when\n&gt; &gt; we start\n&gt; &gt;&gt; showing our=\r\n algorithms problems where we don&#39;t know what the\nsolutions\n&gt; &gt;&gt; should loo=\r\nk like, they will discover good solutions to these\nproblems.\n&gt; &gt;&gt; Specifica=\r\nlly, with regards to generative encodings, one touted\n&gt; &gt; benefit is\n&gt; &gt;&gt; t=\r\nheir ability to exploit regularities that exist in the problem\n&gt; &gt; domain. =\r\nIt\n&gt; &gt;&gt; is the hope that on complex domains there will be many such\nregular=\r\nities\n&gt; &gt;&gt; that can be exploited. It is implied that humans will not always=\r\n\n&gt; &gt; know what\n&gt; &gt;&gt; those regularities are. Even if we did, it would be nic=\r\ne if we did\n&gt; &gt; not have\n&gt; &gt;&gt; to tell the algorithm about each type of regu=\r\nlarity. The hope is that\n&gt; &gt;&gt; generative encodings can discover them and ex=\r\nploit them without our\n&gt; &gt; aid. So,\n&gt; &gt;&gt; while it is nice to be able to inj=\r\nect knowledge, it is also\nimportant to\n&gt; &gt;&gt; devise algorithms that don&#39;t re=\r\nquire such knowledge. Clearly the\nlogical\n&gt; &gt;&gt; extremes of either position =\r\nare untenable: it is uninteresting to\n&gt; &gt; tell the\n&gt; &gt;&gt; network how to do e=\r\nverything but one trivial thing and have it learn\n&gt; &gt; that,\n&gt; &gt;&gt; and NFL te=\r\nlls us we can&#39;t have it be a jack of all trades. But I\n&gt; &gt; think there\n&gt; &gt;&gt;=\r\n are reasons to explore the intermediate ranges of both positions.\n&gt; &gt;&gt; \n&gt; =\r\n&gt;&gt;&gt; That said, if you really did start without the repeating coordinate\n&gt; &gt;=\r\n&gt;&gt; frames, I am guessing it would perform worse as you predict,\nthough I\n&gt; =\r\n&gt;&gt;&gt; don&#39;t know by how much.  It is probably worth doing just to see what\n&gt; =\r\n&gt;&gt;&gt; happens.  Yet my personal view is that there would not be a very\ndeep\n&gt;=\r\n &gt;&gt;&gt; insight to gain from such a result.  After all, why would we\nexpect it=\r\n\n&gt; &gt;&gt;&gt; to consistently discover the right regularity simply by chance every=\r\n\n&gt; &gt;&gt;&gt; time?  Remember that early in evolution, simply discovering this\n&gt; &gt;=\r\n&gt;&gt; regularity may not even be rewarded; just because it somehow gets\n&gt; &gt;&gt;&gt; =\r\nlucky and figures out exactly the right repeating frame of\nreference,\n&gt; &gt;&gt;&gt;=\r\n that does not necessarily mean that within those coordinate\nframes it\n&gt; &gt;&gt;=\r\n&gt; is doing anything useful (i.e. it could be a repetition of a bad\n&gt; &gt;&gt;&gt; po=\r\nlicy), so the discovery is likely to go unnoticed and die out,\njust\n&gt; &gt;&gt;&gt; a=\r\ns easily as it might be leveraged and elaborated properly.\n&gt; &gt;&gt; \n&gt; &gt;&gt; I agr=\r\nee that this is a challenging problem, but I think that it\nis very\n&gt; &gt;&gt; imp=\r\nortant for us to figure out algorithms that can cope with it.\nClearly\n&gt; &gt;&gt; =\r\nnature figured out ways to deal with this issue. How can we set up\n&gt; &gt;&gt; alg=\r\norithms such that if the population early on discovers a\nbauplan that\n&gt; &gt;&gt; =\r\nkeeps it trapped on a local peak, it can eventually discover a\n&gt; &gt; bauplan =\r\nthat\n&gt; &gt;&gt; gives it access to a higher peak?  To me this is a fundamental is=\r\nsue\n&gt; &gt; for our\n&gt; &gt;&gt; field. If we cannot improve upon it, we will be stuck =\r\nevolving\n&gt; &gt; relatively\n&gt; &gt;&gt; trivial solutions and never evolve things as i=\r\nmpressive as jaguars\n&gt; &gt; or poets.\n&gt; &gt;&gt; I am surprised you don&#39;t think rese=\r\narch on this front is important\n&gt; &gt; or would\n&gt; &gt;&gt; provide deep insights. Or=\r\n, is it that you think we can&#39;t make\n&gt; &gt; progress here,\n&gt; &gt;&gt; so documenting=\r\n a further failure isn&#39;t deep?\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt; This problem is related to that=\r\n discussion we had a while back about\n&gt; &gt;&gt;&gt; &quot;target-based evolution&quot; and th=\r\ne phenomena of Picbreeder.  Often the\n&gt; &gt;&gt;&gt; stepping stones (such as discov=\r\nering the right basic regularity) are\n&gt; &gt;&gt;&gt; not recognized by the ultimate =\r\nobjective function, so it&#39;s\npretty much\n&gt; &gt;&gt;&gt; up to luck to find them and k=\r\neep them around long enough to take\n&gt; &gt;&gt;&gt; advantage of them.  My feeling is=\r\n that it is not fruitful for any\n&gt; &gt;&gt;&gt; indirect encoding to try to solve th=\r\nat problem, because it is not a\n&gt; &gt;&gt;&gt; problem with the encoding per say but=\r\n rather with the way fitness is\n&gt; &gt;&gt;&gt; assigned.\n&gt; &gt;&gt; \n&gt; &gt;&gt; Fitness is one p=\r\nart of the mix. But there are also issues of\npopulation\n&gt; &gt;&gt; diversity, rep=\r\nresentation flexibility and evolvability, etc. Any of\n&gt; &gt; these\n&gt; &gt;&gt; could =\r\nassist in allowing deep switches in bauplan.\n&gt; &gt;&gt;  \n&gt; &gt;&gt; I&#39;d like to say on=\r\ne last thing. One of the great innovations of\n&gt; &gt; HyperNEAT\n&gt; &gt;&gt; was that y=\r\nou provided the geometry of the problem such that the\n&gt; &gt; algorithm\n&gt; &gt;&gt; co=\r\nuld exploit it. That strikes me as a bit different than telling the\n&gt; &gt;&gt; al=\r\ngorithm *how* to go about exploiting that geometry. I think that\n&gt; &gt;&gt; disti=\r\nnction is important, and might be being conflated a bit here. I\n&gt; &gt; agree\n&gt;=\r\n &gt;&gt; unequivocally that providing access to the geometry is important,\nbut i=\r\nt\n&gt; &gt;&gt; seems to me that it is an interesting field of research to figure\n&gt; =\r\n&gt; out how to\n&gt; &gt;&gt; create algorithms that learn how to exploit that geometry=\r\n, and its\n&gt; &gt;&gt; regularities, on their own.\n&gt; &gt;&gt; \n&gt; &gt;&gt; \n&gt; &gt;&gt; Cheers,\n&gt; &gt;&gt; Je=\r\nff Clune\n&gt; &gt;&gt; \n&gt; &gt;&gt; Digital Evolution Lab, Michigan State University\n&gt; &gt;&gt; \n=\r\n&gt; &gt;&gt; jclune@\n&gt; &gt;&gt; \n&gt; &gt;&gt; \n&gt; &gt;&gt; \n&gt; &gt;&gt; \n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt; --- In neat@yahoogroups.c=\r\nom, Jeff Clune &lt;jclune@&gt; wrote:\n&gt; &gt;&gt;&gt;&gt; \n&gt; &gt;&gt;&gt;&gt; Hello-\n&gt; &gt;&gt;&gt;&gt; \n&gt; &gt;&gt;&gt;&gt; I enjo=\r\nyed reading this. Thanks for posting it.\n&gt; &gt;&gt;&gt;&gt; \n&gt; &gt;&gt;&gt;&gt; A question: how did=\r\n HyperNEAT perform when you did not provide it\n&gt; &gt;&gt;&gt; with the\n&gt; &gt;&gt;&gt;&gt; repeat=\r\ning coordinate frame for each agent? As you mention in the\n&gt; &gt;&gt;&gt; paper, thi=\r\ns\n&gt; &gt;&gt;&gt;&gt; is something that HyperNEAT could learn on its own. I assume from\n=\r\n&gt; &gt;&gt;&gt; the fact\n&gt; &gt;&gt;&gt;&gt; that you added it that HyperNEAT was not doing a good=\r\n job of\n&gt; &gt;&gt;&gt; learning this.\n&gt; &gt;&gt;&gt;&gt; \n&gt; &gt;&gt;&gt;&gt; If that assumption is right, ho=\r\nw bad was it at learning this\nproblem\n&gt; &gt;&gt;&gt;&gt; decomposition? One of the tout=\r\ned benefits of HyperNEAT, and\n&gt; &gt; generative\n&gt; &gt;&gt;&gt;&gt; encodings in general, i=\r\ns the ability to evolve a module and\nreuse it\n&gt; &gt;&gt;&gt; many\n&gt; &gt;&gt;&gt;&gt; times (pote=\r\nntially with variation).  Here the modularity of the\n&gt; &gt;&gt;&gt; problem was\n&gt; &gt;&gt;=\r\n&gt;&gt; cleanly divided, and should have been relatively easy for\n&gt; &gt; HyperNEAT =\r\nto\n&gt; &gt;&gt;&gt;&gt; discover. Do you find it disconcerting that it couldn&#39;t do so?\n&gt; =\r\n&gt;&gt;&gt;&gt; \n&gt; &gt;&gt;&gt;&gt; \n&gt; &gt;&gt;&gt;&gt; \n&gt; &gt;&gt;&gt;&gt; Cheers,\n&gt; &gt;&gt;&gt;&gt; Jeff Clune\n&gt; &gt;&gt;&gt;&gt; \n&gt; &gt;&gt;&gt;&gt; Digit=\r\nal Evolution Lab, Michigan State University\n&gt; &gt;&gt;&gt;&gt; \n&gt; &gt;&gt;&gt;&gt; jclune@\n&gt; &gt;&gt;&gt;&gt; \n=\r\n&gt; &gt;&gt;&gt;&gt; \n&gt; &gt;&gt;&gt;&gt; \n&gt; &gt;&gt;&gt;&gt; \n&gt; &gt;&gt;&gt;&gt;&gt; From: Kenneth Stanley &lt;kstanley@&gt;\n&gt; &gt;&gt;&gt;&gt;&gt; R=\r\neply-To: &quot;neat@yahoogroups.com&quot; &lt;neat@yahoogroups.com&gt;\n&gt; &gt;&gt;&gt;&gt;&gt; Date: Wed, 1=\r\n6 Apr 2008 22:48:44 -0000\n&gt; &gt;&gt;&gt;&gt;&gt; To: &quot;neat@yahoogroups.com&quot; &lt;neat@yahoogro=\r\nups.com&gt;\n&gt; &gt;&gt;&gt;&gt;&gt; Subject: [neat] Another New Paper:  Multiagent HyperNEAT\n&gt;=\r\n &gt;&gt;&gt;&gt;&gt; \n&gt; &gt;&gt;&gt;&gt;&gt; David D&#39;Ambrosio and I discuss the potential for HyperNEAT\n=\r\n&gt; &gt;&gt;&gt;&gt;&gt; controlling multiple heterogeneous agents in this new\n&gt; &gt;&gt;&gt;&gt;&gt; paper=\r\n, &quot;Generative Encoding for Multiagent Learning,&quot; to appear at\n&gt; &gt;&gt;&gt;&gt;&gt; GECCO=\r\n 2008:\n&gt; &gt;&gt;&gt;&gt;&gt; \n&gt; &gt;&gt;&gt;&gt;&gt; http://eplex.cs.ucf.edu/index.php?\n&gt; &gt;&gt;&gt;&gt;&gt; option=\r\n=3Dcom_content&task=3Dview&id=3D14&Itemid=3D28#dambrosio.gecco08\n&gt; &gt;&gt;&gt;&gt;&gt; \n&gt;=\r\n &gt;&gt;&gt;&gt;&gt; Direct Link:\n&gt; &gt;&gt;&gt;&gt;&gt; \n&gt; &gt;&gt;&gt;&gt;&gt; http://eplex.cs.ucf.edu/papers/dambros=\r\nio_gecco08.pdf\n&gt; &gt;&gt;&gt;&gt;&gt; \n&gt; &gt;&gt;&gt;&gt;&gt; We also have a nice sample of videos that d=\r\nepict various evolved\n&gt; &gt;&gt;&gt;&gt;&gt; teams in action:\n&gt; &gt;&gt;&gt;&gt;&gt; \n&gt; &gt;&gt;&gt;&gt;&gt; http://eple=\r\nx.cs.ucf.edu/multiagenthyperneat\n&gt; &gt;&gt;&gt;&gt;&gt; \n&gt; &gt;&gt;&gt;&gt;&gt; The interesting idea in t=\r\nhis paper is that just as a single\n&gt; &gt;&gt;&gt;&gt;&gt; connective CPPN can encode how a=\r\n single network varies over space,\n&gt; &gt;&gt;&gt;&gt;&gt; it can also encode how a *set* o=\r\nf networks (each representing the\n&gt; &gt;&gt;&gt;&gt;&gt; policy of one agent on the team) =\r\nvaries over space.  In this way,\n&gt; &gt;&gt;&gt;&gt;&gt; HyperNEAT can learn an expression =\r\nthat encodes how policies vary\n&gt; &gt;&gt;&gt;&gt;&gt; over the team geometry.  For example=\r\n, in a soccer team agents vary\n&gt; &gt;&gt;&gt;&gt;&gt; from defensive to offensive as you m=\r\nove away from the goal. \nPart of\n&gt; &gt;&gt;&gt;&gt;&gt; the power of this approach is that=\r\n it means basic skills can be\n&gt; &gt;&gt;&gt;&gt;&gt; learned and shared among the whole te=\r\nam, since the CPPN\nencodes how\n&gt; &gt;&gt;&gt;&gt;&gt; those skills vary across the field.\n=\r\n&gt; &gt;&gt;&gt;&gt;&gt; \n&gt; &gt;&gt;&gt;&gt;&gt; ken\n&gt; &gt;&gt;&gt;&gt;&gt; \n&gt; &gt;&gt;&gt;&gt; \n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt; \n&gt; &gt; \n&gt; &gt;\n&gt;\n\n\n\n"}}