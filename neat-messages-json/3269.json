{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"xIaD0XIsEJzHTZ-iO4t611CzzJoJS8sEjHehbAreFHdzzGqHi3fggER9wIYCvY_kPu0zvftK5egTkksXZED5J7LWlKDqGmyYfMzKrWe38UjG","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Tile Coding and HyperNEAT","postDate":"1178912422","msgId":3269,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGYyMmdyNytwZGsyQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDNGMTQ2NDc4LUFCMTQtNEJGQi1CNEZELTA5QzUwM0VCMUFGNUBjcy51dGV4YXMuZWR1Pg=="},"prevInTopic":3266,"nextInTopic":3270,"prevInTime":3268,"nextInTime":3270,"topicId":3214,"numMessagesInTopic":27,"msgSnippet":"Joe, I agree this is a revealing discussion.  Tile coding to me a is a telling example of a significant misdirection of effort in machine learning right now. ","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 42036 invoked from network); 11 May 2007 19:40:27 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m48.grp.scd.yahoo.com with QMQP; 11 May 2007 19:40:27 -0000\r\nReceived: from unknown (HELO n18a.bullet.sp1.yahoo.com) (69.147.64.127)\n  by mta5.grp.scd.yahoo.com with SMTP; 11 May 2007 19:40:27 -0000\r\nReceived: from [216.252.122.218] by n18.bullet.sp1.yahoo.com with NNFMP; 11 May 2007 19:40:23 -0000\r\nReceived: from [66.218.69.3] by t3.bullet.sp1.yahoo.com with NNFMP; 11 May 2007 19:40:23 -0000\r\nReceived: from [66.218.66.76] by t3.bullet.scd.yahoo.com with NNFMP; 11 May 2007 19:40:23 -0000\r\nDate: Fri, 11 May 2007 19:40:22 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;f22gr7+pdk2@...&gt;\r\nIn-Reply-To: &lt;3F146478-AB14-4BFB-B4FD-09C503EB1AF5@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: Re: Tile Coding and HyperNEAT\r\nX-Yahoo-Group-Post: member; u=54567749; y=yM1HvvBn_5dd7o3wez4UuVdSAv0WhbioPZfSU--crpEw57Y7eeZO\r\nX-Yahoo-Profile: kenstanley01\r\n\r\nJoe, I agree this is a revealing discussion.  Tile coding to me a is \na tel=\r\nling example of a significant misdirection of effort in machine \nlearning r=\r\night now.  \n\nMost of what you said is factually true.  But the spin you put=\r\n on it \nis wrong.  It is correct that tile coding breaks up the state/actio=\r\nn \nspace into little pieces to make the right behavior for each little \nreg=\r\nion easier to compute.  As you put it, &quot;subtiles can better fit \nthe value =\r\nfunction being learned. Note that there is very little \ngeneralization desi=\r\nred here.&quot;  You say that like it&#39;s a good thing.\n\nHowever, the fact that th=\r\nere is a need to do something like that is \nmore a symptom of a serious dis=\r\nease in RL than an accomplishment we \nshould be congratulating ourselves fo=\r\nr.  It&#39;s like using cocaine to \nstay awake at work and claiming that it was=\r\n a good idea because you \nwere more alert.  The fact is the whole approach =\r\nis sick to begin \nwith if it needs cocaine to function properly.\n\nAnd that&#39;=\r\ns what tile coding is really indicating: Much of RL is DOA.  \nTile coding i=\r\ns a symptom of a larger sickness.  You said it \nyourself: &quot;[most] RL is inh=\r\nerently incapable of performing model \nselection.&quot;  Well, if what that mean=\r\ns is that you can&#39;t exploit \ngeometry, it&#39;s all a dead end.  I am not certa=\r\nin that RL (aside from \nNEAT+Q-type stuff) is really incapable of optimizin=\r\ng the model \nbecause who knows what we might realize how to do in the futur=\r\ne.  \nHowever, for now, RL is falling back on tile coding because it is \nmov=\r\ning in the wrong direction.\n\nHere is what is really going on:  Each variabl=\r\ne in the state/action \nspace is a dimension along which the value function =\r\nvaries.  A good \nlearning algorithm would represent how the value function =\r\nvaries with \nrespect to each state variable.  However, such variation may b=\r\ne \ncomplex, i.e. the function could be pretty complicated.  The learning \nm=\r\nethods (i.e. supervised function approximators) inside RL are \nsufficiently=\r\n bad that they cannot handle approximating functions like \nthat.  So what d=\r\no we do?  We break the whole space into chunks.  Now \nthe appropriate actio=\r\nn for each little chunk requires a much simpler \nfunction, so we have a cha=\r\nnce with our poor learning algorithm to \nmaybe get all these little simple =\r\nfunctions right instead of only a \nfew big complicated functions.  \n\nIn oth=\r\ner words, we have a poor algorithm and the cure is to destroy \nwhat variati=\r\nonal structure there was to begin with so that we can \nlook at every little=\r\n bit of the problem separately.   So we have now \nlost the ability to explo=\r\nit all the useful relationships that \ninitially existed in the space.  Stat=\r\nes that are related are now \nbroken apart and must be learned separately, t=\r\nhat is, the geometry \nhas been destroyed! The fact that many see such an op=\r\neration as a \nstep in the right direction is symptomatic of serious misdire=\r\nction in \nthe field.  If that&#39;s the best we can do to make RL easier, than =\r\nRL \nis in serious trouble!\n\nThink of it like this:  Take a game like chess,=\r\n which I learned as a \nlittle kid.  Now take the 64 squares and cut each pi=\r\nece out of the \nboard individually.  Now sprinkle them all randomly all ove=\r\nr your \nliving room.  Each square still represents the same location it was=\r\n \noriginally taken from in the board.  It&#39;s just you can&#39;t see where \nthey =\r\nwere anymore.  Now place the chess pieces in the right starting \nsquares an=\r\nd teach a little kid to play chess.  Think he or she would \nlearn anything =\r\nat all?\n\nWell, that&#39;s exactly what tile coding is!  A method that learns ch=\r\ness \n(or anything else that has implicit or explicit geometry) needs to \nkn=\r\now how the positions relate to each other geometrically because \nthere is m=\r\nassive regularity being lost without that information.  \nWhat kind of crazy=\r\n algorithm would purposely put a chess board into a \nmeaningless order befo=\r\nre learning begins?  A method that &quot;benefits&quot; \nfrom such an approach is cle=\r\narly DOA.  RL researchers should be \nseriously concerned about tile coding =\r\nbeing necessary at all, not \nhappy about it.\n\nSo I stick to my position: Ti=\r\nle coding is anti-geometry and anti-\nrepresentation.  It deserves no credit=\r\n whatsoever for &quot;respecting&quot; \nanything.\n\nken\n\n--- In neat@yahoogroups.com, =\r\nJoseph Reisinger &lt;joeraii@...&gt; wrote:\n&gt;\n&gt; I&#39;ve been aching to reply to this=\r\n post for a while, and I finally  \n&gt; have enough free time to do so. I thin=\r\nk we could have a really  \n&gt; interesting discussion here, hopefully at leas=\r\nt more interesting \nthan  \n&gt; the NFL tangent.\n&gt; \n&gt; &gt;&gt; Sure, but tile-coding=\r\n does respect at least one form of geometry:\n&gt; &gt;&gt; Nearby elements in the st=\r\nate space are known to be nearby, and \nthus\n&gt; &gt;&gt; are grouped in the same ti=\r\nle.\n&gt; &gt;\n&gt; &gt; I have to dispute this characterization of tile coding\n&gt; &gt; as &quot;=\r\nrespecting at least one form of geometry.&quot; I think you are\n&gt; &gt; being unnece=\r\nssarily equitable toward tile coding.\n&gt; &gt;\n&gt; &gt; What you are saying is that i=\r\nn effect taking a nice sculpture and\n&gt; &gt; cutting it into pieces &quot;respects&quot; =\r\nits geometry because those \nlittle\n&gt; &gt; pieces are not broken up any further=\r\n than that. It&#39;s like saying\n&gt; &gt; that someone who cut your head off &quot;respec=\r\nted&quot; your head by \nkeeping\n&gt; &gt; its internal integrity intact. In fact, tile=\r\n coding is peforming a\n&gt; &gt; grievous violation against the existing geometry=\r\n of the domain, \nand\n&gt; &gt; does not deserve to be credited with respecting ge=\r\nometry\n&gt; &gt; whatsoever. I&#39;m hard pressed to imagine how one could do worse\n&gt;=\r\n &gt; beyond cutting things up into even tinier and tinier bits; but \neven\n&gt; &gt;=\r\n then, those bits still contain &quot;nearby elements in the state\n&gt; &gt; space.&quot; S=\r\no that isn&#39;t saying much.\n&gt; \n&gt; Yeah, from the way your framing this argumen=\r\nt, e.g. tile-coding \nused  \n&gt; in the GA model-selection sense, you&#39;re absol=\r\nutely right. I&#39;ll get  \n&gt; back to exactly what I mean by that in a bit. For=\r\n now lets try to  \n&gt; reframe the issue from an RL perspective, which is whe=\r\nre tile-\ncodings  \n&gt; are predominantly used. In RL, the tile-coding is just=\r\n a  \n&gt; representation for a function approximator (in a sense its sort of  =\r\n\n&gt; like a really simple spline cure) that learns in a supervised \nmanner.  =\r\n\n&gt; Tile coding makes a lot of sense in this domain because you can  \n&gt; calc=\r\nulate with a good deal of precision how much some particular \ntile  \n&gt; diff=\r\ners from the expected value of the function being approximated  \n&gt; (in this=\r\n case the Bellman error).\n&gt; Tiles hat is cover a broad area where the value=\r\n function changes a  \n&gt; lot (&quot;have bad fit&quot;, &quot;are too general&quot;, etc) are th=\r\nen split so \nthat  \n&gt; the subtiles can better fit the value function being =\r\nlearned. Note  \n&gt; that there is very little generalization desired here; th=\r\ne best \nthing  \n&gt; given infinite computational resources would be to have a=\r\n whole \nton  \n&gt; of itty-bitty tiles that fit the value function perfectly.\n=\r\n&gt; \n&gt; Anyway, since we&#39;re in the standard RL framework, there is really \nno =\r\n \n&gt; way of learning the &quot;geometry&quot; of a value function (well, \ntechnically =\r\n \n&gt; there is, but thats a long tangent towards a really interesting  \n&gt; res=\r\nearch area). Maybe if the geometry was given by the \nexperimenter  \n&gt; befor=\r\nehand (this would also lead to an interesting extension of \ntile- \n&gt; coding=\r\ns that you might like a little better). But in any case, \nsince  \n&gt; all we&#39;=\r\nre trying to do in RL is supervised function approximation,  \n&gt; the lack of=\r\n geometry isn&#39;t bad.\n&gt; \n&gt; \n&gt; &gt; I&#39;m obviously not a big fan of tile coding :=\r\n) I&#39;m not really\n&gt; &gt; concerned whether it might do better in some cases; th=\r\ne problem \nwith\n&gt; &gt; it is that it is a dead end for future progress because=\r\n it is \nabout\n&gt; &gt; ruining our ability to exploit geometric relationships.\n&gt;=\r\n \n&gt; Ok, this is where the discussion gets really interesting. Remember  \n&gt; =\r\nwhen I mentioned GA&#39;s &quot;performing model selection&quot; or something \nlike  \n&gt; t=\r\nhat before? Thats a fundamental difference in the GA approach and  \n&gt; RL. S=\r\no what do I mean by model selection: roughly speaking, in  \n&gt; Bayesian infe=\r\nrence you have this idea of some separation of the  \n&gt; parameters you are o=\r\nptimizing (e.g. the weights in an NN) and the  \n&gt; model that generates thos=\r\ne parameters (e.g. the topology of the \nNN,  \n&gt; or even whether you use an =\r\nNN or decision tree or something). RL \nis  \n&gt; inherently incapable of perfo=\r\nrming model selection (at least \noutside  \n&gt; of NEAT+Q and some others). On=\r\nce you start learning with  a given  \n&gt; value function representation, you =\r\ncan no longer switch to a  \n&gt; different representation without throwing awa=\r\ny everything you&#39;ve \njust  \n&gt; learned.  GAs on the other hand learn one par=\r\nameterized model per  \n&gt; individual. This is an important distinction.\n&gt; \n&gt;=\r\n Now, what does this have to do with tile coding and learning  \n&gt; geometry?=\r\n  When you talk about &quot;cutting up different variables&quot; \nyou  \n&gt; are inheren=\r\ntly making an argument from the standpoint of model  \n&gt; selection: i.e. wha=\r\nt is the best representation for this learning  \n&gt; problem? This is a valid=\r\n question in the GA world, and I agree \nwith  \n&gt; you tile coding wouldn&#39;t w=\r\nork at all for learning good  \n&gt; representations that allow good future lea=\r\nrning. But from the RL  \n&gt; standpoint, since all tile-coding is used for is=\r\n function  \n&gt; approximation, I don&#39;t think they are as problematic as you i=\r\nmagine.\n&gt; \n&gt; -- Joe\n&gt;\n\n\n\n"}}