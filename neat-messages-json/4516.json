{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":211599040,"authorName":"Jeff Clune","from":"Jeff Clune &lt;jclune@...&gt;","profile":"jeffreyclune","replyTo":"LIST","senderId":"jirYVCNtC-6M8w_ZoW079f-6Spq8QpyspoJjTqplyXubkktYoYZ8fRoFIXqs_c_jufc_B9JqA2VYeGMHw5vrGm7Y","spamInfo":{"isSpam":false,"reason":"2"},"subject":"Re: [neat] Re: Parameter settings for comparing HyperNEAT to P-NEAT","postDate":"1229480215","msgId":4516,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PEM1NkRDQjQ3LjI3MEM4JWpjbHVuZUBtc3UuZWR1Pg==","inReplyToHeader":"PGdpOWYwaytnMDA0QGVHcm91cHMuY29tPg=="},"prevInTopic":4515,"nextInTopic":0,"prevInTime":4515,"nextInTime":4517,"topicId":4496,"numMessagesInTopic":6,"msgSnippet":"Hi Ken- I agree with you. Generalization and scaling are very cool and important results from that paper, and they have nothing to do with having the proper ","rawEmail":"Return-Path: &lt;jclune@...&gt;\r\nX-Sender: jclune@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 10181 invoked from network); 17 Dec 2008 02:28:14 -0000\r\nX-Received: from unknown (66.218.67.94)\n  by m49.grp.scd.yahoo.com with QMQP; 17 Dec 2008 02:28:14 -0000\r\nX-Received: from unknown (HELO qw-out-1920.google.com) (74.125.92.147)\n  by mta15.grp.scd.yahoo.com with SMTP; 17 Dec 2008 02:28:13 -0000\r\nX-Received: by qw-out-1920.google.com with SMTP id 4so691388qwk.60\n        for &lt;neat@yahoogroups.com&gt;; Tue, 16 Dec 2008 18:28:13 -0800 (PST)\r\nX-Received: by 10.214.217.1 with SMTP id p1mr212456qag.36.1229480893267;\n        Tue, 16 Dec 2008 18:28:13 -0800 (PST)\r\nReturn-Path: &lt;jclune@...&gt;\r\nX-Received: from ?192.168.2.19? (cpe-208-84-70-48.dyn.marcocable.net [208.84.70.48])\n        by mx.google.com with ESMTPS id 9sm2364282yws.48.2008.12.16.18.27.59\n        (version=TLSv1/SSLv3 cipher=RC4-MD5);\n        Tue, 16 Dec 2008 18:28:12 -0800 (PST)\r\nUser-Agent: Microsoft-Entourage/12.13.0.080930\r\nDate: Tue, 16 Dec 2008 21:16:55 -0500\r\nTo: &quot;neat@yahoogroups.com&quot; &lt;neat@yahoogroups.com&gt;\r\nMessage-ID: &lt;C56DCB47.270C8%jclune@...&gt;\r\nThread-Topic: [neat] Re: Parameter settings for comparing HyperNEAT to P-NEAT\r\nThread-Index: Aclf7Yf6xFH7fZtDrEa9Z55GN6mYGg==\r\nIn-Reply-To: &lt;gi9f0k+g004@...&gt;\r\nMime-version: 1.0\r\nContent-type: text/plain;\n\tcharset=&quot;ISO-8859-1&quot;\r\nContent-transfer-encoding: quoted-printable\r\nX-eGroups-Msg-Info: 2:2:2:0:0\r\nFrom: Jeff Clune &lt;jclune@...&gt;\r\nSubject: Re: [neat] Re: Parameter settings for comparing HyperNEAT to P-NEAT\r\nX-Yahoo-Group-Post: member; u=211599040; y=38B0jaMM1GUivdW-XWOYL8lUXoJ8doLPQxLvXsDgi-YRmFrp05ID\r\nX-Yahoo-Profile: jeffreyclune\r\n\r\nHi Ken-\n\nI agree with you. Generalization and scaling are very cool and imp=\r\nortant\nresults from that paper, and they have nothing to do with having the=\r\n proper\nparameters. \n\nI guess I have just been thinking more about HyperNEA=\r\nT vs. P-NEAT in general\nand for the specific comparisons I have been making=\r\n in my research.\n\nAs I&#39;ve said, I think the results in your first paper wit=\r\nh Jason are\nprofound and that the parameter issue is orthogonal.\n\nI appreci=\r\nate both of your responses to my question.\n\n\nCheers,\nJeff Clune\n\nDigital Ev=\r\nolution Lab, Michigan State University\n\njclune@...\n\n\n\n\n&gt; From: Kenneth =\r\nStanley &lt;kstanley@...&gt;\n&gt; Reply-To: &quot;neat@yahoogroups.com&quot; &lt;neat@y=\r\nahoogroups.com&gt;\n&gt; Date: Tue, 16 Dec 2008 23:52:52 -0000\n&gt; To: &quot;neat@yahoogr=\r\noups.com&quot; &lt;neat@yahoogroups.com&gt;\n&gt; Subject: [neat] Re: Parameter settings f=\r\nor comparing HyperNEAT to P-NEAT\n&gt; \n&gt; Jeff, you make a legitimate critique:=\r\n The study could be improved with\n&gt; an extensive parameter sweep for P-NEAT=\r\n.  As a routine matter, we do\n&gt; generally try some variation in parameters,=\r\n but we did not\n&gt; systematically sweep parameters as you did.  Of course, p=\r\narameter\n&gt; tuning is always a thorny issue, and a common complaint for any =\r\nmethod\n&gt; that performs worse in a comparison is that it was simply run with=\r\n the\n&gt; wrong parameters, which is very difficult to refute.  Still, as a\n&gt; =\r\nmatter of rigor, it would be most convincing to try to do something\n&gt; syste=\r\nmatic, as it looks like you&#39;ve tried.\n&gt; \n&gt; However, something else should b=\r\ne pointed out about the 2007 results.\n&gt;  I think the main result is *not* h=\r\now fast one method or another finds\n&gt; the solution to the boxes problem.   =\r\nIt&#39;s nice that HyperNEAT does it\n&gt; fast, but the real result is that it *ge=\r\nneralizes* almost perfectly.\n&gt; That result is much less likely to be relate=\r\nd to parameters.\n&gt; Generalization is the gap between performance on the tra=\r\nining set and\n&gt; test set for the *same* method.  Then we can compare that g=\r\nap from one\n&gt; method to the next.  Even if P-NEAT found a solution quickly,=\r\n the gap\n&gt; in generalization would still be big.  The other important resul=\r\nt from\n&gt; that paper is scaling, for which P-NEAT has no mechanism whatsoeve=\r\nr.\n&gt; So I think while the criticism is legitimate, it is not really related=\r\n\n&gt; to the most conceptually important results of the paper.\n&gt; \n&gt; ken\n&gt; \n&gt; -=\r\n-- In neat@yahoogroups.com, Jeff Clune &lt;jclune@...&gt; wrote:\n&gt;&gt; \n&gt;&gt; Jason and=\r\n Ken-\n&gt;&gt; \n&gt;&gt; Thanks for your responses.\n&gt;&gt; \n&gt;&gt; I think comparing HyperNEAT =\r\nand P-NEAT is very instructive, because it\n&gt;&gt; allows us to test the benefit=\r\ns of generative vs. direct encodings. I\n&gt; forgot\n&gt;&gt; that it was not used in=\r\n David=EF=BF=BDs experiments. However, I did use it in my\n&gt;&gt; 2008 PPSN pape=\r\nr (=EF=BF=BDHow a generative encoding fares as problem-regularity\n&gt;&gt; decrea=\r\nses=EF=BF=BD), and I am also using it in current research (not yet\n&gt; publis=\r\nhed).\n&gt;&gt; I mention all of this in order to let people know that I am not\n&gt; =\r\ntrying to be\n&gt;&gt; pedantic with regard to past work, but just trying to ensur=\r\ne that\n&gt;&gt; comparisons that we are making are fair.\n&gt;&gt; \n&gt;&gt; Jason, I was just=\r\n advocating doing a sweep on the mutation rate for\n&gt; P-NEAT\n&gt;&gt; up to N gene=\r\nrations (e.g. however many generations you used in your\n&gt; original\n&gt;&gt; paper=\r\n), and using that same N for HyperNEAT. That is not that large\n&gt; a drain\n&gt;&gt;=\r\n on resources. \n&gt;&gt; \n&gt;&gt; In fact, I have just completed such a sweep and foun=\r\nd that, on the\n&gt; problem I\n&gt;&gt; am testing on, the optimal P-NEAT mutation ra=\r\nte was about an order of\n&gt;&gt; magnitude lower than the one I had been using. =\r\nHowever, the boost in\n&gt;&gt; P-NEAT=EF=BF=BDs performance did not bring it anyw=\r\nhere near HyperNEAT. Of\n&gt; course,\n&gt;&gt; the optimal rate will depend on the pr=\r\noblem, but my point here is\n&gt; that it\n&gt;&gt; will also depend on the encoding. =\r\nWhile I have not swept HyperNEAT=EF=BF=BDs\n&gt;&gt; mutation rate, I=EF=BF=BDll w=\r\nager that HyperNEAT=EF=BF=BDs optimal rate is very\n&gt; different\n&gt;&gt; from P-NE=\r\nAT.  \n&gt;&gt; \n&gt;&gt; Ken- I agree with your assessment that sometimes small changes=\r\n in\n&gt; P-NEAT can\n&gt;&gt; lead to large changes in behavior. However, as you note=\r\n in your updated\n&gt;&gt; reply, in the boxes domain this is probably not the cas=\r\ne.\n&gt;&gt; \n&gt;&gt; It seems that it is difficult to determine a priori whether P-NEA=\r\nT (or\n&gt;&gt; HyperNEAT, or any encoding) will benefit from a higher, or lower,\n=\r\n&gt; mutation\n&gt;&gt; rate. But we know that the mutation rate greatly affects the\n=\r\n&gt; performance of\n&gt;&gt; an evolutionary algorithm. So, to get at the high-level=\r\n issue, it\n&gt; still does\n&gt;&gt; strike me as somewhat unfair to compare two diff=\r\nerent encodings, where\n&gt;&gt; mutations have such different types of effects, w=\r\nith the same\n&gt; mutation rate.\n&gt;&gt; \n&gt;&gt; Our reasoning about the encodings give=\r\ns us expectations that\n&gt; HyperNEAT will\n&gt;&gt; do much better. I would like to =\r\nstate again that I fully agree with\n&gt; those\n&gt;&gt; reasons. However, the point =\r\nof actually running the comparison is to see\n&gt;&gt; whether the data support th=\r\ne intuitions. It is somewhat circular to\n&gt; use our\n&gt;&gt; intuitions as a reaso=\r\nn for not setting up the proper controls, given\n&gt; that\n&gt;&gt; the controls are =\r\nmeant to test the accuracy of our intuitions.\n&gt;&gt; \n&gt;&gt; That said, one can los=\r\ne months doing parameter sweeps. My argument with\n&gt;&gt; regard to mutation rat=\r\ne could also be applied to many other NEAT\n&gt; parameters,\n&gt;&gt; as well as thei=\r\nr interactions. Clearly it would be a waste of energy\n&gt; to try\n&gt;&gt; to sweep =\r\nthrough that multidimensional space. So, maybe it is better\n&gt; to say\n&gt;&gt; tha=\r\nt since the tests are in accordance with what our reasoning\n&gt; suggests, we\n=\r\n&gt;&gt; do not need to spend the time to make sure they were exactly the right\n&gt;=\r\n&gt; control.  Or, perhaps a middle ground is appropriate: doing a\n&gt; cursory s=\r\nweep\n&gt;&gt; of a few parameters (e.g. mutation rate) that we know to have large=\r\n\n&gt; effects.\n&gt;&gt; That is why I was wondering what you feel are the few key pa=\r\nrameters\n&gt; that\n&gt;&gt; should be swept. At present, my thinking is that it is p=\r\nrobably fine\n&gt; to just\n&gt;&gt; sweep MutateLinkProbability. Does that sound righ=\r\nt?\n&gt;&gt;  \n&gt;&gt; In conclusion, I do not think it is particularly fair to use the=\r\n same\n&gt;&gt; settings for both encodings. But I am not sure whether it is worth=\r\n\n&gt; the time\n&gt;&gt; to try to make the controls more fair, given that our intuit=\r\nions largely\n&gt;&gt; predict the outcome of the test anyway, and given that init=\r\nial\n&gt; sweeps (like\n&gt;&gt; the one I just did) corroborate the predictions of th=\r\nose intuitions.\n&gt;&gt; \n&gt;&gt; \n&gt;&gt; \n&gt;&gt; Cheers, \n&gt;&gt; Jeff\n&gt;&gt; \n&gt;&gt; On 12/7/08 3:53 AM, =\r\n&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt; wrote:\n&gt;&gt; \n&gt;&gt;&gt;  \n&gt;&gt;&gt;  \n&gt;&gt;&gt; \n&gt;&gt;&gt; Jeff, a fe=\r\nw other thoughts on the issue...\n&gt;&gt;&gt; \n&gt;&gt;&gt; Note that I believe the only time=\r\n P-NEAT comes up is in the boxes\n&gt;&gt;&gt; domain.  None of David&#39;s experiments i=\r\nnvolve P-NEAT.  I think Jason\n&gt;&gt;&gt; did some cursory checking of other settin=\r\ngs for P-NEAT, but nothing\n&gt;&gt;&gt; systematic.  Jason can correct me if I am wr=\r\nong.\n&gt;&gt;&gt; \n&gt;&gt;&gt; I understand that like Jason and myself, you don&#39;t think it w=\r\nould\n&gt;&gt;&gt; really make a big difference no matter what we do with P-NEAT, but=\r\n\n&gt;&gt;&gt; just for the record, I think the reasoning you give for the\n&gt;&gt;&gt; differ=\r\nences with P-NEAT is not entirely accurate.  In particular, you\n&gt;&gt;&gt; suggest=\r\n that small mutations in HyperNEAT may lead to big changes on\n&gt;&gt;&gt; the subst=\r\nrate, while the same is not true for P-NEAT.  I disagree with\n&gt;&gt;&gt; that pers=\r\npective because ultimately what is important is not the\n&gt;&gt;&gt; substrate, but =\r\nthe behavior produced by the substrate.  The substrate\n&gt;&gt;&gt; is just a level =\r\nof indirection in the mapping between genotype and\n&gt;&gt;&gt; behavior.  In that v=\r\niew, a small mutation in P-NEAT is just as likely\n&gt;&gt;&gt; to produce a large ch=\r\nange in *behavior* as it is in HyperNEAT.\n&gt;&gt;&gt; Consider that behavior is an =\r\nindirect holistic product of genotype as\n&gt;&gt;&gt; well.  That is, a single gene =\r\nmutating in P-NEAT can change how an\n&gt;&gt;&gt; individual behaves in every possib=\r\nle situation it may encounter.\n&gt;&gt;&gt; Hence it is equally holistic as HyperNEA=\r\nT.\n&gt;&gt;&gt; \n&gt;&gt;&gt; In fact, one could argue that the situation is actually opposit=\r\ne of\n&gt;&gt;&gt; what you say:  Because a single mutation in HyperNEAT produces a\n&gt;=\r\n&gt;&gt; systematic concerted change in the substrate, it is less likely to\n&gt;&gt;&gt; p=\r\nroduce a haphazard change in behavior than a single mutation in\n&gt;&gt;&gt; P-NEAT.=\r\n  That is a difficult argument to make concrete, but it&#39;s not\n&gt;&gt;&gt; unreasona=\r\nble.  Just because a lot of connection weights change does\n&gt;&gt;&gt; not mean tha=\r\nt the change is &quot;big.&quot;  If they all change in a concerted\n&gt;&gt;&gt; manner, it ca=\r\nn be quite subtle, or no change at all.  And in fact,\n&gt;&gt;&gt; concerted change =\r\nis exactly what indirect encoding is about.\n&gt;&gt;&gt; \n&gt;&gt;&gt; Note that I am referri=\r\nng to P-NEAT in general.  In the boxes domain,\n&gt;&gt;&gt; it is perhaps more as yo=\r\nu say since individual connections in that\n&gt;&gt;&gt; domain do indeed have small =\r\neffects.\n&gt;&gt;&gt; \n&gt;&gt;&gt; In any case, the larger concern is still valid.  There ma=\r\ny indeed be\n&gt;&gt;&gt; different optimal parameter settings for P-NEAT and HyperNE=\r\nAT, and\n&gt;&gt;&gt; that is something people can look at.  But if there are, in my =\r\nview it\n&gt;&gt;&gt; is probably for different reasons than the ones you cite (as I =\r\nexplain\n&gt;&gt;&gt; above).  Still, barring finding the optimal P-NEAT parameters (=\r\nwhich I\n&gt;&gt;&gt; don&#39;t know), I think the most fair thing is indeed to give them=\r\n the\n&gt;&gt;&gt; same parameters because they are both ultimately variants of NEAT\n=\r\n&gt;&gt;&gt; evolving a solution to the same problem.  Still, I do see that one\n&gt;&gt;&gt; =\r\nmight be interested in optimizing P-NEAT further to see how good it\n&gt;&gt;&gt; can=\r\n really be.  However, as you say and Jason supports, it won&#39;t be\n&gt;&gt;&gt; easy t=\r\no get P-NEAT to optimize a 14,000-dimensional space, whatever\n&gt;&gt;&gt; parameter=\r\ns you give it.\n&gt;&gt;&gt; \n&gt;&gt;&gt; ken\n&gt;&gt;&gt; \n&gt;&gt;&gt; --- In neat@yahoogroups.com &lt;mailto:ne=\r\nat%40yahoogroups.com&gt; ,\n&gt; &quot;Jason G&quot;\n&gt;&gt;&gt; &lt;jgmath2000@&gt; wrote:\n&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt; H=\r\ney Jeff,\n&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt; I understand the concern with the mutation probabilit=\r\ny.  I\n&gt; believe the\n&gt;&gt;&gt;&gt;&gt; reason that NEAT cannot solve the boxes problem i=\r\nn training is\n&gt; a credit\n&gt;&gt;&gt;&gt;&gt; assignment problem. Because the number of co=\r\nnnections is so\n&gt; high, it is\n&gt;&gt;&gt;&gt;&gt; difficult for a direct encoding to lear=\r\nn which connections were\n&gt;&gt;&gt; responsible\n&gt;&gt;&gt;&gt;&gt; for an increase/decrease in =\r\nfitness.\n&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt; It might be possible to improve the credit assignment=\r\n by\n&gt; lowering the\n&gt;&gt;&gt;&gt;&gt; mutation rate.  The problem is that if the mutatio=\r\nn rate was\n&gt; lowered\n&gt;&gt;&gt; to the\n&gt;&gt;&gt;&gt;&gt; point where credit assignment could b=\r\ne manageable by NEAT, I\n&gt; believe\n&gt;&gt;&gt; that\n&gt;&gt;&gt;&gt;&gt; this would require orders =\r\nof magnitude more generations to\n&gt; converge.\n&gt;&gt;&gt; Even if\n&gt;&gt;&gt;&gt;&gt; NEAT converg=\r\ned to a solution, this does not negate the fact that a\n&gt;&gt;&gt; lot of\n&gt;&gt;&gt;&gt;&gt; the=\r\n connections are never used in training, and these connections\n&gt;&gt;&gt; will hav=\r\ne\n&gt;&gt;&gt;&gt;&gt; effectively random values, resulting in poor validation\n&gt; performan=\r\nce.\n&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt; Consider this: if NEAT was able to learn the correct value=\r\n for a\n&gt;&gt;&gt; single link\n&gt;&gt;&gt;&gt;&gt; every generation, and this correct value was p=\r\nreserved through the\n&gt;&gt;&gt; whole run\n&gt;&gt;&gt;&gt;&gt; (i.e. it was not accidentally muta=\r\nted), it would still take\n&gt; about 5000\n&gt;&gt;&gt;&gt;&gt; generations to completely solv=\r\ne the training phase of the boxes\n&gt;&gt;&gt; problem.  At\n&gt;&gt;&gt;&gt;&gt; roughly an hour pe=\r\nr generation, that&#39;s about 7 months per run.\n&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt; Any sweep of the =\r\nmutation rate parameter would require many\n&gt;&gt;&gt; generations in\n&gt;&gt;&gt;&gt;&gt; each ru=\r\nn. I believe that, given a very low mutation rate, NEAT\n&gt; might\n&gt;&gt;&gt; be able=\r\n\n&gt;&gt;&gt;&gt;&gt; to solve the boxes problem; however, it would take tens of\n&gt; thousan=\r\nds of\n&gt;&gt;&gt;&gt;&gt; generations at least, and there&#39;s still the generalization issu=\r\ne.\n&gt;&gt;&gt; Given the\n&gt;&gt;&gt;&gt;&gt; amount of resources necessary to find the magic numb=\r\ners, I didn&#39;t\n&gt;&gt;&gt; see the\n&gt;&gt;&gt;&gt;&gt; utility in trying to pursue it.\n&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;=\r\n&gt; I think what might be more interesting would be to try different\n&gt;&gt;&gt; para=\r\nmeters\n&gt;&gt;&gt;&gt;&gt; for HyperNEAT.  As we fix bugs and develop maturity in the\n&gt; c=\r\nodebase, it\n&gt;&gt;&gt;&gt;&gt; would be important to note how the parameters should be c=\r\nhanged\n&gt; for the\n&gt;&gt;&gt;&gt;&gt; older experiments (e.g. the mutation rate is lower n=\r\now because we\n&gt;&gt;&gt; fixed a\n&gt;&gt;&gt;&gt;&gt; bug in mutation).\n&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt; --- In neat@=\r\nyahoogroups.com &lt;mailto:neat%40yahoogroups.com&gt; ,\n&gt; Jeff Clune\n&gt;&gt;&gt;&gt; &lt;jclune=\r\n@&gt; wrote:\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt;&gt;&gt; Hi all-\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt;&gt;&gt; A quick question for Ke=\r\nn, Jason and Dave.\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt;&gt;&gt; I notice that most of the parameter set=\r\ntings (e.g.\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; MutateLinkWeightsProbability) were the same when you gu=\r\nys\n&gt; compared\n&gt;&gt;&gt;&gt;&gt; HyperNEAT\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; to P-NEAT.\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt;&gt;&gt; Someone i=\r\nn my lab raised the issue yesterday that each\n&gt;&gt;&gt; configuration may\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;=\r\n have entirely different optimal settings, making a\n&gt; comparison with the\n&gt;=\r\n&gt;&gt;&gt;&gt; same\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; settings potentially unfair. Out of curiosity, did you do=\r\n any\n&gt;&gt;&gt; parameter\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; sweeps to see if P-NEAT&#39;s performance did much b=\r\netter, and\n&gt; better\n&gt;&gt;&gt;&gt;&gt; approached\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; HyperNEAT&#39;s, with different pa=\r\nrameter settings?\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt;&gt;&gt; For example, since mutations to HyperNEA=\r\nT have such larger\n&gt; effects\n&gt;&gt;&gt; on the\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; final substrate, it could b=\r\ne argued that P-NEAT needs a much\n&gt; higher\n&gt;&gt;&gt;&gt;&gt; mutation\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; rate to c=\r\nompete.\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt;&gt;&gt; Note: I don&#39;t think P-NEAT will beat HyperNEAT no =\r\nmatter\n&gt; what the\n&gt;&gt;&gt;&gt;&gt; settings,\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; but I do think the question is fa=\r\nir and I would not be surprised\n&gt;&gt;&gt; if the\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; settings that worked gre=\r\nat for HyperNEAT are not the ones that\n&gt;&gt;&gt; work great\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; for P-NEAT (a=\r\nnd vice versa). In short, I am confident that the\n&gt;&gt;&gt; qualitative\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; r=\r\nesults of your paper are still all valid (and represent\n&gt;&gt;&gt; breakthroughs f=\r\nor\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; the field), but am interested to know whether the quantitative\n&gt;=\r\n&gt;&gt; difference\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; between the encodings might be much less with differe=\r\nnt\n&gt; settings.\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt;&gt;&gt; PS. As a side note, there are a ton of sett=\r\nings and sweeping\n&gt; them\n&gt;&gt;&gt; all is\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; nearly impossible, especially w=\r\nhen considering interactions.\n&gt; What does\n&gt;&gt;&gt;&gt;&gt; &g\n&gt;&gt; \n&gt;&gt; \n&gt;&gt; \n&gt;&gt; \n&gt;&gt; \n&gt;&gt; \n=\r\n&gt;&gt; \n&gt;&gt; \n&gt;&gt; \n&gt;&gt; Cheers,\n&gt;&gt; Jeff Clune\n&gt;&gt; \n&gt;&gt; Digital Evolution Lab, Michigan=\r\n State University\n&gt;&gt; \n&gt;&gt; jclune@...\n&gt;&gt; \n&gt;&gt; \n&gt;&gt; \n&gt;&gt; \n&gt;&gt;&gt; From: Kenneth Stanl=\r\ney &lt;kstanley@...&gt;\n&gt;&gt;&gt; Reply-To: &quot;neat@yahoogroups.com&quot; &lt;neat@...=\r\nm&gt;\n&gt;&gt;&gt; Date: Sun, 07 Dec 2008 08:53:00 -0000\n&gt;&gt;&gt; To: &quot;neat@yahoogroups.com&quot;=\r\n &lt;neat@yahoogroups.com&gt;\n&gt;&gt;&gt; Subject: [neat] Re: Parameter settings for comp=\r\naring HyperNEAT to\n&gt; P-NEAT\n&gt;&gt;&gt; \n&gt;&gt;&gt; Jeff, a few other thoughts on the issu=\r\ne...\n&gt;&gt;&gt; \n&gt;&gt;&gt; Note that I believe the only time P-NEAT comes up is in the b=\r\noxes\n&gt;&gt;&gt; domain.  None of David&#39;s experiments involve P-NEAT.  I think Jaso=\r\nn\n&gt;&gt;&gt; did some cursory checking of other settings for P-NEAT, but nothing\n&gt;=\r\n&gt;&gt; systematic.  Jason can correct me if I am wrong.\n&gt;&gt;&gt; \n&gt;&gt;&gt; I understand t=\r\nhat like Jason and myself, you don&#39;t think it would\n&gt;&gt;&gt; really make a big d=\r\nifference no matter what we do with P-NEAT, but\n&gt;&gt;&gt; just for the record, I =\r\nthink the reasoning you give for the\n&gt;&gt;&gt; differences with P-NEAT is not ent=\r\nirely accurate.  In particular, you\n&gt;&gt;&gt; suggest that small mutations in Hyp=\r\nerNEAT may lead to big changes on\n&gt;&gt;&gt; the substrate, while the same is not =\r\ntrue for P-NEAT.  I disagree with\n&gt;&gt;&gt; that perspective because ultimately w=\r\nhat is important is not the\n&gt;&gt;&gt; substrate, but the behavior produced by the=\r\n substrate.  The substrate\n&gt;&gt;&gt; is just a level of indirection in the mappin=\r\ng between genotype and\n&gt;&gt;&gt; behavior.  In that view, a small mutation in P-N=\r\nEAT is just as likely\n&gt;&gt;&gt; to produce a large change in *behavior* as it is =\r\nin HyperNEAT.\n&gt;&gt;&gt; Consider that behavior is an indirect holistic product of=\r\n genotype as\n&gt;&gt;&gt; well.  That is, a single gene mutating in P-NEAT can chang=\r\ne how an\n&gt;&gt;&gt; individual behaves in every possible situation it may encounte=\r\nr.\n&gt;&gt;&gt; Hence it is equally holistic as HyperNEAT.\n&gt;&gt;&gt; \n&gt;&gt;&gt; In fact, one cou=\r\nld argue that the situation is actually opposite of\n&gt;&gt;&gt; what you say:  Beca=\r\nuse a single mutation in HyperNEAT produces a\n&gt;&gt;&gt; systematic concerted chan=\r\nge in the substrate, it is less likely to\n&gt;&gt;&gt; produce a haphazard change in=\r\n behavior than a single mutation in\n&gt;&gt;&gt; P-NEAT.  That is a difficult argume=\r\nnt to make concrete, but it&#39;s not\n&gt;&gt;&gt; unreasonable.  Just because a lot of =\r\nconnection weights change does\n&gt;&gt;&gt; not mean that the change is &quot;big.&quot;  If t=\r\nhey all change in a concerted\n&gt;&gt;&gt; manner, it can be quite subtle, or no cha=\r\nnge at all.  And in fact,\n&gt;&gt;&gt; concerted change is exactly what indirect enc=\r\noding is about.\n&gt;&gt;&gt; \n&gt;&gt;&gt; Note that I am referring to P-NEAT in general.  In=\r\n the boxes domain,\n&gt;&gt;&gt; it is perhaps more as you say since individual conne=\r\nctions in that\n&gt;&gt;&gt; domain do indeed have small effects.\n&gt;&gt;&gt; \n&gt;&gt;&gt; In any cas=\r\ne, the larger concern is still valid.  There may indeed be\n&gt;&gt;&gt; different op=\r\ntimal parameter settings for P-NEAT and HyperNEAT, and\n&gt;&gt;&gt; that is somethin=\r\ng people can look at.  But if there are, in my view it\n&gt;&gt;&gt; is probably for =\r\ndifferent reasons than the ones you cite (as I explain\n&gt;&gt;&gt; above).  Still, =\r\nbarring finding the optimal P-NEAT parameters (which I\n&gt;&gt;&gt; don&#39;t know), I t=\r\nhink the most fair thing is indeed to give them the\n&gt;&gt;&gt; same parameters bec=\r\nause they are both ultimately variants of NEAT\n&gt;&gt;&gt; evolving a solution to t=\r\nhe same problem.  Still, I do see that one\n&gt;&gt;&gt; might be interested in optim=\r\nizing P-NEAT further to see how good it\n&gt;&gt;&gt; can really be.  However, as you=\r\n say and Jason supports, it won&#39;t be\n&gt;&gt;&gt; easy to get P-NEAT to optimize a 1=\r\n4,000-dimensional space, whatever\n&gt;&gt;&gt; parameters you give it.\n&gt;&gt;&gt; \n&gt;&gt;&gt; ken\n=\r\n&gt;&gt;&gt; \n&gt;&gt;&gt; --- In neat@yahoogroups.com, &quot;Jason G&quot; &lt;jgmath2000@&gt; wrote:\n&gt;&gt;&gt;&gt; \n=\r\n&gt;&gt;&gt;&gt; Hey Jeff,\n&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt; I understand the concern with the mutation probab=\r\nility.  I\n&gt; believe the\n&gt;&gt;&gt;&gt; reason that NEAT cannot solve the boxes proble=\r\nm in training is a\n&gt; credit\n&gt;&gt;&gt;&gt; assignment problem. Because the number of =\r\nconnections is so high,\n&gt; it is\n&gt;&gt;&gt;&gt; difficult for a direct encoding to lea=\r\nrn which connections were\n&gt;&gt;&gt; responsible\n&gt;&gt;&gt;&gt; for an increase/decrease in =\r\nfitness.\n&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt; It might be possible to improve the credit assignment b=\r\ny lowering the\n&gt;&gt;&gt;&gt; mutation rate.  The problem is that if the mutation rat=\r\ne was lowered\n&gt;&gt;&gt; to the\n&gt;&gt;&gt;&gt; point where credit assignment could be manage=\r\nable by NEAT, I believe\n&gt;&gt;&gt; that\n&gt;&gt;&gt;&gt; this would require orders of magnitud=\r\ne more generations to converge.\n&gt;&gt;&gt; Even if\n&gt;&gt;&gt;&gt; NEAT converged to a soluti=\r\non, this does not negate the fact that a\n&gt;&gt;&gt; lot of\n&gt;&gt;&gt;&gt; the connections ar=\r\ne never used in training, and these connections\n&gt;&gt;&gt; will have\n&gt;&gt;&gt;&gt; effectiv=\r\nely random values, resulting in poor validation performance.\n&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt; Con=\r\nsider this: if NEAT was able to learn the correct value for a\n&gt;&gt;&gt; single li=\r\nnk\n&gt;&gt;&gt;&gt; every generation, and this correct value was preserved through the\n=\r\n&gt;&gt;&gt; whole run\n&gt;&gt;&gt;&gt; (i.e. it was not accidentally mutated), it would still t=\r\nake about\n&gt; 5000\n&gt;&gt;&gt;&gt; generations to completely solve the training phase of=\r\n the boxes\n&gt;&gt;&gt; problem.  At\n&gt;&gt;&gt;&gt; roughly an hour per generation, that&#39;s abo=\r\nut 7 months per run.\n&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt; Any sweep of the mutation rate parameter wo=\r\nuld require many\n&gt;&gt;&gt; generations in\n&gt;&gt;&gt;&gt; each run. I believe that, given a =\r\nvery low mutation rate, NEAT might\n&gt;&gt;&gt; be able\n&gt;&gt;&gt;&gt; to solve the boxes prob=\r\nlem; however, it would take tens of\n&gt; thousands of\n&gt;&gt;&gt;&gt; generations at leas=\r\nt, and there&#39;s still the generalization issue.\n&gt;&gt;&gt; Given the\n&gt;&gt;&gt;&gt; amount of=\r\n resources necessary to find the magic numbers, I didn&#39;t\n&gt;&gt;&gt; see the\n&gt;&gt;&gt;&gt; u=\r\ntility in trying to pursue it.\n&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt; I think what might be more intere=\r\nsting would be to try different\n&gt;&gt;&gt; parameters\n&gt;&gt;&gt;&gt; for HyperNEAT.  As we f=\r\nix bugs and develop maturity in the\n&gt; codebase, it\n&gt;&gt;&gt;&gt; would be important =\r\nto note how the parameters should be changed\n&gt; for the\n&gt;&gt;&gt;&gt; older experimen=\r\nts (e.g. the mutation rate is lower now because we\n&gt;&gt;&gt; fixed a\n&gt;&gt;&gt;&gt; bug in =\r\nmutation).\n&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt; --- In neat@yahoogroups.com, Jeff Clune &lt;jclune@&gt; wro=\r\nte:\n&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt; Hi all-\n&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt; A quick question for Ken, Jason and D=\r\nave.\n&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt; I notice that most of the parameter settings (e.g.\n&gt;&gt;&gt;&gt;&gt; =\r\nMutateLinkWeightsProbability) were the same when you guys compared\n&gt;&gt;&gt;&gt; Hyp=\r\nerNEAT\n&gt;&gt;&gt;&gt;&gt; to P-NEAT.\n&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt; Someone in my lab raised the issue yes=\r\nterday that each\n&gt;&gt;&gt; configuration may\n&gt;&gt;&gt;&gt;&gt; have entirely different optima=\r\nl settings, making a comparison\n&gt; with the\n&gt;&gt;&gt;&gt; same\n&gt;&gt;&gt;&gt;&gt; settings potenti=\r\nally unfair. Out of curiosity, did you do any\n&gt;&gt;&gt; parameter\n&gt;&gt;&gt;&gt;&gt; sweeps to=\r\n see if P-NEAT&#39;s performance did much better, and better\n&gt;&gt;&gt;&gt; approached\n&gt;&gt;=\r\n&gt;&gt;&gt; HyperNEAT&#39;s, with different parameter settings?\n&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt; For exampl=\r\ne, since mutations to HyperNEAT have such larger effects\n&gt;&gt;&gt; on the\n&gt;&gt;&gt;&gt;&gt; f=\r\ninal substrate, it could be argued that P-NEAT needs a much higher\n&gt;&gt;&gt;&gt; mut=\r\nation\n&gt;&gt;&gt;&gt;&gt; rate to compete.\n&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt; Note: I don&#39;t think P-NEAT will b=\r\neat HyperNEAT no matter what the\n&gt;&gt;&gt;&gt; settings,\n&gt;&gt;&gt;&gt;&gt; but I do think the qu=\r\nestion is fair and I would not be surprised\n&gt;&gt;&gt; if the\n&gt;&gt;&gt;&gt;&gt; settings that =\r\nworked great for HyperNEAT are not the ones that\n&gt;&gt;&gt; work great\n&gt;&gt;&gt;&gt;&gt; for P=\r\n-NEAT (and vice versa). In short, I am confident that the\n&gt;&gt;&gt; qualitative\n&gt;=\r\n&gt;&gt;&gt;&gt; results of your paper are still all valid (and represent\n&gt;&gt;&gt; breakthro=\r\nughs for\n&gt;&gt;&gt;&gt;&gt; the field), but am interested to know whether the quantitati=\r\nve\n&gt;&gt;&gt; difference\n&gt;&gt;&gt;&gt;&gt; between the encodings might be much less with diffe=\r\nrent settings.\n&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt; PS. As a side note, there are a ton of settings=\r\n and sweeping them\n&gt;&gt;&gt; all is\n&gt;&gt;&gt;&gt;&gt; nearly impossible, especially when cons=\r\nidering interactions.\n&gt; What does\n&gt;&gt;&gt;&gt;&gt; everyone in the group consider to b=\r\ne the important ones to sweep?\n&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt; Cheers,\n&gt;&gt;=\r\n&gt;&gt;&gt; Jeff Clune\n&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt; Digital Evolution Lab, Michigan State Universit=\r\ny\n&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt; jclune@\n&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt; On Thu, Dec 4, 2008 at 11:49=\r\n AM, Jeff Clune &lt;jclune@&gt; wrote:\n&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt;   Hi all-\n&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt; A quick=\r\n question for Ken, Jason and Dave.\n&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt; I notice that most of the p=\r\narameter settings (e.g.\n&gt;&gt;&gt;&gt;&gt; MutateLinkWeightsProbability) were the same w=\r\nhen you guys compared\n&gt;&gt;&gt;&gt;&gt; HyperNEAT\n&gt;&gt;&gt;&gt;&gt; to P-NEAT.\n&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt; Someone=\r\n in my lab raised the issue yesterday that each\n&gt;&gt;&gt; configuration may\n&gt;&gt;&gt;&gt;&gt;=\r\n have entirely different optimal settings, making a comparison with\n&gt;&gt;&gt; the=\r\n same\n&gt;&gt;&gt;&gt;&gt; settings potentially unfair. Out of curiosity, did you do any\n&gt;=\r\n&gt;&gt; parameter\n&gt;&gt;&gt;&gt;&gt; sweeps to see if P-NEAT&#39;s performance did much better, a=\r\nnd better\n&gt;&gt;&gt;&gt;&gt; approached\n&gt;&gt;&gt;&gt;&gt; HyperNEAT&#39;s, with different parameter sett=\r\nings?\n&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt; For example, since mutations to HyperNEAT have such larg=\r\ner effects\n&gt;&gt;&gt; on the\n&gt;&gt;&gt;&gt;&gt; final substrate, it could be argued that P-NEAT=\r\n needs a much higher\n&gt;&gt;&gt;&gt;&gt; mutation\n&gt;&gt;&gt;&gt;&gt; rate to compete.\n&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt; Not=\r\ne: I don&#39;t think P-NEAT will beat HyperNEAT no matter what the\n&gt;&gt;&gt; settings=\r\n,\n&gt;&gt;&gt;&gt;&gt; but I do think the question is fair and I would not be surprised\n&gt;&gt;=\r\n&gt; if the\n&gt;&gt;&gt;&gt;&gt; settings that worked great for HyperNEAT are not the ones th=\r\nat\n&gt;&gt;&gt; work great\n&gt;&gt;&gt;&gt;&gt; for P-NEAT (and vice versa). In short, I am confide=\r\nnt that the\n&gt;&gt;&gt; qualitative\n&gt;&gt;&gt;&gt;&gt; results of your paper are still all valid=\r\n (and represent\n&gt;&gt;&gt; breakthroughs for\n&gt;&gt;&gt;&gt;&gt; the field), but am interested t=\r\no know whether the quantitative\n&gt;&gt;&gt; difference\n&gt;&gt;&gt;&gt;&gt; between the encodings =\r\nmight be much less with different settings.\n&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt; PS. As a side note=\r\n, there are a ton of settings and sweeping them\n&gt;&gt;&gt; all is\n&gt;&gt;&gt;&gt;&gt; nearly imp=\r\nossible, especially when considering interactions.\n&gt; What does\n&gt;&gt;&gt;&gt;&gt; everyo=\r\nne in the group consider to be the important ones to sweep?\n&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt; Ch=\r\neers,\n&gt;&gt;&gt;&gt;&gt; Jeff Clune\n&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt; Digital Evolution Lab, Michigan State U=\r\nniversity\n&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt; jclune@ &lt;jclune%40msu.edu&gt;\n&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt;  \n&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;=\r\n&gt; \n&gt;&gt;&gt; \n&gt;&gt;&gt; \n&gt;&gt; \n&gt; \n&gt; \n\n\n\n"}}