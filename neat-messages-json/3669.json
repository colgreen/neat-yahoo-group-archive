{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"MlWbNPBASBPHGlu5cVpXcFScyhHzZnP02WCtClGrUJ9tzeV3ffHW_qixFIkMmjAQMAXPh2Vy8nLDUjO_1jqgnSL8CbbLcPhEVd6ZfgCfOhv8","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Bleeding edge problem domains","postDate":"1197059018","msgId":3669,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGZqY2E0YityYWVmQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGZqNzlhZytycTBuQGVHcm91cHMuY29tPg=="},"prevInTopic":3668,"nextInTopic":0,"prevInTime":3668,"nextInTime":3670,"topicId":3656,"numMessagesInTopic":10,"msgSnippet":"Peter, you are right about the interpretation of both -1 and 1 as white.  This tradition goes back to Mattias Fagerlund s DelphiNEAT- based Genetic Art.  One","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 77502 invoked from network); 7 Dec 2007 20:23:40 -0000\r\nX-Received: from unknown (66.218.67.94)\n  by m50.grp.scd.yahoo.com with QMQP; 7 Dec 2007 20:23:40 -0000\r\nX-Received: from unknown (HELO n13b.bullet.sp1.yahoo.com) (69.147.64.113)\n  by mta15.grp.scd.yahoo.com with SMTP; 7 Dec 2007 20:23:40 -0000\r\nX-Received: from [216.252.122.218] by n13.bullet.sp1.yahoo.com with NNFMP; 07 Dec 2007 20:23:40 -0000\r\nX-Received: from [209.73.164.86] by t3.bullet.sp1.yahoo.com with NNFMP; 07 Dec 2007 20:23:40 -0000\r\nX-Received: from [66.218.66.91] by t8.bullet.scd.yahoo.com with NNFMP; 07 Dec 2007 20:23:40 -0000\r\nDate: Fri, 07 Dec 2007 20:23:38 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;fjca4b+raef@...&gt;\r\nIn-Reply-To: &lt;fj79ag+rq0n@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: Re: Bleeding edge problem domains\r\nX-Yahoo-Group-Post: member; u=54567749; y=QulhsYJy3K3XaY89X0RnzAyF6LRv7G_5j2gNSYrlvcUmrXITcYCX\r\nX-Yahoo-Profile: kenstanley01\r\n\r\nPeter, you are right about the interpretation of both -1 and 1 as \nwhite.  =\r\nThis tradition goes back to Mattias Fagerlund&#39;s DelphiNEAT-\nbased Genetic A=\r\nrt.  One thing is does is make it easier to get things \nthat are made of li=\r\nnes early on.  However, it is not clear if it is \nultimately necessary in t=\r\nhe long run.\n\nIf you are going to interpret the pictures as creatures, you =\r\nmay need \nsome criteria to filter out bad ones that are e.g. a sea of tissu=\r\ne so \nthat they are never even placed in the world.  There may be other \ncr=\r\niteria you can use as well.  You also may want to &quot;cut out&quot; pixels \non the =\r\nfringes that are disconnected from the main body.  \n\nI believe applying a f=\r\nilter that rejects some individuals from being \nborn is acceptable in such =\r\na domain; it is like a miscarriage rate, \nwhich exists in nature as well (i=\r\n.e. where some embryos do not make \nit to birth).  \n\nken\n\n--- In neat@yahoo=\r\ngroups.com, &quot;petar_chervenski&quot; \n&lt;petar_chervenski@...&gt; wrote:\n&gt;\n&gt; I agree, =\r\nbut I need to mention that some domains do not require \n&gt; setting up a subs=\r\ntrate at all. Consider the drawing of a picture \nand \n&gt; transforming it to =\r\na physics object, of just &quot;drawing&quot; the \nproperties \n&gt; of a static object. =\r\nWell this is just CPPN stuff, not involving \n&gt; HyperNEAT, which evolves exp=\r\nlicitly neural networks. \n&gt; \n&gt; Hm.. There is one thing I want to ask about =\r\nit. In Picbreeder for \n&gt; example, both -1 and 1 is considered white, right?=\r\n Is this the \nreason \n&gt; behind the fact that I can evolve negative-like ima=\r\nges as easy as \n&gt; normal ones? \n&gt; \n&gt; I am asking this because in Alife doma=\r\nins, I don&#39;t want creatures \n&gt; that are just a sea of tissue and empty spac=\r\ne in the center to \n&gt; appear. Is there a way to avoid that? Maybe is the to=\r\ntal amount of \n&gt; tissue exceeds a certain limit, to turn the picture negati=\r\nve, or \nwhat?\n&gt; \n&gt; Peter\n&gt; \n&gt; --- In neat@yahoogroups.com, &quot;Kenneth Stanley=\r\n&quot; &lt;kstanley@&gt; wrote:\n&gt; &gt;\n&gt; &gt; For those interested in CPPNs and HyperNEAT I =\r\nwould also suggest \n&gt; &gt; starting out with a domain that is not too complica=\r\nted just to \nget \n&gt; a \n&gt; &gt; little practice since some of the concepts (like=\r\n setting up the \n&gt; &gt; substrate) are novel and take a little practice.   The=\r\nn with that \n&gt; &gt; practice you&#39;ll have a better sense of what is possible in=\r\n more \n&gt; &gt; ambitious domains.\n&gt; &gt; \n&gt; &gt; ken\n&gt; &gt; \n&gt; &gt; --- In neat@yahoogroups=\r\n.com, &quot;Colin Green&quot; &lt;locster01@&gt; wrote:\n&gt; &gt; &gt;\n&gt; &gt; &gt; Hi Peter,\n&gt; &gt; &gt; Thanks =\r\nfor the ideas. I can see now that I really need to go \nback \n&gt; &gt; into\n&gt; &gt; &gt;=\r\n the message archive and acth up on some of the past CPPn \n&gt; &gt; discussions.=\r\n\n&gt; &gt; &gt; e.g. I guess the building/structure domain would need co-\n&gt; evolutio=\r\nn \n&gt; &gt; of\n&gt; &gt; &gt; the substrate (positions of the nodes) and perhaps you coul=\r\nd \nhave \n&gt; &gt; two\n&gt; &gt; &gt; outputs instead of one from the CPPN, one for materi=\r\nal strut \n&gt; width \n&gt; &gt; say\n&gt; &gt; &gt; and one for material type.\n&gt; &gt; &gt; \n&gt; &gt; &gt; Th=\r\ne visual rotation problem I&#39;m unfamilar with also. When you \nsay\n&gt; &gt; &gt; mult=\r\niple variables I take that as being multiple collectors on \n&gt; each\n&gt; &gt; &gt; ne=\r\nuron (one per activation input variable/parameter), which is\n&gt; &gt; &gt; somethin=\r\ng that was discussed ages ago before CPPN. I&#39;ll take a \n&gt; look\n&gt; &gt; &gt; throug=\r\nht archives on that one also.\n&gt; &gt; &gt; \n&gt; &gt; &gt; Cheers,\n&gt; &gt; &gt; \n&gt; &gt; &gt; Colin\n&gt; &gt; &gt;=\r\n \n&gt; &gt; &gt; On 04/12/2007, petar_chervenski &lt;petar_chervenski@&gt; wrote:\n&gt; &gt; &gt; &gt;\n=\r\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; Hi all,\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; I r=\r\nemember that we discussed one issue about visual \nrecognition \n&gt; &gt; with\n&gt; &gt;=\r\n &gt; &gt; HyperNEAT (CPPNs) and one particular problem arised and this \nthe\n&gt; &gt; =\r\n&gt; &gt; problem of evolving the concept of rotation.\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; CPPN-NEAT =\r\nneeds to be extended with neurons that have \nactivation\n&gt; &gt; &gt; &gt; functions s=\r\nupporting multiple variables for input/output for \n&gt; &gt; such a\n&gt; &gt; &gt; &gt; thing=\r\n to be achieved.\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; Another option for a CPPN application is A=\r\nlife domains, where \n&gt; the\n&gt; &gt; &gt; &gt; crature&#39;s genotype is a CPPN, but it is =\r\nstill not clear what \nthe\n&gt; &gt; &gt; &gt; creatures would be and what should they d=\r\no perform in their \n&gt; &gt; world in\n&gt; &gt; &gt; &gt; order to be selected.\n&gt; &gt; &gt; &gt;\n&gt; &gt; =\r\n&gt; &gt; Recently I proposed that a CPPN could evolve the material \n&gt; &gt; properti=\r\nes\n&gt; &gt; &gt; &gt; for static physics objects, where the goal is to find the \nright=\r\n\n&gt; &gt; &gt; &gt; structure for a building or a bridge, for stuff like \neartquakes \n=\r\n&gt; &gt; for\n&gt; &gt; &gt; &gt; example, to make it reliable.\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; There are man=\r\ny more domains, these are just the ones I first \n&gt; &gt; thought\n&gt; &gt; &gt; &gt; of.\n&gt; =\r\n&gt; &gt; &gt;\n&gt; &gt; &gt; &gt; Peter\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; --- In neat@yahoogroups.com, &quot;Colin Gre=\r\nen&quot; &lt;locster01@&gt; wrote:\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; Hi Ken,\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt;=\r\n &gt; &gt; The good people over at http://galaxyzoo.org/ announced \ntoday \n&gt; &gt; th=\r\nat\n&gt; &gt; &gt; &gt; each\n&gt; &gt; &gt; &gt; &gt; galaxy in their database has been categorized by =\r\n30 people \n(on\n&gt; &gt; &gt; &gt; average).\n&gt; &gt; &gt; &gt; &gt; Sounds like they&#39;re building up =\r\na sizeable database we \nmight \n&gt; be\n&gt; &gt; &gt; &gt; able to use.\n&gt; &gt; &gt; &gt; &gt; For the =\r\nuninitiated you basically log in and start looking \nat\n&gt; &gt; &gt; &gt; pictures of\n=\r\n&gt; &gt; &gt; &gt; &gt; galaxies, classifying them as spiral (clockwise or \n&gt; &gt; anticlock=\r\nwise) or\n&gt; &gt; &gt; &gt; &gt; eliptical. The problem they have is that some (actually =\r\na \nlot)\n&gt; &gt; of\n&gt; &gt; &gt; &gt; the images\n&gt; &gt; &gt; &gt; &gt; of galaxies are hard to classif=\r\ny, so they explained what to \n&gt; &gt; look\n&gt; &gt; &gt; &gt; for and\n&gt; &gt; &gt; &gt; &gt; opened the=\r\n task up to Joe Public, the idea being that by \n&gt; &gt; grouping\n&gt; &gt; &gt; &gt; multip=\r\nle\n&gt; &gt; &gt; &gt; &gt; answers they could determine a good answer for each galaxy. \n&gt;=\r\n &gt; Seems\n&gt; &gt; &gt; &gt; the idea\n&gt; &gt; &gt; &gt; &gt; paid off.\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; Now I&#39;ve =\r\nlooked at some of these galaxies and there&#39;s a lof \nof\n&gt; &gt; &gt; &gt; variation an=\r\nd\n&gt; &gt; &gt; &gt; &gt; general noise and mess in these images, and apart from that \n&gt; =\r\n&gt; some of\n&gt; &gt; &gt; &gt; them are\n&gt; &gt; &gt; &gt; &gt; fairly vague. However, Joe Public + pr=\r\nofessional \nastronomers \n&gt; &gt; have\n&gt; &gt; &gt; &gt; trouble\n&gt; &gt; &gt; &gt; &gt; with some of th=\r\nese images also and now we potentially have a\n&gt; &gt; &gt; &gt; database (don&#39;t\n&gt; &gt; &gt;=\r\n &gt; &gt; know if it&#39;s all public yet) that quantifies which images \nare \n&gt; &gt; ha=\r\nrd\n&gt; &gt; &gt; &gt; to\n&gt; &gt; &gt; &gt; &gt; clasify (by humans). An interesting question would =\r\nbe - How \n&gt; &gt; close\n&gt; &gt; &gt; &gt; can the\n&gt; &gt; &gt; &gt; &gt; best machine learning techniq=\r\nue come to Joe Public on this \n&gt; &gt; task?\n&gt; &gt; &gt; &gt; Almost as\n&gt; &gt; &gt; &gt; &gt; good?\n=\r\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; Now I undertand attempts have been made to automate gal=\r\naxy\n&gt; &gt; &gt; &gt; classification\n&gt; &gt; &gt; &gt; &gt; which didn&#39;t turn out well, hence the =\r\nweb site. But this \ndoes \n&gt; &gt; mean\n&gt; &gt; &gt; &gt; there\n&gt; &gt; &gt; &gt; &gt; will/should be a=\r\n body of existing research out there \n&gt; detailing \n&gt; &gt; how\n&gt; &gt; &gt; &gt; well the=\r\n\n&gt; &gt; &gt; &gt; &gt; current best known techniques can do.\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; Just a=\r\n thought. There&#39;s certainly no shortage of galaxies to\n&gt; &gt; &gt; &gt; classify :)\n=\r\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; Colin.\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt;\n=\r\n&gt; &gt;\n&gt;\n\n\n\n"}}