{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"SIXzFpEQ6CM0HaXxCg5LOIeJ2NUn1yLUhmXrnrdPVpvigazwfAHrQtP4-HmJej2zMDooVIXC0j8Tv8KfowPa-oreVvjuBOuIWE0Dx_8eyOgt","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: HyperNEAT: Creating Neural Networks with CPPNs","postDate":"1174964161","msgId":3035,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGV1YTE0MStuM29rQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDQ2MDg2NzI3LjMwMzAzMDZAd2FpdHMubmV0Pg=="},"prevInTopic":3034,"nextInTopic":3036,"prevInTime":3034,"nextInTime":3036,"topicId":3028,"numMessagesInTopic":34,"msgSnippet":"... while ... boil ... the ... In a sense the answer to your answer is yes, except I would not say an extra level of indirection.   It is simply a level of ","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 88800 invoked from network); 27 Mar 2007 02:56:32 -0000\r\nReceived: from unknown (66.218.66.72)\n  by m36.grp.scd.yahoo.com with QMQP; 27 Mar 2007 02:56:32 -0000\r\nReceived: from unknown (HELO n24b.bullet.sp1.yahoo.com) (209.131.38.235)\n  by mta14.grp.scd.yahoo.com with SMTP; 27 Mar 2007 02:56:32 -0000\r\nReceived: from [216.252.122.217] by n24.bullet.sp1.yahoo.com with NNFMP; 27 Mar 2007 02:56:02 -0000\r\nReceived: from [66.218.69.4] by t2.bullet.sp1.yahoo.com with NNFMP; 27 Mar 2007 02:56:02 -0000\r\nReceived: from [66.218.66.80] by t4.bullet.scd.yahoo.com with NNFMP; 27 Mar 2007 02:56:02 -0000\r\nDate: Tue, 27 Mar 2007 02:56:01 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;eua141+n3ok@...&gt;\r\nIn-Reply-To: &lt;46086727.3030306@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: Re: HyperNEAT: Creating Neural Networks with CPPNs\r\nX-Yahoo-Group-Post: member; u=54567749; y=Wa8Sl43TtBEsOCS56Ca8DuZ_7jRmkTRePSG7yh6oITsnqfsiSNhn\r\nX-Yahoo-Profile: kenstanley01\r\n\r\n--- In neat@yahoogroups.com, Stephen Waits &lt;steve@...&gt; wrote:\n&gt;\n&gt; Kenneth S=\r\ntanley wrote:\n&gt; &gt; \n&gt; &gt; Hi everyone, as some of you know, we have been worki=\r\nng for a \nwhile\n&gt; &gt; now on turning the patterns output by CPPNs into neural=\r\n networks.\n&gt; \n&gt; Hi,\n&gt; \n&gt; I have a few naive questions.  First of all, this =\r\nis a novel idea, \n&gt; thanks for publishing it.\n&gt; \n&gt; My first thought as read=\r\ning through the paper: Does this simply \nboil \n&gt; down to an extra (albeit u=\r\nseful) level of indirection?  Of course, \nthe \n&gt; next question: Is this ind=\r\nirection helpful?\n&gt; \n\nIn a sense the answer to your answer is yes, except I=\r\n would not say \nan &quot;extra&quot; level of indirection.   It is simply a level of =\r\n\nindirection.  Of course, if you believe in the importance of \nrepresentati=\r\non to encoding extremely high complexity then \nindirection is a critical fe=\r\nature.  The field of developmental \nencoding has elevated the search for th=\r\ne elusive &quot;ultimate indirect \nencoding&quot; almost to the level of a holy grail=\r\n activity.  \n\nAs you say the next legitimate question is whether connective=\r\n CPPNs \nare indeed a useful form of indirection.  Much of our recent work \n=\r\nhas tried to make a preliminary argument supporting this view, but \nof cour=\r\nse we need to continue to provide evidence in more exciting \ndomains to clo=\r\nse the argument.\n\n&gt; The things not addressed, that I&#39;m wondering..\n&gt; \n&gt; * S=\r\no, this is probably most useful for generating meganode \nnetworks. \n&gt; The k=\r\nind of stuff you might put on an FPGA.  Or, is it beating \nstraight \n&gt; NEAT=\r\n even on smaller problems?\n&gt; \n\nIt depends what you mean by &quot;smaller.&quot;  Ther=\r\ne is a meaning \nfor &quot;smaller&quot; for which NEAT will do better than HyperNEAT:=\r\n  Those \nproblems are ones where the solution can be represented in only a =\r\n\nfew dimensions (i.e. a few connections and nodes).  For example, XOR \nand =\r\npole balancing will probably be better solved by NEAT than \nHyperNEAT.\n\nHow=\r\never, what HyperNEAT is really meant for are extremely difficult \nand high-=\r\ndimensional problems that require a lot of structure to \nsolve, and for whi=\r\nch *regularity* is a key feature.  There is good \nreason to believe that ma=\r\nny of the world&#39;s most difficult problems \ncontain within them regularities=\r\n.  The human brain exemplifies this \nidea, since within it are inumerable r=\r\negularities and repeating \nmotifs.  So we can be reasonably sure that such =\r\nstructures are very \npowerful.\n\nHowever, if a problem has a solution with n=\r\no regularity whatsoever \n(small or large), then HyperNEAT likely is not sui=\r\nted to it either.\n\n&gt; * But, let&#39;s say in that case (meganode networks)..  w=\r\nith the \ncomplexity \n&gt; of the substrate configuration, comes required compl=\r\nexity and \ndetail in \n&gt; the CPNN.  So, I&#39;m wondering, on non &quot;toy&quot; problems=\r\n, how complex \ndo \n&gt; CPNNs need to become, and does NEAT get them there?\n&gt; =\r\n\n\nOf course we need to demonstrate this, but in advance I can say that \nHyp=\r\nerNEAT and connective CPPNs are designed with exactly this type \nof problem=\r\n in mind.  Massive variational structures with subtle \nunderlying motifs wi=\r\nll be applied to very high-resolution and/or \ndifficult problems.\n\n&gt; * And,=\r\n again in the meganode case.. what if you cannot a priori \n&gt; determine a su=\r\nitable substrate configuration?  The \nmore &quot;unnatural&quot; (I&#39;m \n&gt; missing the =\r\nright word for this) the configuration, the more depth \nand \n&gt; complexity n=\r\needed in the CPNN.\n&gt; \n\nSubstrate configuration will probably prove to be a =\r\nresearch area of \nits own.  However, for many problems, there are some intu=\r\nitive and \nstraightforward configurations that will likely become standard.=\r\n  \nFor example, three parallel lines of nodes can be an input layer, \nhidde=\r\nn layer, and output layer.  Of course, there are many other \nways to do thi=\r\ns, which I think is part of the magic of this \napproach:  It allows you to =\r\ninject problem structure into the \nlearning problem through intuitive geome=\r\ntric configurations.  We can \nnow explore different configurations and lear=\r\nn their pros and cons, \nsomething that would have had no meaning in the pas=\r\nt.\n\n&gt; * So, we&#39;re left with manual substrate configuration.. and perhaps \n&gt;=\r\n that&#39;s answered in the yet-to-be-published part of your research?  \nI \n&gt; s=\r\nuppose you might be able to come up with some evolvable substrate \n&gt; encodi=\r\nng schemes.\n&gt; \n\nYes it is manual, and yes it is likely there are ways to ev=\r\nolve it \nautomatically as well.  At the moment we are looking at both optio=\r\nns \nto some extent in various contexts.  We have a lot of ideas on \nsubstra=\r\nte configurations that are interesting and plausible.\n\n&gt; * Finally, I&#39;d sur=\r\ne like to hear more on your ideas about hidden \nnodes \n&gt; in the substrate. =\r\n In the one paper I read (&#39;D&#39;Ambrosio), it&#39;s \nonly \n&gt; briefly mentioned in =\r\na few spots.\n&gt; \n\nYes, again, you raise a current/future work issue that is =\r\nabsolutely \ncritical.  We have done some prelimianry experiments with hidde=\r\nn \nnodes and we have some configuration ideas, but this work is still \nin t=\r\nhe research stage.  For now what I can say is that there is no \nreason in p=\r\nrinciple we cannot exploit any reasonable layout in a way \nthat takes advan=\r\ntage of its geometry.\n\n&gt; Thanks again for the papers.  I like the concept. =\r\n I particularly \nlike \n&gt; the feeling when I get when I read it, that this i=\r\ndea probably \nstemmed \n&gt; from that 2d function evolver/plotter thing you us=\r\ned to make the \n&gt; spaceship some years ago.\n&gt; \n\nYou guessed right.  Evolvin=\r\ng the spaceship was like a revelation for \nme.  I was not expecting to evol=\r\nve anything so regular and \nintricate.  It was shocking in a way because I =\r\nhad been spending \nyears thinking about indirect encoding in terms of itera=\r\ntive \ndevelopmental systems, and then suddenly I realized I had \ninadverten=\r\ntly evolved exactly the type of thing I&#39;d been looking \nfor, but without it=\r\nerative development!  I felt there was an almost \nprofound lesson in that r=\r\nesult, and I wanted to explain it and then \nexploit it.\n\nken\n\n\n\n\n"}}