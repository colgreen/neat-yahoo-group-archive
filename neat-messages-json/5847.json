{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":344770077,"authorName":"Colin Green","from":"Colin Green &lt;colin.green1@...&gt;","profile":"alienseedpod","replyTo":"LIST","senderId":"ZyTF00-YSFNbpLXZbvN_auzZI-G4Kt3YmK-PbWC4GAwkqw_CtCfsbpVGNHs2CqxMJFEbgKGAh2FMNQpr6NX88LYbTFbsXuMsG5jN","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [neat] Re: Lessons From Data Mining - The Power of Ensembles","postDate":"1346365212","msgId":5847,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PENBRTBNK1lkREp3dUs0Yko4OFpYaGQ3ODUrclJ1eDJFbTNHdCtkUStyTW5vWTF3dVo2UUBtYWlsLmdtYWlsLmNvbT4=","inReplyToHeader":"PGsxbjNhcCtjbzBzQGVHcm91cHMuY29tPg==","referencesHeader":"PENBRTBNK1lmOEoyWmNEYjJGOXZEUE83d3RqUnlKeUVXZTc9WEZUTjU3SldxbVdYVWY4d0BtYWlsLmdtYWlsLmNvbT4JPGsxbjNhcCtjbzBzQGVHcm91cHMuY29tPg=="},"prevInTopic":5846,"nextInTopic":0,"prevInTime":5846,"nextInTime":5848,"topicId":5844,"numMessagesInTopic":4,"msgSnippet":"Hi Ken, ... Interesting. A slightly related anecdote - In a chess player rating competition a couple of years ago I did pretty well but I was unable to model","rawEmail":"Return-Path: &lt;colin.green1@...&gt;\r\nX-Sender: colin.green1@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 21325 invoked from network); 30 Aug 2012 22:20:14 -0000\r\nX-Received: from unknown (98.137.34.46)\n  by m7.grp.sp2.yahoo.com with QMQP; 30 Aug 2012 22:20:14 -0000\r\nX-Received: from unknown (HELO mail-ee0-f44.google.com) (74.125.83.44)\n  by mta3.grp.sp2.yahoo.com with SMTP; 30 Aug 2012 22:20:14 -0000\r\nX-Received: by eekb45 with SMTP id b45so864663eek.3\n        for &lt;neat@yahoogroups.com&gt;; Thu, 30 Aug 2012 15:20:13 -0700 (PDT)\r\nMIME-Version: 1.0\r\nX-Received: by 10.14.223.9 with SMTP id u9mr8520914eep.10.1346365212951; Thu, 30\n Aug 2012 15:20:12 -0700 (PDT)\r\nX-Received: by 10.14.136.15 with HTTP; Thu, 30 Aug 2012 15:20:12 -0700 (PDT)\r\nIn-Reply-To: &lt;k1n3ap+co0s@...&gt;\r\nReferences: &lt;CAE0M+Yf8J2ZcDb2F9vDPO7wtjRyJyEWe7=XFTN57JWqmWXUf8w@...&gt;\n\t&lt;k1n3ap+co0s@...&gt;\r\nDate: Thu, 30 Aug 2012 23:20:12 +0100\r\nMessage-ID: &lt;CAE0M+YdDJwuK4bJ88ZXhd785+rRux2Em3Gt+dQ+rMnoY1wuZ6Q@...&gt;\r\nTo: neat@yahoogroups.com\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Colin Green &lt;colin.green1@...&gt;\r\nSubject: Re: [neat] Re: Lessons From Data Mining - The Power of Ensembles\r\nX-Yahoo-Group-Post: member; u=344770077; y=wPBfQbnbyMgabuQMUgwJyCy73QrOKbRoMLA3lxzOoFuajOVh49O4\r\nX-Yahoo-Profile: alienseedpod\r\n\r\nHi Ken,\n\nOn 30 August 2012 08:04, Ken &lt;kstanley@...&gt; wrote:\n&gt;\n&gt; ...one of the few papers I&#39;ve written that has nothing to do with evolution introduced\n&gt; an algorithm called the &quot;Concept Drift Committee&quot; (CDC) for tracking a drifting\n&gt; concept with a &quot;committee&quot; (really an ensemble) of decision trees. It seemed to\n&gt; work pretty well at the time compared to other drift trackers. I wrote it up only as a\n&gt; tech report when I was in grad school:\n\nInteresting. A slightly related anecdote - In a chess player rating\ncompetition a couple of years ago I did pretty well but I was unable\nto model the players&#39; changing skill level over time. It turned out\nthat most others were having the same problem -  trying to do it as a\nfunction of some time variable [t, sqrt(t) or log(t) etc] didn&#39;t seem\nto add value. But those that finished ahead of me had noticed that if\nthey added weight to more recent chess game results, that the\npredictions of near future games improved. So it seemed that there was\nvariation in skill evel that could be modelled, but it was perhaps too\nsubtle to be picked up by a crude mapping function. Hence i can see\nhow a &#39;recent&#39; committee could predict better than one optimised on an\nentire data set/history.\n\n\n\n&gt; If I understand your idea right, it&#39;s to make an individual in an evolving population\n&gt; itself a kind of network ensemble that exhibits the beneficial ensemble averaging\n&gt; effect.\n\nCorrect. This has been probabaly been the main take away from\ncompeting on kaggle comps, but also the Netflix Prize as well which to\nsome extent became a game of who can build the largest ensembles\ntowards the end. Certainly ensembles were used by all of the top\nscorers. I also wonder to what extent decision making processes in\nbiological brains are the result of multiple neuronal circuits being\ncombined by some voting mechanism - just more food for thought.\n\n\n\n[Multiple networks per genome versus combining champs from muliple populations]\n\nOne issue I can see is that single genomes from different populations\naren&#39;t necessarily completely independent, e.g. in the case were there\nare very obvious local maxima then all population may get stuck there\nand produce very similar networks. But more generally the networks\nfrom each pop are a function of the EA and how it &#39;interacts&#39; with the\nfitness landscape, so there will always tend to be similarities\nbetween each population.\n\nWith RF the decision trees are created independently of each other,\nand each one is a very crude model overall, but pretty good in some\nsmall sub region of the problem space. So when we combine 100s, 1000s\nof more of them (which for DTs is achievable with modest CPU\nresources) the ensemble performance can be seen to be approacing some\nasymptote.\n\nSo yes, one approach might be translate this idea directly to EC and\njust perform 100 or so runs with some initial state randomisation, and\nto combe the champs from each run. For certain problems such as the\ntypical data mining scenario of mapping data set X to Y that should\ngive better results, but the CPU cost is high and as I say, the\nnetworks may not be as random as we would like - they may all get\nstuck in the same locality.\n\nFor control problems - combining two radically different but good\ncontrol strategies by averaging their outputs will generally yield an\ninviable strategy I think.\n\nSo I&#39;m trying to determine if there is some variation on the ensemble\nidea that we can pick up and develop a bit further to be applicable to\nEC. And the next step to try or think through seemed to be to have\nagenome that represents an complete ensemble - what sort of dynamics\nwould such an arrangement have? Are there analogues in nature?\n\n\n&gt; Sticking them together in one genome does imply role differentiation and cooperation\n&gt; of a sort that is less likely in a totally free-for-all coevolution.\n\nOn the face of it it seems that this might be a positive\ndynamic/quality that might be achieved - there&#39;s an interesting piece\nof research right there, to try and answer that question.\n\n\n\n\n&gt; On the other hand if you had separate subpopulations of networks from which\n&gt; individuals are put together into ensembles (like in the ESP algorithm) then each\n&gt; subpopulation could differentiate into its own kind of role.\n\nOk so rather than performing independent runs and combining the\nchamps, you&#39;re describing a system where we combine champs but then\nassign fitness to them based on their ensemble score (or scores), such\nthat each sub-pop can specialise on one sub-region of functionality.\nWith that arrangement I guess you would predetermine how many pops\nthere are and combine the champs from each one into a single ensemble\nchamp for evaluation. I can see that working too. With the multi-net\ngenomes though we have multiple ensembles, potentially fostering an\necosystem of different combinations of specialisations... maybe. (just\ntrying to fill out the key differences/strengths between the two\napproaches).\n\n\n\n\n&gt; But I&#39;m not sure that really role differentiation towards specific functionalities is what\n&gt; makes these Random Forest-type systems work?\n\nIt&#39;s not. Each decision tree is constructed in isloation and added to\nthe ensemble, you can do this maybe hundreds of times a minute for a\nsmallish data set and get an ensemble with thousands of DTs, all\ncombined with simple averaging (others may combine them differently,\nnot sure, certainly averaging works very well).\n\n\n&gt; Or is it more just about the idea that having a bunch of amateurs is better than a\n&gt; single expert (but the amateurs aren&#39;t particular &quot;specialized&quot; themselves)?\n\nThey are specialized in the sense that each one is trained on a\nsub-set of data and of the input/prediction variables. So each DT will\ntend to strongly ovefit that sub-region of the problem space, and then\nget averaged back to somewhere nearer a more general/probable\nprediction as part of the ensemble.\n\n\n&gt; In that case, role differentiation would not be that important compared to just some\n&gt; statistical variation in general behavior.\n\nIt&#39;s true that I&#39;m making a bit of a leap of faith here. My hope would\nbe that the specialised nets (if they occur) would be small(er) and\nmore &#39;nimble&#39; in evolutioanry terms, so less likely to get stuck in\nsome local maxima, becuae we can change any one of them to explore\nnearby space for that sub-problem without drastically affecting the\nensemble. So it [maybe] adds an extra bit of of freedom of movement to\nthe search algorithm [conjecture].\n\n\n&gt;\n&gt; To confuse things even more, maybe indirect encoding can apply to this kind of thing\n&gt; after all - for example multiagent HyperNEAT can generate a whole team (you could\n&gt; call it an ensemble) of neural networks that share some regularities but differ in other\n&gt; ways. Whether that is good or bad to get the kind of ensemble effect we&#39;re\n&gt; discussing is unclear to me, but maybe there would be some benefit in the fact that\n&gt; the indirect encoding ensures the networks in the ensemble share some properties.\n\nMy instinct is that we want to differentiate the sub-nets as much as\npossible so that they&#39;re not &#39;overlapping&#39; in terms of what they are\n&#39;bringing&#39; to the ensemble. Perhaps a random input into a CPPN could\nproduce differentiated networks, but it&#39;s not clear to me right now\nwhere the novelty-with-positive-functionality in each generated ANN is\ncoming from. My instinct would be to explore some of the above ideas\nfirst and see where they lead (if anywhere). It may well lead back to\nHyperNEAT down the line.\n\nAlso wondering if someone with more biological knowledeg could chip in\nwith analogues from nature, e.g. was my use of the word chromosomes in\nany way relevant? :)\n\nColin.\n\n"}}