{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":464818732,"authorName":"Jeff Clune","from":"Jeff Clune &lt;jclune@...&gt;","profile":"jeffreyclune","replyTo":"LIST","senderId":"nM_QOkRq7N-XprbR3O-H6oCGWrx-x6qHt50Up91H4Ded833rTdp99dmO04eAFqMA5j1EX0m84yKFaOmUBLHySi27j8M","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [neat] New paper: Automated Generation of Environments to Test the General Learning Capabilities of AI Agents","postDate":"1398876969","msgId":6285,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PEY1MkE3MUQ3LURGMzUtNEEzMC1BM0ZDLTk3RjY0NjIwMzg0N0BnbWFpbC5jb20+","inReplyToHeader":"PENBTnRYaG10eG9oTzRSZmhVYzBCUzRhMmZXMTlKY2pEYmUtOHEwdzlNSDFXalhjbzh6UUBtYWlsLmdtYWlsLmNvbT4=","referencesHeader":"PENBK2R1aW1PMjRzYWtPWFNNVnVxYkVleDgremlCbVFIdmVjb1kza3dBZCt6QUI1Wmt3UUBtYWlsLmdtYWlsLmNvbT4gPENBTnRYaG12dUpHMkxkWXpSRGVXRldpU01HNW1iK3pmQkxWZ0VhQm10dHkyV0ZQaXhFd0BtYWlsLmdtYWlsLmNvbT4gPENBK2R1aW1ONCtZVTMtelN4ZnV1ek9OLVAtcnI4NVBTcHMrMDFDMW5Wa2twSkxjUmR0d0BtYWlsLmdtYWlsLmNvbT4gPENBTnRYaG10eG9oTzRSZmhVYzBCUzRhMmZXMTlKY2pEYmUtOHEwdzlNSDFXalhjbzh6UUBtYWlsLmdtYWlsLmNvbT4="},"prevInTopic":6284,"nextInTopic":6286,"prevInTime":6284,"nextInTime":6286,"topicId":6279,"numMessagesInTopic":11,"msgSnippet":"... Self-adaptive mutation rates are a terrible idea! Please read this for an explanation: Clune J, Misevic D, Ofria C, Lenski RE, Elena SF, and Sanju√ün R","rawEmail":"Return-Path: &lt;jclune@...&gt;\r\nX-Sender: jclune@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 32704 invoked by uid 102); 30 Apr 2014 16:56:16 -0000\r\nX-Received: from unknown (HELO mtaq5.grp.bf1.yahoo.com) (10.193.84.36)\n  by m8.grp.bf1.yahoo.com with SMTP; 30 Apr 2014 16:56:16 -0000\r\nX-Received: (qmail 31925 invoked from network); 30 Apr 2014 16:56:16 -0000\r\nX-Received: from unknown (HELO mail-ig0-f177.google.com) (209.85.213.177)\n  by mtaq5.grp.bf1.yahoo.com with SMTP; 30 Apr 2014 16:56:16 -0000\r\nX-Received: by mail-ig0-f177.google.com with SMTP id h3so2110434igd.10\n        for &lt;neat@yahoogroups.com&gt;; Wed, 30 Apr 2014 09:56:16 -0700 (PDT)\r\nX-Received: by 10.50.154.73 with SMTP id vm9mr39878222igb.14.1398876975946;\n        Wed, 30 Apr 2014 09:56:15 -0700 (PDT)\r\nReturn-Path: &lt;jclune@...&gt;\r\nX-Received: from [10.0.1.12] (host-69-146-98-193.lar-wy.client.bresnan.net. [69.146.98.193])\n        by mx.google.com with ESMTPSA id hx10sm7335034igb.12.2014.04.30.09.56.14\n        for &lt;neat@yahoogroups.com&gt;\n        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);\n        Wed, 30 Apr 2014 09:56:15 -0700 (PDT)\r\nContent-Type: multipart/alternative; boundary=&quot;Apple-Mail=_0D0CD274-F576-4D06-A935-571F91F2772D&quot;\r\nMessage-Id: &lt;F52A71D7-DF35-4A30-A3FC-97F646203847@...&gt;\r\nMime-Version: 1.0 (Mac OS X Mail 7.2 &#92;(1874&#92;))\r\nDate: Wed, 30 Apr 2014 10:56:09 -0600\r\nReferences: &lt;CA+duimO24sakOXSMVuqbEex8+ziBmQHvecoY3kwAd+zAB5ZkwQ@...&gt; &lt;CANtXhmvuJG2LdYzRDeWFWiSMG5mb+zfBLVgEaBmtty2WFPixEw@...&gt; &lt;CA+duimN4+YU3-zSxfuuzON-P-rr85PSps+01C1nVkkpJLcRdtw@...&gt; &lt;CANtXhmtxohO4RfhUc0BS4a2fW19JcjDbe-8q0w9MH1WjXco8zQ@...&gt;\r\nTo: neat users group group &lt;neat@yahoogroups.com&gt;\r\nIn-Reply-To: &lt;CANtXhmtxohO4RfhUc0BS4a2fW19JcjDbe-8q0w9MH1WjXco8zQ@...&gt;\r\nX-Mailer: Apple Mail (2.1874)\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Jeff Clune &lt;jclune@...&gt;\r\nSubject: Re: [neat] New paper: Automated Generation of Environments to Test the General Learning Capabilities of AI Agents\r\nX-Yahoo-Group-Post: member; u=464818732; y=aRhK1hfOkxj6WxdEGb9ZfGn4dg3nZLuaHEPWfiw9kqasnu1gYaPs\r\nX-Yahoo-Profile: jeffreyclune\r\n\r\n\r\n--Apple-Mail=_0D0CD274-F576-4D06-A935-571F91F2772D\r\nContent-Transfer-Encoding: quoted-printable\r\nContent-Type: text/plain;\n\tcharset=windows-1252\r\n\r\n\n&gt; Yes, it is clear now. I guess one could also encode the mutation rates i=\r\nn the genotype, like in Evolution Strategies, and make these parameters sel=\r\nf-adaptive.\n\n\nSelf-adaptive mutation rates are a terrible idea! Please read=\r\n this for an explanation: \n\nClune J, Misevic D, Ofria C, Lenski RE, Elena S=\r\nF, and Sanju=E1n R (2008) \nNatural selection fails to optimize mutation rat=\r\nes for long-term adaptation on rugged fitness landscapes. PLoS Computationa=\r\nl Biology 4(9): e1000187. \npdf: http://jeffclune.com/publications/Clune-Evo=\r\nlvingMutationRates-PLoSCB-2008.pdf\n\nI have spent years asking anyone I enco=\r\nunter who advocates self-adaptive mutation rates for evidence that they wor=\r\nk, or even an argument as to why they could work, and all those conversatio=\r\nns have come up empty. Mostly I end up convincing them that self-adaptive m=\r\nutation rates are a bad idea, or they end up defending something other than=\r\n self-adaptive mutation rates (e.g. Rechenberg=92s 1/5th rule, which is not=\r\n an example of a self adaptive mutation rate: it=92s an externally controll=\r\ned schedule). \n\nFor some reason people believe they are a good idea, but wi=\r\nthout evidence or intuition. I=92m constantly surprised at how persistent t=\r\nhis errant belief is. I think it=92s because ultimately we want to believe =\r\nthat evolution is good at optimizing everything, and we don=92t want to hav=\r\ne to set parameters, so we feel like we should just turn them over to evolu=\r\ntion. But we have lots of evidence of evolution being short-sighted (e.g. i=\r\nt doesn=92t evolve modularity when it would help: http://goo.gl/2vzFv). \n\nS=\r\norry to jump on your side comment on this issue, but I=92m trying to spread=\r\n the word in the community that self-adaptive mutation rates do not work. \n=\r\n\n\n\n&gt; If I remember correctly when reading Soltoggio&#39;s paper, he used some c=\r\nonstraints when evolving the parameters of the plasticity rule and specific=\r\nally, A-D were in the range [-1,1] and eta in the range [-100,100]. Did you=\r\n use any similar constraints? \n&gt; \n&gt; Just out of curiosity, what activation =\r\nfunction did you use for these n+1 outputs that correspond to the classes? =\r\nDid you use a softmax activation function to interpret the outputs as a pro=\r\nbability distribution (and consequently selected the class probabilisticall=\r\ny) or did you just select the class based on the highest output among these=\r\n neurons? \n&gt; \n&gt; \n&gt; The number of states is independent of the number of act=\r\nions. Different actions in state A may all lead to state B but provide diff=\r\nerent reward values.\n&gt; \n&gt; So, how many states did you use for your simulati=\r\nons? Is it 4 (like in Figure 1)? I might have missed that when reading the =\r\npaper, this is why I asked whether the number of actions correspond to the =\r\nnumber of states.\n&gt; \n&gt;   \n&gt; 3) On page 3 you say that &quot;the proportion of st=\r\nate transitions that provide a reward value is 0.5&quot;. It is not clear to me,=\r\n however, what the reward values are. Do all transitions that have a reward=\r\n value have the *same* reward value (e.g. equal to 1), or does this value v=\r\nary?\n&gt; \n&gt; For transitions that provide a reward, the reward is selected uni=\r\nformly from the range [0, 1).\n&gt;  \n&gt; \n&gt; Also, regarding the &quot;maximum possibl=\r\ne reward maxRx&quot;, do you mean the &quot;return (sum of rewards) obtained by the o=\r\nptimal policy&quot;? If you have the *same* reward value on the transitions (as =\r\nmentioned above) then it is easy to calculate maxRx; if the reward values v=\r\nary then I guess you have to calculate maxRx using dynamic programming; the=\r\n initial state and the trial length matters, especially in the case where y=\r\nou have 16 actions (states?) and trial length =3D 4.\n&gt; \n&gt; Because the lengt=\r\nh of trials is relatively small (and the MDPs deterministic) we calculate t=\r\nhe maximum return via a simple brute force method that tries every possible=\r\n sequence of actions for the specific trial length in question.\n&gt; \n&gt; Ok, it=\r\n&#39;s clear now.\n&gt; \n&gt;   \n&gt; Yes, this is an interesting question in general, an=\r\nd certainly previous results on simple deceptive domains indicate that for =\r\ndelayed-reward MDP environments like you describe (neuro)evolution will lik=\r\nely get stuck if the search isn&#39;t aided by something like Novelty Search. I=\r\nn our paper we don&#39;t worry about this issue as we are comparing the perform=\r\nance of the different neural network models relative to each other and are =\r\nnot particularly interested in their performance relative to the maximum po=\r\nssible (we do scale the results relative to the maximum possible but this i=\r\ns simply to make aggregation of results easier).\n&gt; \n&gt; One issue at a time :=\r\n)\n&gt; \n&gt; \n&gt; \n\n\r\n--Apple-Mail=_0D0CD274-F576-4D06-A935-571F91F2772D\r\nContent-Transfer-Encoding: quoted-printable\r\nContent-Type: text/html;\n\tcharset=windows-1252\r\n\r\n&lt;html&gt;&lt;head&gt;&lt;meta http-equiv=3D&quot;Content-Type&quot; content=3D&quot;text/html charset=\r\n=3Dwindows-1252&quot;&gt;&lt;/head&gt;&lt;body style=3D&quot;word-wrap: break-word; -webkit-nbsp-=\r\nmode: space; -webkit-line-break: after-white-space;&quot;&gt;&lt;br&gt;&lt;div&gt;&lt;blockquote t=\r\nype=3D&quot;cite&quot;&gt;&lt;div style=3D&quot;background-color: rgb(255, 255, 255); position: =\r\nstatic; z-index: auto;&quot;&gt;&lt;div id=3D&quot;ygrp-mlmsg&quot; style=3D&quot;position:relative;&quot;=\r\n&gt;&lt;div id=3D&quot;ygrp-msg&quot; style=3D&quot;z-index: 1;&quot;&gt;&lt;div id=3D&quot;ygrp-text&quot;&gt;&lt;div dir=\r\n=3D&quot;ltr&quot;&gt;&lt;div&gt;&lt;div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;div class=3D&quot;gmail_quote&quot;&gt;&lt;d=\r\niv&gt;Yes, it is clear now. I guess one could also encode the mutation rates i=\r\nn the genotype, like in Evolution Strategies, and make these parameters sel=\r\nf-adaptive. &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/b=\r\nlockquote&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;&lt;div style=3D&quot;font-family: Tim=\r\nes-Roman;&quot;&gt;Self-adaptive mutation rates are a terrible idea! Please read th=\r\nis for an explanation:&nbsp;&lt;/div&gt;&lt;div style=3D&quot;font-family: Times-Roman;&quot;&gt;=\r\n&lt;br&gt;&lt;/div&gt;&lt;div style=3D&quot;font-family: Times-Roman;&quot;&gt;Clune J, Misevic D, Ofri=\r\na C, Lenski RE, Elena SF, and Sanju=E1n R (2008)&nbsp;&lt;br&gt;Natural selection=\r\n fails to optimize mutation rates for long-term adaptation on rugged fitnes=\r\ns landscapes. PLoS Computational Biology 4(9): e1000187.&nbsp;&lt;/div&gt;&lt;div st=\r\nyle=3D&quot;font-family: Times-Roman;&quot;&gt;pdf:&nbsp;&lt;a href=3D&quot;http://jeffclune.com=\r\n/publications/Clune-EvolvingMutationRates-PLoSCB-2008.pdf&quot;&gt;http://jeffclune=\r\n.com/publications/Clune-EvolvingMutationRates-PLoSCB-2008.pdf&lt;/a&gt;&lt;/div&gt;&lt;div=\r\n style=3D&quot;font-family: Times-Roman;&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div style=3D&quot;font-family: T=\r\nimes-Roman;&quot;&gt;I have spent years asking anyone I encounter who advocates sel=\r\nf-adaptive mutation rates for evidence that they work, or even an argument =\r\nas to why they could work, and all those conversations have come up empty. =\r\nMostly I end up convincing them that self-adaptive mutation rates are a bad=\r\n idea, or they end up defending something other than self-adaptive mutation=\r\n rates (e.g. Rechenberg=92s 1/5th rule, which is not an example of a self a=\r\ndaptive mutation rate: it=92s an externally controlled schedule).&nbsp;&lt;/di=\r\nv&gt;&lt;div style=3D&quot;font-family: Times-Roman;&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div style=3D&quot;font-fam=\r\nily: Times-Roman;&quot;&gt;For some reason people believe they are a good idea, but=\r\n without evidence or intuition. I=92m constantly surprised at how persisten=\r\nt this errant belief is. I think it=92s because ultimately we want to belie=\r\nve that evolution is good at optimizing everything, and we don=92t want to =\r\nhave to set parameters, so we feel like we should just turn them over to ev=\r\nolution. But we have lots of evidence of evolution being short-sighted (e.g=\r\n. it doesn=92t evolve modularity when it would help:&nbsp;&lt;a href=3D&quot;http:/=\r\n/goo.gl/2vzFv&quot;&gt;http://goo.gl/2vzFv&lt;/a&gt;).&nbsp;&lt;/div&gt;&lt;div style=3D&quot;font-fami=\r\nly: Times-Roman;&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div style=3D&quot;font-family: Times-Roman;&quot;&gt;Sorry =\r\nto jump on your side comment on this issue, but I=92m trying to spread the =\r\nword in the community that self-adaptive mutation rates do not work.&nbsp;&lt;=\r\n/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;br&gt;&lt;blockquote type=3D&quot;cite&quot;&gt;&lt;div=\r\n style=3D&quot;background-color: rgb(255, 255, 255); position: static; z-index: =\r\nauto;&quot;&gt;&lt;div id=3D&quot;ygrp-mlmsg&quot; style=3D&quot;position:relative;&quot;&gt;&lt;div id=3D&quot;ygrp-=\r\nmsg&quot; style=3D&quot;z-index: 1;&quot;&gt;&lt;div id=3D&quot;ygrp-text&quot;&gt;&lt;div dir=3D&quot;ltr&quot;&gt;&lt;div&gt;&lt;div=\r\n&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;div class=3D&quot;gmail_quote&quot;&gt;&lt;div&gt;If I remember c=\r\norrectly when reading Soltoggio&#39;s paper, he used some constraints when evol=\r\nving the parameters of the plasticity rule and specifically, A-D were in th=\r\ne range [-1,1] and eta in the range [-100,100]. Did you use any similar con=\r\nstraints?&nbsp;&lt;/div&gt;\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Just out of curiosity, what activ=\r\nation function did you use for these n+1 outputs that correspond to the cla=\r\nsses? Did you use a softmax activation function to interpret the outputs as=\r\n a probability distribution (and consequently selected the class probabilis=\r\ntically) or did you just select the class based on the highest output among=\r\n these neurons?&nbsp;&lt;/div&gt;\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;blockquote clas=\r\ns=3D&quot;gmail_quote&quot; style=3D&quot;margin:0px 0px 0px 0.8ex;border-left-width:1px;b=\r\norder-left-color:rgb(204,204,204);border-left-style:solid;&quot;&gt;&lt;div dir=3D&quot;ltr=\r\n&quot;&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;\n&lt;div class=3D&quot;gmail_quote&quot;&gt;\n&lt;div&gt;The number o=\r\nf states is independent of the number of actions. Different actions in stat=\r\ne A may all lead to state B but provide different reward values.&lt;/div&gt;&lt;/div=\r\n&gt;&lt;/div&gt;&lt;/div&gt;&lt;/blockquote&gt;\n\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;So, how many states did you=\r\n use for your simulations? Is it 4 (like in Figure 1)? I might have missed =\r\nthat when reading the paper, this is why I asked whether the number of acti=\r\nons correspond to the number of states.&lt;/div&gt;\n\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;&nbsp;&n=\r\nbsp;&lt;/div&gt;&lt;blockquote class=3D&quot;gmail_quote&quot; style=3D&quot;margin:0px 0px 0px 0.8=\r\nex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-sty=\r\nle:solid;&quot;&gt;&lt;div dir=3D&quot;ltr&quot;&gt;\n&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;div class=3D&quot;gmail=\r\n_quote&quot;&gt;\n&lt;div&gt;&lt;blockquote class=3D&quot;gmail_quote&quot; style=3D&quot;margin:0px 0px 0px=\r\n 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left=\r\n-style:solid;&quot;&gt;&lt;div dir=3D&quot;ltr&quot;&gt;&lt;div&gt;&lt;/div&gt;&lt;div&gt;3) On page 3 you say that &quot;=\r\nthe proportion of state transitions that provide a reward value is 0.5&quot;. It=\r\n is not clear to me, however, what the reward values are. Do all transition=\r\ns that have a reward value have the *same* reward value (e.g. equal to 1), =\r\nor does this value vary?&lt;/div&gt;\n\n\n\n&lt;/div&gt;&lt;/blockquote&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;/div&gt;&lt;=\r\ndiv&gt;For transitions that provide a reward, the reward is selected uniformly=\r\n from the range [0, 1).&lt;/div&gt;&lt;div&gt;&lt;div&gt;&nbsp;&lt;/div&gt;&lt;blockquote class=3D&quot;gma=\r\nil_quote&quot; style=3D&quot;margin:0px 0px 0px 0.8ex;border-left-width:1px;border-le=\r\nft-color:rgb(204,204,204);border-left-style:solid;&quot;&gt;\n\n\n\n&lt;div&gt;&lt;div dir=3D&quot;lt=\r\nr&quot;&gt;\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Also, regarding the &quot;maximum possible reward maxRx&quot;=\r\n, do you mean the &quot;return (sum of rewards) obtained by the optimal policy&quot;?=\r\n If you have the *same* reward value on the transitions (as mentioned above=\r\n) then it is easy to calculate&nbsp;maxRx; if the reward&nbsp;values vary t=\r\nhen I guess you have to calculate&nbsp;maxRx&nbsp;using dynamic programming=\r\n; the initial state and the trial length&nbsp;matters, especially in the ca=\r\nse where you have 16 actions (states?) and trial length =3D 4.&lt;/div&gt;\n\n\n\n&lt;/d=\r\niv&gt;&lt;/div&gt;&lt;/blockquote&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;Because the length of trial=\r\ns is relatively small (and the MDPs deterministic) we calculate the maximum=\r\n return via a simple brute force method that tries every possible sequence =\r\nof actions for the specific trial length in question.&lt;/div&gt;\n\n&lt;/div&gt;&lt;/div&gt;&lt;/=\r\ndiv&gt;&lt;/blockquote&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Ok, it&#39;s clear now.&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/di=\r\nv&gt;&lt;div&gt;&nbsp;&nbsp;&lt;/div&gt;&lt;blockquote class=3D&quot;gmail_quote&quot; style=3D&quot;margin:=\r\n0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);=\r\nborder-left-style:solid;&quot;&gt;\n\n&lt;div&gt;&lt;div dir=3D&quot;ltr&quot;&gt;&lt;div class=3D&quot;gmail_extra=\r\n&quot;&gt;&lt;div class=3D&quot;gmail_quote&quot;&gt;&lt;div&gt;Yes, this is an interesting question in g=\r\neneral, and certainly previous results on simple deceptive domains indicate=\r\n that for delayed-reward MDP environments like you describe (neuro)evolutio=\r\nn will likely get stuck if the search isn&#39;t aided by something like Novelty=\r\n Search. In our paper we don&#39;t worry about this issue as we are comparing t=\r\nhe performance of the different neural network models relative to each othe=\r\nr and are not particularly interested in their performance relative to the =\r\nmaximum possible (we do scale the results relative to the maximum possible =\r\nbut this is simply to make aggregation of results easier).&lt;/div&gt;\n\n&lt;/div&gt;&lt;/d=\r\niv&gt;&lt;/div&gt;&lt;/div&gt;&lt;/blockquote&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;One issue at a time :)&lt;/div=\r\n&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;br class=3D&quot;webkit-bloc=\r\nk-placeholder&quot;&gt;&lt;/div&gt;\n\n    &lt;/div&gt;\n     \n\n    \n\n&lt;/div&gt;\n\n\n\n&lt;!-- end group ema=\r\nil --&gt;\n\n&lt;/blockquote&gt;&lt;/div&gt;&lt;br&gt;&lt;/body&gt;&lt;/html&gt;\r\n--Apple-Mail=_0D0CD274-F576-4D06-A935-571F91F2772D--\r\n\n"}}