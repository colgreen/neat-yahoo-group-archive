{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"er3lqm5dB3kiLS73v7g6B3cW8U3penEFW3nsKd6FzzanBCIIEhEaXWXr-T8OFc9leAK2hY9y3DkSXOHko0IocNEQUx2sqs63ugKCVRVDHh4p","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: Growing Neural Gas and CPPNs","postDate":"1230443666","msgId":4529,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGdqNzRhaSszNWNvQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGdqNXQ4NitucWZtQGVHcm91cHMuY29tPg=="},"prevInTopic":4527,"nextInTopic":4530,"prevInTime":4528,"nextInTime":4530,"topicId":4527,"numMessagesInTopic":7,"msgSnippet":"Peter, once again you make another interesting contribution.  Watching the node patterns form certainly got my imagination going.  But let me ask you one","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 41965 invoked from network); 28 Dec 2008 05:54:28 -0000\r\nX-Received: from unknown (66.218.67.95)\n  by m52.grp.scd.yahoo.com with QMQP; 28 Dec 2008 05:54:28 -0000\r\nX-Received: from unknown (HELO n17c.bullet.sp1.yahoo.com) (69.147.64.126)\n  by mta16.grp.scd.yahoo.com with SMTP; 28 Dec 2008 05:54:27 -0000\r\nX-Received: from [69.147.65.151] by n17.bullet.sp1.yahoo.com with NNFMP; 28 Dec 2008 05:54:27 -0000\r\nX-Received: from [66.218.67.200] by t5.bullet.mail.sp1.yahoo.com with NNFMP; 28 Dec 2008 05:54:27 -0000\r\nDate: Sun, 28 Dec 2008 05:54:26 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;gj74ai+35co@...&gt;\r\nIn-Reply-To: &lt;gj5t86+nqfm@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: Re: Growing Neural Gas and CPPNs\r\nX-Yahoo-Group-Post: member; u=54567749; y=_U_eX4uT_Eo2AtjVJ_SHoAGy-vnlrqjC7c81zyCrC_uanAJi0MrL\r\nX-Yahoo-Profile: kenstanley01\r\n\r\nPeter, once again you make another interesting contribution.  Watching\nthe =\r\nnode patterns form certainly got my imagination going.  But let me\nask you =\r\none fundamental question about the idea:  Why is it necessary\nto include a =\r\nself-organizing process like a growing neural gas?  Why\ncan&#39;t you just devi=\r\nse a deterministic rule about whether to\ninstantiate a node at a certain po=\r\nsition based on the pixel\nintensities from the CPPN output in its neighborh=\r\nood?  Then the node\npattern would follow deterministically from any CPPN ou=\r\ntput, and it\nwould work quite quickly.  Is there a reason not to do that?\n\n=\r\nken\n\n--- In neat@yahoogroups.com, &quot;petar_chervenski&quot; &lt;petar_chervenski@...&gt;=\r\n\nwrote:\n&gt;\n&gt; Hi all, \n&gt; \n&gt; I was playing around with my latest DCPPN code, a=\r\nnd decided to go \n&gt; browse the web and find something that may help. And I =\r\nfound it, it \n&gt; was right in front of my eyes for so long time and I didn&#39;t=\r\n pay \n&gt; attention to it. \n&gt; \n&gt; I discussed some issues about evolution/deve=\r\nlopment of neural \n&gt; substrates in one previous thread I started. Among oth=\r\ner things, I \n&gt; stated that an approach capable to map any CPPN pattern to =\r\na finite-\n&gt; sized physics body or substrate must be also capable of scaling=\r\n well. \n&gt; Because the CPPN pattern has constant complexity that can be \n&gt; a=\r\npproximated at any resolution, a good approach would be one that can \n&gt; pro=\r\nduce the same body derived from the CPPN pattern, but on any level \n&gt; of de=\r\ntail. \n&gt; \n&gt; Here is one such approach, which effectively interprets the pla=\r\nin CPPN \n&gt; output picture as a probability map that further guides the \n&gt; d=\r\nevelopment of a Growing Neural Gas network. \n&gt; \n&gt; In GNG, the algorithm sta=\r\nrts with 2 unconnected nodes and adds \n&gt; connections and new nodes as time =\r\nprogresses, while in the same time \n&gt; moving the nodes, resulting in closer=\r\n and closer approximation of the \n&gt; distribution of input vectors. \n&gt; \n&gt; Ho=\r\nw can one CPPN picture be turned into a random distribution of input \n&gt; vec=\r\ntors, which still resembles the pattern? It turns out it is \n&gt; trivial. Sup=\r\npose we allocate an array of 100 000 input vectors that \n&gt; will be used dur=\r\ning the development of the GNG mesh. The task is to \n&gt; fill that array. Ple=\r\nase assume that the CPPN picture contains pixels \n&gt; each in the range of [0=\r\n .. 1]. To get a new (the next) input vector, \n&gt; first pick a random (unifo=\r\nrm distribution) coordinate on the CPPN \n&gt; picture. Then, if the pixel valu=\r\ne at the picked coordinate is less (or \n&gt; higher, depends on choice) than r=\r\nandom (0 .. 1), add the coordinate to \n&gt; the array of input vectors. If not=\r\n, then pick another coordinate and \n&gt; do this until a new input vector is f=\r\ninally obtained. It is good to \n&gt; add a &quot;give up&quot; mechanism to avoid enteri=\r\nng infinite loop in case the \n&gt; CPPN picture is really weird one (all 0s). =\r\n\n&gt; \n&gt; So this in effect turns the raw CPPN output into a probability map. \n=\r\n&gt; Then the GNG algorithm can begin developing the network. The animation \n&gt;=\r\n of a developing GNG network in this way is strikingly natural. It can \n&gt; b=\r\ne easily confused with real embryo development. But even though it is \n&gt; re=\r\nally entertaining to watch the mesh develops under the guide of the \n&gt; CPPN=\r\n in such a way, the approach does not solve all issues I raised \n&gt; before. =\r\n\n&gt; \n&gt; First of all, I can still pick any coordinate, i.e. it is not really =\r\na \n&gt; compact finite body. And second, the maximum density (maximum number \n=\r\n&gt; of nodes) is still a matter of choice, it has to be set by the \n&gt; experim=\r\nenter. \n&gt; \n&gt; The way I see it, this particular method can be used as a way =\r\nto \n&gt; evolve HyperNEAT substrate configurations. Don&#39;t confuse it with \n&gt; d=\r\nensities - they are still predetermined at this point, or \n&gt; connectivity -=\r\n HyperNEAT has a connectivity concept as a pattern in 4D \n&gt; which doesn&#39;t e=\r\nven care about the substrate configuration. \n&gt; \n&gt; But there is definitely a=\r\n merit in the approach. For example, the \n&gt; circular substrate in the food =\r\ngathering task can be represented with \n&gt; 2 concentric circles pattern, whi=\r\nch appears often very early in \n&gt; evolution. Parallel placement schemes are=\r\n created via repeating \n&gt; patterns across x or y. And there are so many man=\r\ny possibilities. I \n&gt; think that now is worth to try out the capability to =\r\nevolve substrate \n&gt; configurations with connectivity at the same time.\n&gt; \n&gt;=\r\n Further exploration of the method will be targeted towards extending \n&gt; th=\r\ne GNG model. For example, another pattern (second output) can be \n&gt; interpr=\r\neted as a probability map for adding new nodes to the network. \n&gt; And anoth=\r\ner pattern specifying neuron types (input/hidden/output). \n&gt; \n&gt; Peter\n&gt; \n&gt; =\r\nP.S. I will upload a binary demonstration of the method to the Files \n&gt; sec=\r\ntion after I complete this message. It will be named \n&gt; &quot;DCPPN_GNG.exe&quot; It =\r\nuses novelty search, where the behavior \n&gt; characterization is an array of =\r\nN 2D vectors, where N is the maximum \n&gt; number of nodes (in this setup - 10=\r\n0). The distance is the average \n&gt; distance between them. It exploits the f=\r\nact that my particular \n&gt; implementation of GNG does not delete nodes - so =\r\nwe know which is \n&gt; which by their order of appearance. Press C to view the=\r\n underlying \n&gt; CPPN probability picture. Press ESC to quit. And if you pres=\r\ns Q, \n&gt; visualization if turned off (runs slightly faster).\n&gt;\n\n\n\n"}}