{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":283334584,"authorName":"petar_chervenski","from":"&quot;petar_chervenski&quot; &lt;petar_chervenski@...&gt;","profile":"petar_chervenski","replyTo":"LIST","senderId":"zf8Ev6OKnEEsr4nKdCfAK-rmjydFD_KluG-n0mTNcwp7cs5HAgSH97-bD1lxNx63FTcycsunrKGKPyLVD7KOcIA0wn-MXV2K1AyMC-pMRo8nsj-9OtA","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Question about hyperneat, NEVH and hidden layers","postDate":"1213082370","msgId":4139,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGcybDl1Mitkbmo5QGVHcm91cHMuY29tPg==","inReplyToHeader":"PGcybDNodis4Z2cxQGVHcm91cHMuY29tPg=="},"prevInTopic":4138,"nextInTopic":4140,"prevInTime":4138,"nextInTime":4140,"topicId":4138,"numMessagesInTopic":4,"msgSnippet":"Well the substrate in NEVH is not a simple single layer perceptron thing, but a Continious Time Recurrent Neural Network (CTRNN). Such networks with enough","rawEmail":"Return-Path: &lt;petar_chervenski@...&gt;\r\nX-Sender: petar_chervenski@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 31267 invoked from network); 10 Jun 2008 07:19:32 -0000\r\nX-Received: from unknown (66.218.67.96)\n  by m52.grp.scd.yahoo.com with QMQP; 10 Jun 2008 07:19:32 -0000\r\nX-Received: from unknown (HELO n26a.bullet.sp1.yahoo.com) (209.131.38.240)\n  by mta17.grp.scd.yahoo.com with SMTP; 10 Jun 2008 07:19:32 -0000\r\nX-Received: from [216.252.122.217] by n26.bullet.sp1.yahoo.com with NNFMP; 10 Jun 2008 07:19:32 -0000\r\nX-Received: from [66.218.69.1] by t2.bullet.sp1.yahoo.com with NNFMP; 10 Jun 2008 07:19:32 -0000\r\nX-Received: from [66.218.67.195] by t1.bullet.scd.yahoo.com with NNFMP; 10 Jun 2008 07:19:32 -0000\r\nDate: Tue, 10 Jun 2008 07:19:30 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;g2l9u2+dnj9@...&gt;\r\nIn-Reply-To: &lt;g2l3hv+8gg1@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;petar_chervenski&quot; &lt;petar_chervenski@...&gt;\r\nSubject: Re: Question about hyperneat, NEVH and hidden layers\r\nX-Yahoo-Group-Post: member; u=283334584; y=rX17vLtB8SLRAHBD_-4qouwn_FM4HfXqadDKRzTF2C6vkYwJA8xTRdqzcg\r\nX-Yahoo-Profile: petar_chervenski\r\n\r\nWell the substrate in NEVH is not a simple single layer perceptron \nthing, =\r\nbut a Continious Time Recurrent Neural Network (CTRNN). Such \nnetworks with=\r\n enough hidden nodes, full interconnectivity and the \nright time constants,=\r\n biases and connection weights, can approximate \nany dynamic system just li=\r\nke the 2 layer feed-forward NNs can \napproximate any continious function. \n=\r\nHyperNEAT is allowed to create any connective pattern in the CTRNN \nsubstra=\r\nte and it also &quot;draws&quot; the time constants and biases through \nthe nodes in =\r\nthe substrate. I can name this thing LeakyHyperNEAT \nbecause the substrate =\r\nhas more properties but it is essentialy the \nsame HyperNEAT/CPPN-NEAT syst=\r\nem at the genetic level. \nOh and one thing about layers.. I don&#39;t assume th=\r\nere are any layers. \nThere are no layers in a fully interconnected network,=\r\n since every \nnode may output to any other node, including itself. There ar=\r\ne no \nlayers in NEAT also, the CPPNs complexify and evolve like in regular =\r\n\nNEAT. \nI believe that the layered topology described in the paper mentione=\r\nd \nis a simplification of the problem based on some kind of priori in \nthe =\r\ntask domain. You can have a completely interconnected network and \nnot assu=\r\nme layered topology but then HyperNEAT may have difficulties \ndiscovering t=\r\nhe right connectivity concept early in evolution. \nSo the short answer to y=\r\nour question is, as long as there are enough \nhidden nodes in the CTRNN net=\r\nwork and HyperNEAT is not restricted to \nparticular types of substrates (li=\r\nke feed-forward-only), it should be \nable to learn to walk. It may be hard,=\r\n though, but with NS applied to \nit, the problems with fitness search disap=\r\npear. \nOne thing I remember you said before some time, if I characterize th=\r\ne \nbehavior as the joint angles over time steps I would be unable to \ndisti=\r\nnguish the good behavior. But I realized that this is not true \nactually. I=\r\nf the humanoid goes forward enough, I can consider it a \ngood behavior, no =\r\nmatter that I don&#39;t keep track of the final XYZ in \nthe BC. \n\nPeter C\n\n--- =\r\nIn neat@yahoogroups.com, &quot;peterberrington&quot; &lt;peterberrington@...&gt; \nwrote:\n&gt;\n=\r\n&gt; Although the papers on hyperneat propose some unique ways of\n&gt; representi=\r\nng the substrate geometrically, I got the impression that\n&gt; the resulting g=\r\nenerated net was always going to be 2 layer\n&gt; feed-forward network (isn&#39;t t=\r\nhe cppn used to construct this?). In \nthe\n&gt; paper on hyperneat checkers how=\r\never, Jason Gauci and Ken Stanley \nquery\n&gt; a single cppn to get connection =\r\nweights for the input/hidden layer,\n&gt; and the hidden/output layer. (A Case =\r\nStudy on the Critical Role of\n&gt; Geometric Regularity in Machine Learning)\n&gt;=\r\n \n&gt; Given that single layer feedforward networks were found incapable of\n&gt; =\r\nlearning linearly inseperable  patterns, it seems like the\n&gt; functionality =\r\nof NEVH could be severely hindered by limiting itself \nto\n&gt; search the spac=\r\ne of single-layer perceptrons. \n&gt; \n&gt; Is there a principled way of extending=\r\n the method used by Gauci and\n&gt; Stanley in the checkers paper to making arb=\r\nitrary amounts hidden\n&gt; layers? Can anyone provide any insights on whether =\r\nlimiting the\n&gt; generated nets to single layer actually inhibits their abili=\r\nty to\n&gt; learn in the domain of humanoid bipedal walking?\n&gt;\n\n\n\n"}}