{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Ken","from":"&quot;Ken&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"A85h4Q9zT_wU6Wm1VEyMVAE3nWxkB4N0j_Fi53_9eT13l5Ni-_ZjCHa2EzkBFJ87d3FX3XfALu2Xf2eBvBbdEuisdttX","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: A question about the Double Pole Benchmark / problems with pole balancing","postDate":"1274202410","msgId":5233,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGhzdWhmYStsdmJjQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGhzc2hobStxamtuQGVHcm91cHMuY29tPg=="},"prevInTopic":5232,"nextInTopic":5234,"prevInTime":5232,"nextInTime":5234,"topicId":5228,"numMessagesInTopic":10,"msgSnippet":"Under Gruau s scheme, only an ANN that can balance for 100,000 timesteps and pass the generalization test is considered a solution. I wanted to add a little","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 79923 invoked from network); 18 May 2010 17:06:51 -0000\r\nX-Received: from unknown (66.196.94.106)\n  by m3.grp.sp2.yahoo.com with QMQP; 18 May 2010 17:06:51 -0000\r\nX-Received: from unknown (HELO n44d.bullet.mail.sp1.yahoo.com) (66.163.169.158)\n  by mta2.grp.re1.yahoo.com with SMTP; 18 May 2010 17:06:51 -0000\r\nX-Received: from [69.147.65.173] by n44.bullet.mail.sp1.yahoo.com with NNFMP; 18 May 2010 17:06:50 -0000\r\nX-Received: from [98.137.34.32] by t15.bullet.mail.sp1.yahoo.com with NNFMP; 18 May 2010 17:06:50 -0000\r\nDate: Tue, 18 May 2010 17:06:50 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;hsuhfa+lvbc@...&gt;\r\nIn-Reply-To: &lt;hsshhm+qjkn@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nFrom: &quot;Ken&quot; &lt;kstanley@...&gt;\r\nSubject: Re: A question about the Double Pole Benchmark / problems with pole balancing\r\nX-Yahoo-Group-Post: member; u=54567749; y=_XdRSYSTspVCQnUTPfEU3fdQVCBabSzgfUraSPofJXvutG1Tr2yU\r\nX-Yahoo-Profile: kenstanley01\r\n\r\n\n\nUnder Gruau&#39;s scheme, only an ANN that can balance for 100,000 timesteps =\r\nand pass the generalization test is considered a solution.\n\nI wanted to add=\r\n a little context on pole balancing.  Pole balancing (especially without ve=\r\nlocities) was once an interesting benchmark because it provided a challenge=\r\n that only the best methods could surpass, and for a few years they would s=\r\nurpass each other in a kind of competition to be the best.  However, I do n=\r\not believe that it any longer provides helpful insight into modern neuroevo=\r\nlution approaches.  This view is only my opinion and open to dispute, but I=\r\n think it&#39;s still worth mentioning why I have begun to take this view:\n\nAll=\r\n of the experimentation on pole balancing in the last 15 years has revealed=\r\n some properties of the problem that distance it considerably from many oth=\r\ner problems of interest:\n\n1) Even the version of the problem without veloci=\r\nties can be solved with one or even zero hidden nodes (with a recurrent loo=\r\np at the top of the network).  This fact suggests that pole balancing is no=\r\nt &quot;hard&quot; in the sense that was once thought.  Usually when we think of a &quot;h=\r\nard&quot; problem in neuroevolution, we mean something that probably requires a =\r\nhigh number of dimensions to solve.  Pole balancing is the opposite: It req=\r\nuires hardly any dimensions.  The discovery that pole balancing can be solv=\r\ned with so few dimensions is initially interesting, but once we know that, =\r\nit is easy to shape algorithms to exploit that fact and thereby seem &quot;bette=\r\nr,&quot; although they would suffer on problems requiring more dimensions.\n\n2) P=\r\nole balancing is about precision rather than about dimensionality.  That is=\r\n, the &quot;hard&quot; aspect of it, once you realize it is low-dimensional, is that =\r\nits weights must be set very precisely or it will be unstable.  Yet the abi=\r\nlity to achieve extreme precision is not really that interesting because ul=\r\ntimately ANNs should not have to hit extremely precise bullseyes or else be=\r\n completely dysfunctional.  Thus a short-term focus on extreme precision (w=\r\nhich can come from strange weight mutation distributions) is probably point=\r\ning us away from the more important problems that face NE in the long run. =\r\n (Ultimately in biological ANNs, precision is achieved through adaptation o=\r\nver lifetime rather than through preset weights).\n\n3) Pole balancing seems =\r\nto have almost no local optima.  This fact was revealed by a few experiment=\r\ns with CMA-ES and NEAT that reduced to population to ridiculously low sizes=\r\n, such as 16.  In fact, NEAT performs better at such a low size, which show=\r\ns that diversity and exploration are liabilities in pole balancing.  The im=\r\nplication is that if you notice a gradient in pole balancing, you should de=\r\ndicate all your resources to following it, because it is almost certainly t=\r\nhe right one.  In other words, there is one big attractor and the aim is to=\r\n shoot up its gradient as fast as possible to a very precise global optimum=\r\n.  The problem is that this characteristic is opposite from almost every in=\r\nteresting problem we would like to solve in neuroevolution, which would be =\r\ncharacterized by deception and profuse local optima.  So being too good at =\r\npole balancing could even be a reason for concern.  (After all, try solving=\r\n other hard problems with a population of 16.)\n\nPole balancing has achieved=\r\n such an iconic status (everyone wants a universal event in which to compet=\r\ne and win) that it has started to be almost a liability for the field.  Thi=\r\ns kind of confusion is a central danger of benchmark obsession.  For exampl=\r\ne, several indirect encoding methods have been tested in pole balancing and=\r\n apparently performed well.  Yet it makes little sense to prove indirect en=\r\ncodings on a problem that require something like 7 or 8 weights and has vir=\r\ntually no regularity in the solution network.  If an indirect encoding is b=\r\netter than a direct encoding at something like that, that may be a reason t=\r\no avoid such an indirect encoding rather than to embrace it.  Unfortunately=\r\n, it is likely that the utility of pole balancing as a benchmark has run it=\r\ns course.\n\nOne last note: I still like pole balancing as a validation metho=\r\nd for a particular NEAT (or other NE) implementation.  That is, it is helpf=\r\nul for making sure your code actually works.  However, that is different fr=\r\nom being a useful benchmark for comparison.\n\nken\n\n\n\n--- In neat@yahoogroups=\r\n.com, &quot;genemerdock&quot; &lt;genemerdock@...&gt; wrote:\n&gt;\n&gt; \n&gt; \n&gt; It should be noted t=\r\nhat the &quot;Single Neuron Solution&quot; to the double pole balancing without veloc=\r\nities has been &quot;discovered&quot; by a number of other TWEANs before NEAT, my own=\r\n system amongst them, that is if the Physics Simulation is correct and the =\r\nspecifications for the agreed solution are the same.\n&gt; \n&gt; What is considere=\r\nd a solution? \n&gt; 1. A NN that can balance the poles for 100000timesteps of =\r\n0.02s, and pass the generalization test with a score over 200?\n&gt; OR\n&gt; 2. A =\r\nNN that can balance the poles for 1000 time steps of 0.02s, and pas the gen=\r\neralization test with a score over 200?\n&gt; \n&gt; \n&gt; --- In neat@yahoogroups.com=\r\n, &quot;Ken&quot; &lt;kstanley@&gt; wrote:\n&gt; &gt;\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; I also gave this answer to Gen=\r\ne in email, but I wanted to answer here as well for others that may be curi=\r\nous:\n&gt; &gt; \n&gt; &gt; In fact, there is a solution to the no-velocity version that =\r\nhas only a single hidden neuron.  In fact, there is a solution with no hidd=\r\nen neurons (which must have a recurrent connection on the output).  The pol=\r\ne balancing tasks turned out to require much less structure than was though=\r\nt, which is something NEAT discovered.\n&gt; &gt; \n&gt; &gt; ken\n&gt; &gt; \n&gt; &gt; --- In neat@ya=\r\nhoogroups.com, &quot;genemerdock&quot; &lt;genemerdock@&gt; wrote:\n&gt; &gt; &gt;\n&gt; &gt; &gt; I&#39;m experien=\r\ncing some strange behavior from NEAT, I&#39;ve rewritten the Double Pole Withou=\r\nt Velocities and with Damping recently using another language, and plugged =\r\nit back in. \n&gt; &gt; &gt; \n&gt; &gt; &gt; After running the experiment many times, every on=\r\nce in a rare while NEAT manages to solve the Double Pole Balancing without =\r\nvelocities and with Damping experiment using 1 Neuron. Is the 1 Neuron solu=\r\ntion possible, or have I made a mistake somewhere while rewriting the exper=\r\niment.\n&gt; &gt; &gt;\n&gt; &gt;\n&gt;\n\n\n\n"}}