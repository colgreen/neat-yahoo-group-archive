{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":7192225,"authorName":"Ian Badcoe","from":"Ian Badcoe &lt;ian_badcoe@...&gt;","profile":"ian_badcoe","replyTo":"LIST","senderId":"mOehCfK_FPwFD0BQB8z_RSqWv4bldo4y3nc6SBd4WKVtHdEHC1UtGFIHFW_fa2feUpRJXRuAt1cuZzyh2TZ0IoU3YXzjUl2wcgk","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] Re: leaky integrator neuron","postDate":"1099143643","msgId":1663,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDUuMS4wLjE0LjAuMjAwNDEwMzAxNDM1NDIuMDI0N2U5MzhAcG9wLm1haWwueWFob28uY28udWs+","inReplyToHeader":"PGNsdTlncytldTVqQGVHcm91cHMuY29tPg==","referencesHeader":"PDUuMS4wLjE0LjAuMjAwNDEwMjkyMTIyMzYuMDI0N2FkNjhAcG9wLm1haWwueWFob28uY28udWs+"},"prevInTopic":1661,"nextInTopic":1664,"prevInTime":1662,"nextInTime":1664,"topicId":1656,"numMessagesInTopic":9,"msgSnippet":"So is it: var = (var + weighted_sum(input)) * f;           f ","rawEmail":"Return-Path: &lt;ian_badcoe@...&gt;\r\nX-Sender: ian_badcoe@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 49852 invoked from network); 30 Oct 2004 15:43:05 -0000\r\nReceived: from unknown (66.218.66.217)\n  by m25.grp.scd.yahoo.com with QMQP; 30 Oct 2004 15:43:05 -0000\r\nReceived: from unknown (HELO cmailm1.svr.pol.co.uk) (195.92.193.18)\n  by mta2.grp.scd.yahoo.com with SMTP; 30 Oct 2004 15:43:05 -0000\r\nReceived: from modem-3808.lemur.dialup.pol.co.uk ([217.135.142.224] helo=giles.yahoo.co.uk)\n\tby cmailm1.svr.pol.co.uk with esmtp (Exim 4.41)\n\tid 1CNvNd-0001y4-4w\n\tfor neat@yahoogroups.com; Sat, 30 Oct 2004 16:43:03 +0100\r\nMessage-Id: &lt;5.1.0.14.0.20041030143542.0247e938@...&gt;\r\nX-Sender: ian_badcoe@...\r\nX-Mailer: QUALCOMM Windows Eudora Version 5.1\r\nDate: Sat, 30 Oct 2004 14:40:43 +0100\r\nTo: neat@yahoogroups.com\r\nIn-Reply-To: &lt;clu9gs+eu5j@...&gt;\r\nReferences: &lt;5.1.0.14.0.20041029212236.0247ad68@...&gt;\r\nMime-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;us-ascii&quot;; format=flowed\r\nX-eGroups-Remote-IP: 195.92.193.18\r\nFrom: Ian Badcoe &lt;ian_badcoe@...&gt;\r\nSubject: Re: [neat] Re: leaky integrator neuron\r\nX-Yahoo-Group-Post: member; u=7192225\r\nX-Yahoo-Profile: ian_badcoe\r\n\r\nSo is it:\n\n          var&#39; = (var + weighted_sum(input)) * f;           f &lt; 1.0\n\nor:\n\n          var&#39; = var * f + weighted_sum(input) * g;           f &lt; 1.0\n\nor even:\n\n         var&#39; = var * (1 - f) + weighted_sum(input) * f;\n\nAll of which are pretty much equivalent, if you fiddle the weights as well...\n\n         Ian\n\nAt 20:34 29/10/2004 +0000, you wrote:\n\n\n&gt;Guys, it should be noted that Ian&#39;s equation is not the actual leaky\n&gt;integrator update equation.  Leaky integrators multiply both the\n&gt;activation level from the previous timestep *and* the incoming\n&gt;activation by a time constant that control how fast activation leaks\n&gt;out and comes in.\n&gt;\n&gt;Actually, (I had forgotten to mention..) my dissertation has a\n&gt;useful introductory section on leaky integrator neurons in its\n&gt;discussion section:\n&gt;\n&gt;http://nn.cs.utexas.edu/keyword?stanley:phd04\n&gt;\n&gt;See section 10.4.2.\n&gt;\n&gt;ken\n&gt;\n&gt;--- In neat@yahoogroups.com, Ian Badcoe &lt;ian_badcoe@y...&gt; wrote:\n&gt; &gt; At 01:59 29/10/2004 +0100, you wrote:\n&gt; &gt;\n&gt; &gt; &gt;Ian Badcoe wrote:\n&gt; &gt; &gt;\n&gt; &gt; &gt; &gt;Hi,\n&gt; &gt; &gt; &gt;         I&#39;ve never heard of it, but an obvious interpretation\n&gt;springs from\n&gt; &gt; &gt; &gt;the name...\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;         What it sounds like, is a neurone with some &quot;memory&quot;\n&gt;in that it\n&gt; &gt; &gt; &gt;keeps a variable from update to update, and its update function\n&gt;would look\n&gt; &gt; &gt; &gt;something like:\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;         var&#39; = var * f + weighted_sum(input);           f &lt; 1.0\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;         So var integrates input across updates, and f acts to\n&gt;slowly leak\n&gt; &gt; &gt; &gt;away the old contents...\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;         whether the output is var itself or f(var) is, I\n&gt;guess, your\n&gt; &gt; &gt; choice...\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;         Ian\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;p.s. presumably you can built one of these from a few standard\n&gt;neurones?\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt;Absolutely. From your explanation I would say that this same\n&gt;effect\n&gt; &gt; &gt;could be achieved with a recursive connection from a neuron&#39;s\n&gt;output\n&gt; &gt; &gt;back into its input. So from that perspective you could say that\n&gt;NEAT as\n&gt; &gt; &gt;it stands can represent this type of functionality through its\n&gt;topology\n&gt; &gt; &gt;searching propoperties. However, relying on topology searching to\n&gt;find\n&gt; &gt; &gt;solutions might not be as effective as actually defining a leaky\n&gt; &gt; &gt;integrater neuron directly. This comes back to our previous\n&gt;[brief]\n&gt; &gt; &gt;discussion regarding implementing different types of neuron and\n&gt; &gt; &gt;activation function - which I hope to look into soon.\n&gt; &gt;\n&gt; &gt; Except after I suggest this, it occurred to me that completely\n&gt;standard\n&gt; &gt; neurones would put unwanted sigmoids onto the definition.  e.g.\n&gt;taking your\n&gt; &gt; idea of a single feedback connection within a single node we would\n&gt;have:\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;           var&#39; = sigmoid(var * f + weighted_sum(input));\n&gt; &gt;\n&gt; &gt; Rather than:\n&gt; &gt;\n&gt; &gt;           var&#39; = var * f + weighted_sum(input);\n&gt; &gt;\n&gt; &gt; [[var * f is the effect of the feedback connection, obviously one\n&gt;would\n&gt; &gt; normally just regard this as part of the weighted sum over inputs\n&gt;but I&#39;m\n&gt; &gt; writting it separately so we can see it :) ]]\n&gt; &gt;\n&gt; &gt; Which brings us back to the point we had briefly some months ago,\n&gt;where\n&gt; &gt; some equations fit reasonably nicely into an ANN framework, but\n&gt;because\n&gt; &gt; they depend so heavily on the linear portion of of the sigmoids,\n&gt;their\n&gt; &gt; weights are constrained so much that it is almost impossible to\n&gt;evolve\n&gt; &gt; them.  Remember when we discusses whether one could evolve\n&gt;networks for A +\n&gt; &gt; B or A * B using sigmoidal neurones?\n&gt; &gt;\n&gt; &gt;          Ian\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; http://livingathome.sourceforge.net/ - evolution for the desktop\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;Yahoo! Groups Links\n&gt;\n&gt;\n&gt;\n&gt;\n\n\n\nhttp://livingathome.sourceforge.net/ - evolution for the desktop\n\n\n\n"}}