{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":281645563,"authorName":"afcarl2","from":"&quot;afcarl2&quot; &lt;a.carl@...&gt;","profile":"afcarl2","replyTo":"LIST","senderId":"3WNNo5awDWG3fs9kLkQcULjMt69pdK14tlDCptUHqvM7yGMSYGCXuqGWKkSsdZUFDZLbG6EXf140DrWWQWMM8sY","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Tile Coding and HyperNEAT","postDate":"1178914400","msgId":3270,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGYyMmlwMCtqamJwQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGYyMmdyNytwZGsyQGVHcm91cHMuY29tPg=="},"prevInTopic":3269,"nextInTopic":3271,"prevInTime":3269,"nextInTime":3271,"topicId":3214,"numMessagesInTopic":27,"msgSnippet":"IMHO, it appears that the extreme cited dismisses an element that holds value. The sole focus on geometry (i.e. 1d, 2d or 3d), is a simplified subset of a","rawEmail":"Return-Path: &lt;a.carl@...&gt;\r\nX-Sender: a.carl@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 18513 invoked from network); 11 May 2007 20:13:41 -0000\r\nReceived: from unknown (66.218.67.36)\n  by m36.grp.scd.yahoo.com with QMQP; 11 May 2007 20:13:41 -0000\r\nReceived: from unknown (HELO n20d.bullet.scd.yahoo.com) (66.218.67.246)\n  by mta10.grp.scd.yahoo.com with SMTP; 11 May 2007 20:13:41 -0000\r\nReceived: from [66.218.69.1] by n20.bullet.scd.yahoo.com with NNFMP; 11 May 2007 20:13:21 -0000\r\nReceived: from [66.218.66.74] by t1.bullet.scd.yahoo.com with NNFMP; 11 May 2007 20:13:21 -0000\r\nDate: Fri, 11 May 2007 20:13:20 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;f22ip0+jjbp@...&gt;\r\nIn-Reply-To: &lt;f22gr7+pdk2@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0\r\nFrom: &quot;afcarl2&quot; &lt;a.carl@...&gt;\r\nSubject: Re: Tile Coding and HyperNEAT\r\nX-Yahoo-Group-Post: member; u=281645563; y=y5qCH9TUrYY86CEOfHLh4I7y_s_SeRObmHQy81PbOFERJw\r\nX-Yahoo-Profile: afcarl2\r\n\r\nIMHO, it appears that the extreme cited dismisses an element that \nholds va=\r\nlue. The sole focus on geometry (i.e. 1d, 2d or 3d), is a \nsimplified subse=\r\nt of a problem dimensionality. Most useful problems \nhave a higher design s=\r\npace dimensionality in which global functions \nare comprised of a collectio=\r\nn of regional &quot;global functions&quot; along \nwith their individual application s=\r\nub-domain definitions, which taken \ntogether makeup the whole.\n\nAn approach=\r\n which works well within a regional sub-domain isn&#39;t bad \nbecause it cannot=\r\n adequately address all of the design space, unless \nof course it can co-ev=\r\nolve different functions in different sub-\nregions along with the correspon=\r\nding regional application /mapping \nbetween them.\n\nAlso, as your example il=\r\nlustrates, it would be seriously \ncounterproductive to throw away proximity=\r\n-based information, whether \ngeometry or other higher dimensional informati=\r\non.\n\nMany useful problems are comprised of discontinuities that have to be =\r\n\ndealt with individually. &quot;Divide-and-conquer&quot; is a powerful approach \nwhen=\r\n properly applied.\n\n\n--- In neat@yahoogroups.com, &quot;Kenneth Stanley&quot; &lt;kstanl=\r\ney@...&gt; wrote:\n&gt;\n&gt; Joe, I agree this is a revealing discussion.  Tile codin=\r\ng to me a \nis \n&gt; a telling example of a significant misdirection of effort =\r\nin \nmachine \n&gt; learning right now.  \n&gt; \n&gt; Most of what you said is factuall=\r\ny true.  But the spin you put on \nit \n&gt; is wrong.  It is correct that tile =\r\ncoding breaks up the \nstate/action \n&gt; space into little pieces to make the =\r\nright behavior for each little \n&gt; region easier to compute.  As you put it,=\r\n &quot;subtiles can better fit \n&gt; the value function being learned. Note that th=\r\nere is very little \n&gt; generalization desired here.&quot;  You say that like it&#39;s=\r\n a good thing.\n&gt; \n&gt; However, the fact that there is a need to do something =\r\nlike that is \n&gt; more a symptom of a serious disease in RL than an accomplis=\r\nhment we \n&gt; should be congratulating ourselves for.  It&#39;s like using cocain=\r\ne to \n&gt; stay awake at work and claiming that it was a good idea because you=\r\n \n&gt; were more alert.  The fact is the whole approach is sick to begin \n&gt; wi=\r\nth if it needs cocaine to function properly.\n&gt; \n&gt; And that&#39;s what tile codi=\r\nng is really indicating: Much of RL is \nDOA.  \n&gt; Tile coding is a symptom o=\r\nf a larger sickness.  You said it \n&gt; yourself: &quot;[most] RL is inherently inc=\r\napable of performing model \n&gt; selection.&quot;  Well, if what that means is that=\r\n you can&#39;t exploit \n&gt; geometry, it&#39;s all a dead end.  I am not certain that=\r\n RL (aside \nfrom \n&gt; NEAT+Q-type stuff) is really incapable of optimizing th=\r\ne model \n&gt; because who knows what we might realize how to do in the future.=\r\n  \n&gt; However, for now, RL is falling back on tile coding because it is \n&gt; m=\r\noving in the wrong direction.\n&gt; \n&gt; Here is what is really going on:  Each v=\r\nariable in the state/action \n&gt; space is a dimension along which the value f=\r\nunction varies.  A good \n&gt; learning algorithm would represent how the value=\r\n function varies \nwith \n&gt; respect to each state variable.  However, such va=\r\nriation may be \n&gt; complex, i.e. the function could be pretty complicated.  =\r\nThe \nlearning \n&gt; methods (i.e. supervised function approximators) inside RL=\r\n are \n&gt; sufficiently bad that they cannot handle approximating functions \nl=\r\nike \n&gt; that.  So what do we do?  We break the whole space into chunks.  \nNo=\r\nw \n&gt; the appropriate action for each little chunk requires a much \nsimpler =\r\n\n&gt; function, so we have a chance with our poor learning algorithm to \n&gt; may=\r\nbe get all these little simple functions right instead of only a \n&gt; few big=\r\n complicated functions.  \n&gt; \n&gt; In other words, we have a poor algorithm and=\r\n the cure is to destroy \n&gt; what variational structure there was to begin wi=\r\nth so that we can \n&gt; look at every little bit of the problem separately.   =\r\nSo we have \nnow \n&gt; lost the ability to exploit all the useful relationships=\r\n that \n&gt; initially existed in the space.  States that are related are now \n=\r\n&gt; broken apart and must be learned separately, that is, the geometry \n&gt; has=\r\n been destroyed! The fact that many see such an operation as a \n&gt; step in t=\r\nhe right direction is symptomatic of serious misdirection \nin \n&gt; the field.=\r\n  If that&#39;s the best we can do to make RL easier, than RL \n&gt; is in serious =\r\ntrouble!\n&gt; \n&gt; Think of it like this:  Take a game like chess, which I learn=\r\ned as \na \n&gt; little kid.  Now take the 64 squares and cut each piece out of =\r\nthe \n&gt; board individually.  Now sprinkle them all randomly all over your \n&gt;=\r\n living room.  Each square still represents the same location it was \n&gt; ori=\r\nginally taken from in the board.  It&#39;s just you can&#39;t see where \n&gt; they wer=\r\ne anymore.  Now place the chess pieces in the right \nstarting \n&gt; squares an=\r\nd teach a little kid to play chess.  Think he or she \nwould \n&gt; learn anythi=\r\nng at all?\n&gt; \n&gt; Well, that&#39;s exactly what tile coding is!  A method that le=\r\narns \nchess \n&gt; (or anything else that has implicit or explicit geometry) ne=\r\neds to \n&gt; know how the positions relate to each other geometrically because=\r\n \n&gt; there is massive regularity being lost without that information.  \n&gt; Wh=\r\nat kind of crazy algorithm would purposely put a chess board into \na \n&gt; mea=\r\nningless order before learning begins?  A method that &quot;benefits&quot; \n&gt; from su=\r\nch an approach is clearly DOA.  RL researchers should be \n&gt; seriously conce=\r\nrned about tile coding being necessary at all, not \n&gt; happy about it.\n&gt; \n&gt; =\r\nSo I stick to my position: Tile coding is anti-geometry and anti-\n&gt; represe=\r\nntation.  It deserves no credit whatsoever for &quot;respecting&quot; \n&gt; anything.\n&gt; =\r\n\n&gt; ken\n&gt; \n&gt; --- In neat@yahoogroups.com, Joseph Reisinger &lt;joeraii@&gt; wrote:=\r\n\n&gt; &gt;\n&gt; &gt; I&#39;ve been aching to reply to this post for a while, and I \nfinally=\r\n  \n&gt; &gt; have enough free time to do so. I think we could have a really  \n&gt; &gt;=\r\n interesting discussion here, hopefully at least more interesting \n&gt; than  =\r\n\n&gt; &gt; the NFL tangent.\n&gt; &gt; \n&gt; &gt; &gt;&gt; Sure, but tile-coding does respect at lea=\r\nst one form of \ngeometry:\n&gt; &gt; &gt;&gt; Nearby elements in the state space are kno=\r\nwn to be nearby, and \n&gt; thus\n&gt; &gt; &gt;&gt; are grouped in the same tile.\n&gt; &gt; &gt;\n&gt; &gt;=\r\n &gt; I have to dispute this characterization of tile coding\n&gt; &gt; &gt; as &quot;respect=\r\ning at least one form of geometry.&quot; I think you are\n&gt; &gt; &gt; being unnecessari=\r\nly equitable toward tile coding.\n&gt; &gt; &gt;\n&gt; &gt; &gt; What you are saying is that in=\r\n effect taking a nice sculpture \nand\n&gt; &gt; &gt; cutting it into pieces &quot;respects=\r\n&quot; its geometry because those \n&gt; little\n&gt; &gt; &gt; pieces are not broken up any f=\r\nurther than that. It&#39;s like saying\n&gt; &gt; &gt; that someone who cut your head off=\r\n &quot;respected&quot; your head by \n&gt; keeping\n&gt; &gt; &gt; its internal integrity intact. I=\r\nn fact, tile coding is \npeforming a\n&gt; &gt; &gt; grievous violation against the ex=\r\nisting geometry of the domain, \n&gt; and\n&gt; &gt; &gt; does not deserve to be credited=\r\n with respecting geometry\n&gt; &gt; &gt; whatsoever. I&#39;m hard pressed to imagine how=\r\n one could do worse\n&gt; &gt; &gt; beyond cutting things up into even tinier and tin=\r\nier bits; but \n&gt; even\n&gt; &gt; &gt; then, those bits still contain &quot;nearby elements=\r\n in the state\n&gt; &gt; &gt; space.&quot; So that isn&#39;t saying much.\n&gt; &gt; \n&gt; &gt; Yeah, from =\r\nthe way your framing this argument, e.g. tile-coding \n&gt; used  \n&gt; &gt; in the G=\r\nA model-selection sense, you&#39;re absolutely right. I&#39;ll \nget  \n&gt; &gt; back to e=\r\nxactly what I mean by that in a bit. For now lets try \nto  \n&gt; &gt; reframe the=\r\n issue from an RL perspective, which is where tile-\n&gt; codings  \n&gt; &gt; are pre=\r\ndominantly used. In RL, the tile-coding is just a  \n&gt; &gt; representation for =\r\na function approximator (in a sense its sort \nof  \n&gt; &gt; like a really simple=\r\n spline cure) that learns in a supervised \n&gt; manner.  \n&gt; &gt; Tile coding make=\r\ns a lot of sense in this domain because you can  \n&gt; &gt; calculate with a good=\r\n deal of precision how much some particular \n&gt; tile  \n&gt; &gt; differs from the =\r\nexpected value of the function being \napproximated  \n&gt; &gt; (in this case the =\r\nBellman error).\n&gt; &gt; Tiles hat is cover a broad area where the value functio=\r\nn changes \na  \n&gt; &gt; lot (&quot;have bad fit&quot;, &quot;are too general&quot;, etc) are then sp=\r\nlit so \n&gt; that  \n&gt; &gt; the subtiles can better fit the value function being l=\r\nearned. \nNote  \n&gt; &gt; that there is very little generalization desired here; =\r\nthe best \n&gt; thing  \n&gt; &gt; given infinite computational resources would be to =\r\nhave a whole \n&gt; ton  \n&gt; &gt; of itty-bitty tiles that fit the value function p=\r\nerfectly.\n&gt; &gt; \n&gt; &gt; Anyway, since we&#39;re in the standard RL framework, there =\r\nis really \n&gt; no  \n&gt; &gt; way of learning the &quot;geometry&quot; of a value function (w=\r\nell, \n&gt; technically  \n&gt; &gt; there is, but thats a long tangent towards a real=\r\nly interesting  \n&gt; &gt; research area). Maybe if the geometry was given by the=\r\n \n&gt; experimenter  \n&gt; &gt; beforehand (this would also lead to an interesting e=\r\nxtension of \n&gt; tile- \n&gt; &gt; codings that you might like a little better). But=\r\n in any case, \n&gt; since  \n&gt; &gt; all we&#39;re trying to do in RL is supervised fun=\r\nction \napproximation,  \n&gt; &gt; the lack of geometry isn&#39;t bad.\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; &gt;=\r\n I&#39;m obviously not a big fan of tile coding :) I&#39;m not really\n&gt; &gt; &gt; concern=\r\ned whether it might do better in some cases; the problem \n&gt; with\n&gt; &gt; &gt; it i=\r\ns that it is a dead end for future progress because it is \n&gt; about\n&gt; &gt; &gt; ru=\r\nining our ability to exploit geometric relationships.\n&gt; &gt; \n&gt; &gt; Ok, this is =\r\nwhere the discussion gets really interesting. \nRemember  \n&gt; &gt; when I mentio=\r\nned GA&#39;s &quot;performing model selection&quot; or something \n&gt; like  \n&gt; &gt; that befor=\r\ne? Thats a fundamental difference in the GA approach \nand  \n&gt; &gt; RL. So what=\r\n do I mean by model selection: roughly speaking, in  \n&gt; &gt; Bayesian inferenc=\r\ne you have this idea of some separation of the  \n&gt; &gt; parameters you are opt=\r\nimizing (e.g. the weights in an NN) and \nthe  \n&gt; &gt; model that generates tho=\r\nse parameters (e.g. the topology of the \n&gt; NN,  \n&gt; &gt; or even whether you us=\r\ne an NN or decision tree or something). RL \n&gt; is  \n&gt; &gt; inherently incapable=\r\n of performing model selection (at least \n&gt; outside  \n&gt; &gt; of NEAT+Q and som=\r\ne others). Once you start learning with  a \ngiven  \n&gt; &gt; value function repr=\r\nesentation, you can no longer switch to a  \n&gt; &gt; different representation wi=\r\nthout throwing away everything you&#39;ve \n&gt; just  \n&gt; &gt; learned.  GAs on the ot=\r\nher hand learn one parameterized model \nper  \n&gt; &gt; individual. This is an im=\r\nportant distinction.\n&gt; &gt; \n&gt; &gt; Now, what does this have to do with tile codi=\r\nng and learning  \n&gt; &gt; geometry?  When you talk about &quot;cutting up different =\r\nvariables&quot; \n&gt; you  \n&gt; &gt; are inherently making an argument from the standpoi=\r\nnt of model  \n&gt; &gt; selection: i.e. what is the best representation for this =\r\n\nlearning  \n&gt; &gt; problem? This is a valid question in the GA world, and I ag=\r\nree \n&gt; with  \n&gt; &gt; you tile coding wouldn&#39;t work at all for learning good  \n=\r\n&gt; &gt; representations that allow good future learning. But from the RL  \n&gt; &gt; =\r\nstandpoint, since all tile-coding is used for is function  \n&gt; &gt; approximati=\r\non, I don&#39;t think they are as problematic as you \nimagine.\n&gt; &gt; \n&gt; &gt; -- Joe\n=\r\n&gt; &gt;\n&gt;\n\n\n\n"}}