{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":115403844,"authorName":"John Arrowwood","from":"&quot;John Arrowwood&quot; &lt;jarrowwx@...&gt;","profile":"jarrowwx","replyTo":"LIST","senderId":"QV5SNIToNxpxve-2eTVa1zSex99agEAl-Q5y1MFF99nlb6oAFzk1a0UHOWF7u32sf3f0pFOJKHYdAS0O85SETJ5N-a_2B2Pf0-Ze74ta","spamInfo":{"isSpam":false,"reason":"0"},"subject":"RE: [neat] Re: IEX musings","postDate":"1093969059","msgId":1482,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PEJBWTItRjI1OXlTeEJMNjlnOVgwMDBiYjQwYUBob3RtYWlsLmNvbT4="},"prevInTopic":1481,"nextInTopic":1483,"prevInTime":1481,"nextInTime":1483,"topicId":1468,"numMessagesInTopic":15,"msgSnippet":"... Cool, I ll check it out. ... Well, if not backprop, evolve. ... Well, it was just an idea, anyway.  :) ... Sure.  Here s hoping my ASCII-art comes","rawEmail":"Return-Path: &lt;jarrowwx@...&gt;\r\nX-Sender: jarrowwx@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 59581 invoked from network); 31 Aug 2004 16:24:54 -0000\r\nReceived: from unknown (66.218.66.167)\n  by m23.grp.scd.yahoo.com with QMQP; 31 Aug 2004 16:24:54 -0000\r\nReceived: from unknown (HELO hotmail.com) (65.54.247.25)\n  by mta6.grp.scd.yahoo.com with SMTP; 31 Aug 2004 16:24:54 -0000\r\nReceived: from mail pickup service by hotmail.com with Microsoft SMTPSVC;\n\t Tue, 31 Aug 2004 09:17:40 -0700\r\nReceived: from 64.122.44.102 by by2fd.bay2.hotmail.msn.com with HTTP;\n\tTue, 31 Aug 2004 16:17:39 GMT\r\nX-Originating-Email: [jarrowwx@...]\r\nX-Sender: jarrowwx@...\r\nTo: neat@yahoogroups.com\r\nBcc: \r\nDate: Tue, 31 Aug 2004 09:17:39 -0700\r\nMime-Version: 1.0\r\nContent-Type: text/plain; format=flowed\r\nMessage-ID: &lt;BAY2-F259ySxBL69g9X000bb40a@...&gt;\r\nX-OriginalArrivalTime: 31 Aug 2004 16:17:40.0011 (UTC) FILETIME=[096A97B0:01C48F76]\r\nX-eGroups-Remote-IP: 65.54.247.25\r\nFrom: &quot;John Arrowwood&quot; &lt;jarrowwx@...&gt;\r\nReply-To: john@...\r\nSubject: RE: [neat] Re: IEX musings\r\nX-Yahoo-Group-Post: member; u=115403844\r\nX-Yahoo-Profile: jarrowwx\r\n\r\n&gt;From: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\n&gt; &gt;  Anyone have any thoughts on this idea?\n&gt;\n&gt;You can definitely train a SOM on an input vector to get higher-\n&gt;level features that are further input into something else, such as a\n&gt;NEAT network.  In fact, you can also layer SOMs on top of each other\n&gt;as you suggest.  The Constructivist Learning Architecture (CLA),\n&gt;developed in our NN research group at UT by Harold Chaput, actually\n&gt;implements this idea already:\n&gt;\n&gt;http://www.cs.utexas.edu/users/chaput/cla/\n\nCool, I&#39;ll check it out.\n\n&gt;However, I&#39;m not sure you can further backprop on SOM weights.\n\nWell, if not backprop, evolve.\n\n&gt;I don&#39;t remember all the details but the SOM activation function and\n&gt;learning mechanism may mean that it develops different weights than\n&gt;a backprop network would.  Maybe they can be made compatible though,\n&gt;but you need to make sure.\n\nWell, it was just an idea, anyway.  :)\n\n&gt; &gt; Let&#39;s analyze the problem a bit.  Suppose the inputs are this:\n&gt; &gt;\n&gt; &gt; 1 2 3\n&gt; &gt; 2 2 3\n&gt; &gt; 3 3 3\n&gt; &gt;\n&gt; &gt; Suppose the &#39;position&#39; inputs were 0,0, 1/3, 1/3.  The expected\n&gt;output is 1.\n&gt; &gt;   At 1/3,0,2/3,1/3, the  expected output is 2.  But at\n&gt;0,0,2/3,2/3, the\n&gt; &gt; expected output is 7/4.  And at 0,0,1,1, the expected output is\n&gt;22/9.\n&gt; &gt;\n&gt;\n&gt;Can you remind me what the coordinates mean in the &quot;position?&quot;  It&#39;s\n&gt;been a while since I&#39;ve looked at all this and I was distracted with\n&gt;my dissertation.  Maybe you can show the result that these position\n&gt;inputs would produce graphically somehow to put it in context.\n\nSure.  Here&#39;s hoping my ASCII-art comes through...\n\n0,0 +----+----+----+\n    |    |    |    |\n    +----+----+----+\n    |    |    |    |\n    +----+----+----+\n    |    |    |    |\n    +----+----+----+ 1,1\n\nThe above is a square, divided up like a tic-tac-toe board.  The upper left \nis 0,0.  The lower right is 1,1.  Obviously, the vertical lines in the \nmiddle would be an X value of 1/3 and 2/3, right?  Likewise, the horizontal \nones would be 1/3 and 2/3.  Or 0.3333 and 0.6666, whichever is easier to \ngrasp.  So, suppose you want to specify the area of the upper-left square.  \nTo do that, you need to specify four values.  The left-most X value, which \nis 0.  The right-most X value (of the region of interest), which is 1/3.  \nThe top-most Y value, 0, and the bottom-most Y value, 1/3.  So the \nspecification of the rectangle is ( 0,0 - 1/3,1/3).  The center square, \nthen, is ( 1/3,1/3 - 2/3, 2/3 ).  And if you happen to specify a rectangle \nof (0,0 - 2/3,2/3) then you specify the four squares in the upper left.  And \n(0,0 - 1,1) specifies the whole square.\n\nThese are the intended usage of the inputs.  Why?  Because it made sense at \nthe time...\n\nSo I am currently running an experiment that just learns to handle\n\n1 2 3\n2 2 3\n3 3 3\n\n(0,0 - 1/3,1/3) = 1\n(1/3, 0 - 2/3, 1/3 ) = 2\n(2/3, 0 - 1, 1/3 ) = 3\n\nAnd so forth.\n\nThe evolution reaches a &#39;peak&#39; of about 60% of the problem fairly early.  \nLike by the time it has evaluated around 50 different topologies, I think.  \nI have left it running for a while, and so far it has evaluated over 2,800 \ndifferent topologies, without any significant improvement.  If someone else \nwould be willing to give this experiment a try with their version of NEAT \n(just this 3x3 grid prediction) and let it run for a few days to see if it \never finds a solution, that would be greatly appreciated.  I can upload the \nperl script that contains the fitness function, if that would help.\n\nBut I&#39;m beginning to think that I&#39;m asking a network to do something other \nthan what they are able to do.  Each output node is really a classifier, not \na mathematical function output.  1 means yes, 0 means no.  Everything in \nbetween is a confidence level.  Now, for a single output pixel in a fixed \nsize/location, a network can work just fine.  But that&#39;s not what I&#39;m asking \nit to do.  I&#39;m asking it not to identify features and classify them, I&#39;m \nasking it to form a mathematical model, and query that model.  But that&#39;s \nnot what an ANN does, does it?  So I may well be doomed from the start, at \nleast for this particular way of doing things.\n\nNot that this means that I&#39;m going to just give up... just means I&#39;ve got to \nre-think things a bit. :)\n\n\n\n"}}