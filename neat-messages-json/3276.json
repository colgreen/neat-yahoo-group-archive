{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":151231063,"authorName":"Joseph Reisinger","from":"Joseph Reisinger &lt;joeraii@...&gt;","profile":"joeraii","replyTo":"LIST","senderId":"SWt7LGi6w2hgo3mcforp6ps_ewEyeR5_G-YIhiqJl2VIwA3Ub2tPcZtvCJfdizP_zqEEc04dj8LBQOgI2oN4_P0bbPvSmVMFG5gigT_8kg","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] Re: Tile Coding and HyperNEAT","postDate":"1179044730","msgId":3276,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PEM2RDM1ODI1LTkwOTAtNDA5RS1CMjVELTkxNDVDMjA2QzYxRkBjcy51dGV4YXMuZWR1Pg==","inReplyToHeader":"PGYyMmdyNytwZGsyQGVHcm91cHMuY29tPg==","referencesHeader":"PGYyMmdyNytwZGsyQGVHcm91cHMuY29tPg=="},"prevInTopic":3274,"nextInTopic":3277,"prevInTime":3275,"nextInTime":3277,"topicId":3214,"numMessagesInTopic":27,"msgSnippet":"Ok, now we re getting somewhere. The argument you re making is much larger, really. Not against tile-coding in specific but against all ML methods that are","rawEmail":"Return-Path: &lt;joeraii@...&gt;\r\nX-Sender: joeraii@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 10546 invoked from network); 13 May 2007 08:25:37 -0000\r\nReceived: from unknown (66.218.66.68)\n  by m43.grp.scd.yahoo.com with QMQP; 13 May 2007 08:25:37 -0000\r\nReceived: from unknown (HELO spunkymail-a9.g.dreamhost.com) (208.97.132.119)\n  by mta11.grp.scd.yahoo.com with SMTP; 13 May 2007 08:25:37 -0000\r\nReceived: from [192.168.1.132] (i60-34-164-113.s02.a009.ap.plala.or.jp [60.34.164.113])\n\tby spunkymail-a9.g.dreamhost.com (Postfix) with ESMTP id 0666F20ECE\n\tfor &lt;neat@yahoogroups.com&gt;; Sun, 13 May 2007 01:25:33 -0700 (PDT)\r\nMime-Version: 1.0 (Apple Message framework v752.2)\r\nIn-Reply-To: &lt;f22gr7+pdk2@...&gt;\r\nReferences: &lt;f22gr7+pdk2@...&gt;\r\nContent-Type: text/plain; charset=US-ASCII; delsp=yes; format=flowed\r\nMessage-Id: &lt;C6D35825-9090-409E-B25D-9145C206C61F@...&gt;\r\nContent-Transfer-Encoding: 7bit\r\nDate: Sun, 13 May 2007 03:25:30 -0500\r\nTo: neat@yahoogroups.com\r\nX-Mailer: Apple Mail (2.752.2)\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Joseph Reisinger &lt;joeraii@...&gt;\r\nSubject: Re: [neat] Re: Tile Coding and HyperNEAT\r\nX-Yahoo-Group-Post: member; u=151231063; y=5iDEmCnr7mLDAZq27VEsfCCu9yABtunD1PuAtj8y1dXDLQ\r\nX-Yahoo-Profile: joeraii\r\n\r\nOk, now we&#39;re getting somewhere. The argument you&#39;re making is much  \nlarger, really. Not against tile-coding in specific but against all  \nML methods that are incapable of taking into account variational  \nproperties (evolvability). Thats fine. But then don&#39;t just attack  \ntile coding, since its really a symptom, not the disease (to use your  \nterminology). In fact, tile-coding is really good at what it does, it  \njust happens to be part of a larger picture that you don&#39;t agree with.\n\nAlso, I would be a little wary of dismissing the entire field just  \nbecause it can&#39;t take into account geometry the way you want it to. I  \nthink the RL community knows that this is an issue: look at Littman&#39;s  \nwork on Relocatable Action Models (RAMs) which is based on the  \nSherstov and Stone stuff I pointed out earlier. Both of these are  \nmoving towards some idea of building in &quot;geometry&quot; into the learning  \nproblem. Geometry in the RAM case is very general, using only  \nsimilarities in function mappings.\n\nBut even that is beside the point. In a sense, the tile-coding  \napproach is more general than the HyperNEAT approach because it does  \nnot require the experimenter to set a priori some geometry. This  \ngenerality is accomplished through being geometry-agnostic, i.e.  \n&quot;cutting the state space up into little pieces and scattering them  \naround the room.&quot; To have a truly general learning method, you need  \none of two things: no a priori assumptions on geometry (most of RL/ML  \nalready does this), or some way of choosing the appropriate geometry  \nduring learning (which, to my knowledge, not one has yet attacked).  \nNote that HyperNEAT does not fulfill either of these criteria. Thats  \nnot to say HyperNEAT isn&#39;t a good first step towards the latter, just  \nthat its not there yet.\n\n&gt; Most of what you said is factually true.  But the spin you put on it\n&gt; is wrong. It is correct that tile coding breaks up the state/action\n&gt; space into little pieces to make the right behavior for each little\n&gt; region easier to compute.  As you put it, &quot;subtiles can better fit\n&gt; the value function being learned. Note that there is very little\n&gt; generalization desired here.&quot;  You say that like it&#39;s a good thing.\n\n&gt; And that&#39;s what tile coding is really indicating: Much of RL is DOA.\n&gt; Tile coding is a symptom of a larger sickness.  You said it\n&gt; yourself: &quot;[most] RL is inherently incapable of performing model\n&gt; selection.&quot;  Well, if what that means is that you can&#39;t exploit\n&gt; geometry, it&#39;s all a dead end.  I am not certain that RL (aside from\n&gt; NEAT+Q-type stuff) is really incapable of optimizing the model\n&gt; because who knows what we might realize how to do in the future.\n&gt; However, for now, RL is falling back on tile coding because it is\n&gt; moving in the wrong direction.\n&gt;\n&gt; Here is what is really going on:  Each variable in the state/action\n&gt; space is a dimension along which the value function varies.  A good\n&gt; learning algorithm would represent how the value function varies with\n&gt; respect to each state variable.  However, such variation may be\n&gt; complex, i.e. the function could be pretty complicated.  The learning\n&gt; methods (i.e. supervised function approximators) inside RL are\n&gt; sufficiently bad that they cannot handle approximating functions like\n&gt; that.  So what do we do?  We break the whole space into chunks.  Now\n&gt; the appropriate action for each little chunk requires a much simpler\n&gt; function, so we have a chance with our poor learning algorithm to\n&gt; maybe get all these little simple functions right instead of only a\n&gt; few big complicated functions.\n&gt;\n&gt; In other words, we have a poor algorithm and the cure is to destroy\n&gt; what variational structure there was to begin with so that we can\n&gt; look at every little bit of the problem separately.   So we have now\n&gt; lost the ability to exploit all the useful relationships that\n&gt; initially existed in the space.  States that are related are now\n&gt; broken apart and must be learned separately, that is, the geometry\n&gt; has been destroyed! The fact that many see such an operation as a\n&gt; step in the right direction is symptomatic of serious misdirection in\n&gt; the field.  If that&#39;s the best we can do to make RL easier, than RL\n&gt; is in serious trouble!\n&gt;\n&gt; Think of it like this:  Take a game like chess, which I learned as a\n&gt; little kid.  Now take the 64 squares and cut each piece out of the\n&gt; board individually.  Now sprinkle them all randomly all over your\n&gt; living room.  Each square still represents the same location it was\n&gt; originally taken from in the board.  It&#39;s just you can&#39;t see where\n&gt; they were anymore.  Now place the chess pieces in the right starting\n&gt; squares and teach a little kid to play chess.  Think he or she would\n&gt; learn anything at all?\n&gt;\n&gt; Well, that&#39;s exactly what tile coding is!  A method that learns chess\n&gt; (or anything else that has implicit or explicit geometry) needs to\n&gt; know how the positions relate to each other geometrically because\n&gt; there is massive regularity being lost without that information.\n&gt; What kind of crazy algorithm would purposely put a chess board into a\n&gt; meaningless order before learning begins?  A method that &quot;benefits&quot;\n&gt; from such an approach is clearly DOA.  RL researchers should be\n&gt; seriously concerned about tile coding being necessary at all, not\n&gt; happy about it.\n&gt;\n&gt; So I stick to my position: Tile coding is anti-geometry and anti-\n&gt; representation.  It deserves no credit whatsoever for &quot;respecting&quot;\n&gt; anything.\n&gt;\n&gt; ken\n&gt;\n&gt; --- In neat@yahoogroups.com, Joseph Reisinger &lt;joeraii@...&gt; wrote:\n&gt;&gt;\n&gt;&gt; I&#39;ve been aching to reply to this post for a while, and I finally\n&gt;&gt; have enough free time to do so. I think we could have a really\n&gt;&gt; interesting discussion here, hopefully at least more interesting\n&gt; than\n&gt;&gt; the NFL tangent.\n&gt;&gt;\n&gt;&gt;&gt;&gt; Sure, but tile-coding does respect at least one form of geometry:\n&gt;&gt;&gt;&gt; Nearby elements in the state space are known to be nearby, and\n&gt; thus\n&gt;&gt;&gt;&gt; are grouped in the same tile.\n&gt;&gt;&gt;\n&gt;&gt;&gt; I have to dispute this characterization of tile coding\n&gt;&gt;&gt; as &quot;respecting at least one form of geometry.&quot; I think you are\n&gt;&gt;&gt; being unnecessarily equitable toward tile coding.\n&gt;&gt;&gt;\n&gt;&gt;&gt; What you are saying is that in effect taking a nice sculpture and\n&gt;&gt;&gt; cutting it into pieces &quot;respects&quot; its geometry because those\n&gt; little\n&gt;&gt;&gt; pieces are not broken up any further than that. It&#39;s like saying\n&gt;&gt;&gt; that someone who cut your head off &quot;respected&quot; your head by\n&gt; keeping\n&gt;&gt;&gt; its internal integrity intact. In fact, tile coding is peforming a\n&gt;&gt;&gt; grievous violation against the existing geometry of the domain,\n&gt; and\n&gt;&gt;&gt; does not deserve to be credited with respecting geometry\n&gt;&gt;&gt; whatsoever. I&#39;m hard pressed to imagine how one could do worse\n&gt;&gt;&gt; beyond cutting things up into even tinier and tinier bits; but\n&gt; even\n&gt;&gt;&gt; then, those bits still contain &quot;nearby elements in the state\n&gt;&gt;&gt; space.&quot; So that isn&#39;t saying much.\n&gt;&gt;\n&gt;&gt; Yeah, from the way your framing this argument, e.g. tile-coding\n&gt; used\n&gt;&gt; in the GA model-selection sense, you&#39;re absolutely right. I&#39;ll get\n&gt;&gt; back to exactly what I mean by that in a bit. For now lets try to\n&gt;&gt; reframe the issue from an RL perspective, which is where tile-\n&gt; codings\n&gt;&gt; are predominantly used. In RL, the tile-coding is just a\n&gt;&gt; representation for a function approximator (in a sense its sort of\n&gt;&gt; like a really simple spline cure) that learns in a supervised\n&gt; manner.\n&gt;&gt; Tile coding makes a lot of sense in this domain because you can\n&gt;&gt; calculate with a good deal of precision how much some particular\n&gt; tile\n&gt;&gt; differs from the expected value of the function being approximated\n&gt;&gt; (in this case the Bellman error).\n&gt;&gt; Tiles hat is cover a broad area where the value function changes a\n&gt;&gt; lot (&quot;have bad fit&quot;, &quot;are too general&quot;, etc) are then split so\n&gt; that\n&gt;&gt; the subtiles can better fit the value function being learned. Note\n&gt;&gt; that there is very little generalization desired here; the best\n&gt; thing\n&gt;&gt; given infinite computational resources would be to have a whole\n&gt; ton\n&gt;&gt; of itty-bitty tiles that fit the value function perfectly.\n&gt;&gt;\n&gt;&gt; Anyway, since we&#39;re in the standard RL framework, there is really\n&gt; no\n&gt;&gt; way of learning the &quot;geometry&quot; of a value function (well,\n&gt; technically\n&gt;&gt; there is, but thats a long tangent towards a really interesting\n&gt;&gt; research area). Maybe if the geometry was given by the\n&gt; experimenter\n&gt;&gt; beforehand (this would also lead to an interesting extension of\n&gt; tile-\n&gt;&gt; codings that you might like a little better). But in any case,\n&gt; since\n&gt;&gt; all we&#39;re trying to do in RL is supervised function approximation,\n&gt;&gt; the lack of geometry isn&#39;t bad.\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;&gt; I&#39;m obviously not a big fan of tile coding :) I&#39;m not really\n&gt;&gt;&gt; concerned whether it might do better in some cases; the problem\n&gt; with\n&gt;&gt;&gt; it is that it is a dead end for future progress because it is\n&gt; about\n&gt;&gt;&gt; ruining our ability to exploit geometric relationships.\n&gt;&gt;\n&gt;&gt; Ok, this is where the discussion gets really interesting. Remember\n&gt;&gt; when I mentioned GA&#39;s &quot;performing model selection&quot; or something\n&gt; like\n&gt;&gt; that before? Thats a fundamental difference in the GA approach and\n&gt;&gt; RL. So what do I mean by model selection: roughly speaking, in\n&gt;&gt; Bayesian inference you have this idea of some separation of the\n&gt;&gt; parameters you are optimizing (e.g. the weights in an NN) and the\n&gt;&gt; model that generates those parameters (e.g. the topology of the\n&gt; NN,\n&gt;&gt; or even whether you use an NN or decision tree or something). RL\n&gt; is\n&gt;&gt; inherently incapable of performing model selection (at least\n&gt; outside\n&gt;&gt; of NEAT+Q and some others). Once you start learning with  a given\n&gt;&gt; value function representation, you can no longer switch to a\n&gt;&gt; different representation without throwing away everything you&#39;ve\n&gt; just\n&gt;&gt; learned.  GAs on the other hand learn one parameterized model per\n&gt;&gt; individual. This is an important distinction.\n&gt;&gt;\n&gt;&gt; Now, what does this have to do with tile coding and learning\n&gt;&gt; geometry?  When you talk about &quot;cutting up different variables&quot;\n&gt; you\n&gt;&gt; are inherently making an argument from the standpoint of model\n&gt;&gt; selection: i.e. what is the best representation for this learning\n&gt;&gt; problem? This is a valid question in the GA world, and I agree\n&gt; with\n&gt;&gt; you tile coding wouldn&#39;t work at all for learning good\n&gt;&gt; representations that allow good future learning. But from the RL\n&gt;&gt; standpoint, since all tile-coding is used for is function\n&gt;&gt; approximation, I don&#39;t think they are as problematic as you imagine.\n&gt;&gt;\n&gt;&gt; -- Joe\n&gt;&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt; Yahoo! Groups Links\n&gt;\n&gt;\n&gt;\n\n\n"}}