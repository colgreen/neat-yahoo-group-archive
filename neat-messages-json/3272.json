{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"TPBTinfaClaZHx57DJ9EluguUnBLjt8b70ER5WmLxDYENWHOR-z02tpAxo3yiDDmiITm4i1srABOheSSNIVYo6BPMi9tc5H2KIsq9JBeppma","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Tile Coding and HyperNEAT","postDate":"1179009636","msgId":3272,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGYyNWZwNCs5YjVkQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGYyMmlwMCtqamJwQGVHcm91cHMuY29tPg=="},"prevInTopic":3271,"nextInTopic":3273,"prevInTime":3271,"nextInTime":3273,"topicId":3214,"numMessagesInTopic":27,"msgSnippet":"I m not certain what the extreme cited refers to, but we should not confuse tile coding with the classic divide and conquer approach to problem solving.","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 8008 invoked from network); 12 May 2007 22:40:47 -0000\r\nReceived: from unknown (66.218.66.167)\n  by m38.grp.scd.yahoo.com with QMQP; 12 May 2007 22:40:47 -0000\r\nReceived: from unknown (HELO n28c.bullet.sp1.yahoo.com) (209.131.38.248)\n  by mta6.grp.scd.yahoo.com with SMTP; 12 May 2007 22:40:47 -0000\r\nReceived: from [216.252.122.216] by n28.bullet.sp1.yahoo.com with NNFMP; 12 May 2007 22:40:37 -0000\r\nReceived: from [66.218.69.4] by t1.bullet.sp1.yahoo.com with NNFMP; 12 May 2007 22:40:37 -0000\r\nReceived: from [66.218.66.87] by t4.bullet.scd.yahoo.com with NNFMP; 12 May 2007 22:40:37 -0000\r\nDate: Sat, 12 May 2007 22:40:36 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;f25fp4+9b5d@...&gt;\r\nIn-Reply-To: &lt;f22ip0+jjbp@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: Re: Tile Coding and HyperNEAT\r\nX-Yahoo-Group-Post: member; u=54567749; y=jpoUhOokNAdWDc5EBnGatdKPbAyQkQp54QaaxQnVJnK5uE3uY8PE\r\nX-Yahoo-Profile: kenstanley01\r\n\r\nI&#39;m not certain what &quot;the extreme cited&quot; refers to, but we should not \nconf=\r\nuse tile coding with the classic &quot;divide and conquer&quot; approach to \nproblem =\r\nsolving.  No one who follows divide and conquer cuts a \nproblem up into com=\r\npletely arbitrary chunks.  In fact, divide and \nconquer is based on the ide=\r\na that when you do divide, you divide *at* \nthe discontinuities that occur =\r\nalong dimensions of design.  Thus it \nimplies an initial assessment of the =\r\noverall geometry of the \nproblem.  Once again, that&#39;s exactly what tile cod=\r\ning throws away.\n\nBy the way, &quot;geometry&quot; does not imply a necessary analogu=\r\ne with the \ndimensions of physical space.  It only implies that there are i=\r\nndeed \northogonal dimensions.  So the &quot;higher design space dimensionality&quot; =\r\n\nis part of the geometry.\n\nken\n\n--- In neat@yahoogroups.com, &quot;afcarl2&quot; &lt;a.c=\r\narl@...&gt; wrote:\n&gt;\n&gt; IMHO, it appears that the extreme cited dismisses an el=\r\nement that \n&gt; holds value. The sole focus on geometry (i.e. 1d, 2d or 3d), =\r\nis a \n&gt; simplified subset of a problem dimensionality. Most useful problems=\r\n \n&gt; have a higher design space dimensionality in which global functions \n&gt; =\r\nare comprised of a collection of regional &quot;global functions&quot; along \n&gt; with =\r\ntheir individual application sub-domain definitions, which \ntaken \n&gt; togeth=\r\ner makeup the whole.\n&gt; \n&gt; An approach which works well within a regional su=\r\nb-domain isn&#39;t bad \n&gt; because it cannot adequately address all of the desig=\r\nn space, \nunless \n&gt; of course it can co-evolve different functions in diffe=\r\nrent sub-\n&gt; regions along with the corresponding regional application /mapp=\r\ning \n&gt; between them.\n&gt; \n&gt; Also, as your example illustrates, it would be se=\r\nriously \n&gt; counterproductive to throw away proximity-based information, \nwh=\r\nether \n&gt; geometry or other higher dimensional information.\n&gt; \n&gt; Many useful=\r\n problems are comprised of discontinuities that have to \nbe \n&gt; dealt with i=\r\nndividually. &quot;Divide-and-conquer&quot; is a powerful \napproach \n&gt; when properly =\r\napplied.\n&gt; \n&gt; \n&gt; --- In neat@yahoogroups.com, &quot;Kenneth Stanley&quot; &lt;kstanley@&gt;=\r\n wrote:\n&gt; &gt;\n&gt; &gt; Joe, I agree this is a revealing discussion.  Tile coding t=\r\no me a \n&gt; is \n&gt; &gt; a telling example of a significant misdirection of effort=\r\n in \n&gt; machine \n&gt; &gt; learning right now.  \n&gt; &gt; \n&gt; &gt; Most of what you said is=\r\n factually true.  But the spin you put on \n&gt; it \n&gt; &gt; is wrong.  It is corre=\r\nct that tile coding breaks up the \n&gt; state/action \n&gt; &gt; space into little pi=\r\neces to make the right behavior for each \nlittle \n&gt; &gt; region easier to comp=\r\nute.  As you put it, &quot;subtiles can better \nfit \n&gt; &gt; the value function bein=\r\ng learned. Note that there is very little \n&gt; &gt; generalization desired here.=\r\n&quot;  You say that like it&#39;s a good \nthing.\n&gt; &gt; \n&gt; &gt; However, the fact that th=\r\nere is a need to do something like that \nis \n&gt; &gt; more a symptom of a seriou=\r\ns disease in RL than an accomplishment \nwe \n&gt; &gt; should be congratulating ou=\r\nrselves for.  It&#39;s like using cocaine \nto \n&gt; &gt; stay awake at work and claim=\r\ning that it was a good idea because \nyou \n&gt; &gt; were more alert.  The fact is=\r\n the whole approach is sick to begin \n&gt; &gt; with if it needs cocaine to funct=\r\nion properly.\n&gt; &gt; \n&gt; &gt; And that&#39;s what tile coding is really indicating: Mu=\r\nch of RL is \n&gt; DOA.  \n&gt; &gt; Tile coding is a symptom of a larger sickness.  Y=\r\nou said it \n&gt; &gt; yourself: &quot;[most] RL is inherently incapable of performing =\r\nmodel \n&gt; &gt; selection.&quot;  Well, if what that means is that you can&#39;t exploit =\r\n\n&gt; &gt; geometry, it&#39;s all a dead end.  I am not certain that RL (aside \n&gt; fro=\r\nm \n&gt; &gt; NEAT+Q-type stuff) is really incapable of optimizing the model \n&gt; &gt; =\r\nbecause who knows what we might realize how to do in the future.  \n&gt; &gt; Howe=\r\nver, for now, RL is falling back on tile coding because it is \n&gt; &gt; moving i=\r\nn the wrong direction.\n&gt; &gt; \n&gt; &gt; Here is what is really going on:  Each vari=\r\nable in the \nstate/action \n&gt; &gt; space is a dimension along which the value f=\r\nunction varies.  A \ngood \n&gt; &gt; learning algorithm would represent how the va=\r\nlue function varies \n&gt; with \n&gt; &gt; respect to each state variable.  However, =\r\nsuch variation may be \n&gt; &gt; complex, i.e. the function could be pretty compl=\r\nicated.  The \n&gt; learning \n&gt; &gt; methods (i.e. supervised function approximato=\r\nrs) inside RL are \n&gt; &gt; sufficiently bad that they cannot handle approximati=\r\nng functions \n&gt; like \n&gt; &gt; that.  So what do we do?  We break the whole spac=\r\ne into chunks.  \n&gt; Now \n&gt; &gt; the appropriate action for each little chunk re=\r\nquires a much \n&gt; simpler \n&gt; &gt; function, so we have a chance with our poor l=\r\nearning algorithm to \n&gt; &gt; maybe get all these little simple functions right=\r\n instead of only \na \n&gt; &gt; few big complicated functions.  \n&gt; &gt; \n&gt; &gt; In other=\r\n words, we have a poor algorithm and the cure is to \ndestroy \n&gt; &gt; what vari=\r\national structure there was to begin with so that we can \n&gt; &gt; look at every=\r\n little bit of the problem separately.   So we have \n&gt; now \n&gt; &gt; lost the ab=\r\nility to exploit all the useful relationships that \n&gt; &gt; initially existed i=\r\nn the space.  States that are related are now \n&gt; &gt; broken apart and must be=\r\n learned separately, that is, the \ngeometry \n&gt; &gt; has been destroyed! The fa=\r\nct that many see such an operation as a \n&gt; &gt; step in the right direction is=\r\n symptomatic of serious \nmisdirection \n&gt; in \n&gt; &gt; the field.  If that&#39;s the =\r\nbest we can do to make RL easier, than \nRL \n&gt; &gt; is in serious trouble!\n&gt; &gt; =\r\n\n&gt; &gt; Think of it like this:  Take a game like chess, which I learned \nas \n&gt;=\r\n a \n&gt; &gt; little kid.  Now take the 64 squares and cut each piece out of \nthe=\r\n \n&gt; &gt; board individually.  Now sprinkle them all randomly all over your \n&gt; =\r\n&gt; living room.  Each square still represents the same location it \nwas \n&gt; &gt;=\r\n originally taken from in the board.  It&#39;s just you can&#39;t see \nwhere \n&gt; &gt; t=\r\nhey were anymore.  Now place the chess pieces in the right \n&gt; starting \n&gt; &gt;=\r\n squares and teach a little kid to play chess.  Think he or she \n&gt; would \n&gt;=\r\n &gt; learn anything at all?\n&gt; &gt; \n&gt; &gt; Well, that&#39;s exactly what tile coding is=\r\n!  A method that learns \n&gt; chess \n&gt; &gt; (or anything else that has implicit o=\r\nr explicit geometry) needs \nto \n&gt; &gt; know how the positions relate to each o=\r\nther geometrically because \n&gt; &gt; there is massive regularity being lost with=\r\nout that information.  \n&gt; &gt; What kind of crazy algorithm would purposely pu=\r\nt a chess board \ninto \n&gt; a \n&gt; &gt; meaningless order before learning begins?  =\r\nA method \nthat &quot;benefits&quot; \n&gt; &gt; from such an approach is clearly DOA.  RL re=\r\nsearchers should be \n&gt; &gt; seriously concerned about tile coding being necess=\r\nary at all, not \n&gt; &gt; happy about it.\n&gt; &gt; \n&gt; &gt; So I stick to my position: Ti=\r\nle coding is anti-geometry and anti-\n&gt; &gt; representation.  It deserves no cr=\r\nedit whatsoever \nfor &quot;respecting&quot; \n&gt; &gt; anything.\n&gt; &gt; \n&gt; &gt; ken\n&gt; &gt; \n&gt; &gt; --- =\r\nIn neat@yahoogroups.com, Joseph Reisinger &lt;joeraii@&gt; wrote:\n&gt; &gt; &gt;\n&gt; &gt; &gt; I&#39;v=\r\ne been aching to reply to this post for a while, and I \n&gt; finally  \n&gt; &gt; &gt; h=\r\nave enough free time to do so. I think we could have a really  \n&gt; &gt; &gt; inter=\r\nesting discussion here, hopefully at least more \ninteresting \n&gt; &gt; than  \n&gt; =\r\n&gt; &gt; the NFL tangent.\n&gt; &gt; &gt; \n&gt; &gt; &gt; &gt;&gt; Sure, but tile-coding does respect at =\r\nleast one form of \n&gt; geometry:\n&gt; &gt; &gt; &gt;&gt; Nearby elements in the state space =\r\nare known to be nearby, \nand \n&gt; &gt; thus\n&gt; &gt; &gt; &gt;&gt; are grouped in the same til=\r\ne.\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; I have to dispute this characterization of tile coding\n&gt;=\r\n &gt; &gt; &gt; as &quot;respecting at least one form of geometry.&quot; I think you are\n&gt; &gt; &gt;=\r\n &gt; being unnecessarily equitable toward tile coding.\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; What y=\r\nou are saying is that in effect taking a nice sculpture \n&gt; and\n&gt; &gt; &gt; &gt; cutt=\r\ning it into pieces &quot;respects&quot; its geometry because those \n&gt; &gt; little\n&gt; &gt; &gt; =\r\n&gt; pieces are not broken up any further than that. It&#39;s like \nsaying\n&gt; &gt; &gt; &gt;=\r\n that someone who cut your head off &quot;respected&quot; your head by \n&gt; &gt; keeping\n&gt;=\r\n &gt; &gt; &gt; its internal integrity intact. In fact, tile coding is \n&gt; peforming =\r\na\n&gt; &gt; &gt; &gt; grievous violation against the existing geometry of the \ndomain, =\r\n\n&gt; &gt; and\n&gt; &gt; &gt; &gt; does not deserve to be credited with respecting geometry\n&gt;=\r\n &gt; &gt; &gt; whatsoever. I&#39;m hard pressed to imagine how one could do worse\n&gt; &gt; &gt;=\r\n &gt; beyond cutting things up into even tinier and tinier bits; \nbut \n&gt; &gt; eve=\r\nn\n&gt; &gt; &gt; &gt; then, those bits still contain &quot;nearby elements in the state\n&gt; &gt; =\r\n&gt; &gt; space.&quot; So that isn&#39;t saying much.\n&gt; &gt; &gt; \n&gt; &gt; &gt; Yeah, from the way your=\r\n framing this argument, e.g. tile-coding \n&gt; &gt; used  \n&gt; &gt; &gt; in the GA model-=\r\nselection sense, you&#39;re absolutely right. I&#39;ll \n&gt; get  \n&gt; &gt; &gt; back to exact=\r\nly what I mean by that in a bit. For now lets try \n&gt; to  \n&gt; &gt; &gt; reframe the=\r\n issue from an RL perspective, which is where tile-\n&gt; &gt; codings  \n&gt; &gt; &gt; are=\r\n predominantly used. In RL, the tile-coding is just a  \n&gt; &gt; &gt; representatio=\r\nn for a function approximator (in a sense its sort \n&gt; of  \n&gt; &gt; &gt; like a rea=\r\nlly simple spline cure) that learns in a supervised \n&gt; &gt; manner.  \n&gt; &gt; &gt; Ti=\r\nle coding makes a lot of sense in this domain because you \ncan  \n&gt; &gt; &gt; calc=\r\nulate with a good deal of precision how much some \nparticular \n&gt; &gt; tile  \n&gt;=\r\n &gt; &gt; differs from the expected value of the function being \n&gt; approximated =\r\n \n&gt; &gt; &gt; (in this case the Bellman error).\n&gt; &gt; &gt; Tiles hat is cover a broad =\r\narea where the value function \nchanges \n&gt; a  \n&gt; &gt; &gt; lot (&quot;have bad fit&quot;, &quot;a=\r\nre too general&quot;, etc) are then split so \n&gt; &gt; that  \n&gt; &gt; &gt; the subtiles can =\r\nbetter fit the value function being learned. \n&gt; Note  \n&gt; &gt; &gt; that there is =\r\nvery little generalization desired here; the best \n&gt; &gt; thing  \n&gt; &gt; &gt; given =\r\ninfinite computational resources would be to have a whole \n&gt; &gt; ton  \n&gt; &gt; &gt; =\r\nof itty-bitty tiles that fit the value function perfectly.\n&gt; &gt; &gt; \n&gt; &gt; &gt; Any=\r\nway, since we&#39;re in the standard RL framework, there is \nreally \n&gt; &gt; no  \n&gt;=\r\n &gt; &gt; way of learning the &quot;geometry&quot; of a value function (well, \n&gt; &gt; technic=\r\nally  \n&gt; &gt; &gt; there is, but thats a long tangent towards a really \ninteresti=\r\nng  \n&gt; &gt; &gt; research area). Maybe if the geometry was given by the \n&gt; &gt; expe=\r\nrimenter  \n&gt; &gt; &gt; beforehand (this would also lead to an interesting extensi=\r\non of \n&gt; &gt; tile- \n&gt; &gt; &gt; codings that you might like a little better). But i=\r\nn any case, \n&gt; &gt; since  \n&gt; &gt; &gt; all we&#39;re trying to do in RL is supervised f=\r\nunction \n&gt; approximation,  \n&gt; &gt; &gt; the lack of geometry isn&#39;t bad.\n&gt; &gt; &gt; \n&gt; =\r\n&gt; &gt; \n&gt; &gt; &gt; &gt; I&#39;m obviously not a big fan of tile coding :) I&#39;m not really\n&gt;=\r\n &gt; &gt; &gt; concerned whether it might do better in some cases; the \nproblem \n&gt; =\r\n&gt; with\n&gt; &gt; &gt; &gt; it is that it is a dead end for future progress because it i=\r\ns \n&gt; &gt; about\n&gt; &gt; &gt; &gt; ruining our ability to exploit geometric relationships=\r\n.\n&gt; &gt; &gt; \n&gt; &gt; &gt; Ok, this is where the discussion gets really interesting. \n&gt;=\r\n Remember  \n&gt; &gt; &gt; when I mentioned GA&#39;s &quot;performing model selection&quot; or som=\r\nething \n&gt; &gt; like  \n&gt; &gt; &gt; that before? Thats a fundamental difference in the=\r\n GA approach \n&gt; and  \n&gt; &gt; &gt; RL. So what do I mean by model selection: rough=\r\nly speaking, in  \n&gt; &gt; &gt; Bayesian inference you have this idea of some separ=\r\nation of \nthe  \n&gt; &gt; &gt; parameters you are optimizing (e.g. the weights in an=\r\n NN) and \n&gt; the  \n&gt; &gt; &gt; model that generates those parameters (e.g. the top=\r\nology of the \n&gt; &gt; NN,  \n&gt; &gt; &gt; or even whether you use an NN or decision tre=\r\ne or something). \nRL \n&gt; &gt; is  \n&gt; &gt; &gt; inherently incapable of performing mod=\r\nel selection (at least \n&gt; &gt; outside  \n&gt; &gt; &gt; of NEAT+Q and some others). Onc=\r\ne you start learning with  a \n&gt; given  \n&gt; &gt; &gt; value function representation=\r\n, you can no longer switch to a  \n&gt; &gt; &gt; different representation without th=\r\nrowing away everything \nyou&#39;ve \n&gt; &gt; just  \n&gt; &gt; &gt; learned.  GAs on the other=\r\n hand learn one parameterized model \n&gt; per  \n&gt; &gt; &gt; individual. This is an i=\r\nmportant distinction.\n&gt; &gt; &gt; \n&gt; &gt; &gt; Now, what does this have to do with tile=\r\n coding and learning  \n&gt; &gt; &gt; geometry?  When you talk about &quot;cutting up dif=\r\nferent variables&quot; \n&gt; &gt; you  \n&gt; &gt; &gt; are inherently making an argument from t=\r\nhe standpoint of model  \n&gt; &gt; &gt; selection: i.e. what is the best representat=\r\nion for this \n&gt; learning  \n&gt; &gt; &gt; problem? This is a valid question in the G=\r\nA world, and I agree \n&gt; &gt; with  \n&gt; &gt; &gt; you tile coding wouldn&#39;t work at all=\r\n for learning good  \n&gt; &gt; &gt; representations that allow good future learning.=\r\n But from the \nRL  \n&gt; &gt; &gt; standpoint, since all tile-coding is used for is =\r\nfunction  \n&gt; &gt; &gt; approximation, I don&#39;t think they are as problematic as yo=\r\nu \n&gt; imagine.\n&gt; &gt; &gt; \n&gt; &gt; &gt; -- Joe\n&gt; &gt; &gt;\n&gt; &gt;\n&gt;\n\n\n\n"}}