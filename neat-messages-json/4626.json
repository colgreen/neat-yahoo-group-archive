{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"-ZG8ZQpITLtujrWvsmXFr52BQdeiqbRkDGKYHttNaMdGSuSZMiCHN2GxK7LRWalHawn5fb80NJNg0nDDMS01x-GIx5QZ9-yZctFU5wOy-lyo","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: The Next Generation of Neural Networks - Geoff Hinton TechTalk","postDate":"1239690762","msgId":4626,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGdzMWFtYStxN25jQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGdydXRsdCtlajdqQGVHcm91cHMuY29tPg=="},"prevInTopic":4625,"nextInTopic":4632,"prevInTime":4625,"nextInTime":4627,"topicId":4620,"numMessagesInTopic":8,"msgSnippet":"JT, my feeling is that there is a critical difference between a network that comes to represent geometry by learning about it and one that sees it from the","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 68800 invoked from network); 14 Apr 2009 06:32:45 -0000\r\nX-Received: from unknown (98.137.34.46)\n  by m1.grp.sp2.yahoo.com with QMQP; 14 Apr 2009 06:32:45 -0000\r\nX-Received: from unknown (HELO n35b.bullet.mail.sp1.yahoo.com) (66.163.168.149)\n  by mta3.grp.sp2.yahoo.com with SMTP; 14 Apr 2009 06:32:45 -0000\r\nX-Received: from [69.147.65.149] by n35.bullet.mail.sp1.yahoo.com with NNFMP; 14 Apr 2009 06:32:43 -0000\r\nX-Received: from [98.137.34.34] by t9.bullet.mail.sp1.yahoo.com with NNFMP; 14 Apr 2009 06:32:43 -0000\r\nDate: Tue, 14 Apr 2009 06:32:42 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;gs1ama+q7nc@...&gt;\r\nIn-Reply-To: &lt;grutlt+ej7j@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: Re: The Next Generation of Neural Networks - Geoff Hinton TechTalk\r\nX-Yahoo-Group-Post: member; u=54567749; y=gt5gyq-vOIegZ97ppK9gBVSUX4VSmSkDmIRYgg9wsLDILsDK4phw\r\nX-Yahoo-Profile: kenstanley01\r\n\r\nJT, my feeling is that there is a critical difference between a network tha=\r\nt comes to represent geometry by learning about it and one that &quot;sees&quot; it f=\r\nrom the get-go.\n\nIt is certainly possible, maybe even likely, that Hinton&#39;s=\r\n network ultimately learns a rough approximation of the geometry of the pix=\r\nel grid.  However, that is not a good thing - rather, it&#39;s a bad thing.  Wh=\r\ny would we want learning algorithms to have to learn the geometry of the wo=\r\nrld *on top of* learning to solve the goal task?  The geometry of the world=\r\n (e.g. of our retina) is an a priori fact that all of us humans know automa=\r\ntically because it is *inherent* in the topographic organization of our vis=\r\nual processing system.\n\nThink about this example:  Imagine that you didn&#39;t =\r\nknow how to play tic tac toe and I decided to teach you.  However, I don&#39;t =\r\ntell you anything about which square is which and simply present the square=\r\ns to you in a vector whose order may or may not mean something geometric.  =\r\nFor example (dots mean empty squares):\n\nOOX.X.X..\n\nNow, you have to learn w=\r\nhere to play and whether someone has won or not.   A game might progress li=\r\nke\n\n.X.......\n\n.X..O....\n\nXX..O....\n\netc..\n\nIn fact, the original vector I =\r\nshowed you (OOX.X.X..) really means the following, but it would take you a =\r\nvery long time and a lot of example to figure out this mapping:\n\n. . X\n\n. X=\r\n O\n\nX O .\n\nInterestingly, if you play enough in the geometry-less represent=\r\nation above, you will eventually *learn* the geometry.  You will start to u=\r\nnderstand it and think about the game in the correct way, because that is t=\r\nhe way you can win, and that happens to be a system in which adjacency mean=\r\ns something.  So, implicitly, you will start to represent its geometry in s=\r\nome &quot;hidden layer&quot; in your brain.\n\nNow the question is, how long will it ta=\r\nke you to become a good tic tac toe player starting that way?  It will take=\r\n you a very long time, because you will only start to become good once you =\r\nimplicitly begin to represent its geometry.  So we would have to wait for t=\r\nhat to happen.\n\nOn the other hand, if we simply showed you the geometry of =\r\nthe game from the beginning, as any sane person would do, you would learn d=\r\nramatically faster because you would not need to reconstruct the geometry o=\r\nf the world before being able to think about it.  \n\nSo the point is, HyperN=\r\nEAT is an algorithm that learns *from* geometry while other algorithms are =\r\nforced to *learn geometry itself*.  That is not a good thing- it&#39;s really i=\r\nrrational when you think about it.  Any learning system should be able to l=\r\nearn from geometry because geometry is an a priori fact of the world.  To h=\r\nave to reconstruct it in the process of learning a task is a significant di=\r\nsability that is alien to the human experience.  In fact, this insight prob=\r\nably explains why some problems are so hard for learning algorithms when th=\r\ney shouldn&#39;t be.  They are blind to geometry and have to reconstruct it fro=\r\nm sparse evidence.\n\nSo I still think something fundamental is missing from =\r\nHinton&#39;s approach.  While it may have some genuinely clever properties, it =\r\nneeds geometry and it doesn&#39;t yet have it.\n\nken\n\n\n--- In neat@yahoogroups.c=\r\nom, &quot;JT&quot; &lt;skybro77@...&gt; wrote:\n&gt;\n&gt; (I don&#39;t post here often, but this talk =\r\nhas been eye-opening.)\n&gt; \n&gt; Hi Ken. I understand your concern. It&#39;s certain=\r\nly a valid one.\n&gt; \n&gt; (I could be totally wrong. I wish I paid more attentio=\r\nn in my statistical mechanics class.)\n&gt; \n&gt; From my understanding of the tal=\r\nk, each pre-training abstract the representations from the inputs of the pr=\r\nevious layer. So even if the original layer of input is individual pixel, i=\r\nt&#39;s very possible that the 2nd or the 3rd layer contains geometry represent=\r\nations. Otherwise, I can&#39;t imagine it will do well in generating such a wid=\r\ne variety of styles of digits.\n&gt; \n&gt; If we want to use this network on tasks=\r\n of different resolutions, I&#39;m willing to hypothesize that we can just chop=\r\n off the pixel layer, attach a higher resolution input layer, and retrain o=\r\nnly the new connections.\n&gt; \n&gt; Better yet, re-train those connections using =\r\nhyperNeat.\n&gt; \n&gt; Does that sound sensable?\n&gt;\n\n\n\n"}}