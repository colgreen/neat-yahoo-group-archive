{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Ken","from":"&quot;Ken&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"ZFZMM0grfdV3cul1-Yt1beqPjcghIBC7HIqAv4UOAzhaNOgaqB5L8YbY8AQc7R1vK9DASi29lzIqIfCoNNVzNhoGLZXe","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: A fresh look at GPUs and OpenCL","postDate":"1372881571","msgId":6164,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGtyMXZyMytpOGhuQGVHcm91cHMuY29tPg==","inReplyToHeader":"PENBRTBNK1ljelZVd3hDVXZGOGRVbk1qYl9TTHk5K19CdVNCal9IdUJrSkdqaVJTczc2QUBtYWlsLmdtYWlsLmNvbT4="},"prevInTopic":6163,"nextInTopic":6165,"prevInTime":6163,"nextInTime":6165,"topicId":6161,"numMessagesInTopic":7,"msgSnippet":"Hi Colin, one other interesting thing I d throw in here is the looming bottleneck in neuroevolution research of extremely large networks.  We re not really","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 86458 invoked by uid 102); 3 Jul 2013 19:59:31 -0000\r\nX-Received: from unknown (HELO mtaq6.grp.bf1.yahoo.com) (10.193.84.37)\n  by m8.grp.bf1.yahoo.com with SMTP; 3 Jul 2013 19:59:31 -0000\r\nX-Received: (qmail 13694 invoked from network); 3 Jul 2013 19:59:31 -0000\r\nX-Received: from unknown (HELO ng2-ip5.bullet.mail.bf1.yahoo.com) (98.139.165.1)\n  by mtaq6.grp.bf1.yahoo.com with SMTP; 3 Jul 2013 19:59:31 -0000\r\nX-Received: from [98.139.164.122] by ng2.bullet.mail.bf1.yahoo.com with NNFMP; 03 Jul 2013 19:59:31 -0000\r\nX-Received: from [10.193.94.110] by tg3.bullet.mail.bf1.yahoo.com with NNFMP; 03 Jul 2013 19:59:31 -0000\r\nDate: Wed, 03 Jul 2013 19:59:31 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;kr1vr3+i8hn@...&gt;\r\nIn-Reply-To: &lt;CAE0M+YczVUwxCUvF8dUnMjb_SLy9+_BuSBj_HuBkJGjiRSs76A@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: &quot;Ken&quot; &lt;kstanley@...&gt;\r\nSubject: Re: A fresh look at GPUs and OpenCL\r\nX-Yahoo-Group-Post: member; u=54567749; y=zozoBGJoRKCzlZk9NCj3r_mpFeCQSpIB_NGbrB2Xbdjgu7xZXEa3\r\nX-Yahoo-Profile: kenstanley01\r\n\r\n\n\nHi Colin, one other interesting thing I&#39;d throw in here is the looming bo=\r\nttleneck in neuroevolution research of extremely large networks.  We&#39;re not=\r\n really there yet today, but at some point making progress will sometimes r=\r\nequire investigating networks with millions or more connections (maybe evol=\r\nved by HyperNEAT or something HyperNEAT-like).  Simply querying all the wei=\r\nghts with the CPPN (which means activating the CPPN millions of times) coul=\r\nd then become a big obstacle to running experiments.  Even if there was som=\r\ne way to streamline just that one aspect of the algorithm it could make a b=\r\nig difference in the future.\n\nBest,\n\nken\n\n--- In neat@yahoogroups.com, Coli=\r\nn Green &lt;colin.green1@...&gt; wrote:\n&gt;\n&gt; Hi all,\n&gt; \n&gt; I know the topic of GPU =\r\nuse has come up before but there have been a\n&gt; few recent developments in t=\r\nhe GPU world so I thought it would be\n&gt; interesting to review the current s=\r\nituation.\n&gt; \n&gt; [CUDA]\n&gt; CUDA has been mentioned previously and I think Ken =\r\nLloyd did some work\n&gt; using CUDA in NEAT, but AFAIK none of the NEAT implem=\r\nentations freely\n&gt; available are using GPUs at all (please correct me if I&#39;=\r\nm wrong). CUDA\n&gt; was notable as being the first platform to provide a gener=\r\nal computing\n&gt; platform/layer over GPUs rather than being graphics accelera=\r\ntion\n&gt; specific. As such it greatly lowered the difficulty of using GPUs fo=\r\nr\n&gt; general computing. A notable point is that CUDA is specific to NVIDIA\n&gt;=\r\n GPUs.\n&gt; \n&gt; [OpenCL]\n&gt; OpenCL is a more recent development that aims to pro=\r\nvide an openly\n&gt; defined GPGPU style platform. OpenCL then is a layer of ab=\r\nstraction\n&gt; from the hardware that allows GPGPU style code to be written\n&gt; =\r\nindependently of any specific h/w and to be executed on any h/w\n&gt; suporting=\r\n OpenCL. At this time there is already a lot of support, e.g.\n&gt; there is su=\r\npport for NVIDIA and ATI/AMD GPUs, IBM&#39;s Cell processor\n&gt; based accelerator=\r\n &#39;blades&#39;, and you can also run OpenCL code on an\n&gt; &#39;normal&#39; Intel multicor=\r\ne CPU (which may be more useful for\n&gt; testing/development than acceleration=\r\n?).\n&gt; \n&gt; The main issue with OpenCL is that the it is an abstraction over\n&gt;=\r\n diverse hardware, thus although a program may run it may not run very\n&gt; fa=\r\nst without specific knowledge of the underlying h/w and what it&#39;s\n&gt; strengt=\r\nhs and weaknesses are.  E.g. if code accesses more RAM that is\n&gt; available =\r\nto each processor then OpenCL will simply compile in\n&gt; instructions to copy=\r\n data between local and main RAM thus eliminating\n&gt; the perf gain of using =\r\nlocal RAM. OpenCL does provide for querying the\n&gt; underlying h/w for some o=\r\nf these factors, so you could in principle\n&gt; perform a set of checks and re=\r\nport that the h/w isn&#39;t suitable for\n&gt; your program, or maybe even dynamica=\r\nlly adjust the program code based\n&gt; on reported parameters.\n&gt; \n&gt; On the who=\r\nle though I see OpenCL as a positive development and\n&gt; something the NEAT c=\r\nommunity can potentially benefit from. It is still\n&gt; a relatively young pla=\r\ntform and therefore may present some challenges\n&gt; to code to as it develops=\r\n, but I think it&#39;s mature and stable enough\n&gt; to consider experimenting wit=\r\nh now.\n&gt; \n&gt; \n&gt; [Current GPU h/w]\n&gt; As a ballpark estimate of the sort of pe=\r\nrformance gains a GPGPU can\n&gt; give us, ATI/AMDs current flagship card (Rade=\r\non 7970) has a peak\n&gt; throughput of about 3.8 TFlops, compared to 100 GFlop=\r\ns for a 4th\n&gt; generation quad core Intel i7. So on paper we&#39;re looking at a=\r\n possible\n&gt; 38x speedup compared to top flight CPUs. However, OpenCL does s=\r\nupport\n&gt; utilising mutliple GPUs, e.g. in the Bitcoin mining world it&#39;s typ=\r\nical\n&gt; to have 4 and sometimes 5 GPUs in one system (using PCI &#39;riser&#39; cabl=\r\nes\n&gt; to distance the GPUs from the motherboard). So for a relatively modest=\r\n\n&gt; investment you could be looking at a possible 100x speedup compared to\n&gt;=\r\n current best CPUs.\n&gt; \n&gt; \n&gt; [NEAT and GPUs]\n&gt; My instinct here is to modify=\r\n current NEAT code to report stats on how\n&gt; much time proportionally is bei=\r\nng spent in each stage of the NEAT\n&gt; algorithm and to target the code that =\r\ntakes up the most time, this\n&gt; will be different across problem domains and=\r\n also for NEAT versus\n&gt; HyperNEAT.\n&gt; \n&gt; Certainly if a problem domain is kn=\r\nown to be CPU heavy (e.g. uses a\n&gt; physics simulation) then it&#39;s probably a=\r\n no-brainer to use OpenCL for\n&gt; that in isolation from the rest of the NEAT=\r\n algorithm. For NEAT itself\n&gt; I&#39;ve observed slowdown as ANNs grow in size a=\r\nnd this is presumably\n&gt; mostly due to time to decode and/or &#39;run&#39; the ANNs,=\r\n and this is of\n&gt; course a greater problem in HyperNEAT where the decode st=\r\nage consists\n&gt; of a NEAT decode and ANN activation. So there might be some =\r\nscope for\n&gt; using OpenCL there. One can envisage multiple GPUs where one ma=\r\ny be\n&gt; dedicated to problem domain physics, one to ANN activation and anoth=\r\ner\n&gt; to ANN genome decoding (say).\n&gt; \n&gt; \n&gt; [Typical GPU Architecture]\n&gt; Fin=\r\nally I&#39;m going to briefly describe the architecture of the Radeon\n&gt; 7970 to=\r\n give an idea of what it is capable of.\n&gt; [Mainly taken from\n&gt; http://www.t=\r\nechradar.com/reviews/pc-mac/pc-components/graphics-cards/amd-radeon-hd-7970=\r\n-1049734/review/2]\n&gt; \n&gt; The 7970 has:\n&gt; \n&gt; 32 x Compute Units (CUs). These =\r\nare completely independent of each\n&gt; other. If you have 2x GPUs then OpenCL=\r\n will see (I think) a block of\n&gt; 64 compute units, hence in some cases code=\r\n can be accelerated just by\n&gt; adding GPUs. Each CU has:\n&gt; \n&gt; 4 x Vector Uni=\r\nts (VUs). And each VU has:\n&gt; 16 x Unified shaders (unified here just means =\r\nthey are no longer\n&gt; specific to a task, e.g. pixel or vector shader, they =\r\nare general\n&gt; purpose processors)\n&gt; \n&gt; So in total there are 32 x 4 x 16 =\r\n=3D  2048 unified shaders.\n&gt; \n&gt; Each CU has 64kB of local RAM that all of t=\r\nhe VUs can access\n&gt; (typically for reading shared state data I would guess)=\r\n. In addition\n&gt; each VU has it&#39;s own 64 kB of RAM  (note. you would typical=\r\nly control\n&gt; what&#39;s in these local memories in code, that is, it&#39;s not a pa=\r\nssive\n&gt; CPU cache). A vector unit is basically a SIMD processor, there is o=\r\nne\n&gt; set of instructions that are executed against all 16 shaders (so e.g.\n=\r\n&gt; you can &#39;shade&#39; 16 pixels at a time). So each of the 128 vector units\n&gt; c=\r\nan execute its own instructions, and in turn those instructions are\n&gt; opera=\r\nting on 16 shaders. A shader then consists of some minimal state\n&gt; data spe=\r\ncific to it and the data it is operating on, and also\n&gt; execution units for=\r\n performing arithmetic, etc.\n&gt; \n&gt; An interesting thing about vector units i=\r\ns that conditional branches\n&gt; are allowed in OpenCL, that is, you can have =\r\nsome shaders executing a\n&gt; different path despite there being only one set =\r\nof instructions and\n&gt; one instruction pointer. However this is merely a tri=\r\nck, if VU code\n&gt; contains a branch then both branches are executed for all =\r\nshaders and\n&gt; the shaders are assigned the correct final result based on wh=\r\nich\n&gt; branch they should have followed. Hence it&#39;s advisable to avoid\n&gt; bra=\r\nnches, but it&#39;s a nice feature to have available so long as you\n&gt; don&#39;t abu=\r\nse it.\n&gt; \n&gt; For more info see:\n&gt;    [From Shader Code to a Tera=EF=AC=82op:=\r\n How Shader Cores Work, Kayvon\n&gt; Fatahalian, Stanford University]\n&gt;    [htt=\r\np://s08.idav.ucdavis.edu/fatahalian-gpu-architecture.pdf]\n&gt; \n&gt; There&#39;s obvi=\r\nously a heck of a lot more to this subject than I&#39;ve\n&gt; described but I thou=\r\nght this might be a reasonably good intro to\n&gt; current possibilities around=\r\n GPU use in NEAT.\n&gt; \n&gt; Colin\n&gt;\n\n\n\n"}}