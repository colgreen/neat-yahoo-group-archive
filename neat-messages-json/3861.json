{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"m54WQcYOlYZm7kBzR_BGaAdo-pT2zyNT5DKLLFQoBRWlpLHG-JT-mayVtwhjYXiBJA1BFKPZz7ZIOqbGe_8IPsHdHlPd9Oykd64FIkSrEv8Q","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Backpropagation and NEAT","postDate":"1205184307","msgId":3861,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGZyNDh2aytudnFkQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGQyNzhlM2FkMDgwMzEwMTEzMHllZWMwZmIwczg2MzYwZjM1NDY4ZTU3N2FAbWFpbC5nbWFpbC5jb20+"},"prevInTopic":3860,"nextInTopic":3862,"prevInTime":3860,"nextInTime":3862,"topicId":3846,"numMessagesInTopic":41,"msgSnippet":"Rafael, thank you for pointing out the connection to memetic algorithms.  That is good to point out that such a combination falls under that category. However,","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 12272 invoked from network); 10 Mar 2008 21:25:08 -0000\r\nX-Received: from unknown (66.218.67.97)\n  by m49.grp.scd.yahoo.com with QMQP; 10 Mar 2008 21:25:08 -0000\r\nX-Received: from unknown (HELO n32a.bullet.sp1.yahoo.com) (209.131.38.211)\n  by mta18.grp.scd.yahoo.com with SMTP; 10 Mar 2008 21:25:08 -0000\r\nX-Received: from [216.252.122.219] by n32.bullet.sp1.yahoo.com with NNFMP; 10 Mar 2008 21:25:08 -0000\r\nX-Received: from [66.218.69.2] by t4.bullet.sp1.yahoo.com with NNFMP; 10 Mar 2008 21:25:08 -0000\r\nX-Received: from [66.218.66.86] by t2.bullet.scd.yahoo.com with NNFMP; 10 Mar 2008 21:25:08 -0000\r\nDate: Mon, 10 Mar 2008 21:25:07 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;fr48vk+nvqd@...&gt;\r\nIn-Reply-To: &lt;d278e3ad0803101130yeec0fb0s86360f35468e577a@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: Re: Backpropagation and NEAT\r\nX-Yahoo-Group-Post: member; u=54567749; y=HX7-XgueGi2PdVqi2WLO1t4fgRPMNyP11Cr8LYKoP3tlZJOaEhDT\r\nX-Yahoo-Profile: kenstanley01\r\n\r\nRafael, thank you for pointing out the connection to memetic \nalgorithms.  =\r\nThat is good to point out that such a combination falls \nunder that categor=\r\ny.\n\nHowever, there are still those who would argue that the local search \nm=\r\nethod should not be encoded back into the genome, that is, that \nevolution =\r\nshould simply search for the best starting point from which \na local search=\r\n would depart.  Because of the Baldwin Effect, that may \neven work better.\n=\r\n\nPersonally, I do not know which approach would work better but both \nare v=\r\niable and it is probably domain dependent.\n\nken\n\n--- In neat@...=\r\nm, &quot;Rafael C.P.&quot; &lt;kurama.youko.br@...&gt; \nwrote:\n&gt;\n&gt; Ken, it doesn&#39;t fit pure=\r\n evolution but it fits memetic algorithms, \nthat\n&gt; consists exactly of evol=\r\nution alternated with local search methods \nfor fine\n&gt; tunning (just few st=\r\neps). NEAT+BP may become a good memetic \nalgorithm for\n&gt; neural networks.\n&gt;=\r\n \n&gt; On Mon, Mar 10, 2008 at 2:19 PM, Kenneth Stanley &lt;kstanley@...&gt;\n&gt; wrote=\r\n:\n&gt; \n&gt; &gt;   Peter, I believe that backprop can potentially improve the\n&gt; &gt; a=\r\nccuracy. It has been shown to work effectively with neurevolution\n&gt; &gt; in cl=\r\nassification tasks in the past. So in principle it could\n&gt; &gt; help. Of cours=\r\ne, there is always the chance that it will not\n&gt; &gt; enhance performance as w=\r\nell.\n&gt; &gt;\n&gt; &gt; One issue I would also consider is that some people disagree o=\r\nn\n&gt; &gt; whether the changes to weights from backprop should be encoded \nback\n=\r\n&gt; &gt; into the genome or not. If it is actually encoded back into the\n&gt; &gt; gen=\r\nome, that is &quot;Lamarckian&quot; evolution because in effect what the\n&gt; &gt; organism=\r\n learned over its lifetime is encoded into its own\n&gt; &gt; offspring. That is o=\r\nbviously not how real evolution works.\n&gt; &gt;\n&gt; &gt; However, of course, it doesn=\r\n&#39;t have to work like real evolution \nand\n&gt; &gt; some people believe that Lamar=\r\nckian evolution will work better.\n&gt; &gt; However, there are arguments that in =\r\nfact it works worse because \nit\n&gt; &gt; hurts the diversity of the population. =\r\nBecause of the Baldwin\n&gt; &gt; effect, some would argue that evolution+backprop=\r\n is most powerful \nif\n&gt; &gt; the learned weights are not encoded back into the=\r\n genome. This \ntopic\n&gt; &gt; is fairly extensive. A lot is written about the &quot;B=\r\naldwin effect.&quot;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; ken\n&gt; &gt;\n&gt; &gt; --- In neat@yahoogroups.com &lt;neat%\n=\r\n40yahoogroups.com&gt;, &quot;petar_chervenski&quot;\n&gt; &gt; &lt;petar_chervenski@&gt; wrote:\n&gt; &gt; &gt;=\r\n\n&gt; &gt; &gt; Hi Ken,\n&gt; &gt; &gt;\n&gt; &gt; &gt; I am evolving time series predictors, in fact ev=\r\nen a simplified\n&gt; &gt; &gt; version of time series predictors, where the network =\r\nhas to \nanswer\n&gt; &gt; is\n&gt; &gt; &gt; the future value going up or down. The actual o=\r\nutput neuron is a\n&gt; &gt; &gt; simple step function, but back-prop can be applied =\r\nif it is \nturned\n&gt; &gt; &gt; out to a sigmoid with a very steep slope.\n&gt; &gt; &gt; The =\r\nnetworks are allowed to have any topology and they are\n&gt; &gt; evaluated\n&gt; &gt; &gt; =\r\non the run, meaning that on each timestep, an error is being\n&gt; &gt; &gt; calculat=\r\ned (being 0 or 1, depending on the prediction made).\n&gt; &gt; &gt; First of all, do=\r\n you think that applying back-prop to these\n&gt; &gt; networks\n&gt; &gt; &gt; may bring an=\r\ny accuracy improvement? I know that it is going to \neat\n&gt; &gt; &gt; the CPU resou=\r\nrses, so it can be applied at regular intervals, \nsay\n&gt; &gt; &gt; each 50 generat=\r\nions, to push the networks&#39;s weights in the right\n&gt; &gt; &gt; direction, a kind o=\r\nf a hint to the search. I am still thinking\n&gt; &gt; of &quot;is\n&gt; &gt; &gt; it worth it?&quot;.=\r\n.\n&gt; &gt; &gt;\n&gt; &gt; &gt; Peter\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt; --- In neat@yahoogroups.com &lt;ne=\r\nat%40yahoogroups.com&gt;, &quot;Kenneth \nStanley&quot;\n&gt; &gt; &lt;kstanley@&gt; wrote:\n&gt; &gt; &gt; &gt;\n&gt; =\r\n&gt; &gt; &gt; A number of people have programmed backprop into NEAT. Chris\n&gt; &gt; &gt; &gt; =\r\nChristenson did a Masters thesis on combining NEAT and \nbackprop;\n&gt; &gt; a\n&gt; &gt;=\r\n &gt; &gt; paper based on this work is actually in the files section of\n&gt; &gt; this\n=\r\n&gt; &gt; &gt; group:\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; \nhttp://f1.grp.yahoofs.com/v1/UP7SR8=\r\nrDovimxlLlvcmGOziLUBIVncb2Tfr7sruo\n&gt; &gt; B\n&gt; &gt; &gt; 8b\n&gt; &gt; &gt; &gt; taAfELU62JLyQ9XCx=\r\nXF_Akhcmi-\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; \nTH4gpVHIikwnzB59ArOMQfPOAzyw25/Evolving_Train=\r\nable_Neural_Networks_6_p\n&gt; &gt; a\n&gt; &gt; &gt; ge\n&gt; &gt; &gt; &gt; s.doc\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; Shimo=\r\nn Whiteson implemented it as part of his NEAT+Q\n&gt; &gt; reinforcement\n&gt; &gt; &gt; &gt; l=\r\nearning method:\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; \nhttp://staff.science.uva.nl/~whiteson/pubs=\r\n/whitesonaaai06.pdf&lt;http://s\ntaff.science.uva.nl/%7Ewhiteson/pubs/whitesona=\r\naai06.pdf&gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; There has been a lot written on backprop in NEAT=\r\n in the \narchives\n&gt; &gt; of\n&gt; &gt; &gt; &gt; this group: just search for &quot;backprop&quot; fro=\r\nm the yahoo page for\n&gt; &gt; this\n&gt; &gt; &gt; &gt; group and many messages will pop up.\n=\r\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; In general, if you do not allow recurrence then I believe \n=\r\nthere\n&gt; &gt; is\n&gt; &gt; &gt; no\n&gt; &gt; &gt; &gt; special change needed in the traditional back=\r\nprop algorithm.\n&gt; &gt; With\n&gt; &gt; &gt; &gt; recurrence you would need something like r=\r\necurrent backprop \nlike\n&gt; &gt; &gt; Derek\n&gt; &gt; &gt; &gt; suggested. But let&#39;s just say y=\r\nou are evolving nonrecurrent\n&gt; &gt; &gt; networks-\n&gt; &gt; &gt; &gt; is there a particular =\r\nproblem you have in mind that comes up\n&gt; &gt; with\n&gt; &gt; &gt; &gt; applying backprop t=\r\no such networks?\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; ken\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; --- In neat@yahoogroup=\r\ns.com &lt;neat%40yahoogroups.com&gt;,\n&gt; &gt; &quot;petar_chervenski&quot;\n&gt; &gt; &lt;petar_chervensk=\r\ni@&gt;\n&gt; &gt; &gt; &gt; wrote:\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; Hello there.\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; I a=\r\nm looking for any back-propagation algorithm that can \nwork\n&gt; &gt; on\n&gt; &gt; &gt; &gt; =\r\n&gt; networks with arbitrary topology such as these that NEAT\n&gt; &gt; evolves.\n&gt; &gt;=\r\n &gt; All\n&gt; &gt; &gt; &gt; &gt; libraries I found so far either assume layered networks or=\r\n\n&gt; &gt; only\n&gt; &gt; &gt; feed-\n&gt; &gt; &gt; &gt; &gt; forward ones.. I am confused. Is there any =\r\nsource code that\n&gt; &gt; might\n&gt; &gt; &gt; &gt; help\n&gt; &gt; &gt; &gt; &gt; me? Any back-prop impleme=\r\nntation that can work on NEAT\n&gt; &gt; networks\n&gt; &gt; &gt; such\n&gt; &gt; &gt; &gt; &gt; that it can=\r\n easily be integrated. Or maybe some papers on \nthe\n&gt; &gt; &gt; topic?\n&gt; &gt; &gt; &gt; &gt; =\r\nI appreciate any help from the community.\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; Peter\n&gt; &gt; &gt; &gt;=\r\n &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt;\n&gt; &gt;  \n&gt; &gt;\n&gt; \n&gt; \n&gt; \n&gt; -- \n&gt; =3D=3D=3D=3D=3D=3D=3D=3D=3D=\r\n\n&gt; Rafael C.P.\n&gt; =3D=3D=3D=3D=3D=3D=3D=3D=3D\n&gt;\n\n\n\n"}}