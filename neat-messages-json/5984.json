{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Ken","from":"&quot;Ken&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"KDJumEIRBrB8LQ3Z1cx9iNxAOPfTsch1Zvpe48YvaGHhUW_Gs4R94izwWSHmT_TNKgBgZzrTtfc9PLpjz_5jpkz8euuk","spamInfo":{"isSpam":false,"reason":"3"},"subject":"Re: New paper on why modules evolve, and how to evolve modular artificial neural networks","postDate":"1360470126","msgId":5984,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGtmNzc5ZStxY2o0QGVHcm91cHMuY29tPg==","inReplyToHeader":"PDEzNjAyMDA4MzUuMjU4NzAuWWFob29NYWlsTmVvQHdlYjE3MTkwNS5tYWlsLmlyMi55YWhvby5jb20+"},"prevInTopic":5982,"nextInTopic":5985,"prevInTime":5983,"nextInTime":5985,"topicId":5976,"numMessagesInTopic":30,"msgSnippet":"I wanted to follow up on Alex s second question, on whether any approach biased to low connectivity could work. I think that s a really interesting question","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 84852 invoked from network); 10 Feb 2013 04:22:08 -0000\r\nX-Received: from unknown (10.193.84.135)\n  by m3.grp.bf1.yahoo.com with QMQP; 10 Feb 2013 04:22:08 -0000\r\nX-Received: from unknown (HELO ng2-vm5.bullet.mail.gq1.yahoo.com) (98.136.219.18)\n  by mta1.grp.bf1.yahoo.com with SMTP; 10 Feb 2013 04:22:07 -0000\r\nX-Received: from [98.137.0.83] by ng2.bullet.mail.gq1.yahoo.com with NNFMP; 10 Feb 2013 04:22:07 -0000\r\nX-Received: from [10.193.94.46] by tg3.bullet.mail.gq1.yahoo.com with NNFMP; 10 Feb 2013 04:22:07 -0000\r\nDate: Sun, 10 Feb 2013 04:22:06 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;kf779e+qcj4@...&gt;\r\nIn-Reply-To: &lt;1360200835.25870.YahooMailNeo@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 2:3:4:0:0\r\nFrom: &quot;Ken&quot; &lt;kstanley@...&gt;\r\nSubject: Re: New paper on why modules evolve, and how to evolve modular artificial neural networks\r\nX-Yahoo-Group-Post: member; u=54567749; y=FusHzWcr6Hj9x50Rmz8m50-JFkjcKDuUVLhSqcZKzhglbDktusXL\r\nX-Yahoo-Profile: kenstanley01\r\n\r\n\n\nI wanted to follow up on Alex&#39;s second question, on whether any approach =\r\nbiased to low connectivity could work. I think that&#39;s a really interesting =\r\nquestion with a lot of deep implications and deserves some discussion.\n\nFir=\r\nst let me echo others&#39; congratulations to Jeff, Jean-Baptiste, and Hod for =\r\nthis important publication, which dispels a lot of myths about the origins =\r\nof modularity.  Jeff has heard some of what I&#39;m posting here before in our =\r\nown private discussions, but it&#39;s something I think worth sharing on this g=\r\nroup as well given the importance of the issue.\n\nIn particular, the questio=\r\nn of how to bias a search is significant not only for biasing towards low c=\r\nonnectivity (for the purpose of producing modularity), but potentially for =\r\nall kinds of different properties we might realize are important in the fut=\r\nure.  And I believe there are two primary options:  (1) Use the fitness fun=\r\nction to bias the search, as Jeff et al. do; or (2) use the encoding to bia=\r\ns the search.  That is, in the second option, you would not manipulate the =\r\nfitness function in any way, but instead do something to the genetic encodi=\r\nng to make it more likely to produce the phenotypic properties you want to =\r\nsee.  In the case of low connectivity, one example of such a genetic bias i=\r\ns through an additional link expression output (LEO) on CPPNs in HyperNEAT,=\r\n which was introduced by Philip Verbancsics and myself:\n\nhttp://eplex.cs.uc=\r\nf.edu/publications/2011/verbancsics-gecco11\n\nTo bias towards low connectivi=\r\nty with the LEO, you start with a CPPN that tends to express short connecti=\r\nons (instead of long ones), which keeps connectivity down overall, and also=\r\n thereby seemed to help lead to modular structures.  \n\nBut like I said ther=\r\ne is a larger issue here than just modularity or low connectivity.  The iss=\r\nue is how you should implement a search bias of any type.  And I believe th=\r\nat doing it by manipulating the fitness function is generally dangerous in =\r\nthe long run.  Like with many ideas in EC, it is likely to work well for re=\r\nlatively simple problems, but the more we aim for high complexity, the more=\r\n I think it will burden the search with unintended consequences that increa=\r\nsingly warp the search off the best path.  \n\nAs a simple example, if you di=\r\nrectly manipulate fitness to be lower when connectivity is high, then you c=\r\nreate a &quot;niche&quot; for networks of low connectivity that otherwise do nothing =\r\nof any substance because they will be able to survive by having a higher fi=\r\ntness than networks of high connectivity that also do nothing of any substa=\r\nnce.  This niche of non-functional low-connectivity networks is essentially=\r\n a permanent dead-weight in the population that will last forever.    Maybe=\r\n it&#39;s not enough to kill you in simple problems, but in complex problems it=\r\n&#39;s something you probably can&#39;t afford.\n\nMore generally the issue is the us=\r\nual problem of deception, which is compounded by anything you do with fitne=\r\nss.  For example, in a complex search space, there is a reasonable chance t=\r\nhat the stepping stone to a good low-connectivity solution is something wit=\r\nh higher connectivity.  By manipulating fitness, you are cutting out all ch=\r\nances of encountering such a deceptive stepping stone.  But even if you don=\r\n&#39;t believe that could be true, the single-mindedness of always favoring low=\r\n-connectivity could deceive you from many parts of the search space that mi=\r\nght be stepping stones to something worthwhile, relating to connection dens=\r\nity or not.\n\nOn the other hand, manipulating the encoding is different beca=\r\nuse in effect it actually reorganizes the structure of the search space its=\r\nelf, which seems to me a more principled thing to do (if you can figure out=\r\n a way to do it).  Because the thing is, in that case, you do not need to w=\r\norry about a permanent dead weight taking up some proportion of your popula=\r\ntion forever.  Instead, while the encoding may *tend* to produce e.g. low-c=\r\nonnectivity solutions, it can still escape that tendency without any penalt=\r\ny to fitness.  Furthermore, in reality the best situation regarding modular=\r\nity and connectivity is probably rather subtle, with most of the brain resp=\r\necting the principle of low connectivity, but with a number of critical exc=\r\neptions in key areas, such as major inter-module hubs.  A sophisticated enc=\r\noding can allow its bias to bend to make such nuanced exceptions (e.g. base=\r\nd on locations within a geometry), whereas a fitness penalty is a heavy han=\r\nd and blunt instrument that cannot but help always to demand global and hol=\r\nistic subservience to dogmatic universals (unless you are willing to take a=\r\n hit in fitness).\n\nAn interesting question in nature (where our brains evol=\r\nved modular structure) is whether its tendency towards low connectivity is =\r\na result of an aspect of fitness in the wild, or an aspect of encoding bias=\r\n.  I think there is a lot of room in this question for arguing either way, =\r\nbut my hunch is that the bias is mostly in the encoding.  My logic is that =\r\nI think the reason that the connectivity of the brain is so much lower than=\r\n what it could be (e.g. it is a tiny fraction of everything-to-everything c=\r\nonnectivity) is an artifact of physics rather than an artifact of fitness. =\r\n It is simply physically impossible for a giant 100-billion-to-100-billion =\r\nconnectivity to fit in a head anything close to our size.  And physical imp=\r\nossibility is in some sense a property of encoding.  That is, mutations tha=\r\nt could step from a low-connectivity brain to a high one are few and far be=\r\ntween simply because of physical constraint.  So high-connectivity structur=\r\nes are simply a very small part of the search space of brains in the physic=\r\nal universe.  However, at the same time, you can still get long-range conne=\r\nctions from time to time because there is no universal penalty for doing so=\r\n, just a lower a priori probability of such mutations occurring.\n\nIn summar=\r\ny, the key difference between the alternatives is that with fitness you are=\r\n saying &quot;stay out of this part of the search space&quot; whereas with encoding y=\r\nou are saying &quot;this part of the search space is much smaller and hence less=\r\n likely to encounter.&quot;\n\nSo, my speculation is that if you want to bias the =\r\nsearch in highly complex domains, the best way is through the encoding.  Fi=\r\ntness is a nasty quagmire that is deceptively tempting to manipulate, but n=\r\never plays by the rules you wish it would.  Of course, these are merely my =\r\nown unproven intuitions and their veracity remains to be demonstrated.  But=\r\n at least it&#39;s something to think about.\n\nBest,\n\nken\n\n--- In neat@yahoogrou=\r\nps.com, Alexandre Devert  wrote:\n&gt;\n&gt; Hi,\n&gt; \n&gt; =C2=A0 Simple, clean experime=\r\nnt, with sharp results, congrats on that, definitely\n&gt; a step forward ! Of =\r\ncourse, it begs for more questions. I would love to hear\n&gt; you on such (fai=\r\nrly open) questions\n&gt; \n&gt; =C2=A0 =C2=A01) Do you think that selection pressu=\r\nre for low connectivity is sufficient in\n&gt; itself to evolve large coherent =\r\nnetworks, or is it just a piece of the puzzle ?\n&gt; =C2=A0 =C2=A02) Do you se=\r\ne your work as an indication that any approach biased to low\n&gt; connectivity=\r\n would reproduce the result ? Or does the way you guys enforced\n&gt; this bias=\r\n matters ?\n&gt; \n&gt; To me=C2=A0\n&gt; 1) =3D&gt; Part of the puzzle. Should see how we=\r\nll it scales for increasingly\n&gt; complex task, when the connection graph get=\r\ns bigger. A randomized=C2=A0\n&gt; search process=C2=A0on large graph sounds no=\r\nt so efficient, need something to guide it.\n&gt; I advocate construction proce=\r\nss that have a feedback from what the neuron=C2=A0\n&gt; network is computing. =\r\nDon&#39;t know how to do it without creepling computational\n&gt; cost tho...\n&gt; 2) =\r\n=3D&gt; I guess that the bias alone is enough, the way to introduce it might\n&gt;=\r\n not be such a big deal.=C2=A0\n&gt; \n&gt; Again, great work, very helpful contrib=\r\nution :)\n&gt; \n&gt; Alex\n&gt; =C2=A0\n&gt; Dr. Devert Alexandre\n&gt; Researcher at the Natu=\r\nre Inspired Computation and Applications Laboratory (NICAL)\n&gt; Lecturer at S=\r\nchool Of Software Engineering of USTC\n&gt; -----------------------------------=\r\n-----------------\n&gt; Homepage :=C2=A0http://www.marmakoide.org\n&gt; -----------=\r\n-----------------------------------------\n&gt; 166 Renai Road, Dushu Lake High=\r\ner Education Town\n&gt; Suzhou Industrial Park,\n&gt; Suzhou, Jiangsu, People&#39;s Rep=\r\nublic of China\n&gt; \n&gt; \n&gt; ________________________________\n&gt;  From: Jeff Clune=\r\n \n&gt; To: neat users group group  \n&gt; Cc: Jean-Baptiste Mouret ; Hod Lipson  \n=\r\n&gt; Sent: Thursday, February 7, 2013 1:57 AM\n&gt; Subject: [neat] New paper on w=\r\nhy modules evolve, and how to evolve modular artificial neural networks\n&gt;  =\r\n\n&gt; \n&gt; =C2=A0 \n&gt; Hello all,\n&gt; \n&gt; I&#39;m extremely pleased to announce a new pap=\r\ner on a subject that many--including myself--think is critical to making si=\r\ngnificant progress in our field: the evolution of modularity.=C2=A0\n&gt; \n&gt; Je=\r\nan-Baptiste Mouret, Hod Lipson and I have a new paper that=C2=A0\n&gt; \n&gt; 1) sh=\r\neds light on why modularity may evolve in biological networks (e.g. neural,=\r\n genetic, metabolic, protein-protein, etc.)\n&gt; \n&gt; 2) provides a simple techn=\r\nique for evolving neural networks that are modular and have increased evolv=\r\nability, in that they adapt faster to new environments. The modules that fo=\r\nrmed solved subproblems in the domain.=C2=A0\n&gt; Cite:=C2=A0Clune J, Mouret J=\r\n-B, Lipson H (2013) The evolutionary origins of modularity. Proceedings of =\r\nthe Royal Society B. 280: 20122863.=C2=A0http://dx.doi.org/10.1098/rspb.201=\r\n2.2863=C2=A0(pdf)\n&gt; \n&gt; Abstract: A central biological question is how natur=\r\nal organisms are so evolvable (capable of quickly adapting to new environme=\r\nnts). A key driver of evolvability is the widespread modularity of biologic=\r\nal networks=E2=80&quot;their organization as functional, sparsely connected subu=\r\nnits=E2=80&quot;but there is no consensus regarding why modularity itself evolve=\r\nd. Although most hypotheses assume indirect selection for evolvability, her=\r\ne we demonstrate that the ubiquitous, direct selection pressure to reduce t=\r\nhe cost of connections between network nodes causes the emergence of modula=\r\nr networks. Computational evolution experiments with selection pressures to=\r\n maximize network performance and minimize connection costs yield networks =\r\nthat are significantly more modular and more evolvable than control experim=\r\nents that only select for performance. These results will catalyse research=\r\n in numerous disciplines, such as neuroscience and genetics, and enhance ou=\r\nr ability to harness\n&gt;  evolution for engineering purposes.\n&gt; \n&gt; Video:=C2=\r\n=A0http://www.youtube.com/watch?feature=3Dplayer_embedded&v=3DSG4_aW8LMng\n&gt;=\r\n \n&gt; There has been some nice coverage of this work in the popular press, in=\r\n case you are interested:\n&gt; National Geographic:=C2=A0http://phenomena.nati=\r\nonalgeographic.com/2013/01/30/the-parts-of-life/MIT&#39;s Technology Review:=C2=\r\n=A0http://www.technologyreview.com/view/428504/computer-scientists-reproduc=\r\ne-the-evolution-of-evolvability/=C2=A0Fast Company:=C2=A0http://www.fastcom=\r\npany.com/3005313/evolved-brains-robots-creep-closer-animal-learningCornell =\r\nChronicle:=C2=A0http://www.news.cornell.edu/stories/Jan13/modNetwork.htmlSc=\r\nienceDaily:=C2=A0http://www.sciencedaily.com/releases/2013/01/130130082300.=\r\nhtm\n&gt; \n&gt; Please let me know what you think and if you have any questions. I=\r\n hope this work will help our field move forward!\n&gt; \n&gt; \n&gt; \n&gt; \n&gt; Best regard=\r\ns,\n&gt; Jeff Clune\n&gt; \n&gt; Assistant Professor\n&gt; Computer Science\n&gt; University of=\r\n Wyoming\n&gt; jclune@...\n&gt; jeffclune.com\n&gt;\n\n\n\n"}}