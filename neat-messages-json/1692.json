{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":7192225,"authorName":"Ian Badcoe","from":"Ian Badcoe &lt;ian_badcoe@...&gt;","profile":"ian_badcoe","replyTo":"LIST","senderId":"SJ19UR74D0DPIVnXBm4oexZPW2grKrX2tkkCCEhky5A9UEtBHQznC_rPl0eJwfAiSJN6fLNnqwBnlBuAOqOKIalEwCyQV1lHYyg","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] Neuron functions","postDate":"1099674988","msgId":1692,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDYuMS4yLjAuMC4yMDA0MTEwNTExMTYwNy4wMjRmOGIxMEBwb3AubWFpbC55YWhvby5jby51az4=","inReplyToHeader":"PDQxOEFBQzQ0LjgwNDA4MDBAZHNsLnBpcGV4LmNvbT4=","referencesHeader":"PDQxODY0NDM3LjEwNTA2MDJAZHNsLnBpcGV4LmNvbT4gPDYuMS4yLjAuMC4yMDA0MTEwMjExNTgzMC4wMjUxNDcwOEBwb3AubWFpbC55YWhvby5jby51az4gPDQxODdGMjhDLjUwNTAxMDBAZHNsLnBpcGV4LmNvbT4gPDYuMS4yLjAuMC4yMDA0MTEwMzE2MTY0Mi4wMjUwNmM2OEBwb3AubWFpbC55YWhvby5jby51az4gPDQxOEFBQzQ0LjgwNDA4MDBAZHNsLnBpcGV4LmNvbT4="},"prevInTopic":1691,"nextInTopic":1693,"prevInTime":1691,"nextInTime":1693,"topicId":1668,"numMessagesInTopic":20,"msgSnippet":"... No, I think I fumbled that paragraph, disregard it.... ... w.r.t development in natural brains... Development is a whole complex and fascinating subject in","rawEmail":"Return-Path: &lt;ian_badcoe@...&gt;\r\nX-Sender: ian_badcoe@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 99718 invoked from network); 5 Nov 2004 17:22:07 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m25.grp.scd.yahoo.com with QMQP; 5 Nov 2004 17:22:07 -0000\r\nReceived: from unknown (HELO smtp003.mail.ukl.yahoo.com) (217.12.11.34)\n  by mta5.grp.scd.yahoo.com with SMTP; 5 Nov 2004 17:22:06 -0000\r\nReceived: from unknown (HELO ian2k.yahoo.co.uk) (ian?badcoe@212.159.73.108 with login)\n  by smtp003.mail.ukl.yahoo.com with SMTP; 5 Nov 2004 17:11:31 -0000\r\nMessage-Id: &lt;6.1.2.0.0.20041105111607.024f8b10@...&gt;\r\nX-Sender: ian_badcoe@...\r\nX-Mailer: QUALCOMM Windows Eudora Version 6.1.2.0\r\nDate: Fri, 05 Nov 2004 17:16:28 +0000\r\nTo: neat@yahoogroups.com\r\nIn-Reply-To: &lt;418AAC44.8040800@...&gt;\r\nReferences: &lt;41864437.1050602@...&gt;\n &lt;6.1.2.0.0.20041102115830.02514708@...&gt;\n &lt;4187F28C.5050100@...&gt;\n &lt;6.1.2.0.0.20041103161642.02506c68@...&gt;\n &lt;418AAC44.8040800@...&gt;\r\nMime-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;us-ascii&quot;; format=flowed\r\nX-eGroups-Remote-IP: 217.12.11.34\r\nFrom: Ian Badcoe &lt;ian_badcoe@...&gt;\r\nSubject: Re: [neat] Neuron functions\r\nX-Yahoo-Group-Post: member; u=7192225\r\nX-Yahoo-Profile: ian_badcoe\r\n\r\nAt 22:25 04/11/2004, you wrote:\n\n&gt;Ian Badcoe wrote:\n&gt;\n&gt; &gt;&gt;On the other hand you could take the view that some functions are so\n&gt; &gt;&gt;fundamental, that incorporating them into the neuron makes sense for all\n&gt; &gt;&gt;NEAT variations, modular or otherwise. As I say, I think there&#39;s a\n&gt; &gt;&gt;balance between providing functions and increasing the parameter/search\n&gt; &gt;&gt;space, and not having so many and reducing the search space. Biological\n&gt; &gt;&gt;neural nets suggest the right balance might be towards adding functions.\n&gt; &gt;&gt;\n&gt; &gt;&gt;\n&gt; &gt;\n&gt; &gt;Do they?  There are a few different types of neurones in biology, it is\n&gt; &gt;true, but in some ways it is more as if each type plays a different role,\n&gt; &gt;globally, rather than just being chosen for a different role in the local\n&gt; &gt;circuit.\n&gt; &gt;\n&gt; &gt;\n&gt;So what you&#39;re saying here is that any given circuit is only made up of\n&gt;one type of neurone...\n\nNo, I think I fumbled that paragraph, disregard it....\n\n&gt; &gt;  e.g. by this I mean the idea that a whole class of neurone can be\n&gt; &gt;labelled as (say) &quot;inhibitory&quot; and that it is always found to inhibit and\n&gt; &gt;further, it is localised to particular areas and also always attached to\n&gt; &gt;the neurones of some other class.\n&gt; &gt;\n&gt; &gt;\n\nw.r.t development in natural brains...\n\nDevelopment is a whole complex and fascinating subject in its own right.\n\nA lot of it is directed by its own evolutionary history, rather than any \ncoherent plan, or even status quo.\n(ontogeny recapitulates phylogeny, and all that)\n\nOTOH, it is very stable and very flexible, so some aspect of that approach \nmust work.\n\nNote that I am not an expert on this.  I have a bio-science background, and \nI read the first 1/3 of an introductory text a few months ago, as a \nrefresher...\n\nHowever, the general picture is this.\n\n1) ball of cells\n2) top bottom polarity established\n3) front back polarity established\n\nand then several hierarchic levels of:\n\nXa) cell types become specialized based on location\nXb) cells move according to their specialization\n\nSo, for example, the first &quot;X&quot; stage consists of forming three layers \n(inside, middle, outside) (endoderm, mesoderm, ectoderm).  Then in the next \nstage, for example, ectoderm cells (which have moved to various places) \nspecialize again into a couple of further types.\n\nThere are about four hierarchic levels in the diagram I have of \nthis.  Which is not to say that it is the whole process but you have to \nunderstand that the further down we go, the smaller, more complex, and more \nspecialized the structures involved...\n\nThe process of one population of cells specializing in two different ways \nto form two populations in called &quot;differentiation&quot; and is not \nreversible.  (Another view on the same thing is that the original \npopulation was &quot;competent&quot; to become a range of derived types but after \ndifferentiation, each sub-population is only competent to become a subset \nof the those types.\n\nNow, the diagram I have handy only goes as far as &quot;neurones&quot; but ISRT \nfurther differentiations occur during brain development.  And one example \nof that is between &quot;inhibitory&quot; and &quot;excitatory&quot; neurones.  Now I may be \nmisremembering and this is certainly drawn from only one example (and other \nneurones may do both behaviours) but in this context at least, exciting \nneurones and inhibiting neurones are different animals, and cannot \ninterconvert.\n\ne.g. the process is that inhibitory neurones form in one location, and \nmigrate to where they are needed, and excitory ones form elsehwere and also \nmigrate.  So there&#39;s no question of them developing inhibition or \nexcitation behaviour just on the basis of signal exchange with their \nneighbours.  The choice is at a higher level and about whether to connect \nor not.\n\nThat&#39;s what I was getting at...\n\nBut I may be wrong, it has been a long time...\n\nAnd that&#39;s not to say there isn&#39;t some other class of neurone which has \nboth types of behaviour...\n\n&gt;On the other hand we have this other model where all neurones are the\n&gt;same, but their functionality is modified by the current chemical and\n&gt;electrical state of the neurone. This seems like a far more powerful\n&gt;model in that the signals in the network can actually dramatically\n&gt;modify the functionality that a network describes.\n\nRight.  Remember the distinction between evolution and learning, \nhowever.  Evolution can act on an arbitrarily complex neurone model, \nbecause all that is needed is &quot;fitter&quot; or &quot;less fit&quot;.  Real neurone&#39;s \nlearning (which you seem to be touching on) would (it seems to me) find too \npowerful a model on the individual neurone inhibiting (e.g. too many ways \nto change may cause an inability to pick one change to make).\n\n&gt;I really need to do some reading on biological neurones, I suspect that\n&gt;both models exist to some degree - a hybrid of the two models might be\n&gt;even more powerful.\n\nEven if not directly comparable, I find occasional back-reference to \nbiology really puts things in some sort of context.\n\n&gt; &gt;&gt;&gt;        Did you consider making it an homologous pool of functions.  e.g.\n&gt; &gt;&gt;&gt;not distinguishing collectors from activators but allowing them to be\n&gt; &gt;&gt;&gt;plugged in any order.  That way if the system needs linearity through some\n&gt; &gt;&gt;&gt;sub-net, it does not need to select a whole load of &quot;linear&quot; \n&gt; activators, it\n&gt; &gt;&gt;&gt;just omits the activation functions altogether.  That would cover the \n&gt; leaky\n&gt; &gt;&gt;&gt;integrator as well...\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;I don&#39;t follow. One of the proposed activation functions is a\n&gt; &gt;&gt;straight-through/linear function, surely selecting that function has the\n&gt; &gt;&gt;same effect?\n&gt; &gt;&gt;\n&gt; &gt;&gt;\n&gt; &gt;\n&gt; &gt;True, but I guess I was getting towards the idea that for some sorts of\n&gt; &gt;net, activation functions might be the exception, rather than the rule...\n&gt; &gt;\n&gt; &gt;A completely homologous set of functions is also clearer for future\n&gt; &gt;expansion, because if you have something a bit different, like leaky\n&gt; &gt;integrator, it is still clear where it fits into the system.\n&gt; &gt;\n&gt; &gt;\n&gt;I&#39;m not forming a clear idea of how this would work - apart from\n&gt;previously mention idea of a set of functions each with a weighting\n&gt;factor. You then have a choice of evolving the weight factor (model 1)\n&gt;or calculating it based on the signal on each function&#39;s input channel\n&gt;(model 2).\n\nI think you are mixing up too different areas of conversation.  This bit \nwas about your plan to have two classes of function, &quot;collectors&quot; and \n&quot;activators&quot; (I think those were the terms).  I was just suggesting not \ndistinguishing between them, so that C1 -&gt; C2 -&gt; C3 -&gt; A was a valid \nsequence (C = collector type, A = activator type).\n\n&gt; &gt;No, I meant two different control parameters on the activation curve (I\n&gt; &gt;need to draw this but don&#39;t have time just now) one controlling the Y-scale\n&gt; &gt;\n&gt; &gt;\n&gt;err, you mean the X-scale right?\n\nOh, yes :)\n\n&gt; &gt;of the sigmoid (activation in your eqn) and one controlling the curviness\n&gt; &gt;of the sigmoid (so that if it were zero, the sgmoid would be a straight\n&gt; &gt;line).  I&#39;m not sure the exp-based eqn is easily adapted for that, but I&#39;ll\n&gt; &gt;think about it later....\n&gt; &gt;\n&gt; &gt;\n&gt;Ahh I see now. I was thinking that the curviness and the activation\n&gt;(above) where the same thing. But yeh that seems like a sound idea, I&#39;ll\n&gt;have a play around in gnuplot.\n\nCool.\n\n&gt; &gt;OTOH, this does lead up to another point I&#39;ve been meaning to discuss for\n&gt; &gt;some time.  Which is how easy is it for NEAT to adjust scaling?  I thought\n&gt; &gt;about ti before in terms of input, but it applies just the same for\n&gt; &gt;output.  e.g. suppose we have a fit network, where to improve it we need to\n&gt; &gt;adjust the &quot;activation&quot; parameter (from your eqn) of one neurone.  The\n&gt; &gt;standard ANN scheme can do this, all you do is scale _all_ the input\n&gt; &gt;weights on the neurone by the same amount.  However, that is a set of a\n&gt; &gt;large number of linked mutations, and doing one of them in isolation might\n&gt; &gt;be unadaptive.  I see this as a very strong argument that neurones should\n&gt; &gt;have a mutatable &quot;activation&quot; (which I would call &quot;gain&quot;) parameter...\n&gt; &gt;\n&gt; &gt;\n&gt;Yes, what you say makes a lot of sense.\n&gt;\n&gt;Going to go and search out some biological neurone research now.\n\nHave fun :)\n\n&gt;Colin.\n&gt;\n&gt;P.S. Is a neurone just a posh neuron? :)\n\nI think it might be an English one.\n\n\nLiving@Home - Open Source Evolving Organisms - \nhttp://livingathome.sourceforge.net/\n\n\n\n\n"}}