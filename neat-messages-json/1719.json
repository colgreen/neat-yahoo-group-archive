{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":127853030,"authorName":"Colin Green","from":"Colin Green &lt;cgreen@...&gt;","profile":"alienseedpod","replyTo":"LIST","senderId":"VEKmtobDDwtbU8s0yrVBxjw2zOspwPKek_seRjqiO7p9mLmqLe69bjSSozUUURd1CCSyDtLPLHomfXZg_PqItymJwZQNrXfnog","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] Neuron functions","postDate":"1100120124","msgId":1719,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQxOTI4MDNDLjIwODAxQGRzbC5waXBleC5jb20+","inReplyToHeader":"PDYuMS4yLjAuMC4yMDA0MTEwNTExMTYwNy4wMjRmOGIxMEBwb3AubWFpbC55YWhvby5jby51az4=","referencesHeader":"PDQxODY0NDM3LjEwNTA2MDJAZHNsLnBpcGV4LmNvbT4gPDYuMS4yLjAuMC4yMDA0MTEwMjExNTgzMC4wMjUxNDcwOEBwb3AubWFpbC55YWhvby5jby51az4gPDQxODdGMjhDLjUwNTAxMDBAZHNsLnBpcGV4LmNvbT4gPDYuMS4yLjAuMC4yMDA0MTEwMzE2MTY0Mi4wMjUwNmM2OEBwb3AubWFpbC55YWhvby5jby51az4gPDQxOEFBQzQ0LjgwNDA4MDBAZHNsLnBpcGV4LmNvbT4gPDYuMS4yLjAuMC4yMDA0MTEwNTExMTYwNy4wMjRmOGIxMEBwb3AubWFpbC55YWhvby5jby51az4="},"prevInTopic":1718,"nextInTopic":0,"prevInTime":1718,"nextInTime":1720,"topicId":1668,"numMessagesInTopic":20,"msgSnippet":"... [...] ... This then would tend to suggest that the activation flipping does not have a biological basis. It still might be useful though, but that s ","rawEmail":"Return-Path: &lt;cgreen@...&gt;\r\nX-Sender: cgreen@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 13615 invoked from network); 10 Nov 2004 20:55:26 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m18.grp.scd.yahoo.com with QMQP; 10 Nov 2004 20:55:26 -0000\r\nReceived: from unknown (HELO astro.systems.pipex.net) (62.241.163.6)\n  by mta5.grp.scd.yahoo.com with SMTP; 10 Nov 2004 20:55:25 -0000\r\nReceived: from [10.0.0.10] (81-86-175-101.dsl.pipex.com [81.86.175.101])\n\tby astro.systems.pipex.net (Postfix) with ESMTP id AD874E000056\n\tfor &lt;neat@yahoogroups.com&gt;; Wed, 10 Nov 2004 20:55:19 +0000 (GMT)\r\nMessage-ID: &lt;4192803C.20801@...&gt;\r\nDate: Wed, 10 Nov 2004 20:55:24 +0000\r\nUser-Agent: Mozilla Thunderbird 0.7.1 (Windows/20040626)\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: neat@yahoogroups.com\r\nReferences: &lt;41864437.1050602@...&gt; &lt;6.1.2.0.0.20041102115830.02514708@...&gt; &lt;4187F28C.5050100@...&gt; &lt;6.1.2.0.0.20041103161642.02506c68@...&gt; &lt;418AAC44.8040800@...&gt; &lt;6.1.2.0.0.20041105111607.024f8b10@...&gt;\r\nIn-Reply-To: &lt;6.1.2.0.0.20041105111607.024f8b10@...&gt;\r\nContent-Type: text/plain; charset=us-ascii; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Remote-IP: 62.241.163.6\r\nFrom: Colin Green &lt;cgreen@...&gt;\r\nSubject: Re: [neat] Neuron functions\r\nX-Yahoo-Group-Post: member; u=127853030\r\nX-Yahoo-Profile: alienseedpod\r\n\r\nIan Badcoe wrote:\n\n&gt;At 22:25 04/11/2004, you wrote:\n&gt;  \n&gt;\n[...]\n\n&gt;Now, the diagram I have handy only goes as far as &quot;neurones&quot; but ISRT \n&gt;further differentiations occur during brain development.  And one example \n&gt;of that is between &quot;inhibitory&quot; and &quot;excitatory&quot; neurones.  Now I may be \n&gt;misremembering and this is certainly drawn from only one example (and other \n&gt;neurones may do both behaviours) but in this context at least, exciting \n&gt;neurones and inhibiting neurones are different animals, and cannot \n&gt;interconvert.\n&gt;\n&gt;e.g. the process is that inhibitory neurones form in one location, and \n&gt;migrate to where they are needed, and excitory ones form elsehwere and also \n&gt;migrate.  So there&#39;s no question of them developing inhibition or \n&gt;excitation behaviour just on the basis of signal exchange with their \n&gt;neighbours.  The choice is at a higher level and about whether to connect \n&gt;or not.\n&gt;  \n&gt;\nThis then would tend to suggest that the activation flipping does not \nhave a biological basis. It still might be useful though, but that&#39;s \ncompletely open for investigation at this point.\n\n&gt;That&#39;s what I was getting at...\n&gt;\n&gt;But I may be wrong, it has been a long time...\n&gt;  \n&gt;\nAnd you never know what might be going on elsewhere in the brain. \nScientific American&#39;s April issue had a special feature about glial \ncells and how their role has perhaps been underestimated by science up \nuntil recently. As with neurons there are a range of glial cell types \nand they typically outnumber neurons by 9 to 1 They seem to listen in on \nneurotransmitter signals at synapses as well as communicating chemically \namongst themselves, releasing chemicals that can dampen the transmission \nof signals or cause synapses and dendrites to grow. So it seems they \nhave a range of functions but the main one might be the actual forming \nof connections. E.g. if you have two isolated electrical circuits nearby \nthat are both activated, then the glia will secrete some chemical \nglobally and nearby glia can therefor work out a path between the two \ncircuits and start releasing chemicals to build connections between the \ntwo circuits.\n\n&gt;And that&#39;s not to say there isn&#39;t some other class of neurone which has \n&gt;both types of behaviour...\n&gt;  \n&gt;\n&gt;  \n&gt;\n&gt;&gt;On the other hand we have this other model where all neurones are the\n&gt;&gt;same, but their functionality is modified by the current chemical and\n&gt;&gt;electrical state of the neurone. This seems like a far more powerful\n&gt;&gt;model in that the signals in the network can actually dramatically\n&gt;&gt;modify the functionality that a network describes.\n&gt;&gt;    \n&gt;&gt;\n&gt;\n&gt;Right.  Remember the distinction between evolution and learning, \n&gt;however.  Evolution can act on an arbitrarily complex neurone model, \n&gt;because all that is needed is &quot;fitter&quot; or &quot;less fit&quot;.  Real neurone&#39;s \n&gt;learning (which you seem to be touching on) would (it seems to me) find too \n&gt;powerful a model on the individual neurone inhibiting (e.g. too many ways \n&gt;to change may cause an inability to pick one change to make).\n&gt;  \n&gt;\nI understand what you&#39;re saying but I don&#39;t think there&#39;s any learning \ninvolved in such a network (one with connection channels to modify \nneuron behaviour). If there were some kind scheme in place to alter \ninternal neuron parameters during a run, or to adjust weights then I \nwould take that as being learning, but with our current model the neuron \nfunctionality remains a constant, it&#39;s just that the functionality as \nfar richer than a fixed sigmoid.\n\nOn the other hand maybe such a system does allow some degree of learning \n- e.g. by somehow locking down a neuron&#39;s functionality after a number \nof activations, perhaps using some kind of semi-isolated recursive \nconnection - that can maintain a steady signals into one of our function \nmodifier channels.\n\n&gt;&gt;I really need to do some reading on biological neurones, I suspect that\n&gt;&gt;both models exist to some degree - a hybrid of the two models might be\n&gt;&gt;even more powerful.\n&gt;&gt;    \n&gt;&gt;\n&gt;\n&gt;Even if not directly comparable, I find occasional back-reference to \n&gt;biology really puts things in some sort of context.\n&gt;  \n&gt;\nBecause there are so many options I&#39;m hoping some reasearch into \nbiological neurons will help choose a direction or directions to \ninvestigate. It might well be that our dreamt up appraoches would be \njust as good, but biology has already found a working model, so that \nseems like a good starting point to me.\n\n&gt;  \n&gt;\n&gt;&gt;&gt;OTOH, this does lead up to another point I&#39;ve been meaning to discuss for\n&gt;&gt;&gt;some time.  Which is how easy is it for NEAT to adjust scaling?  I thought\n&gt;&gt;&gt;about ti before in terms of input, but it applies just the same for\n&gt;&gt;&gt;output.  e.g. suppose we have a fit network, where to improve it we need to\n&gt;&gt;&gt;adjust the &quot;activation&quot; parameter (from your eqn) of one neurone.  The\n&gt;&gt;&gt;standard ANN scheme can do this, all you do is scale _all_ the input\n&gt;&gt;&gt;weights on the neurone by the same amount.  However, that is a set of a\n&gt;&gt;&gt;large number of linked mutations, and doing one of them in isolation might\n&gt;&gt;&gt;be unadaptive.  I see this as a very strong argument that neurones should\n&gt;&gt;&gt;have a mutatable &quot;activation&quot; (which I would call &quot;gain&quot;) parameter...\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;      \n&gt;&gt;&gt;\n&gt;&gt;Yes, what you say makes a lot of sense.\n&gt;&gt;    \n&gt;&gt;\nAlternatively you could just have some semi-intelligent weight mutation \nthat is able to mutate the incoming weights on one neuron at a time. I \nalready have some semi-intelligent mutation routines that I use during \npruning. It would be fairly easy then to add this to my now modularised \nweight mutation code that was put in place for the meta algorithm search \nwork.\n\nColin.\n\n\n\n\n"}}