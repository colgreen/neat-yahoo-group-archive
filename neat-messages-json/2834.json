{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":119941855,"authorName":"cpchristenson","from":"&quot;cpchristenson&quot; &lt;cpchristenson@...&gt;","profile":"cpchristenson","replyTo":"LIST","senderId":"Die3R928I5KofBuNnrIUVYn0tarXEZAfjaBtJ6OrfizKvpPledasTqSiG2B4rf6ncCYyKG4r-XeOLcCK5nwVnArWVd3HkWJyD3ZUuhnBQbE","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Some questions from beginner in NEAT","postDate":"1164041671","msgId":2834,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGVqc21rNys5dWtnQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGVqbzdvMysyNjY4QGVHcm91cHMuY29tPg=="},"prevInTopic":2824,"nextInTopic":2836,"prevInTime":2833,"nextInTime":2835,"topicId":2819,"numMessagesInTopic":9,"msgSnippet":"I still read, just never comment.  I did have success combining evolution and learning.  However, unlike what you described, my thesis included the learning","rawEmail":"Return-Path: &lt;cpchristenson@...&gt;\r\nX-Sender: cpchristenson@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 45024 invoked from network); 20 Nov 2006 16:55:51 -0000\r\nReceived: from unknown (66.218.66.167)\n  by m23.grp.scd.yahoo.com with QMQP; 20 Nov 2006 16:55:51 -0000\r\nReceived: from unknown (HELO n8c.bullet.sp1.yahoo.com) (69.147.64.170)\n  by mta6.grp.scd.yahoo.com with SMTP; 20 Nov 2006 16:55:51 -0000\r\nReceived: from [216.252.122.219] by n8.bullet.sp1.yahoo.com with NNFMP; 20 Nov 2006 16:54:33 -0000\r\nReceived: from [66.218.69.4] by t4.bullet.sp1.yahoo.com with NNFMP; 20 Nov 2006 16:54:33 -0000\r\nReceived: from [66.218.66.86] by t4.bullet.scd.yahoo.com with NNFMP; 20 Nov 2006 16:54:33 -0000\r\nDate: Mon, 20 Nov 2006 16:54:31 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;ejsmk7+9ukg@...&gt;\r\nIn-Reply-To: &lt;ejo7o3+2668@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0\r\nFrom: &quot;cpchristenson&quot; &lt;cpchristenson@...&gt;\r\nSubject: Re: Some questions from beginner in NEAT\r\nX-Yahoo-Group-Post: member; u=119941855; y=TVwa7INcg-l9a_C-Ry-WIBqFAx4oaJklxCtmr4IXWay2NlucS75rGw\r\nX-Yahoo-Profile: cpchristenson\r\n\r\nI still read, just never comment.  I did have success combining \nevolution =\r\nand learning.  However, unlike what you described, my \nthesis included the =\r\nlearning within the life of the network rather \nthan after the evolution is=\r\n complete.  So, evolution would create a \nnetwork, then that network would =\r\nlearn to perform a task, then it \nwas evaluated based on its ability to lea=\r\nrn the task.  The learned \nweights were NOT passed onto the next generation=\r\n.  The idea was to \nevolve a network that was better at learning.  I was ab=\r\nle to show \nthis was possible using the task of learning to solve polynomia=\r\nls.  \nA very simple example, but functional.\n\n--- In neat@yahoogroups.com, =\r\n&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt; wrote:\n&gt;\n&gt; I know people have combined NEA=\r\nT with BP in the past.  For \nexample, \n&gt; Chris Christenson had some success=\r\n with it, but I&#39;m not sure he&#39;s \n&gt; still reading this group.\n&gt; \n&gt; In any ca=\r\nse, the number of generations (and hence the length of \n&gt; time) it takes to=\r\n solve a problem depends on problem difficulty, \nso \n&gt; it&#39;s difficult to gi=\r\nve an accurate estimate, but 7 hours is \nprobably \n&gt; a reasonable expectati=\r\non for the problem you cite.  \n&gt; \n&gt; In fact, supervised classification prob=\r\nlems tend to be faster than \n&gt; control problems since they don&#39;t involve a =\r\ndomain simulator, so \nyou \n&gt; can expect it to be possibly a lot faster.  Ho=\r\nwever, if you use BP \n&gt; you have to factor in the time that adds to each ev=\r\naluation, \n&gt; depending how you fold it into your procedure.\n&gt; \n&gt; I&#39;m also g=\r\nuessing you will not need two hidden layers of 8 hidden \n&gt; nodes each to so=\r\nlve your problem.  NEAT usually discovers that it \n&gt; takes a lot less than =\r\nyou expect to solve problems that seem like \n&gt; they should be difficult.\n&gt; =\r\n\n&gt; Finally, NEAT has been applied to real world industrial problems \nin \n&gt; =\r\nsimulation, but I&#39;m not sure if it&#39;s been literally used in e.g a \n&gt; real f=\r\nactory or something like that.  But a 300 instance \n&gt; classification sounds=\r\n fairly straightforward. \n&gt; \n&gt; ken \n&gt; \n&gt; --- In neat@yahoogroups.com, &quot;aimi=\r\nke002&quot; &lt;aimike002@&gt; wrote:\n&gt; &gt;\n&gt; &gt; Sorry , just to clarify for (3) if we to=\r\nok an example with \n&gt; &gt; say 300 training samples, with say 100 samples in t=\r\nhe test set.\n&gt; &gt; Just to try and get an idea.\n&gt; &gt; \n&gt; &gt; Mike\n&gt; &gt; \n&gt; &gt; --- In=\r\n neat@yahoogroups.com, &quot;aimike002&quot; &lt;aimike002@&gt; wrote:\n&gt; &gt; &gt;\n&gt; &gt; &gt; Hi I&#39;m t=\r\naking a look for possibly using in for my Masters \n&gt; project,\n&gt; &gt; &gt; which  =\r\nneeds evolving the architecture for a neural net for a \n&gt; real\n&gt; &gt; &gt; world =\r\napplication. \n&gt; &gt; &gt; I have read some of Ken Stanley&#39;s papers on the web but=\r\n beyond \n&gt; that\n&gt; &gt; &gt; I&#39;m a beginner, so if I may ask some beginner&#39;s quest=\r\nions:\n&gt; &gt; &gt; 1) I think my neural net may be evolved using GAs , but fine \n&gt;=\r\n tuned to\n&gt; &gt; &gt; optimise using standard BP (ie hybrid approach). Has anybod=\r\ny \n&gt; else been\n&gt; &gt; &gt; down that path using NEAT, taking e.g. the weights evo=\r\nlved \nusing \n&gt; NEAT\n&gt; &gt; &gt; & feeding them into some standard BP software?\n&gt; =\r\n&gt; &gt; 2) Has NEAT been used in any &#39;real world&#39;/industrial \n&gt; applications ye=\r\nt?\n&gt; &gt; &gt; \n&gt; &gt; &gt; and a &#39;how long is a piece of string question&#39;...\n&gt; &gt; &gt; \n&gt; =\r\n&gt; &gt; 3)  As yet  have no feel for how long the evolution might \ntake. \n&gt; I&#39;v=\r\ne\n&gt; &gt; &gt; ran a few of the downloaded sample executables they seem to \ntake \n=\r\n&gt; quite\n&gt; &gt; &gt; a while to run. My neural net has 8 inputs, one output and if=\r\n \nwe \n&gt; guess\n&gt; &gt; &gt; at a final neural net of 2 hidden layers with 8 nodes p=\r\ner \nlayer. \n&gt; If we\n&gt; &gt; &gt; used say a 2GHz PC doing nothing else - could we =\r\nguess at 7 \nhours\n&gt; &gt; &gt; (i.e. an overnight run) to evolve or is it more lik=\r\nely to be \n&gt; several\n&gt; &gt; &gt; days? Would appreciate any best guesses or exper=\r\nience on this.\n&gt; &gt; &gt; \n&gt; &gt; &gt; Thanks in advance\n&gt; &gt; &gt; \n&gt; &gt; &gt; Mike\n&gt; &gt; &gt;\n&gt; &gt;\n&gt;=\r\n\n\n\n\n\n"}}