{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":199382914,"authorName":"Mike Woodhouse","from":"&quot;Mike Woodhouse&quot; &lt;mikewoodhouse@...&gt;","profile":"mikewoodhouse","replyTo":"LIST","senderId":"Kbr3McZ8oP06wBqZVq6isKnFty-eKJo6JPkykX1S7pGm_hfNNOQgYnuTjOVmYu4VeIxSEHfp-Ho5K1VmbNtCkXmjzXdYMLkAis8fK_NkaAYW","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: 378 days later...","postDate":"1143025799","msgId":2581,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGR2cmJhNytoYTY2QGVHcm91cHMuY29tPg==","inReplyToHeader":"PDQ0MjA5MEFBLjcwMDA2MDNAZHNsLnBpcGV4LmNvbT4="},"prevInTopic":2580,"nextInTopic":0,"prevInTime":2580,"nextInTime":2582,"topicId":2571,"numMessagesInTopic":6,"msgSnippet":"... CPU ... My working practice mostly tends to involve making adjustments at lunchtime, then kicking off the model before leaving work and leaving to simmer","rawEmail":"Return-Path: &lt;mikewoodhouse@...&gt;\r\nX-Sender: mikewoodhouse@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 31891 invoked from network); 22 Mar 2006 11:10:32 -0000\r\nReceived: from unknown (66.218.67.36)\n  by m13.grp.scd.yahoo.com with QMQP; 22 Mar 2006 11:10:31 -0000\r\nReceived: from unknown (HELO n24.bullet.scd.yahoo.com) (66.94.237.53)\n  by mta10.grp.scd.yahoo.com with SMTP; 22 Mar 2006 11:10:22 -0000\r\nComment: DomainKeys? See http://antispam.yahoo.com/domainkeys\r\nReceived: from [66.218.69.5] by n24.bullet.scd.yahoo.com with NNFMP; 22 Mar 2006 11:09:59 -0000\r\nReceived: from [66.218.66.82] by t5.bullet.scd.yahoo.com with NNFMP; 22 Mar 2006 11:09:59 -0000\r\nDate: Wed, 22 Mar 2006 11:09:59 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;dvrba7+ha66@...&gt;\r\nIn-Reply-To: &lt;442090AA.7000603@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: &quot;Mike Woodhouse&quot; &lt;mikewoodhouse@...&gt;\r\nSubject: Re: 378 days later...\r\nX-Yahoo-Group-Post: member; u=199382914; y=5bx3B341ITg1XpfGS1iYhNAp9KbaHSVqll0tZAV5kS4TfhI9D_eqCQ\r\nX-Yahoo-Profile: mikewoodhouse\r\n\r\n\n&gt; ... I purposely fixed the size of my graphs in\n&gt; sharpneat because they =\r\nuse GDI+ and I noticed it gets real slow as the\n&gt; painting area increases -=\r\n exponentially so. ZedGraph may suffer from\n&gt; this also, so just be aware t=\r\nhat graph refreshes may be eating more\nCPU\n&gt; than you would like.\n\nMy worki=\r\nng practice mostly tends to involve making adjustments at\nlunchtime, then k=\r\nicking off the model before leaving work and leaving to\nsimmer overnight. T=\r\nhe overhead is probably there, but at 1 or 2\nevaluations per second (there =\r\nare 30 to 40 thousand matches to process\nper run at 4 or 5 steps per eval -=\r\n one a second is pretty snappy and a\nfew extra milliseconds per generation =\r\nwon&#39;t hurt).\n\n&gt; &lt;deep frown&gt; Just because you /can/ set silly values doesn&#39;=\r\nt mean you\n&gt; should! ;) If the # of species does crash then you risk loosin=\r\ng\n&gt; diversity in your population, diversity that may take a long time to\n&gt; =\r\nreacquire, especially if you are well into a search and your genomes\nare\n&gt; =\r\nquite large. In fact at such an advance stage the crash in diversity\n&gt; woul=\r\nd be quite bad I think.\n\nAwww, can&#39;t a chap have a little fun?\n\n&gt; &gt;I&#39;ve loo=\r\nked at the number of evaluations taken to converge to a 0.01\n&gt; &gt;delta. With=\r\n simple random networks it averages between 1 and 2, as\nable\n&gt; &gt;networks ev=\r\nolve it stretches to 4 to 6. Looking at the networks that\nare\n&gt; &gt;emerging, =\r\nthere&#39;s considerable connecting between the output nodes,\nso\n&gt; &gt;multi-stepp=\r\ning looks appropriate. Results have been generally better\n&gt; &gt;than single-st=\r\nep models (where the output layer connectivity doesn&#39;t\n&gt; &gt;tend to help much=\r\n). And CPU cycles are cheap!\n&gt;\n&gt; Single step? So have you tried fixing the =\r\n# of activations at 4 or 5?\n&gt; The extra overhead of RelaxNetwork() is proba=\r\nbly about equal to a\nwhole\n&gt; activation, so if you end up with solutions ar=\r\nound the 4/5 activations\n&gt; mark then it might make sense to just fix it. It=\r\n is potentially more\n&gt; complex than this because e.g. networks that relax m=\r\nay be more\n&gt; compatible during crossover, but I&#39;ve no reason to believe tha=\r\nt this\nis\n&gt; or isn&#39;t the case at this time.\n\nIt&#39;s on my to-do list. I&#39;m gue=\r\nssing that networks that haven&#39;t relaxed\nby say, 5 steps, will probably ten=\r\nd to underperform and will likely find\nselection working against them as a =\r\npenalty for dithering.\n\n&gt; Could you not just split off some of your existin=\r\ng training data into\na\n&gt; test set? Well, I&#39;m guessing your already invested=\r\n in the existing\n&gt; trainign data as it is, so maybe not eh.\n\nI&#39;m a little c=\r\noncerned about the size of the dataset already (risk of\nmemorising). I&#39;m st=\r\nill exploring the behaviour of the experiment really,\nso the acquisition of=\r\n test sets has yet to float to the top. That to-do\nlist again...\n\n&gt; I also =\r\nwant to follow the evolution of team\n&gt; &gt;rating values over time against res=\r\nults to see if I can infer what\nthe\n&gt; &gt;network is interested in.\n&gt;\n&gt; In my =\r\nexperience analysis of this type of data will lead to insights,\n&gt; although =\r\nin my case it&#39;s usually discovering something both really\n&gt; obvious (in hin=\r\ndsight) and bad and that needs some rethinking to\navoid.\n&gt; In fact it&#39;s an =\r\nobservation of mine that I NEAT often finds some\nreally\n&gt; interesting and c=\r\nlever ways of cheating at my fitness functions, but\n&gt; when I finally close =\r\nall of the loopholes that clerverness just sort\nof\n&gt; evaporates :( Ok that&#39;=\r\ns not entirely correct, there have been some\n&gt; pretty good successes really=\r\n.\n\nMy experience with anything GA-related over the years has been that the\n=\r\nlittle whatsits will always find loopholes in fitness functions as a\nprefer=\r\nence to finding a solution to the problem being presented, which\nis, after =\r\nall, only visible to evolution through the FF. It&#39;s hard to\ndefine a functi=\r\non for a complex problem space that plugs the holes\nwithout starting to inc=\r\norporate assumptions from the modeller -\nsomething I&#39;m keen to avoid: I hav=\r\ne a fiar idea what I think, I want to\nknow what the network can tell me tha=\r\nt&#39;s new!\n\n&gt; [mental note - amend license agreement to limit usage by evil-d=\r\noers]\n\n:)\n\n&gt; &gt;I&#39;m taking the precaution of reworking the model to utilise a=\r\n Manager\n&gt; &gt;class that understands working with a day&#39;s matches at a time. =\r\nThe\nfront\n&gt; &gt;end could then be little more than a console app and some text=\r\n files,\n&gt; &gt;which stands half a chance...\n&gt;\n&gt; Yeh, don&#39;t get bogged down in =\r\nGUI stuff. It&#39;s nice to have but not\n&gt; always essential, and also a bit bor=\r\ning really (IMHO).\n\nI rather like dreaming up GUIs (my VB heritage, probabl=\r\ny). But it&#39;s\nsomewhere near the bottom of the ol&#39; to-do list. A console app=\r\n and a\nfistful of text files will do fine  - I can use Excel for analysis.\n=\r\n\n&gt; &gt; I&#39;m\n&gt; &gt;thinking about ways to reward networks for other attributes suc=\r\nh as\ngood\n&gt; &gt;statistical reliabilty - chi-squared or similar, indicating th=\r\nat the\n&gt; &gt;results aren&#39;t just luck. Networks that learn to bet big on a few=\r\n\n&gt; &gt;long-odds winners would be penalised undr this. I have already\n&gt; &gt;exper=\r\nimented with scaling fitness down for networks selecting\nverysmall\n&gt; &gt;numbe=\r\nrs of possible bets.\n&gt;\n&gt; Sure, although you shouldn&#39;t discount the possibil=\r\nity that your champ\n&gt; network(s) may actually be able to accurately spot so=\r\nme very specific\n&gt; but rare scenario and profit from it with high a success=\r\n rate.\n\nAlthough in theory one network should be able, with sufficient\ncomp=\r\nlexity, to evaluate sensibly any set of input values, it wouldn&#39;t\nsurprise =\r\nme to discover that it&#39;s easier to subdivide the space train\nseveral specia=\r\nlist networks. The reduced sample sizes make me worry\nabout overfitting. Ke=\r\neping to one model and removing the &quot;tail data&quot;\nfrom the training set becau=\r\nse there may be small-sample distortions\n(matches where either side is a hu=\r\nge favourite) is helpful, because it\nstops the networks learning to back Bo=\r\nlton at Man U a few years back (a\n18-1 winner).\n\n&gt; &gt;&gt;P.S. If you make your =\r\nfortune then mines a pint. ;)\n&gt; &gt;I already owe you at least one for the cod=\r\ne you&#39;ve written so far!\n&gt; Ok well throw in a packet of crisps as well then=\r\n.\n\nDon&#39;t push yer luck...\n\n\n\nMike\n\n\n\n\n\n"}}