{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":211599040,"authorName":"Jeff Clune","from":"Jeff Clune &lt;jclune@...&gt;","profile":"jeffreyclune","replyTo":"LIST","senderId":"ieMggzcdOE9KfkTP8Vf7gxhcUgWQJ2XgQ5ONZGlsl-_raYFCvodGmNEDnYNQII2Au3ZIS3gq49hVm6hem_WpN7ZZ","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [neat] Re: Another New Paper:  Multiagent HyperNEAT","postDate":"1209060327","msgId":3983,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PEM0MzY0NDI3LjIyNjg1JWpjbHVuZUBtc3UuZWR1Pg==","inReplyToHeader":"PGZ1cDd2YytoazJiQGVHcm91cHMuY29tPg=="},"prevInTopic":3982,"nextInTopic":3984,"prevInTime":3982,"nextInTime":3984,"topicId":3955,"numMessagesInTopic":49,"msgSnippet":"... Hi Ken. I think it is indeed cool that you demonstrate an easy way to inject user knowledge in a way that appropriately biases the algorithm toward better","rawEmail":"Return-Path: &lt;jclune@...&gt;\r\nX-Sender: jclune@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 25202 invoked from network); 24 Apr 2008 18:05:32 -0000\r\nX-Received: from unknown (66.218.67.97)\n  by m52.grp.scd.yahoo.com with QMQP; 24 Apr 2008 18:05:32 -0000\r\nX-Received: from unknown (HELO wx-out-0506.google.com) (66.249.82.229)\n  by mta18.grp.scd.yahoo.com with SMTP; 24 Apr 2008 18:05:32 -0000\r\nX-Received: by wx-out-0506.google.com with SMTP id h27so3279786wxd.3\n        for &lt;neat@yahoogroups.com&gt;; Thu, 24 Apr 2008 11:05:32 -0700 (PDT)\r\nX-Received: by 10.70.109.12 with SMTP id h12mr3823071wxc.0.1209060331509;\n        Thu, 24 Apr 2008 11:05:31 -0700 (PDT)\r\nReturn-Path: &lt;jclune@...&gt;\r\nX-Received: from ?192.168.2.2? ( [67.167.130.112])\n        by mx.google.com with ESMTPS id i17sm1053122wxd.19.2008.04.24.11.05.29\n        (version=TLSv1/SSLv3 cipher=OTHER);\n        Thu, 24 Apr 2008 11:05:30 -0700 (PDT)\r\nUser-Agent: Microsoft-Entourage/12.1.0.080305\r\nDate: Thu, 24 Apr 2008 14:05:27 -0400\r\nTo: &quot;neat@yahoogroups.com&quot; &lt;neat@yahoogroups.com&gt;\r\nMessage-ID: &lt;C4364427.22685%jclune@...&gt;\r\nThread-Topic: [neat] Re: Another New Paper:  Multiagent HyperNEAT\r\nThread-Index: AcimNcZFmmGrg5Cm80OfzuT/YF3mRQ==\r\nIn-Reply-To: &lt;fup7vc+hk2b@...&gt;\r\nMime-version: 1.0\r\nContent-type: text/plain;\n\tcharset=&quot;US-ASCII&quot;\r\nContent-transfer-encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Jeff Clune &lt;jclune@...&gt;\r\nSubject: Re: [neat] Re: Another New Paper:  Multiagent HyperNEAT\r\nX-Yahoo-Group-Post: member; u=211599040; y=4la34xDAvkm0kFFlAiqcSYj32UwQCC_YmpOzPi5xXZ99YBYLCeEB\r\nX-Yahoo-Profile: jeffreyclune\r\n\r\n&gt; Perhaps one way to explain why we thought about that first is to\n&gt; consider that one important philosophical motivation behind HyperNEAT\n&gt; is that machine learning needs a way for humans to convey to the\n&gt; learner a priori known domain geometry.  In effect, we are running\n&gt; away from the black box of No Free Lunch (which is a nasty trap) by\n&gt; finding new ways to convey critical a priori domain information.\n&gt; While arguments can be made that because certain techniques align with\n&gt; certain problem classes we should not pay too much heed to NFL, why\n&gt; would we purposefully move *towards* the black box when we don&#39;t have\n&gt; to?  The real excitement, I think, is to find very general techniques\n&gt; for conveying to the learner standard kinds of a priori practical\n&gt; information (or bias), e.g. geometry.\n\nHi Ken. I think it is indeed cool that you demonstrate an easy way to inject\nuser knowledge in a way that appropriately biases the algorithm toward\nbetter solutions. However, in my opinion, the reason we are interested in\nmachine learning is because it can solve the problems we *don&#39;t* know how to\nsolve. The reason we use simple toy problems is because we know what the\nexpected solutions should look like, and we want to demonstrate that our\nalgorithms can find them. That gives us some confidence that, when we start\nshowing our algorithms problems where we don&#39;t know what the solutions\nshould look like, they will discover good solutions to these problems.\nSpecifically, with regards to generative encodings, one touted benefit is\ntheir ability to exploit regularities that exist in the problem domain. It\nis the hope that on complex domains there will be many such regularities\nthat can be exploited. It is implied that humans will not always know what\nthose regularities are. Even if we did, it would be nice if we did not have\nto tell the algorithm about each type of regularity. The hope is that\ngenerative encodings can discover them and exploit them without our aid. So,\nwhile it is nice to be able to inject knowledge, it is also important to\ndevise algorithms that don&#39;t require such knowledge. Clearly the logical\nextremes of either position are untenable: it is uninteresting to tell the\nnetwork how to do everything but one trivial thing and have it learn that,\nand NFL tells us we can&#39;t have it be a jack of all trades. But I think there\nare reasons to explore the intermediate ranges of both positions.\n\n&gt; That said, if you really did start without the repeating coordinate\n&gt; frames, I am guessing it would perform worse as you predict, though I\n&gt; don&#39;t know by how much.  It is probably worth doing just to see what\n&gt; happens.  Yet my personal view is that there would not be a very deep\n&gt; insight to gain from such a result.  After all, why would we expect it\n&gt; to consistently discover the right regularity simply by chance every\n&gt; time?  Remember that early in evolution, simply discovering this\n&gt; regularity may not even be rewarded; just because it somehow gets\n&gt; lucky and figures out exactly the right repeating frame of reference,\n&gt; that does not necessarily mean that within those coordinate frames it\n&gt; is doing anything useful (i.e. it could be a repetition of a bad\n&gt; policy), so the discovery is likely to go unnoticed and die out, just\n&gt; as easily as it might be leveraged and elaborated properly.\n\nI agree that this is a challenging problem, but I think that it is very\nimportant for us to figure out algorithms that can cope with it. Clearly\nnature figured out ways to deal with this issue. How can we set up\nalgorithms such that if the population early on discovers a bauplan that\nkeeps it trapped on a local peak, it can eventually discover a bauplan that\ngives it access to a higher peak?  To me this is a fundamental issue for our\nfield. If we cannot improve upon it, we will be stuck evolving relatively\ntrivial solutions and never evolve things as impressive as jaguars or poets.\nI am surprised you don&#39;t think research on this front is important or would\nprovide deep insights. Or, is it that you think we can&#39;t make progress here,\nso documenting a further failure isn&#39;t deep?\n&gt; \n&gt; This problem is related to that discussion we had a while back about\n&gt; &quot;target-based evolution&quot; and the phenomena of Picbreeder.  Often the\n&gt; stepping stones (such as discovering the right basic regularity) are\n&gt; not recognized by the ultimate objective function, so it&#39;s pretty much\n&gt; up to luck to find them and keep them around long enough to take\n&gt; advantage of them.  My feeling is that it is not fruitful for any\n&gt; indirect encoding to try to solve that problem, because it is not a\n&gt; problem with the encoding per say but rather with the way fitness is\n&gt; assigned.\n\nFitness is one part of the mix. But there are also issues of population\ndiversity, representation flexibility and evolvability, etc. Any of these\ncould assist in allowing deep switches in bauplan.\n \nI&#39;d like to say one last thing. One of the great innovations of HyperNEAT\nwas that you provided the geometry of the problem such that the algorithm\ncould exploit it. That strikes me as a bit different than telling the\nalgorithm *how* to go about exploiting that geometry. I think that\ndistinction is important, and might be being conflated a bit here. I agree\nunequivocally that providing access to the geometry is important, but it\nseems to me that it is an interesting field of research to figure out how to\ncreate algorithms that learn how to exploit that geometry, and its\nregularities, on their own.\n\n\nCheers,\nJeff Clune\n\nDigital Evolution Lab, Michigan State University\n\njclune@...\n\n\n\n\n&gt; \n&gt; --- In neat@yahoogroups.com, Jeff Clune &lt;jclune@...&gt; wrote:\n&gt;&gt; \n&gt;&gt; Hello-\n&gt;&gt; \n&gt;&gt; I enjoyed reading this. Thanks for posting it.\n&gt;&gt; \n&gt;&gt; A question: how did HyperNEAT perform when you did not provide it\n&gt; with the\n&gt;&gt; repeating coordinate frame for each agent? As you mention in the\n&gt; paper, this\n&gt;&gt; is something that HyperNEAT could learn on its own. I assume from\n&gt; the fact\n&gt;&gt; that you added it that HyperNEAT was not doing a good job of\n&gt; learning this.\n&gt;&gt; \n&gt;&gt; If that assumption is right, how bad was it at learning this problem\n&gt;&gt; decomposition? One of the touted benefits of HyperNEAT, and generative\n&gt;&gt; encodings in general, is the ability to evolve a module and reuse it\n&gt; many\n&gt;&gt; times (potentially with variation).  Here the modularity of the\n&gt; problem was\n&gt;&gt; cleanly divided, and should have been relatively easy for HyperNEAT to\n&gt;&gt; discover. Do you find it disconcerting that it couldn&#39;t do so?\n&gt;&gt; \n&gt;&gt; \n&gt;&gt; \n&gt;&gt; Cheers,\n&gt;&gt; Jeff Clune\n&gt;&gt; \n&gt;&gt; Digital Evolution Lab, Michigan State University\n&gt;&gt; \n&gt;&gt; jclune@...\n&gt;&gt; \n&gt;&gt; \n&gt;&gt; \n&gt;&gt; \n&gt;&gt;&gt; From: Kenneth Stanley &lt;kstanley@...&gt;\n&gt;&gt;&gt; Reply-To: &quot;neat@yahoogroups.com&quot; &lt;neat@yahoogroups.com&gt;\n&gt;&gt;&gt; Date: Wed, 16 Apr 2008 22:48:44 -0000\n&gt;&gt;&gt; To: &quot;neat@yahoogroups.com&quot; &lt;neat@yahoogroups.com&gt;\n&gt;&gt;&gt; Subject: [neat] Another New Paper:  Multiagent HyperNEAT\n&gt;&gt;&gt; \n&gt;&gt;&gt; David D&#39;Ambrosio and I discuss the potential for HyperNEAT\n&gt;&gt;&gt; controlling multiple heterogeneous agents in this new\n&gt;&gt;&gt; paper, &quot;Generative Encoding for Multiagent Learning,&quot; to appear at\n&gt;&gt;&gt; GECCO 2008:\n&gt;&gt;&gt; \n&gt;&gt;&gt; http://eplex.cs.ucf.edu/index.php?\n&gt;&gt;&gt; option=com_content&task=view&id=14&Itemid=28#dambrosio.gecco08\n&gt;&gt;&gt; \n&gt;&gt;&gt; Direct Link:\n&gt;&gt;&gt; \n&gt;&gt;&gt; http://eplex.cs.ucf.edu/papers/dambrosio_gecco08.pdf\n&gt;&gt;&gt; \n&gt;&gt;&gt; We also have a nice sample of videos that depict various evolved\n&gt;&gt;&gt; teams in action:\n&gt;&gt;&gt; \n&gt;&gt;&gt; http://eplex.cs.ucf.edu/multiagenthyperneat\n&gt;&gt;&gt; \n&gt;&gt;&gt; The interesting idea in this paper is that just as a single\n&gt;&gt;&gt; connective CPPN can encode how a single network varies over space,\n&gt;&gt;&gt; it can also encode how a *set* of networks (each representing the\n&gt;&gt;&gt; policy of one agent on the team) varies over space.  In this way,\n&gt;&gt;&gt; HyperNEAT can learn an expression that encodes how policies vary\n&gt;&gt;&gt; over the team geometry.  For example, in a soccer team agents vary\n&gt;&gt;&gt; from defensive to offensive as you move away from the goal.  Part of\n&gt;&gt;&gt; the power of this approach is that it means basic skills can be\n&gt;&gt;&gt; learned and shared among the whole team, since the CPPN encodes how\n&gt;&gt;&gt; those skills vary across the field.\n&gt;&gt;&gt; \n&gt;&gt;&gt; ken\n&gt;&gt;&gt; \n&gt;&gt; \n&gt; \n&gt; \n\n\n\n"}}