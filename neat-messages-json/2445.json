{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":244748320,"authorName":"myhabalixyp","from":"&quot;myhabalixyp&quot; &lt;myhabalixyp@...&gt;","profile":"myhabalixyp","replyTo":"LIST","senderId":"K-lE3zNiZa5ubZgL0i0KxGLXVs8fNxCjew9NZV6Fq8BrI8JuenzRvdmbEn_ZVgteuPNcWnBl4i04Mx8Kox7NCJKwScZdWXDr0k6Ig-SMPg","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: question about the topology of ANN","postDate":"1133268514","msgId":2445,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGRtaGluMitvcDQzQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGRtZHY4ZStyZG5oQGVHcm91cHMuY29tPg=="},"prevInTopic":2444,"nextInTopic":2446,"prevInTime":2444,"nextInTime":2446,"topicId":2441,"numMessagesInTopic":7,"msgSnippet":"Thanks for all the reply first :) I m excited to know that gradient-based algorithms such as BP can be applied to train ANN with an arbitrary topology (Colin","rawEmail":"Return-Path: &lt;myhabalixyp@...&gt;\r\nX-Sender: myhabalixyp@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 64162 invoked from network); 29 Nov 2005 12:49:07 -0000\r\nReceived: from unknown (66.218.66.216)\n  by m35.grp.scd.yahoo.com with QMQP; 29 Nov 2005 12:49:07 -0000\r\nReceived: from unknown (HELO n25.bulk.scd.yahoo.com) (66.94.237.54)\n  by mta1.grp.scd.yahoo.com with SMTP; 29 Nov 2005 12:49:07 -0000\r\nComment: DomainKeys? See http://antispam.yahoo.com/domainkeys\r\nReceived: from [66.218.69.5] by n25.bulk.scd.yahoo.com with NNFMP; 29 Nov 2005 12:48:35 -0000\r\nReceived: from [66.218.66.92] by mailer5.bulk.scd.yahoo.com with NNFMP; 29 Nov 2005 12:48:34 -0000\r\nDate: Tue, 29 Nov 2005 12:48:34 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;dmhin2+op43@...&gt;\r\nIn-Reply-To: &lt;dmdv8e+rdnh@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: &quot;myhabalixyp&quot; &lt;myhabalixyp@...&gt;\r\nSubject: Re: question about the topology of ANN\r\nX-Yahoo-Group-Post: member; u=244748320; y=gBmvqU0S0V6Qrw0arJphYsGlCCR5nBLJ1VqmSczw_XiIfNXMudQ\r\nX-Yahoo-Profile: myhabalixyp\r\n\r\nThanks for all the reply first :)\n\nI&#39;m excited to know that gradient-based =\r\nalgorithms such as BP can be \napplied to train ANN with an arbitrary topolo=\r\ngy (Colin and Yuri). \nBut I don&#39;t know how to implement the application :(.=\r\n Could someone \nteach me or recommend some paper or books to me?\n\nI&#39;m sorry=\r\n I didn&#39;t describe my question about the speed problem \nclearly. My puzzle =\r\nis, for example, a problem, such as the XOR \nproblem, can be approximated s=\r\natisfactorily by a 3-layers ANN with 4 \nhiden nodes. After being trained, w=\r\nhen we want to get the output of \na input such (0,0), we only need to do ma=\r\ntrix multiplication:[1,2]*\n[2,4] ,4+1 times logsin() and several times of a=\r\nddition operation. \nBut with a arbitrary topology ANN, we must calculate it=\r\neratively \nuntil the output of all the nodes no longer changed, more \nmulti=\r\nplication and logsin() calculation are need. the activation \nfunction is lo=\r\ngsin() in this example. With more complex topology the \nproblem must be mor=\r\ne serious. I&#39;m not sure I express my puzzle \nclearly this time :(. The kenr=\r\nal of my problem is, the algorithm \ncomplexity of an ANN producting the out=\r\nput of a input with a layed \ntopology may be much better than with an arbit=\r\nrary topology when \nsolve the same problem and use the same activation func=\r\ntion. \n\n\n\n\n"}}