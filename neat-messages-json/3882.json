{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":203637135,"authorName":"Eric Mohlenhoff","from":"&quot;Eric Mohlenhoff&quot; &lt;eric.mohlenhoff@...&gt;","profile":"er042685","replyTo":"LIST","senderId":"Sx2x8zW953iS46V8VyYP9x5d6nZ7Wy1S-fX7Ltr3wrloA-c_JZQcIdq5MG0BvRi3GJhhvOzAMJSj7-2CwQ7zDbzE7fYW5Z-0XFah8yFY2gYopY7P","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Yet another question on combining NEAT with backprop (or another non-genetic training algorithm)","postDate":"1205773005","msgId":3882,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGFiZGQwOTRlMDgwMzE3MDk1Nnk1ZTcwMjQzM2xhYmQxZDA0YzhlZGE3ZTdiQG1haWwuZ21haWwuY29tPg=="},"prevInTopic":0,"nextInTopic":3885,"prevInTime":3881,"nextInTime":3883,"topicId":3882,"numMessagesInTopic":3,"msgSnippet":"Hey all, Long-time reader, first time writer here. The recent discussion on using NEAT with backprop inspired me to ask if anyone has done or knows of any","rawEmail":"Return-Path: &lt;eric.mohlenhoff@...&gt;\r\nX-Sender: eric.mohlenhoff@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 86420 invoked from network); 17 Mar 2008 16:56:46 -0000\r\nX-Received: from unknown (66.218.67.95)\n  by m56.grp.scd.yahoo.com with QMQP; 17 Mar 2008 16:56:46 -0000\r\nX-Received: from unknown (HELO rn-out-0910.google.com) (64.233.170.185)\n  by mta16.grp.scd.yahoo.com with SMTP; 17 Mar 2008 16:56:46 -0000\r\nX-Received: by rn-out-0910.google.com with SMTP id i24so3617685rng.0\n        for &lt;neat@yahoogroups.com&gt;; Mon, 17 Mar 2008 09:56:45 -0700 (PDT)\r\nX-Received: by 10.115.79.1 with SMTP id g1mr18322440wal.2.1205773005149;\n        Mon, 17 Mar 2008 09:56:45 -0700 (PDT)\r\nX-Received: by 10.114.192.2 with HTTP; Mon, 17 Mar 2008 09:56:45 -0700 (PDT)\r\nMessage-ID: &lt;abdd094e0803170956y5e702433labd1d04c8eda7e7b@...&gt;\r\nDate: Mon, 17 Mar 2008 12:56:45 -0400\r\nTo: neat@yahoogroups.com\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Transfer-Encoding: 7bit\r\nContent-Disposition: inline\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: &quot;Eric Mohlenhoff&quot; &lt;eric.mohlenhoff@...&gt;\r\nSubject: Yet another question on combining NEAT with backprop (or another non-genetic training algorithm)\r\nX-Yahoo-Group-Post: member; u=203637135; y=r_rV8uiZjdAPylGTwt92K0W-ydLOaInicJaqL8JDiDgQCmc\r\nX-Yahoo-Profile: er042685\r\n\r\nHey all,\n\nLong-time reader, first time writer here.\n\nThe recent discussion on using NEAT with backprop inspired me to ask\nif anyone has done or knows of any research on a topic similar to the\nfollowing:\n\nThe idea is to use NEAT for developing the topology of a given\nnetwork, and at a later time complete the training with another\nnon-genetic training algorithm such as gradient descent, etc. The\n&#39;NEAT&#39; phase of the network &#39;training&#39; should try to develop a network\ntopology that is _conducive_ to being trained (ideally, quickly) using\na non-genetic algorithm on a limited subset of the domain of a given\nclass of problem. In other words, has anyone ever tried integrating a\nbackprop algorithm into the fitness function of a given NEAT\nimplementation, with the intention of not merging the weight changes\nmade by said backprop algorithm back into the population, but rather\nonly factoring information relating to how well/quickly that network\ncould be trained using backprop (against data set(s) from the problem\nclass, on the limited domain desired) into the fitness score.\n\nIf you need a reason for why this may be useful, consider the\nfollowing scenario. An embedded system with limited memory (making GA\nsomewhat prohibitive), in which the network   (including topology) is\nimplemented in hardware (making GA practically _impossible_), but can\nuse fast dedicated multipliers for algorithms such as gradient\ndescent. One desires to fine-tune this network to data sets\nencountered by the system in the field, hopefully by using a backprop\nalgorithm that can be implemented (mostly) in hardware. It would be\ndesirable to run a GA in the lab prior to deployment to determine the\nbest topology to implement in hardware (if possible). The constraints\nof the data/problem domain are known prior to deployment in the field.\n\nIf anyone knows of any research done on this or similar, please let me\nknow. I&#39;ve been doing some research and have found some information\nabout applying gradient descent to NEAT-generated topologies, but\nnever anything about developing a topology that was actually\n_conducive_ to being trained efficiently with a backprop algorithm.\n\nEric\n\n"}}