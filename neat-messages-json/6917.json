{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":311320802,"authorName":"Peter Chervenski","from":"Peter Chervenski &lt;spookey@...&gt;","profile":"petar_chervenski","replyTo":"LIST","senderId":"beb4FApRFEmHK95OIq6HL1F8kK-986ys_MXhv0Oh0En-yJyrC3fTeqpWqXw1SMwLmb_7tse1DG6vYM3ipEgqVMSp2HlgA6uw","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] Welcoming the Era of Deep Neuroevolution","postDate":"1513630860","msgId":6917,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGE2ZWQyNjFhLTE0NzgtNzhiZC1kZTEyLTYzYjg2ZTc2MzE1NkBhYnYuYmc+","inReplyToHeader":"PHAxOTg3cCtmZGs2YXVAWWFob29Hcm91cHMuY29tPg==","referencesHeader":"PHAxOTg3cCtmZGs2YXVAWWFob29Hcm91cHMuY29tPg=="},"prevInTopic":6916,"nextInTopic":6918,"prevInTime":6916,"nextInTime":6918,"topicId":6916,"numMessagesInTopic":6,"msgSnippet":"Hi Ken, I ve done some experiments in the past month with a GA evolving the weights of deep convolutional NNs, but with NEAT involved. The topology of the CNN","rawEmail":"Return-Path: &lt;spookey@...&gt;\r\nX-Sender: spookey@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 9397 invoked by uid 102); 18 Dec 2017 21:01:06 -0000\r\nX-Received: from unknown (HELO mtaq1.grp.bf1.yahoo.com) (10.193.84.32)\n  by m7.grp.bf1.yahoo.com with SMTP; 18 Dec 2017 21:01:06 -0000\r\nX-Received: (qmail 9387 invoked from network); 18 Dec 2017 21:01:06 -0000\r\nX-Received: from unknown (HELO mta1006.groups.mail.bf1.yahoo.com) (98.139.245.165)\n  by mtaq1.grp.bf1.yahoo.com with SMTP; 18 Dec 2017 21:01:06 -0000\r\nX-Original-Return-Path: &lt;spookey@...&gt;\r\nX-Received-SPF: pass (domain of abv.bg designates 194.153.145.31 as permitted sender)\r\nX-YMailISG: LDbmeZYWLDsha3DMatJgG5_ZUV0urObGNZCjAD9RGDBd4O..\n c..XnJLuc0j.4xesJnSTgWPxnq1MzCkejFEiLsBDFhlhS2YrAiLbbsV_iQPH\n hPsaFIyU5rFs6ZEUX2R_8kwjVKiBpjM_IXE8imsV2emcQfZsY04q.S8xY_Pl\n 6nUMAwFxVLR_PliZxzGdQdiOdG4PlQne.621AgeFN6uPDZnrN00OEbA3i3E9\n ccrmPBns2EOqaMt2OcsnNXRyV2inMTq8X4DbaLQVVdYUsvwaELJg7W2i971h\n u6lG18X.5wdszPkeLN3UciOAmXUyP5YCPALX4G9BJi5JL8UpD3qoCsqclcp7\n d04.LXXYUjM88EL.2awYy2wgN84s8cyl6GQRuvgVYeGiN89KCiqhXgvR_S90\n FEiQzhwMJGQQkvLqcSszJhL2UKhoRrZDs3bnksU8b6lEndDDqt1DQestePFH\n 8sf2fIo3kjtHEtaKsKbSrZjxwroeY3o_e1acq8dNQgXz1RvOSZX63Sl6Ffzq\n RC3mz8NBF0FmAxWnbLnJT6NVthl0AIzQnrEklqsylzk7O3z4hpOAbK0wisIG\n Nf0_1dvDyYZ.wdiyPGrUYUQuuK5oChOTeWEjG5DF5ViXCrtjphdEYt6EFWzk\n l5Binro9DEFd7IahPbmyYQo5vg7jlL8mO166zxtwTHfEaf.PhJPOQTQDiDeC\n NjL_ToISlcsdH8tvXsw_4HzMliNcQLjpTpV_HW5OgiIbttn6GGUtfNKA.vUJ\n VGHw4i1fZ.dcnbh7FismTolcFBHBVY2_kXG1Yc9VKyNv6T1CrEtMB.LpssPi\n XtOURc.oOoS9SqZaD7F2pUz4RbHt9vzS8YWKUb6eh0QgplBFkvwh381KnI0j\n 1TvmpCJwcp6iDqq2.TWxUaa.P_LYWtCFOfbjf10q5IhgoUQ4AHMy232_driU\n tZ4H_W9nYJptjumZmhNW_ZU2muxnrH6b0QB5xMKmGL3iesSwtPdXcB.HK1ko\n IBNVohOH53jeFvLJ..GtC5NalCGRRGaMpHO74Cetx7qW12E.Fqi03CgFuxkC\n Ul4ZGKFn0LhxnH5nQbled_JGDaZCTe4T1pK3ti.vsqQPYMnYWec5in4r0chN\n 2BmaTuPSXn7SGlxF7ifGA5Edq7RBdNaErUHfrNRDrp4_amKNt6vkszaxn_n1\n wGO6x9S2zVU23mOrFQAhSgvs6MwLKu8PpJcRpNeOBHTXwpkrY3YyQbGvj2NK\n uM_fPrV7SDHkNTsvNyfEbzBIPqIXcGcV9OqD76BW.Sfqkh_QYaoO_HNV.rzH\n KKcifw9CB__kTgTE4YZRaTI9bDkWyKLJnDhoegSWLV4JxHS9d7F5qQsZGpeF\n lbqirVYvmP68MDwPji0zp8Sjbnn0F41Od3WdLu.VuHp_isHqUNLe5xe6k9Cn\r\nAuthentication-Results: mta1006.groups.mail.bf1.yahoo.com  from=abv.bg; domainkeys=neutral (no sig);  from=abv.bg; dkim=neutral (no sig)\r\nX-Received: from 127.0.0.1  (EHLO pop32.abv.bg) (194.153.145.31)\n  by mta1006.groups.mail.bf1.yahoo.com with SMTPS; Mon, 18 Dec 2017 21:01:06 +0000\r\nX-Received: from smtp.abv.bg (localhost [127.0.0.1])\n\tby pop32.abv.bg (Postfix) with ESMTP id 9C3611FC1B\n\tfor &lt;neat@yahoogroups.com&gt;; Mon, 18 Dec 2017 23:01:02 +0200 (EET)\r\nX-HELO: [192.168.1.8]\r\nAuthentication-Results: smtp.abv.bg; auth=pass (plain) smtp.auth=spookey@...\r\nX-Received: from 89-25-63-70.ip.btc-net.bg (HELO [192.168.1.8]) (89.25.63.70)\n by smtp.abv.bg (qpsmtpd/0.96) with ESMTPSA (ECDHE-RSA-AES256-GCM-SHA384 encrypted); Mon, 18 Dec 2017 23:01:02 +0200\r\nTo: neat@yahoogroups.com\r\nReferences: &lt;p1987p+fdk6au@...&gt;\r\nMessage-ID: &lt;a6ed261a-1478-78bd-de12-63b86e763156@...&gt;\r\nDate: Mon, 18 Dec 2017 23:01:00 +0200\r\nUser-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:52.0) Gecko/20100101\n Thunderbird/52.5.0\r\nMIME-Version: 1.0\r\nIn-Reply-To: &lt;p1987p+fdk6au@...&gt;\r\nContent-Type: multipart/alternative;\n boundary=&quot;------------0D9AB3958AB325C71FB5380E&quot;\r\nContent-Language: en-US\r\nSubject: Re: [neat] Welcoming the Era of Deep Neuroevolution\r\nX-Yahoo-Group-Post: member; u=311320802; y=JLtrLXdLixtTkqUzNio6aCEP6d9LVIlkkyFn9SzWGIDZrPQkw-VI0wCQVg\r\nX-Yahoo-Profile: petar_chervenski\r\nFrom: Peter Chervenski &lt;spookey@...&gt;\r\n\r\n\r\n--------------0D9AB3958AB325C71FB5380E\r\nContent-Type: text/plain; charset=utf-8; format=flowed\r\nContent-Transfer-Encoding: 8bit\r\n\r\nHi Ken,\n\nI&#39;ve done some experiments in the past month with a GA evolving the \nweights of deep convolutional NNs, but with NEAT involved. The topology \nof the CNN layers is fixed, but it complexifies between them and the \noutputs. It works great in Doom. But this isn&#39;t like evolving the whole \nthing, right? It might be possible to implement complexifying CNNs with \nthe existing functionality of MultiNEAT, which already allows arbitrary \nobjects like NumPy arrays (for the CNN weights) to be contained within \ngenes. I think neurons/links can be generalized to layers/weight \nmatrices, but I&#39;m not sure yet how crossover and compatibility distance \nwill work. The problem is that the algorithm will have to compare/cross \nmatrices of different shapes. If we ignore that issue, I don&#39;t think any \nchanges to NEAT&#39;s genome encoding are necessary to do it, so it might be \neasy.\n\nPeter\n\nOn 12/18/2017 10:25 PM, kstanley@... [neat] wrote:\n&gt;\n&gt; You may have been wondering what we&#39;ve been up to at Uber regarding \n&gt; neuroevolution.  In fact, we have a lot of compute power at Uber and \n&gt; we put it to the test to see what it can do for neuroevolution.  The \n&gt; outcome is a whole bunch of interesting and surprising results, mostly \n&gt; on large and/or deep networks, including with millions of connections \n&gt; and over 100 layers.  It is clear from these investigations that \n&gt; neuroevolution is a deep learning method:\n&gt;\n&gt;\n&gt; https://eng.uber.com/deep-neuroevolution/\n&gt;\n&gt;\n&gt; Because this is the NEAT Users Group, it&#39;s worth adding a bit about \n&gt; NEAT.  These results were all obtained either with simple GAs or ES.  \n&gt; So they don&#39;t yet probe the opportunities of NEAT-like evolution.  But \n&gt; they show something surprising - that actually it&#39;s now possible with \n&gt; a lot of computation to evolve in very high dimensions.  That insight \n&gt; changes some of the intuitions we had about the need for \n&gt; complexification. I believe what is going on is that, as has been \n&gt; discovered in deep learning in general, very high-dimensional space \n&gt; seems to have fewer local optima (instead with a lot of saddle \n&gt; points), and as you move from low to very high-d space, you seem to \n&gt; cross some kind of phase change where, if you have enough computation, \n&gt; search in some ways becomes easier.  This realization needs to be \n&gt; digested because it has a lot of implications.\n&gt;\n&gt;\n&gt; But I still think NEAT-like processes can play an interesting role \n&gt; here.  Almost certainly architecture matters too, and indeed we&#39;ve \n&gt; seen a lot of papers in 2017 on evolving the architecture of NNs (a \n&gt; few inspired by NEAT), but so far mainly for SGD (stochastic gradient \n&gt; descent) to operate on the evolved architecture.  Our new research \n&gt; hints that maybe you could once again evolve it all - architecture and \n&gt; weights (and other stuff too) - and it might actually work well.  So \n&gt; there are still opportunities for historical markings, speciation, \n&gt; etc., to play a role still at this higher level.\n&gt;\n&gt;\n&gt; That said, there are obviously going to be a new issues to address in \n&gt; these larger spaces that will lead to changes in the conventional \n&gt; application of NEAT-like processes.  And of course there are new \n&gt; opportunities opened up in other areas of neuroevolution as well.  So \n&gt; there&#39;s a lot of stuff to revisit.\n&gt;\n&gt;\n&gt; Best,\n&gt;\n&gt;\n&gt; ken\n&gt;\n&gt;\n&gt; \n\n\n\r\n--------------0D9AB3958AB325C71FB5380E\r\nContent-Type: text/html; charset=utf-8\r\nContent-Transfer-Encoding: 8bit\r\n\r\n&lt;html&gt;\n  &lt;head&gt;\n    &lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=utf-8&quot;&gt;\n  &lt;/head&gt;\n  &lt;body text=&quot;#000000&quot; bgcolor=&quot;#FFFFFF&quot;&gt;\n    &lt;div class=&quot;moz-cite-prefix&quot;&gt;Hi Ken, &lt;br&gt;\n      &lt;br&gt;\n      I&#39;ve done some experiments in the past month with a GA evolving\n      the weights of deep convolutional NNs, but with NEAT involved. The\n      topology of the CNN layers is fixed, but it complexifies between\n      them and the outputs. It works great in Doom. But this isn&#39;t like\n      evolving the whole thing, right? It might be possible to implement\n      complexifying CNNs with the existing functionality of MultiNEAT,\n      which already allows arbitrary objects like NumPy arrays (for the\n      CNN weights) to be contained within genes. I think neurons/links\n      can be generalized to layers/weight matrices, but I&#39;m not sure yet\n      how crossover and compatibility distance will work. The problem is\n      that the algorithm will have to compare/cross matrices of\n      different shapes. If we ignore that issue, I don&#39;t think any\n      changes to NEAT&#39;s genome encoding are necessary to do it, so it\n      might be easy. &lt;br&gt;\n      &lt;br&gt;\n      Peter&lt;br&gt;\n      &lt;br&gt;\n      On 12/18/2017 10:25 PM, &lt;a class=&quot;moz-txt-link-abbreviated&quot; href=&quot;mailto:kstanley@...&quot;&gt;kstanley@...&lt;/a&gt; [neat] wrote:&lt;br&gt;\n    &lt;/div&gt;\n    &lt;blockquote type=&quot;cite&quot; cite=&quot;mid:p1987p+fdk6au@...&quot;&gt; &lt;span\n        style=&quot;display:none&quot;&gt; &lt;/span&gt;\n      \n          &lt;div id=&quot;ygrp-text&quot;&gt;\n            &lt;p&gt;&lt;span&gt;You may have been wondering what we&#39;ve been up to\n                at Uber regarding neuroevolution.  In fact, we have a\n                lot of compute power at Uber and we put it to the test\n                to see what it can do for neuroevolution.  The outcome\n                is a whole bunch of interesting and surprising results,\n                mostly on large and/or deep networks, including with\n                millions of connections and over 100 layers.  It is\n                clear from these investigations that neuroevolution is a\n                deep learning method:&lt;br&gt;\n              &lt;/span&gt;&lt;/p&gt;\n            &lt;p&gt;&lt;span&gt;&lt;br&gt;\n              &lt;/span&gt;&lt;/p&gt;\n            &lt;p&gt;&lt;span&gt;&lt;a rel=&quot;nofollow&quot; target=&quot;_blank&quot;\n                  href=&quot;https://eng.uber.com/deep-neuroevolution/&quot;\n                  moz-do-not-send=&quot;true&quot;&gt;https://eng.uber.com/deep-neuroevolution/&lt;/a&gt;&lt;br&gt;\n              &lt;/span&gt;&lt;/p&gt;\n            &lt;p&gt;&lt;span&gt;&lt;br&gt;\n              &lt;/span&gt;&lt;/p&gt;\n            &lt;p&gt;&lt;span&gt;Because this is the NEAT Users Group, it&#39;s worth\n                adding a bit about NEAT.  These results were all\n                obtained either with simple GAs or ES.  So they don&#39;t\n                yet probe the opportunities of NEAT-like evolution.  But\n                they show something surprising - that actually it&#39;s now\n                possible with a lot of computation to evolve in very\n                high dimensions.  That insight changes some of the\n                intuitions we had about the need for complexification. \n                I believe what is going on is that, as has been\n                discovered in deep learning in general, very\n                high-dimensional space seems to have fewer local optima\n                (instead with a lot of saddle points), and as you move\n                from low to very high-d space, you seem to cross some\n                kind of phase change where, if you have enough\n                computation, search in some ways becomes easier.  This\n                realization needs to be digested because it has a lot of\n                implications.  &lt;br&gt;\n              &lt;/span&gt;&lt;/p&gt;\n            &lt;p&gt;&lt;span&gt;&lt;br&gt;\n              &lt;/span&gt;&lt;/p&gt;\n            &lt;p&gt;&lt;span&gt;But I still think NEAT-like processes can play an\n                interesting role here.  Almost certainly architecture\n                matters too, and indeed we&#39;ve seen a lot of papers in\n                2017 on evolving the architecture of NNs (a few inspired\n                by NEAT), but so far mainly for SGD (stochastic gradient\n                descent) to operate on the evolved architecture.  Our\n                new research hints that maybe you could once again\n                evolve it all - architecture and weights (and other\n                stuff too) - and it might actually work well.  So there\n                are still opportunities for historical markings,\n                speciation, etc., to play a role still at this higher\n                level.&lt;/span&gt;&lt;/p&gt;\n            &lt;p&gt;&lt;span&gt;&lt;br&gt;\n              &lt;/span&gt;&lt;/p&gt;\n            &lt;p&gt;&lt;span&gt;That said, there are obviously going to be a new\n                issues to address in these larger spaces that will lead\n                to changes in the conventional application of NEAT-like\n                processes.  And of course there are new opportunities\n                opened up in other areas of neuroevolution as well.  So\n                there&#39;s a lot of stuff to revisit.&lt;/span&gt;&lt;/p&gt;\n            &lt;p&gt;&lt;span&gt;&lt;br&gt;\n              &lt;/span&gt;&lt;/p&gt;\n            &lt;p&gt;&lt;span&gt;Best,&lt;/span&gt;&lt;/p&gt;\n            &lt;p&gt;&lt;span&gt;&lt;br&gt;\n              &lt;/span&gt;&lt;/p&gt;\n            &lt;p&gt;&lt;span&gt;ken&lt;br class=&quot;yui-cursor&quot;&gt;\n              &lt;/span&gt;&lt;/p&gt;\n            &lt;p&gt;&lt;span&gt;&lt;br class=&quot;yui-cursor&quot;&gt;\n              &lt;/span&gt;&lt;/p&gt;\n          &lt;/div&gt;\n          \n      \n      &lt;!-- end group email --&gt;\n    &lt;/blockquote&gt;\n    &lt;p&gt;&lt;br&gt;\n    &lt;/p&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n\r\n--------------0D9AB3958AB325C71FB5380E--\r\n\n"}}