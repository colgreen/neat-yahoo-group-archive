{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"LX9vClX0nHxjQkVe28CuYBtoZSzkUzkH6sZ0PTY8_WMNnGtIg_HA0fyFhjzlJTOJNLd9TctU-Sze-1qdx72XB1TB7Noy4-03Z43anQXEy91y","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Backpropagation and NEAT","postDate":"1205169554","msgId":3858,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGZyM3FpaStvdDhiQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGZxdmM5NitzcW81QGVHcm91cHMuY29tPg=="},"prevInTopic":3853,"nextInTopic":3859,"prevInTime":3857,"nextInTime":3859,"topicId":3846,"numMessagesInTopic":41,"msgSnippet":"Peter, I believe that backprop can potentially improve the accuracy.  It has been shown to work effectively with neurevolution in classification tasks in the","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 35790 invoked from network); 10 Mar 2008 17:19:16 -0000\r\nX-Received: from unknown (66.218.67.96)\n  by m56.grp.scd.yahoo.com with QMQP; 10 Mar 2008 17:19:16 -0000\r\nX-Received: from unknown (HELO n39a.bullet.mail.sp1.yahoo.com) (66.163.168.133)\n  by mta17.grp.scd.yahoo.com with SMTP; 10 Mar 2008 17:19:16 -0000\r\nX-Received: from [216.252.122.217] by n39.bullet.mail.sp1.yahoo.com with NNFMP; 10 Mar 2008 17:19:16 -0000\r\nX-Received: from [66.218.69.1] by t2.bullet.sp1.yahoo.com with NNFMP; 10 Mar 2008 17:19:16 -0000\r\nX-Received: from [66.218.66.74] by t1.bullet.scd.yahoo.com with NNFMP; 10 Mar 2008 17:19:16 -0000\r\nDate: Mon, 10 Mar 2008 17:19:14 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;fr3qii+ot8b@...&gt;\r\nIn-Reply-To: &lt;fqvc96+sqo5@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: Re: Backpropagation and NEAT\r\nX-Yahoo-Group-Post: member; u=54567749; y=_jSgSxjL5VUmW3LuEESbWx9Eb2CYgJilAwYjmPPw8JxLJ_hOlirv\r\nX-Yahoo-Profile: kenstanley01\r\n\r\nPeter, I believe that backprop can potentially improve the \naccuracy.  It h=\r\nas been shown to work effectively with neurevolution \nin classification tas=\r\nks in the past.  So in principle it could \nhelp.  Of course, there is alway=\r\ns the chance that it will not \nenhance performance as well.\n\nOne issue I wo=\r\nuld also consider is that some people disagree on \nwhether the changes to w=\r\neights from backprop should be encoded back \ninto the genome or not.  If it=\r\n is actually encoded back into the \ngenome, that is &quot;Lamarckian&quot; evolution =\r\nbecause in effect what the \norganism learned over its lifetime is encoded i=\r\nnto its own \noffspring.  That is obviously not how real evolution works.\n\nH=\r\nowever, of course, it doesn&#39;t have to work like real evolution and \nsome pe=\r\nople believe that Lamarckian evolution will work better.  \nHowever, there a=\r\nre arguments that in fact it works worse because it \nhurts the diversity of=\r\n the population.  Because of the Baldwin \neffect, some would argue that evo=\r\nlution+backprop is most powerful if \nthe learned weights are not encoded ba=\r\nck into the genome. This topic \nis fairly extensive.  A lot is written abou=\r\nt the &quot;Baldwin effect.&quot;\n\nken\n\n--- In neat@yahoogroups.com, &quot;petar_chervensk=\r\ni&quot; \n&lt;petar_chervenski@...&gt; wrote:\n&gt;\n&gt; Hi Ken, \n&gt; \n&gt; I am evolving time seri=\r\nes predictors, in fact even a simplified \n&gt; version of time series predicto=\r\nrs, where the network has to answer \nis \n&gt; the future value going up or dow=\r\nn. The actual output neuron is a \n&gt; simple step function, but back-prop can=\r\n be applied if it is turned \n&gt; out to a sigmoid with a very steep slope. \n&gt;=\r\n The networks are allowed to have any topology and they are \nevaluated \n&gt; o=\r\nn the run, meaning that on each timestep, an error is being \n&gt; calculated (=\r\nbeing 0 or 1, depending on the prediction made). \n&gt; First of all, do you th=\r\nink that applying back-prop to these \nnetworks \n&gt; may bring any accuracy im=\r\nprovement? I know that it is going to eat \n&gt; the CPU resourses, so it can b=\r\ne applied at regular intervals, say \n&gt; each 50 generations, to push the net=\r\nworks&#39;s weights in the right \n&gt; direction, a kind of a hint to the search. =\r\nI am still thinking \nof &quot;is \n&gt; it worth it?&quot;.. \n&gt; \n&gt; Peter\n&gt; \n&gt; \n&gt; \n&gt; --- I=\r\nn neat@yahoogroups.com, &quot;Kenneth Stanley&quot; &lt;kstanley@&gt; wrote:\n&gt; &gt;\n&gt; &gt; A numb=\r\ner of people have programmed backprop into NEAT.  Chris \n&gt; &gt; Christenson di=\r\nd a Masters thesis on combining NEAT and backprop; \na \n&gt; &gt; paper based on t=\r\nhis work is actually in the files section of \nthis \n&gt; group:\n&gt; &gt; \n&gt; &gt; \n&gt; \nh=\r\nttp://f1.grp.yahoofs.com/v1/UP7SR8rDovimxlLlvcmGOziLUBIVncb2Tfr7sruo\nB\n&gt; 8b=\r\n\n&gt; &gt; taAfELU62JLyQ9XCxXF_Akhcmi-\n&gt; &gt; \n&gt; \nTH4gpVHIikwnzB59ArOMQfPOAzyw25/Evo=\r\nlving_Trainable_Neural_Networks_6_p\na\n&gt; ge\n&gt; &gt; s.doc\n&gt; &gt; \n&gt; &gt; Shimon Whites=\r\non implemented it as part of his NEAT+Q \nreinforcement \n&gt; &gt; learning method=\r\n:\n&gt; &gt; \n&gt; &gt; http://staff.science.uva.nl/~whiteson/pubs/whitesonaaai06.pdf\n&gt; =\r\n&gt; \n&gt; &gt; There has been a lot written on backprop in NEAT in the archives \nof=\r\n \n&gt; &gt; this group: just search for &quot;backprop&quot; from the yahoo page for \nthis =\r\n\n&gt; &gt; group and many messages will pop up.\n&gt; &gt; \n&gt; &gt; In general, if you do no=\r\nt allow recurrence then I believe there \nis \n&gt; no \n&gt; &gt; special change neede=\r\nd in the traditional backprop algorithm.  \nWith \n&gt; &gt; recurrence you would n=\r\need something like recurrent backprop like \n&gt; Derek \n&gt; &gt; suggested.  But le=\r\nt&#39;s just say you are evolving nonrecurrent \n&gt; networks- \n&gt; &gt; is there a par=\r\nticular problem you have in mind that comes up \nwith \n&gt; &gt; applying backprop=\r\n to such networks?\n&gt; &gt; \n&gt; &gt; ken\n&gt; &gt; \n&gt; &gt; --- In neat@yahoogroups.com, &quot;peta=\r\nr_chervenski&quot; \n&lt;petar_chervenski@&gt; \n&gt; &gt; wrote:\n&gt; &gt; &gt;\n&gt; &gt; &gt; Hello there. \n&gt; =\r\n&gt; &gt; \n&gt; &gt; &gt; I am looking for any back-propagation algorithm that can work \no=\r\nn \n&gt; &gt; &gt; networks with arbitrary topology such as these that NEAT \nevolves.=\r\n \n&gt; All \n&gt; &gt; &gt; libraries I found so far either assume layered networks or \n=\r\nonly \n&gt; feed-\n&gt; &gt; &gt; forward ones.. I am confused. Is there any source code =\r\nthat \nmight \n&gt; &gt; help \n&gt; &gt; &gt; me? Any back-prop implementation that can work=\r\n on NEAT \nnetworks \n&gt; such \n&gt; &gt; &gt; that it can easily be integrated. Or mayb=\r\ne some papers on the \n&gt; topic? \n&gt; &gt; &gt; I appreciate any help from the commun=\r\nity. \n&gt; &gt; &gt; \n&gt; &gt; &gt; Peter\n&gt; &gt; &gt;\n&gt; &gt;\n&gt;\n\n\n\n"}}