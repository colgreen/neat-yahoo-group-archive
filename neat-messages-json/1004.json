{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":177632070,"authorName":"Michael Gillam","from":"&quot;Michael Gillam&quot; &lt;gillam@...&gt;","replyTo":"LIST","senderId":"H9qO7sXh5fydTO7C19hrCpXiEXn_5er73otyNlW61yMdtVMQNqUbHb_NJVxWY_x1oKg6xWBRRPmnO--VV-2o5q2Uc-eJjpjuPgZ3vu1gFA","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] Re: Bloat","postDate":"1086506628","msgId":1004,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGdpbGxhbS0wUlYrZUFha0hybW9PSjR6SEMwdXA2Skd2eC9nZVJyQG1haWxibG9ja3MuY29tPg==","referencesHeader":"PGM5dG41dSt1djByQGVHcm91cHMuY29tPiA8MDU4ZjAxYzQ0Yjk3JDVkN2IxOWIwJDY1MDFhOGMwQEpvdXJuZXk+"},"prevInTopic":1003,"nextInTopic":1005,"prevInTime":1003,"nextInTime":1005,"topicId":904,"numMessagesInTopic":68,"msgSnippet":"Excellent post Ken - and I agree completely (from a biologic and intuitively computational perspective.) Your statements are what attracted my interest","rawEmail":"Return-Path: &lt;gillam-0RV+eAaoHrmoYGZRK5bnQZCysX4pnaJ@...&gt;\r\nX-Sender: gillam-0RV+eAaoHrmoYGZRK5bnQZCysX4pnaJ@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 55273 invoked from network); 6 Jun 2004 07:25:54 -0000\r\nReceived: from unknown (66.218.66.167)\n  by m16.grp.scd.yahoo.com with QMQP; 6 Jun 2004 07:25:54 -0000\r\nReceived: from unknown (HELO smtp3.mailblocks.com) (140.174.9.93)\n  by mta6.grp.scd.yahoo.com with SMTP; 6 Jun 2004 07:25:54 -0000\r\nReceived: (qmail 8018 invoked from network); 6 Jun 2004 07:24:53 -0000\r\nReceived: from 10.10.0.50 (HELO mailblocks.com) (10.10.0.50)\n  by 10.10.0.93 with SMTP; 6 Jun 2004 07:24:53 -0000\r\nMIME-Version: 1.0\r\nTo: &lt;neat@yahoogroups.com&gt;\r\nContent-Type: text/plain;\tcharset=&quot;iso-8859-1&quot;\r\nContent-Transfer-Encoding: 7bit\r\nReferences: &lt;c9tn5u+uv0r@...&gt; &lt;058f01c44b97$5d7b19b0$6501a8c0@Journey&gt;\r\nX-Mailer: Microsoft Outlook Express 6.00.2800.1158\r\nX-MSMail-Priority: Normal\r\nX-MB-Message-Source: ClientSMTP\r\nReceived: from 69.138.187.11 by app1.mailblocks.com (10.10.0.50) with SMTP (authenticated); Sun, 06 Jun 2004 00:24:53 -0700\r\nX-MB-Message-Type: User\r\nMessage-Id: &lt;gillam-0RV+eAakHrmoOJ4zHC0up6JGvx/geRr@...&gt;\r\nX-MimeOLE: Produced By Microsoft MimeOLE V6.00.2800.1165\r\nDate: Sun, 6 Jun 2004 03:23:48 -0400\r\nX-Priority: 3\r\nX-eGroups-Remote-IP: 140.174.9.93\r\nFrom: &quot;Michael Gillam&quot; &lt;gillam@...&gt;\r\nSubject: Re: [neat] Re: Bloat\r\nX-Yahoo-Group-Post: member; u=177632070\r\n\r\n\nExcellent post Ken - and I agree completely (from a biologic and intuitively\ncomputational perspective.)\n\nYour statements are what attracted my interest originally to NEAT.  I built\na significantly simplified but ideological similar system to NEAT back in\nmedical school almost 10 1/2 years ago(!)  (An old computer bag with my\nApple Powerbook 170 recently yielded the disks on which it was written!)\n\nIt seemed clear at that time that neural networks were mired in rigid\npre-defined structures that did not adequately imitate the neural\ncomplexification we see in the fossil and evolutionary record..  I\naffectionately called the project Prevolve to represent this idea of taking\na step-backwards (away fro pre-defined rigid structures) to move forwards.\nUnfortunately, Prevolve took a back seat to other strenuous demands for\ntime... : ( ...but I am glad to see that an amazingly active group of\nprogrammers have independently created their own torch and are leading the\nway into this Great Frontier.  Ironically, a striking example of convergent\ncognitive evolution, if I ever saw one!\n\nThis leads me to my second point.\n\nThe next great leap forward just might be taking another big step backwards\nfrom what we rigidly define a priori.    Many systems today *rigidly* define\nthe translation steps from genome to neural phenome.  How do we know that\nour translation of the genetic code of our organisms into neural structure\nis *the* optimal translation algorithm?  Perhaps other translation methods\nthat use structures like homeotic boxes, replicative units, optimally\nbalanced &#39;exxons&#39; or others might more optimally step through topologies.\nEarly work in Artificial Life strongly suggests that life was created and\ndestroyed millions perhaps trillions of times.  Early life competed in a\npalette of likely vastly different translation methods of their genotypes to\nneural phenotypes.  Ultimately, the most rapid system for topological and\nphenotypical evolution likely triumphed and is the system we see today.\n\nCall this process Genotypic Evolutionary Translation (GET).  Though many\nmight dismiss GET as unnecessary, strong evidence points to its need.\nPerhaps most strongly is the fact that the only evolutionary system we know\nof that has successfully evolved sentience, had GET in its toolbox.  : )\n\nTo me, creating GET is one of the pre-eminent intellectual challenges of\nneural-evolution today.  Kudos to the individual who solves this conundrum\nand is able to still obtain the evolutionary simulation speeds we see today.\n\n-Mike\n------------------------------------------------------------\nMichael Gillam, MD\nDirector, Medical Media Lab (http://imedi.org)\nResearch Director, National Center for Emergency Medicine Informatics\n(http://ncemi.org)\nWashington D.C.\n\nInformatics Director, Division of Emergency Medicine\nEvanston Northwestern Healthcare\n\nChair, Section on Web Education\nSociety of Academic Emergency Medicine\n\n\n\n\n\n\n----- Original Message ----- \nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\nTo: &lt;neat@yahoogroups.com&gt;\nSent: Saturday, June 05, 2004 8:06 PM\nSubject: [neat] Re: Bloat\n\n\n&gt; Jim, I hope you will allow me a rather long and detailed response to\n&gt; your point.  I feel this is the right time for me to respond\n&gt; broadly, since you have touched on the central theme behind much\n&gt; discussion on this group, and ultimately behind my own motivations\n&gt; for introducing NEAT.  Therefore, forgive me for a long-winded\n&gt; response, but one I would to get on the record.\n&gt;\n&gt; I doubt that the importance of topology can be overstated.  That\n&gt; said, I want to concede up front that there is no question that most\n&gt; of the key steps in exploration are through weight mutation, and\n&gt; that weight mutation will get you far.  In fact, there are very\n&gt; sophisticated methods for altering the weight mutation distribution\n&gt; to point it in more promising directions, and these methods can be\n&gt; quite powerful.\n&gt;\n&gt; Nevertheless, weight mutation is no more than exploring a fixed\n&gt; space, and exploring a fixed space is well understood and tried and\n&gt; tested.  In fact, it is proven that there  is only so much a black\n&gt; box method can do to explore a space.  No method can promise always\n&gt; to escape local optima, and no method ever will make such a promise\n&gt; (so says the No Free Lunch Theorem).\n&gt;\n&gt; There are fundamental questions at the core of AI that fixed-space\n&gt; exploration can never address.  Most perplexing and fundamental is\n&gt; the question of what space should we be exploring in the first\n&gt; place?  Fogel tried 3 topologies (and probably more, off the\n&gt; ecord).  But where did those topologies come from?  What was the\n&gt; basis of the decisions to use them?  Isn&#39;t our mission, as\n&gt; researchers in AI, to make *that* decision automatic?  After all,\n&gt; *that* decision- the decision of what topology to search, i.e. what\n&gt; space to search in- is really the only hard decision, the one that\n&gt; requires &quot;intelligence&quot;.  It is a relatively trivial matter, once\n&gt; you know what to search, just to go searching.  The fact that weight\n&gt; mutation alone (once the correct topology has been identified) is\n&gt; sufficient to solve checkers says more about checkers and human\n&gt; intelligence (intelligence for choosing the right space to search)\n&gt; than about the prowess of weight mutation.\n&gt;\n&gt; Yet this is not only a philosophical argument about what AI should\n&gt; be able to do automatically.  It is also a critical practical\n&gt; matter.  Contrary to your reasoning, the real danger is not in\n&gt; adding a single dimension to a search space, but in beginning search\n&gt; in a bad space in the first place.  If you are concerned that\n&gt; addition of a single dimension has some exponential expense (which I\n&gt; believe is not correct anyway), what cost then must there be in\n&gt; searching in a topology with dozens or even hundreds of unnecessary\n&gt; dimensions?  The effect on search could be catastrophic.\n&gt;\n&gt; And yet for most difficult problems we have not the slightest idea\n&gt; what the right space is to search, other than that it is large.  How\n&gt; many dimensions are in the brain of a robotic maid?  Surely at least\n&gt; thousands; maybe millions.  Should we begin search then in a network\n&gt; of a million connections?  Weight space exploration offers no\n&gt; comfort: The search is intractable in million dimensional space,\n&gt; even if the solution is somewhere within.\n&gt;\n&gt; Yet even as there is danger from above in the form of too-high\n&gt; dimensional space, there is danger from below in spaces of too-few\n&gt; dimensions, where a solution may not even exist.  What if Fogel had\n&gt; chosen to search in networks with 2 fewer neurons?  5 fewer? At some\n&gt; point, the good player just doesn&#39;t exist in that space anymore.\n&gt; But how could we know this in advance?  There is no analysis that\n&gt; can tell us a priori how many dimensions we need.  And if we try to\n&gt; go lean and get just the right amount, we might miss the boat\n&gt; entirely, even by a single connection, and end up searching forever\n&gt; in futility in a space without a solution.\n&gt;\n&gt; Worse, even if we knew *exactly* the minimal number of connections\n&gt; necessary to solve a problem *and* the perfect topology, even then,\n&gt; if the space is too large, weight mutation alone is likely to fail.\n&gt; The problem is, where in a large space do you *begin* to search? And\n&gt; that problem is impossible to address since by definition you don&#39;t\n&gt; know anything about the space before you begin searching!\n&gt; Therefore, in a high-dimensional space, you are highly likely to\n&gt; begin in an unpromising part of the space; it&#39;s simply too large.\n&gt;\n&gt; Therefore, to begin minimally and complexify into the the proper\n&gt; space is addressing a fundamental issue and I believe is ultimately\n&gt; unavoidable as a critical component of any black box search for\n&gt; complex behaviors.  Rather than adding expense as you imply, it is\n&gt; reducing expense by spending most of search in lower-dimensional\n&gt; space than the final solution.  A complexifying method only may be\n&gt; searching in the space of the final solution for 10% of the run.\n&gt; Fixed-topology search spends 100% of the run in the high dimensional\n&gt; space of the final solution, which, according to your formulation\n&gt; should incur an incomprehensibly vast exponential penalty.\n&gt;\n&gt; I think ultimately what you are misunderstanding is that NEAT is not\n&gt; an attempt to search in high-dimensional space.  It is a method for\n&gt; spending most of your search in *lower-dimensional space* than the\n&gt; final solution, and complexifying up to the complexity of the final\n&gt; solution.  The goal is to be able to find solutions that *exist* in\n&gt; high-dimensional space.  That&#39;s not the same as a goal of searching\n&gt; directly in high-dimensional space no matter the problem.  The\n&gt; latter goal is the antithesis of what NEAT is about.  NEAT is\n&gt; designed to avoid searching in unnecessarily high-dimensional space.\n&gt;\n&gt; Thus, I feel strongly that the idea of searching through topologies\n&gt; must be taken seriously, and should not be viewed as merely\n&gt; a &quot;fun&quot;, &quot;sexy,&quot; or &quot;somwhat spatially interesting&quot;  recreation.  It\n&gt; is not mere intellectual exercise.  Prior topology-evolving systems\n&gt; before NEAT were perhaps better targets for your criticism, since\n&gt; they were essentially aimed at flipping through random topologies\n&gt; unsystematically for its own sake.  However, NEAT is designed to to\n&gt; use topology as a way of minimizing dimensionality in search, and\n&gt; ultimately to automatically address that fundamental question of\n&gt; what space to be searching in, a completely different endeavor.\n&gt;\n&gt; (\n&gt;\n&gt; A couple side notes:\n&gt;\n&gt; I agree that structural mutation needs to be relatively rare.  In\n&gt; NEAT, it is generally 5% or lower.  Years of experimentation with\n&gt; NEAT have gone into testing different rates of structure-adding.\n&gt;\n&gt; Finally, I believe your mathematical formulation is incorrect.\n&gt; Adding a dimensions to an already-partially-optimized structure\n&gt; certainly does not incur exponential expense in the search process.\n&gt; In fact, the effect can be quite the opposite, adding new routes off\n&gt; the top of a local optimum.\n&gt;\n&gt; Not to be picky, and this isn&#39;t really important, but here&#39;s what\n&gt; doesn&#39;t make sense to me about your formal argument:\n&gt;\n&gt; -&quot;What is occurring to me is that just doing weight mutation is a\n&gt; search at a rate X in a huge space.&quot;  How do you define &quot;search at\n&gt; rate X?&quot;  This does not seem to mean anything formally speaking.\n&gt; What are the units of search rate?  How is it derived?\n&gt;\n&gt; -&quot;And it seems to me adding topological variation is not just a\n&gt; multiplier, but an exponent increasing X.&quot;  If X is a rate (as you\n&gt; defined it), then increasing X means the rate becomes faster.  So I\n&gt; assume X is not a rate.  But then what is it?\n&gt;\n&gt; -&quot;topological complexity curve for having a more fit player is\n&gt; exponetial&quot;  What is a topological complexity curve?  Is it based\n&gt; somehow on rate X?  Complexity is usually defined as the size of the\n&gt; space or number of connections in a network.  Under that usual\n&gt; definition, complexity goes up linearly with the addition of new\n&gt;   structure, not exponentially.\n&gt;\n&gt; -&quot;X^T, where T = Y^Z and Z is the complexity curve&quot;  I still am not\n&gt; sure what X really means formally, but you haven&#39;t given a\n&gt; definition for T or Y or Z either.  What are these variables?\n&gt;\n&gt; Ultimately I think you are arguing from intuition rather than\n&gt; formally, and intuitions can be misleading.\n&gt; )\n&gt;\n&gt;\n&gt; Sorry to all for the long-windedness of this response!  I hope it is\n&gt; still useful!\n&gt;\n&gt; --- In neat@yahoogroups.com, &quot;Jim O&#39;Flaherty, Jr.&quot;\n&gt; &lt;jim_oflaherty_jr@y...&gt; wrote:\n&gt; &gt; John, Colin, Ken and Derek,\n&gt; &gt;\n&gt; &gt; I am wondering if there is not a wee bit too much focus on\n&gt; topological vairation and insignificant focus on just weight\n&gt; mutation.  I get that NEAT is unique in the fact that it has a very\n&gt; effective search mechanism for topoligical variation, with the\n&gt; ability to stress both additive and subtractive aspects of change.\n&gt; And I get that it is fun to focus on the topological variation as it\n&gt; is somewhat spatially interesting.\n&gt; &gt;\n&gt; &gt; However, I am realizing that just doing effective weight\n&gt; mutations, sans topological changes, can end up producing solutions\n&gt; that are very &quot;fit&quot;.  I have been focused on reproducing the\n&gt; experiments Fogel and Kumar produced which are covered in their book\n&gt; Blondie24.  In that, they had only 3 topologies they experimented\n&gt; with.  All of the GA searching was just done with weight mutation\n&gt; within a step size that was both a GA parameter and nudged towards\n&gt; smaller values.  In the Fogel experiments, they arrived at an expert\n&gt; player (well, at least against human opponents) using just co-\n&gt; evolution and a static topology.  And in my own replication of the\n&gt; experiments, something as simple as turn on/off biases had a\n&gt; substantial effect in how long it took to arrive at a specimen of\n&gt; similar fitness.\n&gt; &gt;\n&gt; &gt; Now, I realize that mutating weights only is not near as sexy\n&gt; sounding as both weight mutation and topological variation.\n&gt; However, what I am wondering and hope to be able to evaluate with\n&gt; experimentation is whether the topological mutation rates ought not\n&gt; be very small with the focus more on trying out many weight\n&gt; mutations within a given topology?  What is occurring to me is that\n&gt; just doing weight mutation is a search at a rate X in a huge space.\n&gt; And it seems to me adding topological variation is not just a\n&gt; multiplier, but an exponent increasing X.  Perhaps the space is\n&gt; being made too large too quickly, before a search just in the weight\n&gt; mutation space might demonstrate a uniquely fit individual.\n&gt; &gt;\n&gt; &gt; My understanding is that the &quot;search in higher dimensional space&quot;\n&gt; might produce more robust and fit players.  At what computational\n&gt; cost?  If the topological complexity curve for having a &quot;more fit\n&gt; player&quot; is exponetial, then doesn&#39;t that mean that there is a\n&gt; threshold of diminishing returns somewhere?  Granted, it may not\n&gt; be.  However, when it is exponential (and my intuition says it is\n&gt; more of the time), we now have an exponent on an exponent of\n&gt; complexification which massively enlarges the search space, X^T,\n&gt; where T = Y^Z and Z is the complexity curve.  If this is true, then\n&gt; we are massive amounts of processing power away from achieving\n&gt; result in anything but the simplest of domains, like XOR and Tic-Tac-\n&gt; Toe.\n&gt; &gt;\n&gt; &gt; Am I missing something here?  Perhaps I need to do more direct\n&gt; experimentation and examine the results before jumping to this kind\n&gt; of conclusion.  I just get the sense that simple weight mutation\n&gt; achieved quite a bit in Checkers, a domain more complex than Tic-Tac-\n&gt; Toe.  It would be interesting to see how Checkers might do with NEAT\n&gt; and see what kinds of mutation rates might be more/less effective\n&gt; and why.\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; Jim\n&gt; &gt;\n&gt; &gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt; Yahoo! Groups Links\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n\n\n\n"}}