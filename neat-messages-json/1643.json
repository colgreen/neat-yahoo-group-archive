{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"vZktxfSEmOQDzlaH2fgwrDLHmJGP9w5NuZlL-mUTMgQBQYAYH3WC-sTXSZ4kFRKtuyoDyywdcpvmIp9BCdhpSC91E2gMMGojs6AkuouXT4_M","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: Connection weight mutation schemes","postDate":"1097674572","msgId":1643,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGNramIwYytjMDJnQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDYuMS4yLjAuMC4yMDA0MTAxMzExNDMyMS4wMjRlZWZhMEBwb3AubWFpbC55YWhvby5jby51az4="},"prevInTopic":1642,"nextInTopic":1644,"prevInTime":1642,"nextInTime":1644,"topicId":1618,"numMessagesInTopic":27,"msgSnippet":"I don t think we should necessarily lump any algorithm that computes a gradient in with backprop.  To me, what really sets backprop apart from evolutionary","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 95597 invoked from network); 13 Oct 2004 13:36:22 -0000\r\nReceived: from unknown (66.218.66.217)\n  by m11.grp.scd.yahoo.com with QMQP; 13 Oct 2004 13:36:22 -0000\r\nReceived: from unknown (HELO n12a.bulk.scd.yahoo.com) (66.94.237.20)\n  by mta2.grp.scd.yahoo.com with SMTP; 13 Oct 2004 13:36:22 -0000\r\nReceived: from [66.218.69.3] by n12.bulk.scd.yahoo.com with NNFMP; 13 Oct 2004 13:36:12 -0000\r\nReceived: from [66.218.67.153] by mailer3.bulk.scd.yahoo.com with NNFMP; 13 Oct 2004 13:36:12 -0000\r\nDate: Wed, 13 Oct 2004 13:36:12 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;ckjb0c+c02g@...&gt;\r\nIn-Reply-To: &lt;6.1.2.0.0.20041013114321.024eefa0@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Length: 3623\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Remote-IP: 66.94.237.20\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: Re: Connection weight mutation schemes\r\nX-Yahoo-Group-Post: member; u=54567749\r\nX-Yahoo-Profile: kenstanley01\r\n\r\n\nI don&#39;t think we should necessarily lump any algorithm that computes \na gradient in with backprop.  To me, what really sets backprop apart \nfrom evolutionary computation is that backprop requires training \ntargets, making it supervised.  If you can compute a gradient in a \nsparse feedback algorithm such as an ES or GA, you can do a lot \nmore.  In other words, in a reinforcement learning problem, we can \nknow which directions are most promising without ever knowing what \nthe outputs should look like, and that&#39;s more than backprop can \nsay.   Evolutionary methods at least implicitly climb gradients \n(among other things) anyway, so making it more explicit is not \nreally a radical change in spirit.  It doesn&#39;t stop you from still \ndoing crossover.\n\nken\n\n--- In neat@yahoogroups.com, Ian Badcoe &lt;ian_badcoe@y...&gt; wrote:\n&gt; At 22:12 12/10/2004, you wrote:\n&gt; \n&gt; &gt; &gt; Hi,\n&gt; &gt; &gt;          I don&#39;t get the connection with back-propagation -- \ncan you \n&gt; &gt; explain?\n&gt; &gt; &gt;\n&gt; &gt; &gt;          Ian\n&gt; &gt;\n&gt; &gt;Sure...let me quote from one of your posts...\n&gt; &gt;\n&gt; &gt; &gt;&gt;There actually is a third, more sophisticated method for \ngetting\n&gt; &gt; &gt;&gt;weight mutation parameters, which is to measure the success of\n&gt; &gt; &gt;&gt;mutations in various directions in the multidimensional space \nand\n&gt; &gt; &gt;&gt;actually construct a model of the most promising mutation \ntrajectory\n&gt; &gt; &gt;&gt;based on the experience over past generations (as a multi-\ndimensional\n&gt; &gt; &gt;&gt;probability distribution).  That has recently popped up in ES \nas well,\n&gt; &gt; &gt;&gt;though the math behind it is not simple at all.\n&gt; &gt;\n&gt; &gt; &gt;But that is a step away from the basis of an evolutionary \nsearch and\n&gt; &gt; &gt;towards a gradient-based method.  Gradient-based methods are\n&gt; &gt; &gt;well-established and the choice for many problems, but they \nhave certain\n&gt; &gt; &gt;weaknesses which evolutionary methods address... IMHO, that is.\n&gt; &gt;\n&gt; &gt;Basically, I was saying what you were saying...that this takes us \none\n&gt; &gt;step closer to back-propagation / gradient-based methods.\n&gt; &gt;\n&gt; &gt;And that perhaps we might learn from both the successes and \nfailures\n&gt; &gt;of that technique.  But that we would do well do do so \nconscientiously\n&gt; &gt;as opposed to re-inventing it in an effort to make improvements \nto our\n&gt; &gt;current approach.  Make sense?\n&gt; \n&gt; Ah!  Yes I get you now.  This is a very good idea.\n&gt; \n&gt; I think that only that one angle of the discussion directly \nrelates to \n&gt; back-prop however.  The more-general point, about including \nparameters for \n&gt; mutation rate in the genome has less to do with it.  Although I \nsuppose \n&gt; that when you have a mutability on every gene you kind-of have an \nunsigned \n&gt; &quot;gradient&quot; parameter.  Intuitively I feel that if the mutabilities \nare \n&gt; randomly evolving it is a more-robust search technique than if we \nwere \n&gt; trying to calculate them from a single point on the fitness \nsurface (or \n&gt; even a historical path across it).\n&gt; \n&gt; I did see one study where what they did was:\n&gt; \n&gt; 1) maintain an evolving population (fixed topology)\n&gt; 2) before evaluating the fitness of each individual, train it with \nback-prop\n&gt;          (they actually did it as a basic weight with a learned \nmodifier \n&gt; added on, but that was just to help with analysis)\n&gt; \n&gt; -what they were looking at was the Baldwin effect, which is where \na \n&gt; property is initially learned, but over a longer-time gets \ntransferred into \n&gt; the genome (because individuals who learn it faster are fitter, \nand \n&gt; eventually do not need to learn it at all...)\n&gt; \n&gt; I can find the reference if you need it...\n&gt; \n&gt;          Ian\n&gt; \n&gt; \n&gt; \n&gt; Living@Home - Open Source Evolving Organisms - \n&gt; http://livingathome.sourceforge.net/\n\n\n\n\n"}}