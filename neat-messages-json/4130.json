{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":206967455,"authorName":"Julian Togelius","from":"&quot;Julian Togelius&quot; &lt;julian@...&gt;","profile":"jtogel","replyTo":"LIST","senderId":"C4LZOWVhrztIhaT7htRxQwlDOkXaAmdISxbIVD_I4X1poQ_dAzIGRkMHrwUpSsgz9q4be0VT81GG7sIUR27wuMdee991pjVqWtQkJCNc","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [neat] Re: Can novelty search be treated as an optimization technique?","postDate":"1212820318","msgId":4130,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDIzMGU0NjNlMDgwNjA2MjMzMW43YzkwOGI4ZXM5YjFjOTE2MTI0NDNjMjBmQG1haWwuZ21haWwuY29tPg==","inReplyToHeader":"PGcyYTZxbSsyaTY1QGVHcm91cHMuY29tPg==","referencesHeader":"PDIzMGU0NjNlMDgwNTMwMDUxMHExZDJkNjZiYnQxOTdmMjE5N2FiMmU5OTg0QG1haWwuZ21haWwuY29tPgkgPGcyYTZxbSsyaTY1QGVHcm91cHMuY29tPg=="},"prevInTopic":4129,"nextInTopic":4131,"prevInTime":4129,"nextInTime":4131,"topicId":4113,"numMessagesInTopic":6,"msgSnippet":"Ken, I presented our first memetic climber paper at WCCI yesterday, and one of the people in the audience was Nils T. Siebel, coinventor of the EANT-II","rawEmail":"Return-Path: &lt;julian.togelius@...&gt;\r\nX-Sender: julian.togelius@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 32399 invoked from network); 7 Jun 2008 06:32:06 -0000\r\nX-Received: from unknown (66.218.67.97)\n  by m57.grp.scd.yahoo.com with QMQP; 7 Jun 2008 06:32:06 -0000\r\nX-Received: from unknown (HELO wa-out-1112.google.com) (209.85.146.178)\n  by mta18.grp.scd.yahoo.com with SMTP; 7 Jun 2008 06:32:06 -0000\r\nX-Received: by wa-out-1112.google.com with SMTP id j4so877093wah.19\n        for &lt;neat@yahoogroups.com&gt;; Fri, 06 Jun 2008 23:32:06 -0700 (PDT)\r\nX-Received: by 10.114.53.18 with SMTP id b18mr1089869waa.220.1212820318980;\n        Fri, 06 Jun 2008 23:31:58 -0700 (PDT)\r\nX-Received: by 10.114.132.6 with HTTP; Fri, 6 Jun 2008 23:31:58 -0700 (PDT)\r\nMessage-ID: &lt;230e463e0806062331n7c908b8es9b1c91612443c20f@...&gt;\r\nDate: Sat, 7 Jun 2008 14:31:58 +0800\r\nTo: neat@yahoogroups.com\r\nIn-Reply-To: &lt;g2a6qm+2i65@...&gt;\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Transfer-Encoding: 7bit\r\nContent-Disposition: inline\r\nReferences: &lt;230e463e0805300510q1d2d66bbt197f2197ab2e9984@...&gt;\n\t &lt;g2a6qm+2i65@...&gt;\r\nX-Google-Sender-Auth: 82e7885a181bb74e\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: &quot;Julian Togelius&quot; &lt;julian@...&gt;\r\nSubject: Re: [neat] Re: Can novelty search be treated as an optimization technique?\r\nX-Yahoo-Group-Post: member; u=206967455; y=Z7N3eQYf6qFxhdJOE1_cmEMUc07QcEYoylZxbAbILQzB\r\nX-Yahoo-Profile: jtogel\r\n\r\nKen,\n\nI presented our first memetic climber paper at WCCI yesterday, and one\nof the people in the audience was Nils T. Siebel, coinventor of the\nEANT-II algorithm. His algorithm is based on complexification and\nCMA-ES, and he claims that it works very well (better than NEAT on his\nown robot benchmark, apparently). Like in our papers (the PPSN one was\naccepted) he separates the evolution of weights and topologies. I\ndon&#39;t know exactly how he handles the varying dimensionality, but I\ngot the impression from what he said that he simply restarts the\nCMA-ES when needed. I was very surprised that I had not heard of this\nalgorithm before - have you?\n\nAs I&#39;m still in Hong Kong, I haven&#39;t had time to read his papers yet,\nbut I will definitely look into this.\n\nJulian\n\nOn 06/06/2008, Kenneth Stanley &lt;kstanley@...&gt; wrote:\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt; Julian,\n&gt;\n&gt; Thanks for the further insight on CMA-ES. I also think there may be a\n&gt; way to combine it &quot;properly&quot; with a variable-dimension encoding.\n&gt; Somehow the dimensions for which there is no information (e.g. because\n&gt; they were just added) need to be possible to incorporate into the\n&gt; existing model seamlessly on the fly, instead of starting over again.\n&gt; It seems like in principle that should be possible?\n&gt;\n&gt; Anyway, like you said, it is still not a panacea since it can be greedy.\n&gt;\n&gt; ken\n&gt;\n&gt; --- In neat@yahoogroups.com, &quot;Julian Togelius&quot; &lt;julian@...&gt; wrote:\n&gt; &gt;\n&gt; &gt; Hi Peter and Ken,\n&gt; &gt;\n&gt; &gt; We actually just recently combined CMA-ES with topology search (though\n&gt; &gt; not NEAT-based) in a paper submitted to PPSN. I&#39;ll make the paper\n&gt; &gt; available as soon as it&#39;s accepted and revised, but send me a private\n&gt; &gt; mail if you want a draft before that.\n&gt; &gt;\n&gt; &gt; Basically, what we found in this respect is that for two standard\n&gt; &gt; reinforcement learning problems, our &quot;memetic CMA-ES&quot; did much worse\n&gt; &gt; than standard CMA. This is probably because it takes time to get a\n&gt; &gt; good covariance matrix, typically many hundreds of evaluations, and\n&gt; &gt; everytime you change the topology you have to restart the CMA-ES.\n&gt; &gt; However, in a version of the problem with lots of extra inputs (and\n&gt; &gt; thus lots of deceptiveness) the memetic CMA-ES could outperform both a\n&gt; &gt; normal CMA-ES and a memetic ES.\n&gt; &gt;\n&gt; &gt; Finding a way of dealing &quot;properly&quot; with varying number of dimensions\n&gt; &gt; would be very interesting indeed. But I also agree with Ken that\n&gt; &gt; CMA-ES is not a panacea in any way, and in fact might be too greedy\n&gt; &gt; for many problems.\n&gt; &gt;\n&gt; &gt; By the way, I just dropped down in Hong Kong - any members of this\n&gt; &gt; group attending WCCI? Maybe even know of any interesting papers being\n&gt; &gt; presented?\n&gt; &gt;\n&gt; &gt; Julian\n&gt; &gt;\n&gt; &gt;\n&gt;\n&gt; &gt; On 26/05/2008, Kenneth Stanley &lt;kstanley@...&gt; wrote:\n&gt; &gt; &gt;\n&gt; &gt; &gt; Peter,\n&gt; &gt; &gt;\n&gt; &gt; &gt; I also think NEAT could work with a CMA-ES (or some kind of EDA, which\n&gt; &gt; &gt; seems to do something similar). While EANT cycles through phases of\n&gt; &gt; &gt; topology and weight optimization, I think maybe there is a more\n&gt; &gt; &gt; elegant way to just combine the two by dealing properly with variable\n&gt; &gt; &gt; numbers of dimensions in the CMA-ES.\n&gt; &gt; &gt;\n&gt; &gt; &gt; Anyway, that&#39;s not the main point you&#39;re making. Your main question\n&gt; &gt; &gt; is whether something like CMA-ES, which focuses on optimization, can\n&gt; &gt; &gt; be used in a novelty search. Of course, novelty search is\n&gt; &gt; &gt; philosophically almost the antithesis of optimization. However, in\n&gt; &gt; &gt; practice the search for novelty itself could be viewed as the\n&gt; &gt; &gt; objective, and then, in principle, an optimization method might be\n&gt; &gt; &gt; able to optimize to that end.\n&gt; &gt; &gt;\n&gt; &gt; &gt; It is true that from one generation to the next, the scores change and\n&gt; &gt; &gt; therefore you cannot rely on a constant gradient. However, isn&#39;t it\n&gt; &gt; &gt; true that CMA-ES (and EDAs) computes its covariance matrix from only\n&gt; &gt; &gt; the current generation (i.e. current population). If so, it could\n&gt; &gt; &gt; compute how genetic parameters correlate to novelty scores, and\n&gt; &gt; &gt; thereby try to optimize novelty for that particular population. In\n&gt; &gt; &gt; other words, in a fixed population, I don&#39;t see a reason it couldn&#39;t\n&gt; &gt; &gt; be applied (and then reapplied for the next population all over\n&gt; &gt; &gt; again). However, I guess you are saying that you can&#39;t just launch a\n&gt; &gt; &gt; &quot;phase&quot; of CMA-ES on its own since it would not optimize topologies.\n&gt; &gt; &gt; But I don&#39;t think that would necessarily matter. Still, even better\n&gt; &gt; &gt; would be to seamlessly combine the CMA-ES with NEAT instead of\n&gt; &gt; &gt; splitting them into phases.\n&gt; &gt; &gt;\n&gt; &gt; &gt; Whether or not CMA-ES works better than stochastic mutation over the\n&gt; &gt; &gt; long run is a different question, but it&#39;s a question even in regular\n&gt; &gt; &gt; objective-based optimization. I actually think that CMA-ES and the\n&gt; &gt; &gt; like are greedy, although you say it is not. It computes the most\n&gt; &gt; &gt; promising vector based exclusively on the current distribution. Thus\n&gt; &gt; &gt; it is in a sense overly focused on the initial population\n&gt; &gt; &gt; distribution, which may be highly misleading with respect to the\n&gt; &gt; &gt; ultimate objective. Basically, it&#39;s the problem of deception as\n&gt; &gt; &gt; usual, but I think CMA-ES is even more susceptible to it because it\n&gt; &gt; &gt; actively seeks out a model of the deceptive distribution. That is\n&gt; &gt; &gt; pretty greedy in my view.\n&gt; &gt; &gt;\n&gt; &gt; &gt; Now, if the problem is not deceptive, then that is great, because the\n&gt; &gt; &gt; model will be way better than random perturbations. So you will find\n&gt; &gt; &gt; some problems where it will be fantastic. But in highly deceptive\n&gt; &gt; &gt; problems I am not sure. Some of the results for CMA-ES reported to\n&gt; &gt; &gt; date may be misleading because they do not focus on deceptive\n&gt; &gt; &gt; problems. One way to see this fact is that they use extremely small\n&gt; &gt; &gt; populations. That simply cannot work well in a highly deceptive\n&gt; &gt; &gt; domain. In fact, as you can see in my dissertation, regular NEAT is\n&gt; &gt; &gt; also extremely fast at pole balancing problems with a tiny population.\n&gt; &gt; &gt; So that tells you more about pole balancing having a really large\n&gt; &gt; &gt; basin of attraction than about either method in general.\n&gt; &gt; &gt;\n&gt; &gt; &gt; Anyway, I think ultimately that it can be combined with NEAT and that\n&gt; &gt; &gt; would be interesting, and such a system could even work with novelty\n&gt; &gt; &gt; search. Whether it could work better is an open question.\n&gt; &gt; &gt;\n&gt; &gt; &gt; ken\n&gt; &gt; &gt;\n&gt; &gt; &gt; --- In neat@yahoogroups.com &lt;neat%40yahoogroups.com&gt;,\n&gt;\n&gt; &quot;peterberrington&quot;\n&gt; &gt; &gt; &lt;peterberrington@&gt;\n&gt; &gt; &gt; wrote:\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; I&#39;ve been tinkering with many optimization techniques and\n&gt; considering\n&gt; &gt; &gt; &gt; the possibility of integrating some with the mechanics of neat,\n&gt; in the\n&gt; &gt; &gt; &gt; vein of EANT, which sadly has no open source implementation.\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; While I&#39;m trying to add that functionality to the neat-python\n&gt; &gt; &gt; &gt; implementation I use, I&#39;m really wondering if its at all\n&gt; applicable to\n&gt; &gt; &gt; &gt; novelty search. In novelty search, there is a specific pressure\n&gt; to do\n&gt; &gt; &gt; &gt; something new and indeed we do assign a numeric novelty value to\n&gt; each\n&gt; &gt; &gt; &gt; individual behaviour at the time of its evaluation for addition\n&gt; to the\n&gt; &gt; &gt; &gt; archive. However, the score an individual receives depends on\n&gt; who its\n&gt; &gt; &gt; &gt; up against, so its not objective. Without a function to\n&gt; minimize, the\n&gt; &gt; &gt; &gt; dynamics which allow optimization techniques to work may be\n&gt; damaged to\n&gt; &gt; &gt; &gt; the point of rendering it no better than random search.\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; I&#39;m still puzzled on the discussion Peter C raised about\n&gt; reevaluating\n&gt; &gt; &gt; &gt; the novelty of behaviours in the archive at future intervals.\n&gt; &gt; &gt; &gt; Shouldn&#39;t the novelty score assigned to each point be useless after\n&gt; &gt; &gt; &gt; we&#39;ve decided whether or not to archive the behaviour? I&#39;m not sure\n&gt; &gt; &gt; &gt; that joint angles over every timestep is really the best way the\n&gt; &gt; &gt; &gt; characterize behaviour for a 3d ragdoll rig, as you are essentially\n&gt; &gt; &gt; &gt; providing no clear way to distinguish a good behaviour from a bad\n&gt; &gt; &gt; &gt; behaviour. Although I haven&#39;t been able to use optimized libraries\n&gt; &gt; &gt; &gt; with my current simulation configuration (this is hopefully changing\n&gt; &gt; &gt; &gt; very soon), I have noticed a significant boost in speed with my\n&gt; &gt; &gt; &gt; ragdoll evolution by defining behaviour simply as the final x y z\n&gt; &gt; &gt; &gt; triplet for the center of mass. In this way the search quickly\n&gt; &gt; &gt; &gt; exhausts all the easy ways of falling close to its origin and\n&gt; pressure\n&gt; &gt; &gt; &gt; mounts for it explore end locations successively farther and farther\n&gt; &gt; &gt; &gt; away from its origin; in essence the objective function is\n&gt; realized in\n&gt; &gt; &gt; &gt; that characterization of behavioural distance.\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; In any case, we are explicitly trying to reward novelty by selecting\n&gt; &gt; &gt; &gt; for further evaluation any individuals which satisfy a minimum\n&gt; &gt; &gt; &gt; threshold of how different their behaviour is. Something about that\n&gt; &gt; &gt; &gt; gives me a feeling that maybe optimization techniques are\n&gt; applicable,\n&gt; &gt; &gt; &gt; but only if the dynamics of optimization can be adapted to the\n&gt; &gt; &gt; &gt; unwieldy workings of novelty search.\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; Rather than maximizing fitness, optimization techniques usually\n&gt; &gt; &gt; &gt; minimize an objective function so the fitness scale is simply\n&gt; reversed\n&gt; &gt; &gt; &gt; with a perfect score being 0.0\n&gt; &gt; &gt; &gt; Since in this domain theres no such thing as perfect novelty, can\n&gt; &gt; &gt; &gt; anyone provide any insights as to a framework where optimization\n&gt; &gt; &gt; &gt; techniques can be harnessed to maximize novelty? I feel this is a\n&gt; &gt; &gt; &gt; really important area to focus attention on, as it could potentially\n&gt; &gt; &gt; &gt; lead to a dramatic increase in the fitness per number of\n&gt; evaluations.\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; As some background: I&#39;ve long been tempted after reading papers on\n&gt; &gt; &gt; &gt; EANT to try dropping in a more advanced optimization function into\n&gt; &gt; &gt; &gt; NEAT, like some of the new variations on the covariance matrix\n&gt; &gt; &gt; &gt; adaption evolution strategy. The way this was implemented in\n&gt; EANT was\n&gt; &gt; &gt; &gt; by splitting NEATs main loop into a &quot;structural exploration&quot;\n&gt; loop for\n&gt; &gt; &gt; &gt; building networks, and optimizing the weight connection values\n&gt; &gt; &gt; &gt; separately within a nested loop, using CMA-ES. This division of work\n&gt; &gt; &gt; &gt; permits the net to be treated as an n-dimensional equation where\n&gt; n is\n&gt; &gt; &gt; &gt; the number of connection or node weights (or other properties)\n&gt; we wish\n&gt; &gt; &gt; &gt; to optimize (i.e. everything that isn&#39;t defining the topology of the\n&gt; &gt; &gt; &gt; net). I think it can be argued that standard fitness based neat is a\n&gt; &gt; &gt; &gt; greedier approach because it cartwheels through topological\n&gt; space and\n&gt; &gt; &gt; &gt; weight parameter space at the same time. I want to plug in an\n&gt; &gt; &gt; &gt; optimization function, but I can&#39;t think of how anymore, or even if\n&gt; &gt; &gt; &gt; the two search techniques are reconcilable.\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; Theres no point optimizing within a specific net topology in novelty\n&gt; &gt; &gt; &gt; search because you want to kind of push out and fill behavioural\n&gt; space\n&gt; &gt; &gt; &gt; as evenly as possible, so theres no nook or crack which escapes the\n&gt; &gt; &gt; &gt; poking and prodding of your search; without the\n&gt; &gt; &gt; &gt; competition/coevolutionary aspect of defining fitness against the\n&gt; &gt; &gt; &gt; backdrop of current rivals and ancestors, I doubt you&#39;d see anything\n&gt; &gt; &gt; &gt; more effective than random selection. If you can&#39;t optimize each net\n&gt; &gt; &gt; &gt; topology in a vacuum, is there another way fitness-based\n&gt; searches and\n&gt; &gt; &gt; &gt; novelty-based searches can reach a compromise where each complements\n&gt; &gt; &gt; &gt; the other?\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; I&#39;ve given some thought to the idea of phased searching, but you\n&gt; would\n&gt; &gt; &gt; &gt; have to find some way of defining stopping critera for when\n&gt; you&#39;d want\n&gt; &gt; &gt; &gt; to optimize an objective function and when you&#39;d want to diversify\n&gt; &gt; &gt; &gt; from bottom up complexity wise.\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; As an interesting side note, to anyone whos had a sneak peak at\n&gt; Peter\n&gt; &gt; &gt; &gt; C&#39;s maze navigation novelty search program, isn&#39;t it uncanny, the\n&gt; &gt; &gt; &gt; resemblance between the past behaviours that have accumulated,\n&gt; and the\n&gt; &gt; &gt; &gt; growth of a plant? As more organisms fill up the space of possible\n&gt; &gt; &gt; &gt; behaviours its tempting to imagine it as molasses which slowly oozes\n&gt; &gt; &gt; &gt; its way into every crack and crevice it can reach. Perhaps there\n&gt; is a\n&gt; &gt; &gt; &gt; useful insight to be drawn from that, I have no idea, I just thought\n&gt; &gt; &gt; &gt; it looked very &quot;organic&quot;.\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; --\n&gt; &gt; Julian Togelius\n&gt; &gt; IDSIA\n&gt; &gt; Galleria 2\n&gt; &gt; 6928 Manno-Lugano\n&gt; &gt; Switzerland\n&gt; &gt; julian@...\n&gt; &gt; http://julian.togelius.com\n&gt; &gt; http://www.idsia.ch/~togelius\n&gt; &gt; +41-764-110679\n&gt; &gt; +46-705-192088\n&gt; &gt;\n&gt;\n&gt;\n&gt;\n&gt; \n\n\n\n-- \nJulian Togelius\nIDSIA\nGalleria 2\n6928 Manno-Lugano\nSwitzerland\njulian@...\nhttp://julian.togelius.com\nhttp://www.idsia.ch/~togelius\n+41-764-110679\n+46-705-192088\n\n"}}