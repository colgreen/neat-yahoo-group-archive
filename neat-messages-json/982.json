{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":127853030,"authorName":"Colin Green","from":"Colin Green &lt;cgreen@...&gt;","profile":"alienseedpod","replyTo":"LIST","senderId":"RmM1ZmFw3Ile7m1sMBZVHQqfRIWwJHnES3gKWn9p4R-OOmvtvPGiNaelXOEO38yuLyN4Ia0IYCdbpOzY44uanN0EdM3RwwlHmQ","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] Re: Computation Time","postDate":"1086301694","msgId":982,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQwQkZBNUZFLjIwNzA5MDlAZHNsLnBpcGV4LmNvbT4=","inReplyToHeader":"PDYuMS4wLjYuMC4yMDA0MDYwMzE2NDkzMi4wMjUxMmUzMEBwb3AubWFpbC55YWhvby5jby51az4=","referencesHeader":"PDFkNDEwMWM0NDY3ZiQzZDkyODU2MCQ3NGNiMDEwYUBtYWlsMndvcmxkLmNvbT4gPDQwQkNGNDE4LjcwMzA5MDVAZHNsLnBpcGV4LmNvbT4gPDYuMS4wLjYuMC4yMDA0MDYwMzE2NDkzMi4wMjUxMmUzMEBwb3AubWFpbC55YWhvby5jby51az4="},"prevInTopic":978,"nextInTopic":989,"prevInTime":981,"nextInTime":983,"topicId":845,"numMessagesInTopic":99,"msgSnippet":"[Discussion about optimization using integer maths and fast activation functions] ... I just ran a benchmark om my AMD XP machine and it gives 8261 MIPS and ","rawEmail":"Return-Path: &lt;cgreen@...&gt;\r\nX-Sender: cgreen@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 58520 invoked from network); 3 Jun 2004 22:28:21 -0000\r\nReceived: from unknown (66.218.66.167)\n  by m24.grp.scd.yahoo.com with QMQP; 3 Jun 2004 22:28:21 -0000\r\nReceived: from unknown (HELO shockwave.systems.pipex.net) (62.241.160.9)\n  by mta6.grp.scd.yahoo.com with SMTP; 3 Jun 2004 22:28:21 -0000\r\nReceived: from dsl.pipex.com (81-86-175-101.dsl.pipex.com [81.86.175.101])\n\tby shockwave.systems.pipex.net (Postfix) with ESMTP id 0ACBD1C0014E\n\tfor &lt;neat@yahoogroups.com&gt;; Thu,  3 Jun 2004 23:28:12 +0100 (BST)\r\nMessage-ID: &lt;40BFA5FE.2070909@...&gt;\r\nDate: Thu, 03 Jun 2004 23:28:14 +0100\r\nUser-Agent: Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.5) Gecko/20031007\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: neat@yahoogroups.com\r\nReferences: &lt;1d4101c4467f$3d928560$74cb010a@...&gt; &lt;40BCF418.7030905@...&gt; &lt;6.1.0.6.0.20040603164932.02512e30@...&gt;\r\nIn-Reply-To: &lt;6.1.0.6.0.20040603164932.02512e30@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Remote-IP: 62.241.160.9\r\nFrom: Colin Green &lt;cgreen@...&gt;\r\nSubject: Re: [neat] Re: Computation Time\r\nX-Yahoo-Group-Post: member; u=127853030\r\nX-Yahoo-Profile: alienseedpod\r\n\r\n[Discussion about optimization using integer maths and fast activation \nfunctions]\n\nIan Badcoe wrote:\n\n&gt;         I used to be a games programmer so I know about squeezing the last \n&gt;cycle out of the CPU.  Or used to...\n&gt;\n&gt;         The difference between everyday float functions (multiply, add and \n&gt;subtract) and integer is no longer worth worrying about on many \n&gt;CPUs.  Divide is slower, as it always was, but nothing like as bad as it \n&gt;used to be.  And transcendental functions (Sqrt, Sin, Tan) are all still \n&gt;real killers.\n&gt;\nI just ran a benchmark om my AMD XP machine and it gives 8261 MIPS and \n3408 MFLOPS. This is a little tenuous but it does roughly show that \nfloating point stuff is about half as fast as integer stuff, doesn&#39;t it?\n\n&gt;  So, if I were doing this, I would try to code an activation \n&gt;function using:\n&gt;\n&gt;i) float maths\n&gt;ii) no loops\n&gt;iii) minimal terms\n&gt;iv) no transcendental functions\n&gt;v) _one_ divide (on the basis that you won&#39;t manage none)\n&gt;  \n&gt;\n&gt;         But I would want to see profiling results to prove this was a \n&gt;significant CPU hot spot before putting too much work into it.  There&#39;s so \n&gt;much else can eat CPU these days.  For example, if the network can be \n&gt;updated as a straight loop through a set of arrays, then most of the CPU \n&gt;may indeed be in the maths.  OTOH, if the loop contains a couple of \n&gt;conditional jumps and a function call (or worse a call through a pointer) \n&gt;then you may be spending as much CPU on the body of the loop.\n&gt;  \n&gt;\nYes except that modern compilers tend to help you out quite a bit. Those \nheavily used function calls are good candidates for getting in-lined for \nexample.\n\n&gt;         I was just playing with some equations, it&#39;s not easy to get the \n&gt;right properties.  But how about x/(n+abs(x))?  That&#39;s a sigmoid, symmetric \n&gt;about zero, equal to +1 at +infinity and -1 at -infinity.  n controls the \n&gt;steepness and is equal to the x value where the curve reaches +0.5...\n&gt;\n&gt;  \n&gt;\nYeh nice idea. I think normally you want an output range of 0 to 1 \nthough or at least a lower bound of 0 anyway. That way large negative \nweights can drag the output to 0 rather than down to -1, which would \ncause the neuron to send out a signal of -1. So we could add one and get \na range of 0 to 2, which would be better. Also it doesn&#39;t approach the \nbounds as fast as the sigmoids I have been using until now - depending \non what value you use for n though of course.\n\nI just did a rough test and there was no noticable difference between \nyour function and 1.0/(1.0+(exp(-x)))  . Actually I wonder if this fn is \nalready quite optimal? is exp() expensive? Anyway like you say some \nprofiling needs to be done. If there is significant improvement in the \nactivation fn (say twice as fast) then it will probably only become \nnoticeable when dealing with relatively large networks.\n\nColin.\n\n\n"}}