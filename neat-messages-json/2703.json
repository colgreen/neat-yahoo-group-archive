{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":150579383,"authorName":"Sidhant Dash","from":"Sidhant Dash &lt;sidhantdash@...&gt;","profile":"sidhantdash","replyTo":"LIST","senderId":"XpqP1TgtNI31PSH3P8BUz0VcOjRAoLDp9twe3iTKMWmllaO5xTuNPb8II_Z0HSG2EpiGvd3TFWtpgogC4blXDmlqXJSf2kgtYSVb","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] Re: NEAT enhancements","postDate":"1155110894","msgId":2703,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDIwMDYwODA5MDgwODE0LjM4NTQ1LnFtYWlsQHdlYjYxMjI1Lm1haWwueWFob28uY29tPg==","inReplyToHeader":"PGViOW5oNytwcjBkQGVHcm91cHMuY29tPg=="},"prevInTopic":2702,"nextInTopic":2704,"prevInTime":2702,"nextInTime":2704,"topicId":2684,"numMessagesInTopic":17,"msgSnippet":"Hi, I could do only 20 generations or less for most of my experiments. I understand that with more generations, I could have achieved much better networks. The","rawEmail":"Return-Path: &lt;sidhantdash@...&gt;\r\nX-Sender: sidhantdash@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 13362 invoked from network); 9 Aug 2006 08:08:16 -0000\r\nReceived: from unknown (66.218.67.35)\n  by m40.grp.scd.yahoo.com with QMQP; 9 Aug 2006 08:08:16 -0000\r\nReceived: from unknown (HELO web61225.mail.yahoo.com) (209.73.179.59)\n  by mta9.grp.scd.yahoo.com with SMTP; 9 Aug 2006 08:08:15 -0000\r\nReceived: (qmail 38547 invoked by uid 60001); 9 Aug 2006 08:08:14 -0000\r\nMessage-ID: &lt;20060809080814.38545.qmail@...&gt;\r\nReceived: from [203.200.95.130] by web61225.mail.yahoo.com via HTTP; Wed, 09 Aug 2006 01:08:14 PDT\r\nDate: Wed, 9 Aug 2006 01:08:14 -0700 (PDT)\r\nTo: neat@yahoogroups.com\r\nIn-Reply-To: &lt;eb9nh7+pr0d@...&gt;\r\nMIME-Version: 1.0\r\nContent-Type: multipart/alternative; boundary=&quot;0-1880278716-1155110894=:57428&quot;\r\nContent-Transfer-Encoding: 8bit\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Sidhant Dash &lt;sidhantdash@...&gt;\r\nSubject: Re: [neat] Re: NEAT enhancements\r\nX-Yahoo-Group-Post: member; u=150579383; y=Nh4-T7iN3RWZsBpjA7orv_INvmZzA7aRV6RdbN97jtI6U8C52hY\r\nX-Yahoo-Profile: sidhantdash\r\n\r\n\r\n--0-1880278716-1155110894=:57428\r\nContent-Type: text/plain; charset=iso-8859-1\r\nContent-Transfer-Encoding: 8bit\r\n\r\nHi,\n   \n  I could do only 20 generations or less for most of my experiments. I understand that with more generations, I could have achieved much better networks. The main problem was that my implementation of NEAT happened to be slow. It was in Matlab, and I was using a population of 300 networks in each of my runs. This coupled with a 1000 point data set made things very slow. As I was working under a time bound program which required me to have a report ready by a certain date, I had little time to complete my experiments the way I wanted to,  so I kept the starting mutation rates high for the runs. Such rates made NEAT aggressive in the initial generations, giving me reasonable good networks by the 20th or so generation.\n   \n  I had however used an adaptive mutation rate (as explained in the NEAT faq) to keep the number of individuals in a species stable, this I think would have led evolution to discover better networks by stabilising the rates towards the later generations. I am trying out more experiments to test this idea. \n\n  The RW model does do a good job of foecasting the time series in question. It is true that the improvements achieved through NEAT are quite small. This aspect needs more attention. I have been told that NN models give &#39;much&#39; better predictions that the RW model. I am still trying to see if there are ways to improve the performance of the ensemble. One way, I think, is to let NEAT work for more generations. Another maybe is to increase the number of networks in the ensemble, I had used only 4.  \n   \n  As far as the utility of the prediction goes, I do not think there is much that can be done with the results in their present form. But with some more improvement in the predictions, I believe that such a setup can be used to atleast predict the direction of a currency&#39;s movement, if not the actual values themselves. \n   \n  thanks a lot for going through my report,\n  I hope to come up with better results soon.\n   \n  regards,\n  Sidhant\n  \nKenneth Stanley &lt;kstanley@...&gt; wrote:\n          Sidhant, I have to say that your paper is a great accomplishment for \na 3-month undergraduate research project. The scope of this project \nand amount of technical work involved is enormous. It is \nfurthermore impressive that this was your first attempt at technical \nwriting (Sidhant told me this in separate email), since I found it \nto be more clear than many papers I have to review for scientific \njournals.\n\nFor anyone who is interested in time series prediction with NEAT \n(including financial time series), I highly recommend this work. It \nis a bit long, but I was able to read it in one sitting since it was \nso clear and well-organized. I would even recommend this to people \nwho have no interest or experience in this area, since it makes \nquite a good tutorial on a number of technologies outside NEAT \nitself, including conjugate gradient descent, elman networks, RTRL \nnetworks, boosting, and ensemble methods. Sidhant, how did you \nprogram all this stuff in 3 months?\n\nThe work is furthermore creative in that it modifies NEAT to evolve \nnew kinds of NNs (Elman and RTRL), and comes up with ways to keep \nall 3 types of nets (the new ones plus MLPs) in the population at \nonce.\n\nSome of the people in this group know that I am not generally in \nfavor of combining numerous methods simply for the sake of combining \nthem, but in this case the combinations seem to have been selected \ncleverly, making for a compelling case for a very effective overall \nmethod for time series predicition.\n\nI do have a few questions for Sidhant: The primary issue that I \ndon&#39;t understand is why the runs in the financial time series only \nlasted under 20 generations? How much structure can even evolve in \nsuch a short time? It appears in one case that 6 new hidden nodes \nwere added, but in only 20 generations? That seems perhaps \nunnecessarily fast in terms of structural mutation rate. Maybe you \nwould have gotten even better results with a lower node-\naddition/link-addition rate and a run of 100 or more generations, \ngiving NEAT more time to weight-optimize each new structural \naddition.\n\nMy second major question is how the RW model can produce such a low \nerror (3.7113e-5) on this specific currency data compared to the \nNEAT ensemble error of 3.582e-5? While that difference may be \nsignificant, it is surprisingly small. When I look at figure 6.10, \nNEAT&#39;s performance looks amazingly accurate, and I don&#39;t understand \nhow a random walk can come so close? What does figure 6.10 if you \ngraph the random walk performance?\n\nFinally, out of curiosity, how much money would you have been able \nto make using the NEAT-ensemble if you had based your currency \ntrades on its predicitons? (i.e. pretending that you had been using \nit for real purposes after the training period) Is this kind of \npredicition financially profitable or useful to trading companies?\n\nOverall, excellent work; thank you for sharing! I suggest that you \nconsider publishing this work in a conference or journal.\n\nken\n\n--- In neat@yahoogroups.com, Sidhant Dash &lt;sidhantdash@...&gt; wrote:\n&gt;\n&gt; As part of an undergraduate research project, I did implement \nsomething on those lines. \n&gt; \n&gt; I worked on a NEAT based approach to predicting a noisy \nfinancial time series (Yen-USD exchange rate). The original NEAT \nalgorithm was modified to start with an initial population of 3 \ndifferent neural network architectures, which included Elman \nnetworks and MLPs apart from the normal recurrent networks that NEAT \nbegins with. The networks (winners) produced by NEAT were then put \nthrough a conjugate gradient based optimization process, and finally \nthe optimized networks were combined using an ensembling technique \nto produce the final results. \n&gt; \n&gt; I personally think the local optimization is necessary when we \nare using NEAT to produce networks to accomplish tasks that require \na high degree of precision. It kind of fine tunes the performance of \nthe networks. Ensembling then is one of the standard tools in the ML \nrepertoire to produce results that none of NEAT&#39;s &#39;winner&#39; networks \ncould individually produce. \n&gt; \n&gt; A detailed project report is posted in the Files section.\n&gt; \n&gt; regards\n&gt; Sidhant\n&gt; \n&gt; aklein07 &lt;a.klein@...&gt; wrote:\n&gt; All,\n&gt; \n&gt; Ken&#39;s papers mention (at least I think that they do) that \n(dynamic) \n&gt; annealing of mutation rates might be advantageous in order to \nconfine \n&gt; global searching after a while to a promising region in search \nspace. \n&gt; Having read some papers, I also found indications that a hybrid \n&gt; weight training algorithm (global search by means of genetic \n&gt; algorithms and local search using some gradient descent algorithm) \n&gt; might be able to produce better results than genetic algorithm \nsearch \n&gt; alone. The reason is, that each type of algorithm performs well \n&gt; mostly only in its specific field. Has anyone further insights \ninto \n&gt; this topic ? Has anyone tried to implement any of the approaches \n&gt; mentioned ? \n&gt; \n&gt; Also time delay network connections might provide improvements for \n&gt; some model estimation tasks. Has anyone tested / implemented yet \nany \n&gt; of these ideas ?\n&gt; \n&gt; Thanks for any comments,\n&gt; Achim\n&gt; \n&gt; \n&gt; \n&gt; \n&gt; \n&gt; \n&gt; Fear is only as deep as the mind allows. \n&gt; --Japanese proverb \n&gt; \n&gt; My blog \n&gt; \n&gt; \n&gt; ---------------------------------\n&gt; Groups are talking. We&acute;re listening. Check out the handy \nchanges to Yahoo! Groups.\n&gt;\n\n\n\n         \n\n\nFear is only as deep as the mind allows. \n--Japanese proverb \n\nMy blog   \n\n \t\t\n---------------------------------\nGet your email and more, right on the  new Yahoo.com \r\n--0-1880278716-1155110894=:57428\r\nContent-Type: text/html; charset=iso-8859-1\r\nContent-Transfer-Encoding: 8bit\r\n\r\n&lt;div&gt;Hi,&lt;/div&gt;  &lt;div&gt;&nbsp;&lt;/div&gt;  &lt;div&gt;I could do only 20 generations or less for most of my experiments. I understand that with more generations, I could have achieved much better networks. The main problem was that my implementation of NEAT happened to be slow. It was in Matlab, and I was using a population of 300 networks in each of my runs. This coupled with a 1000 point data set made things very slow.&nbsp;As I was working under a time bound program which required me to have a report ready by a certain date, I had little time to complete my experiments the way I wanted to,&nbsp; so I kept the starting mutation rates high for the runs.&nbsp;Such rates made NEAT aggressive in&nbsp;the initial generations, giving me reasonable good networks by the 20th or so generation.&lt;/div&gt;  &lt;div&gt;&nbsp;&lt;/div&gt;  &lt;div&gt;I had however used an adaptive&nbsp;mutation rate&nbsp;(as explained in the NEAT faq) to keep the number of individuals in a species stable, this I think would have led\n evolution to discover better networks by stabilising the&nbsp;rates towards the later generations. I am trying out more experiments to test this idea. &lt;BR&gt;&lt;/div&gt;  &lt;div&gt;The RW model does do a good job of foecasting the time series in question. It is true that the improvements achieved through NEAT are quite small. This aspect needs more attention. I have been told that NN models give &#39;much&#39; better predictions that the RW model. I am still trying to see if there are ways to improve the performance of the ensemble. One way,&nbsp;I think, is to let NEAT work for more generations. Another maybe is to increase the number of networks in the ensemble, I had used only 4. &nbsp;&lt;/div&gt;  &lt;div&gt;&nbsp;&lt;/div&gt;  &lt;div&gt;As far as the utility of the prediction goes, I do not think there is much that can be done with the results in their present form. But with some more improvement in the predictions, I believe that such a setup can be used to atleast predict the direction of a currency&#39;s\n movement, if not the actual values themselves. &lt;/div&gt;  &lt;div&gt;&nbsp;&lt;/div&gt;  &lt;div&gt;thanks a lot for going through my report,&lt;/div&gt;  &lt;div&gt;I hope to come up with better results soon.&lt;/div&gt;  &lt;div&gt;&nbsp;&lt;/div&gt;  &lt;div&gt;regards,&lt;/div&gt;  &lt;div&gt;Sidhant&lt;/div&gt;  &lt;div&gt;&lt;BR&gt;&lt;B&gt;&lt;I&gt;Kenneth Stanley &lt;kstanley@...&gt;&lt;/I&gt;&lt;/B&gt; wrote:&lt;/div&gt;  &lt;BLOCKQUOTE class=replbq style=&quot;PADDING-LEFT: 5px; MARGIN-LEFT: 5px; BORDER-LEFT: #1010ff 2px solid&quot;&gt;&lt;!-- Network content --&gt;  &lt;DIV id=ygrp-text&gt;  &lt;div&gt;Sidhant, I have to say that your paper is a great accomplishment for &lt;BR&gt;a 3-month undergraduate research project. The scope of this project &lt;BR&gt;and amount of technical work involved is enormous. It is &lt;BR&gt;furthermore impressive that\n this was your first attempt at technical &lt;BR&gt;writing (Sidhant told me this in separate email), since I found it &lt;BR&gt;to be more clear than many papers I have to review for scientific &lt;BR&gt;journals.&lt;BR&gt;&lt;BR&gt;For anyone who is interested in time series prediction with NEAT &lt;BR&gt;(including financial time series), I highly recommend this work. It &lt;BR&gt;is a bit long, but I was able to read it in one sitting since it was &lt;BR&gt;so clear and well-organized. I would even recommend this to people &lt;BR&gt;who have no interest or experience in this area, since it makes &lt;BR&gt;quite a good tutorial on a number of technologies outside NEAT &lt;BR&gt;itself, including conjugate gradient descent, elman networks, RTRL &lt;BR&gt;networks, boosting, and ensemble methods. Sidhant, how did you &lt;BR&gt;program all this stuff in 3 months?&lt;BR&gt;&lt;BR&gt;The work is furthermore creative in that it modifies NEAT to evolve &lt;BR&gt;new kinds of NNs (Elman and RTRL), and comes up with ways to keep &lt;BR&gt;all 3 types of nets (the new ones plus\n MLPs) in the population at &lt;BR&gt;once.&lt;BR&gt;&lt;BR&gt;Some of the people in this group know that I am not generally in &lt;BR&gt;favor of combining numerous methods simply for the sake of combining &lt;BR&gt;them, but in this case the combinations seem to have been selected &lt;BR&gt;cleverly, making for a compelling case for a very effective overall &lt;BR&gt;method for time series predicition.&lt;BR&gt;&lt;BR&gt;I do have a few questions for Sidhant: The primary issue that I &lt;BR&gt;don&#39;t understand is why the runs in the financial time series only &lt;BR&gt;lasted under 20 generations? How much structure can even evolve in &lt;BR&gt;such a short time? It appears in one case that 6 new hidden nodes &lt;BR&gt;were added, but in only 20 generations? That seems perhaps &lt;BR&gt;unnecessarily fast in terms of structural mutation rate. Maybe you &lt;BR&gt;would have gotten even better results with a lower node-&lt;BR&gt;addition/link-&lt;WBR&gt;addition rate and a run of 100 or more generations, &lt;BR&gt;giving NEAT more time to weight-optimize each new structural\n &lt;BR&gt;addition.&lt;BR&gt;&lt;BR&gt;My second major question is how the RW model can produce such a low &lt;BR&gt;error (3.7113e-5) on this specific currency data compared to the &lt;BR&gt;NEAT ensemble error of 3.582e-5? While that difference may be &lt;BR&gt;significant, it is surprisingly small. When I look at figure 6.10, &lt;BR&gt;NEAT&#39;s performance looks amazingly accurate, and I don&#39;t understand &lt;BR&gt;how a random walk can come so close? What does figure 6.10 if you &lt;BR&gt;graph the random walk performance?&lt;BR&gt;&lt;BR&gt;Finally, out of curiosity, how much money would you have been able &lt;BR&gt;to make using the NEAT-ensemble if you had based your currency &lt;BR&gt;trades on its predicitons? (i.e. pretending that you had been using &lt;BR&gt;it for real purposes after the training period) Is this kind of &lt;BR&gt;predicition financially profitable or useful to trading companies?&lt;BR&gt;&lt;BR&gt;Overall, excellent work; thank you for sharing! I suggest that you &lt;BR&gt;consider publishing this work in a conference or journal.&lt;BR&gt;&lt;BR&gt;ken&lt;BR&gt;&lt;BR&gt;---\n In &lt;A href=&quot;mailto:neat%40yahoogroups.com&quot;&gt;neat@yahoogroups.&lt;WBR&gt;com&lt;/A&gt;, Sidhant Dash &lt;sidhantdash@&lt;WBR&gt;...&gt; wrote:&lt;BR&gt;&gt;&lt;BR&gt;&gt; As part of an undergraduate research project, I did implement &lt;BR&gt;something on those lines. &lt;BR&gt;&gt; &lt;BR&gt;&gt; I worked on a NEAT based approach to predicting a noisy &lt;BR&gt;financial time series (Yen-USD exchange rate). The original NEAT &lt;BR&gt;algorithm was modified to start with an initial population of 3 &lt;BR&gt;different neural network architectures, which included Elman &lt;BR&gt;networks and MLPs apart from the normal recurrent networks that NEAT &lt;BR&gt;begins with. The networks (winners) produced by NEAT were then put &lt;BR&gt;through a conjugate gradient based optimization process, and finally &lt;BR&gt;the optimized networks were combined using an ensembling technique &lt;BR&gt;to produce the final results. &lt;BR&gt;&gt; &lt;BR&gt;&gt; I personally think the local optimization is necessary when we &lt;BR&gt;are using NEAT to produce networks to accomplish tasks that require &lt;BR&gt;a\n high degree of precision. It kind of fine tunes the performance of &lt;BR&gt;the networks. Ensembling then is one of the standard tools in the ML &lt;BR&gt;repertoire to produce results that none of NEAT&#39;s &#39;winner&#39; networks &lt;BR&gt;could individually produce. &lt;BR&gt;&gt; &lt;BR&gt;&gt; A detailed project report is posted in the Files section.&lt;BR&gt;&gt; &lt;BR&gt;&gt; regards&lt;BR&gt;&gt; Sidhant&lt;BR&gt;&gt; &lt;BR&gt;&gt; aklein07 &lt;a.klein@...&lt;WBR&gt;&gt; wrote:&lt;BR&gt;&gt; All,&lt;BR&gt;&gt; &lt;BR&gt;&gt; Ken&#39;s papers mention (at least I think that they do) that &lt;BR&gt;(dynamic) &lt;BR&gt;&gt; annealing of mutation rates might be advantageous in order to &lt;BR&gt;confine &lt;BR&gt;&gt; global searching after a while to a promising region in search &lt;BR&gt;space. &lt;BR&gt;&gt; Having read some papers, I also found indications that a hybrid &lt;BR&gt;&gt; weight training algorithm (global search by means of genetic &lt;BR&gt;&gt; algorithms and local search using some gradient descent algorithm) &lt;BR&gt;&gt; might be able to produce better results than genetic algorithm\n &lt;BR&gt;search &lt;BR&gt;&gt; alone. The reason is, that each type of algorithm performs well &lt;BR&gt;&gt; mostly only in its specific field. Has anyone further insights &lt;BR&gt;into &lt;BR&gt;&gt; this topic ? Has anyone tried to implement any of the approaches &lt;BR&gt;&gt; mentioned ? &lt;BR&gt;&gt; &lt;BR&gt;&gt; Also time delay network connections might provide improvements for &lt;BR&gt;&gt; some model estimation tasks. Has anyone tested / implemented yet &lt;BR&gt;any &lt;BR&gt;&gt; of these ideas ?&lt;BR&gt;&gt; &lt;BR&gt;&gt; Thanks for any comments,&lt;BR&gt;&gt; Achim&lt;BR&gt;&gt; &lt;BR&gt;&gt; &lt;BR&gt;&gt; &lt;BR&gt;&gt; &lt;BR&gt;&gt; &lt;BR&gt;&gt; &lt;BR&gt;&gt; Fear is only as deep as the mind allows. &lt;BR&gt;&gt; --Japanese proverb &lt;BR&gt;&gt; &lt;BR&gt;&gt; My blog &lt;BR&gt;&gt; &lt;BR&gt;&gt; &lt;BR&gt;&gt; ------------&lt;WBR&gt;---------&lt;WBR&gt;---------&lt;WBR&gt;---&lt;BR&gt;&gt; Groups are talking. We&amp;acute;re listening. Check out the handy &lt;BR&gt;changes to Yahoo! Groups.&lt;BR&gt;&gt;&lt;BR&gt;&lt;BR&gt;&lt;/div&gt;&lt;/DIV&gt;&lt;!--End group email --&gt;&lt;/BLOCKQUOTE&gt;&lt;BR&gt;&lt;BR&gt;&lt;BR&gt;&lt;DIV&gt;Fear is only as deep as the mind allows. &lt;BR&gt;--Japanese proverb &lt;BR&gt;&lt;BR&gt;&lt;A href=&quot;http://sidhantdash.blogspot.com&quot;&gt;My blog&lt;/A&gt;&nbsp;&nbsp; &lt;/DIV&gt;&lt;p&gt;&#32;\n\t\t&lt;hr size=1&gt;Get your email and more, right on the &lt;a href=&quot;http://us.rd.yahoo.com/evt=42973/*http://www.yahoo.com/preview&quot;&gt; new Yahoo.com&lt;/a&gt; \n\r\n--0-1880278716-1155110894=:57428--\r\n\n"}}