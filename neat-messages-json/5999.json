{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Ken","from":"&quot;Ken&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"IivajqrSYTnwRgBulAyKymbUmrzujI61lCN7uqzjImrJqt4fawVyLn7jeZzYAbHuhGym1h79xuUAqF1I3nwylfEsNGgy","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: New paper on why modules evolve, and how to evolve modular artificial neural networks","postDate":"1361072005","msgId":5999,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGtmcGoyNSs0ZWhmQGVHcm91cHMuY29tPg==","inReplyToHeader":"PEQ1RkQyMjE5LTIzNEEtNDlFNS04MjBDLTVDNkQ0QzkxNDAyMkB1d3lvLmVkdT4="},"prevInTopic":5997,"nextInTopic":6000,"prevInTime":5998,"nextInTime":6000,"topicId":5976,"numMessagesInTopic":30,"msgSnippet":"Hi Jeff, I wanted to follow up on the dead weight concept I brought up, because I feel I may not have made it entirely clear what that is. In a","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 21807 invoked from network); 17 Feb 2013 03:33:27 -0000\r\nX-Received: from unknown (10.193.84.163)\n  by m9.grp.bf1.yahoo.com with QMQP; 17 Feb 2013 03:33:27 -0000\r\nX-Received: from unknown (HELO ng8-vm5.bullet.mail.gq1.yahoo.com) (98.136.219.96)\n  by mta3.grp.bf1.yahoo.com with SMTP; 17 Feb 2013 03:33:27 -0000\r\nX-Received: from [98.137.0.88] by ng8.bullet.mail.gq1.yahoo.com with NNFMP; 17 Feb 2013 03:33:26 -0000\r\nX-Received: from [10.193.94.106] by tg8.bullet.mail.gq1.yahoo.com with NNFMP; 17 Feb 2013 03:33:26 -0000\r\nDate: Sun, 17 Feb 2013 03:33:25 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;kfpj25+4ehf@...&gt;\r\nIn-Reply-To: &lt;D5FD2219-234A-49E5-820C-5C6D4C914022@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: &quot;Ken&quot; &lt;kstanley@...&gt;\r\nSubject: Re: New paper on why modules evolve, and how to evolve modular artificial neural networks\r\nX-Yahoo-Group-Post: member; u=54567749; y=4aFzbLtBqQe_tdqD4N3XpHNBloxLb_BUxW02hDC2StMyhS2u1EXL\r\nX-Yahoo-Profile: kenstanley01\r\n\r\n\n\nHi Jeff, I wanted to follow up on the &quot;dead weight&quot; concept I brought up,=\r\n because I feel I may not have made it entirely clear what that is. In a mu=\r\nltiobjective formulation, if one of the objectives is low connectivity, the=\r\nn you can dominate on that objective by having extremely low connectivity a=\r\nnd no other saving grace whatsoever.  In other words we are talking about c=\r\nompletely nonfunctional and essentially inert garbage that does well on tha=\r\nt one objective by effectively being brain dead.  In effect, you have creat=\r\ned a special &quot;niche&quot; for brain-dead networks with radically low connectivit=\r\ny.\n\nPerhaps early on this niche is a fruitful point of departure for better=\r\n places, but the problem is that this niche will never go away.  You will a=\r\nlways have this protective pocket for brain-dead low-connectivity networks =\r\nfor as long as evolution runs.  In fact it&#39;s a very attractive niche becaus=\r\ne it&#39;s so easy - you don&#39;t have to worry about performance and simply need =\r\nto keep your structure down.  So these types of inert blobs will be around =\r\nforever.\n\nYou suggest that my concern about this &quot;unfit&quot; niche is not consi=\r\nstent with my support of novelty search, but novelty search is not really a=\r\nnalogous.  Novelty search is about constantly searching for *new* departure=\r\n points.  Your dead-weight niche is about keeping around the same bottom-le=\r\nvel junk forever.  Novelty search would quickly tire of such a black hole a=\r\nnd abandon it (it doesn&#39;t stay novel).  But multiobjective search will embr=\r\nace it for eternity.\n\nNature doesn&#39;t have anything analogous either, which =\r\nmeans there is at least some evidence that the &quot;fitness bias&quot; analogy with =\r\nnature is not lining up perfectly.  You might point to the continuing exist=\r\nence of single-celled organisms as something similar to the perpetual dead-=\r\nweight in this formulation, but they aren&#39;t really analogous because single=\r\n-celled organisms are functional - they retain the ability to make copies o=\r\nf themselves and continue to evolve in their own right - while the low-conn=\r\nectivity deadweight maintains no capability whatsoever.  On the other hand,=\r\n suspiciously, as in nature, nothing similar to such a deadweight niche is =\r\nperpetuated by a biased encoding.\n\nDoesn&#39;t it seem a little strange that th=\r\ne price we have to pay to obtain modular structure is to maintain a perpetu=\r\nal dead pool of genetic junk?  Note that it doesn&#39;t suggest that such a sys=\r\ntem won&#39;t work in some cases, but it&#39;s inelegant enough to raise questions =\r\nabout the best realization of the concept..\n\nAlso on the analogy with natur=\r\ne, while your argument for fitness pressure in nature based on the size of =\r\nthe head is logically possible (and the kind of argument that is probably a=\r\nttractive to a lot of people), it&#39;s only speculative.  In fact what little =\r\nevidence there is on such a speculative issue (i.e. whether a baby&#39;s head m=\r\night fit through a mother&#39;s pelvis in some alternate evolutionary path)  do=\r\nesn&#39;t support the idea that size is the main issue here.  After all, whale =\r\nbrains are far bigger than human brains and I&#39;m sure they are also dominate=\r\nd by local connectivity.  Conversely, human brains, or brains of any size f=\r\nor that matter, could have been exactly the same size but dominated by long=\r\n-range connections rather than short range ones.  In fact, in such a situat=\r\nion connectivity would actually be lower overall because long-range connect=\r\nions take up more space.  But the mother&#39;s anatomy poses no obstacle to suc=\r\nh a scenario.  The fact that we don&#39;t see such a connectivity in any specie=\r\ns is therefore plausibly a result of the fact that the encoding simply can&#39;=\r\nt describe it easily.\n\nI feel that you may not see what I&#39;m saying about en=\r\ncoding here, because you speak about encoding as if it has similar effects =\r\nto fitness pressure, but I think it&#39;s not the same.  You say: \n\n&quot;The reason=\r\ns biases work is because they do bias search towards some areas and away fr=\r\nom others: so I think both encoding biases and fitness penalties have simil=\r\nar effects in this regard.&quot;\n\nBut I don&#39;t think that&#39;s true for encoding.  T=\r\nhe difference with encoding is that it is not pushing the search towards an=\r\ny particular area within the space it induces.  Absent any kind of fitness =\r\nfunction or  selective pressure, encoding says nothing about which areas ar=\r\ne accessible.  Rather it simply says which types of phenotypes are overrepr=\r\nesented or underrepresented throughout the whole space of genotypes.  In ot=\r\nher words, even if an encoding is &quot;biased&quot; towards low connectivity, if you=\r\n happen to get into an area of the space with high connectivity, the encodi=\r\nng does not have to push you out of that area (it could be a dense subspace=\r\n full of high-connectivity structures).  But fitness bias would have to pus=\r\nh you out because all it does it push you out.  It can&#39;t bend or change dep=\r\nending on where you go.  Encoding can change with the times and has the won=\r\nderful additional potential for canalization, a bonus entirely absent from =\r\nfitness pressure.\n\nThe right level of modularity is likely on a delicate co=\r\nntinuum - not entirely one way or another, and probably varies by species. =\r\n You believe evolution can pay a kind of tax for going against the pressure=\r\n towards low connectivity: &quot;if a certain phenotype pays for its wiring by i=\r\nncreasing fitness, it can add high-connectivity areas anywhere that they ar=\r\ne useful.&quot;  But in a delicate balancing act where the best option is likely=\r\n some middle ground, that sounds too much like gambling and it&#39;s vulnerable=\r\n to deception in cases where there is no immediate fitness benefit (whereas=\r\n encoding is orthogonal to fitness).  With encoding you don&#39;t have the play=\r\n that game.  Encoding can create its own tendency towards some middle groun=\r\nd and canalize that tendency over time.  While you worry that modularity mi=\r\nght &quot;evolve away,&quot; the idea that it cannot evolve away to varying degrees s=\r\nounds worse to me.  Natural evolution is generally good about not keeping a=\r\nll its eggs in one basket - a trait may evolve away in some branches but no=\r\nt in others.  But for you to make an all-out attempt to bar such a deviatio=\r\nn from square one is making a lot of strong assumptions about what we want =\r\nto see 1 billion years in the future.\n\nSo I&#39;m still a fan of manipulating e=\r\nncoding over manipulating fitness.  But I would not entirely despair on fit=\r\nness because there will still be cases where there is no clear option for m=\r\nanipulating the encoding.  But such scenarios are not ones we should be hop=\r\ning for.\n\nBest,\n\nken\n\n\n--- In neat@yahoogroups.com, Jeff Clune &lt;jclune@...&gt;=\r\n wrote:\n&gt;\n&gt; Hello all,\n&gt; \n&gt; As Ken mentioned, we&#39;ve discussed these issues =\r\nin private. I&#39;m going to include some of my comments from one of those emai=\r\nl threads with slight modification, as I believe they summarize the views o=\r\nf Jean-Baptiste and I on the issues Ken raises. I&#39;ll then respond to a few =\r\nindividual comments by Ken afterwards. \n&gt; \n&gt; ---------------\n&gt; Ken,\n&gt; \n&gt; It=\r\n&#39;s great to hear your feedback on our paper. Thanks for sending it.\n&gt; \n&gt; Fi=\r\nrst off, thanks for the kind words. We&#39;re very glad you liked the paper and=\r\n think it is important. \n&gt; \n&gt; Regarding a selection pressure vs. an encodin=\r\ng bias. We&#39;re not convinced that an initial encoding bias is a good way to =\r\nencourage properties that one wants in phenotypes throughout evolution, suc=\r\nh as modularity. If there is any deceptiveness (or even neutrality) regardi=\r\nng modularity at any point during the run then the bias will disappear, and=\r\n then for the rest of evolutionary time nothing will encourage modularity. =\r\nWe are more convinced of the power of mutational bias in the encoding (i.e.=\r\n a constant encoding bias instead of just an initial encoding bias), and we=\r\n think it would be interesting to investigate area. However, if the encodin=\r\ng bias is under selection, then you have the same issue where it might evol=\r\nve away. Selective pressures are interesting because they are constant, so =\r\nyou&#39;re more likely to get what you want. That raises the point you mention =\r\nabout our pressure being too strong, such that evolution could not deviate =\r\nwhen it would be beneficial not to have modularity. That might be a problem=\r\n if the pressure is too strong, but it seems likely that in many cases the =\r\nbenefits in terms of performance for being non-modular will outweigh the co=\r\nst. In other words, evolution can decide to pay the cost of non-modularity =\r\nwhen it is useful (e.g. in your example of a hub of connections between mod=\r\nules).\n&gt; \n&gt; Regarding playing with an encoding being safer than playing wit=\r\nh selection pressures. Our view is that both are very complicated and can h=\r\nave unintended consequences, so playing with one is just as bad as the othe=\r\nr. I think our field is more familiar with unintended consequences of selec=\r\ntive pressures just because we historically tend to play with them more (an=\r\nd make simple encodings), but it is also very hard to intuit the consequenc=\r\nes of choices regarding biases in complex encodings. In your case the conse=\r\nquences are relatively intuitive, precisely because they are so minimally i=\r\nnterventionist...but that is also why I think they are not strong enough to=\r\n cause modularity except in cases (like retina) where all you have to do is=\r\n initially place evolution in the right attractor basin.\n&gt; \n&gt; Regarding the=\r\n resource hog waste of having a cost objective. I have to have a little fun=\r\n here and point out the irony of the co-champion of novelty search worrying=\r\n about the resources consumed by non-high-performing individuals! Hehe. As =\r\nyou&#39;ve persuaded me, I&#39;m more interested in an algorithm that is interestin=\r\ng or that works than spending a little computation inefficiently.\n&gt; \n&gt; I&#39;d =\r\nalso like to point out an innovation we came up with to mitigate the proble=\r\nm of preventing evolution from exploring solutions that are contrary to one=\r\n of the objectives. We recognized that the cost objective is ultimately les=\r\ns important than the performance objective. We wanted evolution to periodic=\r\nally ignore the cost objective to explore stepping stones that had higher c=\r\nonnectivity. To do that, we invented a technique that involves &quot;probabilist=\r\nic pareto dominance&quot;, wherein secondary objectives (in this case cost) are =\r\nfactored into pareto dominance only a small percentage of the time. That wo=\r\nn&#39;t solve the problem you mention if you have to take a long, many-multi-ge=\r\nnerational walk through high-connectivity areas of the search space, but it=\r\n does allow quick forays into that terrain without any fitness penalty. Thi=\r\ns technique could be used for any cost (or other) objective, so it is not s=\r\npecific to connectivity costs. \n&gt; \n&gt; See below for a few specific responses=\r\n to your comments. I should note that below this the thoughts are my own an=\r\nd Jean-Baptiste should not be blamed for any of them! (Feel free to blame h=\r\nim for things above this line=85we went over that text together a while bac=\r\nk). ;-)\n&gt; \n&gt; &gt; More generally the issue is the usual problem of deception, =\r\nwhich is compounded by anything you do with fitness. For example, in a comp=\r\nlex search space, there is a reasonable chance that the stepping stone to a=\r\n good low-connectivity solution is something with higher connectivity. By m=\r\nanipulating fitness, you are cutting out all chances of encountering such a=\r\n deceptive stepping stone. But even if you don&#39;t believe that could be true=\r\n, the single-mindedness of always favoring low-connectivity could deceive y=\r\nou from many parts of the search space that might be stepping stones to som=\r\nething worthwhile, relating to connection density or not.\n&gt; &gt; \n&gt; &gt; \n&gt; True.=\r\n But the same exact thing can be said for biases in the encoding: they prev=\r\nent you from searching large areas of the search space. You may reply that =\r\nit is only a bias, not a strict ban, but of course we know that in large se=\r\narch spaces biases hugely affect the landscape such that certain areas will=\r\n practically never be visited. \n&gt; \n&gt; &gt; On the other hand, manipulating the =\r\nencoding is different because in effect it actually reorganizes the structu=\r\nre of the search space itself, which seems to me a more principled thing to=\r\n do (if you can figure out a way to do it). Because the thing is, in that c=\r\nase, you do not need to worry about a permanent dead weight taking up some =\r\nproportion of your population forever. Instead, while the encoding may *ten=\r\nd* to produce e.g. low-connectivity solutions, it can still escape that ten=\r\ndency without any penalty to fitness.\n&gt; &gt; \n&gt; My instincts tell me that we c=\r\nreate dead weight with encoding biases too. For example, an overly regular =\r\ngenerative encoding (e.g. context free L-systems) is great if good solution=\r\ns are perfectly regular, but if what is required is a mix of regularity and=\r\n irregularity, then you spend your entire time producing only highly regula=\r\nr phenotypes that never wander into the appropriately irregular areas of th=\r\ne search space. Our IEEE TEC paper, for example, shows that HyperNEAT can s=\r\npend thousands of generations spinning its wheels never generating solution=\r\ns that HybrID could easily generate, demonstrating a &quot;overly regular&quot; dead =\r\nweight associated with the biases of even the best known* generative encodi=\r\nng! Both fitness penalties and biases can cause you to focus your search in=\r\n unproductive areas=85which is ultimately what dead weight is. \n&gt; \n&gt; * in o=\r\nur opinion! :0)\n&gt; \n&gt; &gt; Furthermore, in reality the best situation regarding=\r\n modularity and connectivity is probably rather subtle, with most of the br=\r\nain respecting the principle of low connectivity, but with a number of crit=\r\nical exceptions in key areas, such as major inter-module hubs. A sophistica=\r\nted encoding can allow its bias to bend to make such nuanced exceptions (e.=\r\ng. based on locations within a geometry), whereas a fitness penalty is a he=\r\navy hand and blunt instrument that cannot but help always to demand global =\r\nand holistic subservience to dogmatic universals (unless you are willing to=\r\n take a hit in fitness).\n&gt; &gt; \n&gt; &gt; \n&gt; I think the last clause you offer is t=\r\nhe key exception though. As I mentioned above, if a certain phenotype pays =\r\nfor its wiring by increasing fitness, it can add high-connectivity areas an=\r\nywhere that they are useful (without even needing to carve out that area in=\r\n geometric space, which is often a difficult task for CPPNs). Instead of be=\r\ning a blunt instrument, a fitness penalty can be quite subtle, because it c=\r\nan allow connection-by-connection exceptions if they produce fitness improv=\r\nements, and do so without any search overhead. \n&gt; \n&gt; &gt; An interesting quest=\r\nion in nature (where our brains evolved modular structure) is whether its t=\r\nendency towards low connectivity is a result of an aspect of fitness in the=\r\n wild, or an aspect of encoding bias. I think there is a lot of room in thi=\r\ns question for arguing either way, but my hunch is that the bias is mostly =\r\nin the encoding. My logic is that I think the reason that the connectivity =\r\nof the brain is so much lower than what it could be (e.g. it is a tiny frac=\r\ntion of everything-to-everything connectivity) is an artifact of physics ra=\r\nther than an artifact of fitness. It is simply physically impossible for a =\r\ngiant 100-billion-to-100-billion connectivity to fit in a head anything clo=\r\nse to our size. And physical impossibility is in some sense a property of e=\r\nncoding. That is, mutations that could step from a low-connectivity brain t=\r\no a high one are few and far between simply because of physical constraint.=\r\n So high-connectivity structures are simply a very small part of the search=\r\n space of brains in the physical universe. However, at the same time, you c=\r\nan still get long-range connections from time to time because there is no u=\r\nniversal penalty for doing so, just a lower a priori probability of such mu=\r\ntations occurring.\n&gt; &gt; \n&gt; &gt; \n&gt; Here I completely disagree with you. I see t=\r\nhe force preventing the volume of neural connections from getting too large=\r\n as a direct fitness cost, not an encoding bias. If mutations increase the =\r\nsize of the head, the baby and the mother are more likely to die in childbi=\r\nrth. Anthropologists have long known that evolution&#39;s desire to have larger=\r\n and larger brains is the main reason why humans have such ridiculously hig=\r\nh maternal and infant mortality &quot;in the wild&quot; (pre modern health care, and =\r\neven post). Our encoding keeps producing such mutants, and it&#39;s death (via =\r\nthe physical constraints of the pelvis) that keep them from being kept arou=\r\nnd. The historical accident of birthing through the pelvis aside, there wou=\r\nld still be fitness consequences for more vastly neural connections (e.g. n=\r\neurons are metabolically expensive, neural connections require energy to bu=\r\nild and maintain, and housing such large brains would create a large, clunk=\r\ny bobble head of a being that would be ungainly). It is possible that low c=\r\nonnectivity is an encoding bias, but were that true I think it would look l=\r\nike some sort of growth rule that only grew a few connections per neuron (e=\r\n.g. 10k). Evolution could have learned such a rule, and canalized that rule=\r\n in a way that makes it unlikely to have mutations that produce orders of m=\r\nagnitude more neurons, but my guess is that if it has done so, it was becau=\r\nse of the fitness costs associated with large numbers of neurons, not becau=\r\nse of evolvability (or due to some historical accident). I do think it is i=\r\nnteresting to study whether such biases exist in the encoding of neural gro=\r\nwth rules, but even if they do exist I don&#39;t think that shows that a fitnes=\r\ns cost was not the ultimate cause. Note: I recognize that you admit that th=\r\nis could be argued &quot;either way&quot;=85are these the sorts of arguments you envi=\r\nsioned as being the other way?\n&gt; \n&gt; &gt; In summary, the key difference betwee=\r\nn the alternatives is that with fitness you are saying &quot;stay out of this pa=\r\nrt of the search space&quot; whereas with encoding you are saying &quot;this part of =\r\nthe search space is much smaller and hence less likely to encounter.&quot;\n&gt; &gt; \n=\r\n&gt; &gt; \n&gt; I don&#39;t precisely understand your latter clause, but I think I under=\r\nstand the spirit of it. I disagree, though. The reasons biases work is beca=\r\nuse they do bias search towards some areas and away from others: so I think=\r\n both encoding biases and fitness penalties have similar effects in this re=\r\ngard. \n&gt; \n&gt; &gt; So, my speculation is that if you want to bias the search in =\r\nhighly complex domains, the best way is through the encoding. Fitness is a =\r\nnasty quagmire that is deceptively tempting to manipulate, but never plays =\r\nby the rules you wish it would. Of course, these are merely my own unproven=\r\n intuitions and their veracity remains to be demonstrated. But at least it&#39;=\r\ns something to think about.\n&gt; &gt; \n&gt; &gt; \n&gt; \n&gt; Thanks again for your feedback. =\r\nAs always, I appreciate your input and enjoy discussing these fascinating s=\r\nubjects. \n&gt; \n&gt; I want to end by clarifying that I agree that there are posi=\r\ntives and negatives to both approaches. I do not see it as such an obvious =\r\nchoice between the two as you do. As I mentioned up top: I definitely don&#39;t=\r\n think initial encoding biases that selection can get rid of will get us ve=\r\nry far. For simple problems they will work, but for any challenging problem=\r\n the initial bias will have long disappeared by the time it will matter. Wh=\r\nat we need is some constant force encouraging search to take promising path=\r\ns. A fitness cost is one way to do that, but a *constant* (or, at least, *p=\r\neriodic*) encoding bias could do that just as well, and perhaps better. \n&gt; =\r\n\n&gt; \n&gt; Best regards,\n&gt; Jeff Clune\n&gt; \n&gt; Assistant Professor\n&gt; Computer Scienc=\r\ne\n&gt; University of Wyoming\n&gt; jeffclune@...\n&gt; jeffclune.com\n&gt; \n&gt; \n&gt; &gt; Best,\n&gt;=\r\n &gt; \n&gt; &gt; ken\n&gt; &gt; \n&gt; &gt; --- In neat@yahoogroups.com, Alexandre Devert wrote:\n&gt;=\r\n &gt; &gt;\n&gt; &gt; &gt; Hi,\n&gt; &gt; &gt; \n&gt; &gt; &gt; =C2  Simple, clean experiment, with sharp resul=\r\nts, congrats on that, definitely\n&gt; &gt; &gt; a step forward ! Of course, it begs =\r\nfor more questions. I would love to hear\n&gt; &gt; &gt; you on such (fairly open) qu=\r\nestions\n&gt; &gt; &gt; \n&gt; &gt; &gt; =C2  =C2 1) Do you think that selection pressure for l=\r\now connectivity is sufficient in\n&gt; &gt; &gt; itself to evolve large coherent netw=\r\norks, or is it just a piece of the puzzle ?\n&gt; &gt; &gt; =C2  =C2 2) Do you see yo=\r\nur work as an indication that any approach biased to low\n&gt; &gt; &gt; connectivity=\r\n would reproduce the result ? Or does the way you guys enforced\n&gt; &gt; &gt; this =\r\nbias matters ?\n&gt; &gt; &gt; \n&gt; &gt; &gt; To me=C2 \n&gt; &gt; &gt; 1) =3D&gt; Part of the puzzle. Sho=\r\nuld see how well it scales for increasingly\n&gt; &gt; &gt; complex task, when the co=\r\nnnection graph gets bigger. A randomized=C2 \n&gt; &gt; &gt; search process=C2 on lar=\r\nge graph sounds not so efficient, need something to guide it.\n&gt; &gt; &gt; I advoc=\r\nate construction process that have a feedback from what the neuron=C2 \n&gt; &gt; =\r\n&gt; network is computing. Don&#39;t know how to do it without creepling computati=\r\nonal\n&gt; &gt; &gt; cost tho...\n&gt; &gt; &gt; 2) =3D&gt; I guess that the bias alone is enough,=\r\n the way to introduce it might\n&gt; &gt; &gt; not be such a big deal.=C2 \n&gt; &gt; &gt; \n&gt; &gt;=\r\n &gt; Again, great work, very helpful contribution :)\n&gt; &gt; &gt; \n&gt; &gt; &gt; Alex\n&gt; &gt; &gt; =\r\n=C2 \n&gt; &gt; &gt; Dr. Devert Alexandre\n&gt; &gt; &gt; Researcher at the Nature Inspired Com=\r\nputation and Applications Laboratory (NICAL)\n&gt; &gt; &gt; Lecturer at School Of So=\r\nftware Engineering of USTC\n&gt; &gt; &gt; ------------------------------------------=\r\n----------\n&gt; &gt; &gt; Homepage :=C2 http://www.marmakoide.org\n&gt; &gt; &gt; ------------=\r\n----------------------------------------\n&gt; &gt; &gt; 166 Renai Road, Dushu Lake H=\r\nigher Education Town\n&gt; &gt; &gt; Suzhou Industrial Park,\n&gt; &gt; &gt; Suzhou, Jiangsu, P=\r\neople&#39;s Republic of China\n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; &gt; _____________________________=\r\n___\n&gt; &gt; &gt; From: Jeff Clune \n&gt; &gt; &gt; To: neat users group group \n&gt; &gt; &gt; Cc: Jea=\r\nn-Baptiste Mouret ; Hod Lipson \n&gt; &gt; &gt; Sent: Thursday, February 7, 2013 1:57=\r\n AM\n&gt; &gt; &gt; Subject: [neat] New paper on why modules evolve, and how to evolv=\r\ne modular artificial neural networks\n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; &gt; =C2  \n&gt; &gt; &gt; Hello =\r\nall,\n&gt; &gt; &gt; \n&gt; &gt; &gt; I&#39;m extremely pleased to announce a new paper on a subjec=\r\nt that many--including myself--think is critical to making significant prog=\r\nress in our field: the evolution of modularity.=C2 \n&gt; &gt; &gt; \n&gt; &gt; &gt; Jean-Bapti=\r\nste Mouret, Hod Lipson and I have a new paper that=C2 \n&gt; &gt; &gt; \n&gt; &gt; &gt; 1) shed=\r\ns light on why modularity may evolve in biological networks (e.g. neural, g=\r\nenetic, metabolic, protein-protein, etc.)\n&gt; &gt; &gt; \n&gt; &gt; &gt; 2) provides a simple=\r\n technique for evolving neural networks that are modular and have increased=\r\n evolvability, in that they adapt faster to new environments. The modules t=\r\nhat formed solved subproblems in the domain.=C2 \n&gt; &gt; &gt; Cite:=C2 Clune J, Mo=\r\nuret J-B, Lipson H (2013) The evolutionary origins of modularity. Proceedin=\r\ngs of the Royal Society B. 280: 20122863.=C2 http://dx.doi.org/10.1098/rspb=\r\n.2012.2863=C2 (pdf)\n&gt; &gt; &gt; \n&gt; &gt; &gt; Abstract: A central biological question is=\r\n how natural organisms are so evolvable (capable of quickly adapting to new=\r\n environments). A key driver of evolvability is the widespread modularity o=\r\nf biological networks=E2=80&quot;their organization as functional, sparsely conn=\r\nected subunits=E2=80&quot;but there is no consensus regarding why modularity its=\r\nelf evolved. Although most hypotheses assume indirect selection for evolvab=\r\nility, here we demonstrate that the ubiquitous, direct selection pressure t=\r\no reduce the cost of connections between network nodes causes the emergence=\r\n of modular networks. Computational evolution experiments with selection pr=\r\nessures to maximize network performance and minimize connection costs yield=\r\n networks that are significantly more modular and more evolvable than contr=\r\nol experiments that only select for performance. These results will catalys=\r\ne research in numerous disciplines, such as neuroscience and genetics, and =\r\nenhance our ability to harness\n&gt; &gt; &gt; evolution for engineering purposes.\n&gt; =\r\n&gt; &gt; \n&gt; &gt; &gt; Video:=C2 http://www.youtube.com/watch?feature=3Dplayer_embedded=\r\n&v=3DSG4_aW8LMng\n&gt; &gt; &gt; \n&gt; &gt; &gt; There has been some nice coverage of this wor=\r\nk in the popular press, in case you are interested:\n&gt; &gt; &gt; National Geograph=\r\nic:=C2 http://phenomena.nationalgeographic.com/2013/01/30/the-parts-of-life=\r\n/MIT&#39;s Technology Review:=C2 http://www.technologyreview.com/view/428504/co=\r\nmputer-scientists-reproduce-the-evolution-of-evolvability/=C2 Fast Company:=\r\n=C2 http://www.fastcompany.com/3005313/evolved-brains-robots-creep-closer-a=\r\nnimal-learningCornell Chronicle:=C2 http://www.news.cornell.edu/stories/Jan=\r\n13/modNetwork.htmlScienceDaily:=C2 http://www.sciencedaily.com/releases/201=\r\n3/01/130130082300.htm\n&gt; &gt; &gt; \n&gt; &gt; &gt; Please let me know what you think and if=\r\n you have any questions. I hope this work will help our field move forward!=\r\n\n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; &gt; Best regards,\n&gt; &gt; &gt; Jeff Clune\n&gt; &gt; &gt; \n&gt; =\r\n&gt; &gt; Assistant Professor\n&gt; &gt; &gt; Computer Science\n&gt; &gt; &gt; University of Wyoming\n=\r\n&gt; &gt; &gt; jclune@\n&gt; &gt; &gt; jeffclune.com\n&gt; &gt; &gt;\n&gt; &gt; \n&gt; &gt; \n&gt; &gt;\n&gt;\n\n\n\n"}}