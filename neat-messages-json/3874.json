{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"s05hZ3zbSmcG2UKZEPrJmQWySUufja_ucL7rSpuVaGZbC2fNozbXxdJ_qCj3CREi9_ZPw260hPsJAgGazuSXKAaIeXCZog4R4joqRSDXkk7D","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Backpropagation and NEAT","postDate":"1205705133","msgId":3874,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGZyazVqZCtkazdiQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGZyaWw1OCtrM2I0QGVHcm91cHMuY29tPg=="},"prevInTopic":3873,"nextInTopic":3875,"prevInTime":3873,"nextInTime":3875,"topicId":3846,"numMessagesInTopic":41,"msgSnippet":"Andy, all I m saying is that local gradient-based optimization is not a magic bullet.    It s certainly useful in some problems, but in others it won t be a","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 65036 invoked from network); 16 Mar 2008 22:05:34 -0000\r\nX-Received: from unknown (66.218.67.96)\n  by m50.grp.scd.yahoo.com with QMQP; 16 Mar 2008 22:05:34 -0000\r\nX-Received: from unknown (HELO n39a.bullet.mail.sp1.yahoo.com) (66.163.168.133)\n  by mta17.grp.scd.yahoo.com with SMTP; 16 Mar 2008 22:05:34 -0000\r\nX-Received: from [216.252.122.216] by n39.bullet.mail.sp1.yahoo.com with NNFMP; 16 Mar 2008 22:05:34 -0000\r\nX-Received: from [66.218.69.2] by t1.bullet.sp1.yahoo.com with NNFMP; 16 Mar 2008 22:05:34 -0000\r\nX-Received: from [66.218.66.81] by t2.bullet.scd.yahoo.com with NNFMP; 16 Mar 2008 22:05:34 -0000\r\nDate: Sun, 16 Mar 2008 22:05:33 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;frk5jd+dk7b@...&gt;\r\nIn-Reply-To: &lt;fril58+k3b4@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: Re: Backpropagation and NEAT\r\nX-Yahoo-Group-Post: member; u=54567749; y=D1iy2rZ3Y4bySqQhNo0tQJJn7P84OM6k85jToKU9vj7HghOTiqqQ\r\nX-Yahoo-Profile: kenstanley01\r\n\r\nAndy, all I&#39;m saying is that local gradient-based optimization is not\na mag=\r\nic bullet.    It&#39;s certainly useful in some problems, but in\nothers it won&#39;=\r\nt be a big help.  In any case, the big-picture point I&#39;m\nmaking is that if =\r\nyou really want to focus on the most difficult\nmachine learning problems, t=\r\nhen local search is not much of a\nbreakthrough.  We&#39;re going to need a lot =\r\nmore than that.\n\nThe problem is that it can be a liability just as much as =\r\nit can be an\nadvantage.  Just as it can allow you to precisely tune very lo=\r\ncalized\nvalues where a GA may fail, it can also cause you to prematurely\nco=\r\nnverge.  In combination with a GA, it will sometimes work as you are\nhoping=\r\n, wherein the GA does the global work and the gradient search\nnicely optimi=\r\nzes each local area; however, it can just as easily work\nagainst that desir=\r\ned dynamic: It can cause the population to be pulled\ninto local optima too =\r\nearly and thereby thwart the GA&#39;s otherwise\nuseful ability to spread out ov=\r\ner the global space.  \n\nBasically, any blanket statement one might make wou=\r\nld be an\novergeneralization because the structure of individual problems ar=\r\ne so\ncomplex and varied.  \n\nIn fact, the real operational issue that you ar=\r\ne getting at is just\nthe step size of the search operator, i.e. the power o=\r\nf mutation and\nthe size of the learning rate constant in backpropagation.  =\r\nYou could\neven create a topsy-turvy hybrid in which backprop has a much hig=\r\nher\nlearning rate and the GA a much smaller mutation power!  In that case,\n=\r\nthe &quot;local&quot; search would actually be more global and the GA would be\noptimi=\r\nzing locally!  \n\nYou could even have multilevel GAs with different mutation=\r\n strengths\nat different granularities.  If you created a GA with a very tin=\r\ny\nmutation power, it will start to look a lot like the &quot;local search&quot;\nthat =\r\nyou are talking about, even though it is still a GA.\n\nThere are plenty of a=\r\nlgorithms that do mix among local and global\nsearch in various ways.  There=\r\n are GAs that change the mutation power\nover the course of the run accordin=\r\ng to stagnation.   There are\nvariants of backprop that change the learning =\r\nrate (as in simulated\nannealing) to let the solution simmer down into a loc=\r\nal area.\n\nYet none of these are guaranteed to work, and all can be thwarted=\r\n\nunder the right (i.e. wrong) conditions.  That&#39;s No Free Lunch.  And\nso it=\r\n&#39;s a little too simple to just say our solution is a hybrid.  A\nhybrid is u=\r\nseful in some cases, wrong in others.  \n\nIf you have experience where a hyb=\r\nrid worked better than a GA alone,\nthat is problem-specific.  There are cou=\r\nnterexamples for every example.  \n\nBut the really important issue is not wh=\r\nether or not we need\nglobal/local search.  That can work fine when applied =\r\nappropriately,\nbut the big issue is that we need *new* ideas if we are real=\r\nly going\nto tackle problems that are currently beyond our capabilities.  We=\r\n&#39;re\ntalking about things like information reuse in representation, high\ndim=\r\nensionality, high complexity, automatically learning regularities,\nand rest=\r\nructuring the search space to make it less deceptive.  These\nare issues tha=\r\nt are not addressed by gradient-based search.\n\nOne thing however is clear t=\r\no me:  The hardest problems are going to\nrequire the ability to change the =\r\ndimensionality of the representation\non the fly.  Of that much I am confide=\r\nnt.\n\nken\n\n\n\n\n--- In neat@yahoogroups.com, &quot;afcarl2&quot; &lt;a.carl@...&gt; wrote:\n&gt;\n&gt;=\r\n Perhaps the following two quotes from the Dakota Users Manual ( \n&gt; http://=\r\nwww.cs.sandia.gov/DAKOTA/software.html ) may be helpful to \n&gt; bring focus o=\r\nn the salient issues:\n&gt; \n&gt; \n&gt; 1) &quot;Multilevel Hybrid Optimization: This stra=\r\ntegy allows the user to \n&gt; specify a sequence of optimization methods, with=\r\n the results from one \n&gt; method providing the starting point for the next m=\r\nethod in the \n&gt; sequence. An example which is useful in many engineering de=\r\nsign \n&gt; problems involves the use of a nongradient-based global optimizatio=\r\nn\n&gt; method (e.g., genetic algorithm) to identify a promising region of \n&gt; t=\r\nhe parameter space, which feeds its results into a gradient-based \n&gt; method=\r\n (quasi-Newton, SQP, etc.) to perform an efficient local search \n&gt; for the =\r\noptimum point.&quot;, Sec. 3.8 Optimization Strategies, p. 59.\n&gt; \n&gt; \n&gt; 2) &quot;Rathe=\r\nr, EAs are better suited to optimization problems where \n&gt; conventional gra=\r\ndient-based optimization fails, such as situations \n&gt; where there are multi=\r\nple local optima and/or gradients are not \n&gt; available. In such cases, the =\r\ncomputational expense of an EA is \n&gt; warranted since other optimization met=\r\nhods are not applicable or \n&gt; impractical. In many optimization problems, E=\r\nAs often quickly \n&gt; identify promising regions of the design space where th=\r\ne global \n&gt; minimum may be located. However, an EA can be slow to converge =\r\nto\n&gt; the optimum. For this reason, it can be an effective approach to \n&gt; co=\r\nmbine the global search capabilities of a EA with the efficient \n&gt; local se=\r\narch of a gradient-based algorithm in a multilevel hybrid \n&gt; optimization s=\r\ntrategy. In this approach, the optimization starts by \n&gt; using a few iterat=\r\nions of a EA to provide the initial search for a \n&gt; good region of the para=\r\nmeter space (low objective function and/or \n&gt; feasible constraints), and th=\r\nen it switches to a gradient-based\n&gt; algorithm (using the best design point=\r\n found by the EA as its \n&gt; starting point) to perform an efficient local se=\r\narch for an optimum \n&gt; design point.&quot;, Sec. 2.4.7 Nongradient-based Optimiz=\r\nation via \n&gt; Evolutionary Algorithm, p. 43.\n&gt; \n&gt; Most real world problems a=\r\nre multiobjective in nature, with multiple \n&gt; non-linear inequality constra=\r\nints. The determination of the pareto \n&gt; front only magnifies the reference=\r\nd issues.\n&gt; \n&gt; And no, it is not an exageration, it is based upon experienc=\r\ne. \n&gt; Consider the computational sequence of events of the two alternatives=\r\n \n&gt; as measured in total overall fitness evaluations required to achieve \n&gt;=\r\n convergence to a local minima. The robust nature of GA to find a \n&gt; global=\r\n minima also works against quick convergence. NEAT is a \n&gt; significant inpr=\r\nomement over conventional GA, but the improvement is \n&gt; still inadequate to=\r\n overcome the computational cost in problem \n&gt; domains in which the fitness=\r\n evaluation is non-trivial.\n&gt; \n&gt; \n&gt; --- In neat@yahoogroups.com, &quot;Kenneth S=\r\ntanley&quot; &lt;kstanley@&gt; wrote:\n&gt; &gt;\n&gt; &gt; Andy, I may be misunderstanding some of =\r\nyour points about local \n&gt; search.\n&gt; &gt; \n&gt; &gt; First, the term &quot;local search&quot; =\r\nitself implies that we are talking\n&gt; &gt; about a method that has been develop=\r\ned exclusively to search in one\n&gt; &gt; local part of the search space.  But wh=\r\nat method is really intended\n&gt; &gt; for that purpose and why is that a good th=\r\ning?  The problem in\n&gt; &gt; difficult tasks in general is usually that whateve=\r\nr algorithm you \n&gt; are\n&gt; &gt; using gets *trapped* in a local area.  Therefore=\r\n, generally \n&gt; speaking,\n&gt; &gt; an algorithm that is designed to stay in one a=\r\nrea is probably a bad\n&gt; &gt; thing, not a good thing.  Only the most trivial p=\r\nroblems entail \n&gt; simply\n&gt; &gt; running up the first hillside that you see.  F=\r\nor that, all we would\n&gt; &gt; need is hill climbing.  \n&gt; &gt; \n&gt; &gt; In fact, one of=\r\n the liabilities of backprop is that it tends to be\n&gt; &gt; caught on local opt=\r\nima.  That is a serious problem and by no means a\n&gt; &gt; reason to recommend i=\r\nt.  \n&gt; &gt; \n&gt; &gt; You say, &quot;There are many times in which it is orders of magni=\r\ntude\n&gt; &gt; quicker for a non-GA local gradient search method to find a local\n=\r\n&gt; &gt; minima, than an equivalent GA to mutate through generations to find\n&gt; &gt;=\r\n the same local minima.&quot;\n&gt; &gt; \n&gt; &gt; Generally speaking finding a local minimu=\r\nm is easy for any \n&gt; algorithm.\n&gt; &gt;    A gradient technique is likely faste=\r\nr than a GA, but &quot;orders of\n&gt; &gt; magnitude?&quot;  In general, that sounds exager=\r\nated.  In fact, in the \n&gt; 90s\n&gt; &gt; before NEAT existed there were a number o=\r\nf papers written that\n&gt; &gt; compared neuroevolution to backprop and concluded=\r\n that \n&gt; neuroevolution\n&gt; &gt; was faster.  I do not think these results are m=\r\neaningful because the\n&gt; &gt; issue is highly domain-dependent, but it does sho=\r\nw that neither\n&gt; &gt; approach has a massive advantage over the other in gener=\r\nal.  In \n&gt; fact,\n&gt; &gt; claiming otherwise just winds up running into No Free =\r\nLunch.\n&gt; &gt; \n&gt; &gt; Yet in most problems what we really care about finding are =\r\nnot just\n&gt; &gt; any local optima but *particular* local optima (or the global =\r\n\n&gt; optimum)\n&gt; &gt; that satisfy a &quot;good enough&quot; criterion.  Thus we need to be=\r\n able to\n&gt; &gt; leave the local neighborhood entirely and go on a real search.=\r\n  For\n&gt; &gt; that kind of search, you need an algorithm that is designed not t=\r\no\n&gt; &gt; simply rush up the nearest hill.  \n&gt; &gt;  \n&gt; &gt; You also say, &quot;But a loc=\r\nal search method could just as easily wander\n&gt; &gt; free in the local fitness =\r\nlandscape, un-encumbered by organism\n&gt; &gt; topology issues, to find the local=\r\n minima.&quot;\n&gt; &gt; \n&gt; &gt; I do not understand what you mean by &quot;un-encumbered.&quot;  A=\r\nll search is\n&gt; &gt; within the confines of a certain topology, even if that to=\r\npology is\n&gt; &gt; set by the user.  The topology is what defines the search spa=\r\nce.  \n&gt; You\n&gt; &gt; cannot isolate search from the space being searched.  Furth=\r\nermore, \n&gt; if\n&gt; &gt; there is any encumbrance, it is the inability to change t=\r\nhe topology\n&gt; &gt; when it is found wanting.  Thus searching through different=\r\n \n&gt; topologies\n&gt; &gt; is the opposite of an encumbrance; it is a liberation.  =\r\n\n&gt; &gt; \n&gt; &gt; Along the same lines, this statement also seems misleading: &quot;The =\r\n\n&gt; moral\n&gt; &gt; of the story is that local search does not have to be constrai=\r\nned by\n&gt; &gt; organism topology, beyond that of providing the start point.&quot;\n&gt; =\r\n&gt; \n&gt; &gt; Again, local search is always constrained by topology.  Topology\n&gt; &gt;=\r\n defines the dimensions of the search. \n&gt; &gt; \n&gt; &gt; What you may not be consid=\r\nering is that changing the topology is\n&gt; &gt; changing the search space itself=\r\n.  It is an entirely different type \n&gt; of\n&gt; &gt; operation than moving *within=\r\n* a particular search space.  Adding a\n&gt; &gt; new connection adds a new dimens=\r\nion to space.  That is not the same \n&gt; as\n&gt; &gt; moving in any particular dire=\r\nction along any particular dimension,\n&gt; &gt; which is what you mean by &quot;local =\r\nsearch.&quot;\n&gt; &gt; \n&gt; &gt; In a broader picture, gradient search is not the panacea =\r\nthat we \n&gt; need.\n&gt; &gt; It is exactly the failures of such search that are the=\r\n reason that\n&gt; &gt; we are not able to handle these very difficult real world =\r\nproblems\n&gt; &gt; that you mention.  The big challenge is that the gradient in a=\r\n \n&gt; massive\n&gt; &gt; multidimensional space on a complex problem is almost certa=\r\ninly \n&gt; highly\n&gt; &gt; deceptive, and gradient techniques have nothing going fo=\r\nr them other\n&gt; &gt; than the gradient!  If the gradient is all we have to go o=\r\nn, how can\n&gt; &gt; that be a good thing?  \n&gt; &gt; \n&gt; &gt; It will certainly be necess=\r\nary to change topologies as a part of any\n&gt; &gt; effective formula for success=\r\n on extremely hard problems, among \n&gt; other\n&gt; &gt; ingredients that are are st=\r\nill being discovered and invented today.\n&gt; &gt; \n&gt; &gt; ken\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; --- In =\r\nneat@yahoogroups.com, &quot;afcarl2&quot; &lt;a.carl@&gt; wrote:\n&gt; &gt; &gt;\n&gt; &gt; &gt; Peter,\n&gt; &gt; &gt; \n=\r\n&gt; &gt; &gt; It appears that you are implicitly assuming that that the only \n&gt; way=\r\n \n&gt; &gt; &gt; to &quot;reach out and touch&quot; the fitness evaluation is via a NEAT \n&gt; &gt; =\r\n&gt; fabricated organism topology. Local search does not have to \n&gt; proceed \n&gt;=\r\n &gt; &gt; via organism topology. \n&gt; &gt; &gt; \n&gt; &gt; &gt; Usage of NEAT as a global search =\r\nmethod, as part of a \n&gt; hierarchical \n&gt; &gt; &gt; methodology, surely uses organi=\r\nsm topology as the local search \n&gt; start \n&gt; &gt; &gt; point. And in the case of y=\r\nour proposed backprop search on \n&gt; organism \n&gt; &gt; &gt; weight values, maintains=\r\n the same number and composition of nodes \n&gt; and \n&gt; &gt; &gt; associated connecti=\r\nvity. But a local search method could just as \n&gt; &gt; &gt; easily wander free in =\r\nthe local fitness landscape, un-encumbered \n&gt; by \n&gt; &gt; &gt; organism topology i=\r\nssues, to find the local minima.\n&gt; &gt; &gt; \n&gt; &gt; &gt; The very strong point of GA c=\r\nan also be its greatest weakness, in \n&gt; &gt; &gt; instances in which the computat=\r\nional resource requirements of the \n&gt; &gt; &gt; fitness evaluation, in light of t=\r\nhe dimensionality and hyper \n&gt; volume \n&gt; &gt; &gt; size and complexity, become no=\r\nn-trivial.\n&gt; &gt; &gt; \n&gt; &gt; &gt; This is especially true in the case of non-adaptive=\r\n mutation \n&gt; &gt; &gt; parameters, as is currently the case with NEAT. There are =\r\nmany \n&gt; times \n&gt; &gt; &gt; in which it is orders of magnitude quicker for a non-G=\r\nA local \n&gt; &gt; &gt; gradient search method to find a local minima, than an equiv=\r\nalent \n&gt; GA \n&gt; &gt; &gt; to mutate through generations to find the same local min=\r\nima. \n&gt; &gt; &gt; \n&gt; &gt; &gt; NEAT speciation and niche protection help to mitigate th=\r\ne problem \n&gt; via \n&gt; &gt; &gt; population size and maintaining multiple species, b=\r\nut at a \n&gt; &gt; &gt; computational cost. But a hierarchical search methodology ca=\r\nn \n&gt; apply \n&gt; &gt; &gt; the strengths of both GA and local search, without having=\r\n to use \n&gt; &gt; &gt; computational power to cover-up GA&#39;s weak points.\n&gt; &gt; &gt; \n&gt; &gt;=\r\n &gt; The remaining question as to whether to a) re-encode the final \n&gt; &gt; &gt; de=\r\nstination of the local search back into the organism&#39;s \n&gt; topology, or \n&gt; &gt;=\r\n &gt; b) simply take the final/best fitness derived by the local search \n&gt; &gt; &gt;=\r\n (using the organism&#39;s original topology as the start point), and \n&gt; &gt; &gt; as=\r\nsociate it with the organism and it&#39;s original topology, is up \n&gt; for \n&gt; &gt; =\r\n&gt; debate and/or personal preference.\n&gt; &gt; &gt; \n&gt; &gt; &gt; The moral of the story is=\r\n that local search does not have to be \n&gt; &gt; &gt; constrained by organism topol=\r\nogy, beyond that of providing the \n&gt; start \n&gt; &gt; &gt; point. \n&gt; &gt; &gt; \n&gt; &gt; &gt; Real=\r\n world problems are so complex in light of current computer \n&gt; &gt; &gt; speeds a=\r\nnd fitness computation requirements, to render GA alone \n&gt; to be \n&gt; &gt; &gt; com=\r\nputationally unpractical in many instances, even with the \n&gt; obvious \n&gt; &gt; &gt;=\r\n benefits that NEAT brings to the table.\n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; &gt; --- In =\r\nneat@yahoogroups.com, &quot;petar_chervenski&quot; \n&gt; &gt; &gt; &lt;petar_chervenski@&gt; wrote:\n=\r\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; Well actually speciation takes care of this. Species are \n&gt;=\r\n allowed to \n&gt; &gt; &gt; &gt; exist until they stagnate for too long time. If some n=\r\new \n&gt; structure \n&gt; &gt; &gt; &gt; appears through mutations, the mutated individuals=\r\n are \n&gt; separated in \n&gt; &gt; &gt; &gt; another species. Each species is a local prot=\r\nected competition \n&gt; &gt; &gt; among \n&gt; &gt; &gt; &gt; individuals grouped by similarity. =\r\nConsider it as a GA \n&gt; performed on \n&gt; &gt; &gt; &gt; near identical topologies. The=\r\nn you can see NEAT as a algorithm \n&gt; &gt; &gt; &gt; running multiple GAs. So this is=\r\n actually what you mean by \n&gt; dynamic \n&gt; &gt; &gt; &gt; programming.. or something. =\r\nIn fact this scheme is far better \n&gt; than \n&gt; &gt; &gt; &gt; it. \n&gt; &gt; &gt; &gt; As for the =\r\nidea of speculative structure, this is the core of \n&gt; NEAT \n&gt; &gt; &gt; &gt; and it =\r\nis actually Ken&#39;s idea :) \n&gt; &gt; &gt; &gt; Colin Green&#39;s idea is about phased searc=\r\nhing, as far as I know. \n&gt; It \n&gt; &gt; &gt; is \n&gt; &gt; &gt; &gt; that after some structure =\r\nis added through complexifying, a \n&gt; &gt; &gt; &gt; simplifying phase kicks in, remo=\r\nving any unnecessary structure, \n&gt; &gt; &gt; thus \n&gt; &gt; &gt; &gt; returning the search d=\r\nown to a baseline low dimentional space \n&gt; while \n&gt; &gt; &gt; &gt; retaining the fit=\r\nness (because of elitism). \n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; Peter\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; --- In =\r\nneat@yahoogroups.com, c f &lt;christofer_fransson@&gt; wrote:\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt;=\r\n In dynamic programming the idea is to divide the\n&gt; &gt; &gt; &gt; &gt; solution in ste=\r\nps and then for each step present a\n&gt; &gt; &gt; &gt; &gt; fixed number of possible solu=\r\ntions.\n&gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; Collin Greens idea is that speculative nodes ar=\r\ne added\n&gt; &gt; &gt; &gt; &gt; to the solutions but it might take time/generations\n&gt; &gt; &gt;=\r\n &gt; &gt; before an added node are shown to be useful. \n&gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; Is =\r\nit possible to apply dynamic programming approach\n&gt; &gt; &gt; &gt; &gt; to this area, t=\r\no evolve NEAT driven networks?\n&gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; To combine local optimi=\r\nzation and dynamic programming\n&gt; &gt; &gt; &gt; &gt; ideas?\n&gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; Br,\n&gt; =\r\n&gt; &gt; &gt; &gt; Christofer\n&gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; \n&gt;=\r\n &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; =\r\n\n&gt; &gt; &gt; &gt; &gt; --- petar_chervenski &lt;petar_chervenski@&gt;\n&gt; &gt; &gt; &gt; &gt; wrote:\n&gt; &gt; &gt; =\r\n&gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; Given the simplest topology (a perceptron\n&gt; &gt; &gt; &gt; &gt; &gt; stru=\r\ncture), the local \n&gt; &gt; &gt; &gt; &gt; &gt; minima is just one. Perceptrons are always\n&gt;=\r\n &gt; &gt; &gt; &gt; &gt; guaranteed to converge on \n&gt; &gt; &gt; &gt; &gt; &gt; correct weights. But incr=\r\neasing the dimentionality\n&gt; &gt; &gt; &gt; &gt; &gt; of the solution \n&gt; &gt; &gt; &gt; &gt; &gt; increase=\r\ns the error surface&#39;s curvature as well. So\n&gt; &gt; &gt; &gt; &gt; &gt; more dimentions \n&gt; =\r\n&gt; &gt; &gt; &gt; &gt; means more complex error surface. The coolest thing\n&gt; &gt; &gt; &gt; &gt; &gt; i=\r\nn NEAT is that \n&gt; &gt; &gt; &gt; &gt; &gt; when it increases the dimentionality of the\n&gt; &gt;=\r\n &gt; &gt; &gt; &gt; solution, the individuals \n&gt; &gt; &gt; &gt; &gt; &gt; are already located in a pr=\r\nomising area of the new\n&gt; &gt; &gt; &gt; &gt; &gt; space. In fact \n&gt; &gt; &gt; &gt; &gt; &gt; those space=\r\ns are related to each other - you don&#39;t\n&gt; &gt; &gt; &gt; &gt; &gt; know how the error \n&gt; &gt;=\r\n &gt; &gt; &gt; &gt; surface is going to look like when you enter the new\n&gt; &gt; &gt; &gt; &gt; &gt; s=\r\npace with more \n&gt; &gt; &gt; &gt; &gt; &gt; dimentions. There are unlimited possibilities. =\r\n\n&gt; &gt; &gt; &gt; &gt; &gt; So what local gradient search will do in essence is\n&gt; &gt; &gt; &gt; &gt; =\r\n&gt; pushing the \n&gt; &gt; &gt; &gt; &gt; &gt; weights towards the *local* minumim.. It is not\n=\r\n&gt; &gt; &gt; &gt; &gt; &gt; guaranteed that this \n&gt; &gt; &gt; &gt; &gt; &gt; is the *solution*! It is simp=\r\nly because you don&#39;t\n&gt; &gt; &gt; &gt; &gt; &gt; know the solution&#39;s \n&gt; &gt; &gt; &gt; &gt; &gt; dimention=\r\nality at first. It may require 3 or\n&gt; &gt; &gt; &gt; &gt; &gt; 21342532 dimentions. \n&gt; &gt; &gt;=\r\n &gt; &gt; &gt; Don&#39;t forget that NEAT complexifies solutions\n&gt; &gt; &gt; &gt; &gt; &gt; incrementa=\r\nly. \n&gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; Peter\n&gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; --- In neat@ya=\r\nhoogroups.com, &quot;afcarl2&quot; &lt;a.carl@&gt;\n&gt; &gt; &gt; &gt; &gt; &gt; wrote:\n&gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;=\r\n &gt; &gt; &gt; In fact, it may be that a substancial portion of\n&gt; &gt; &gt; &gt; &gt; &gt; the val=\r\nue-added of \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; speciation and niche protection of infant\n&gt; &gt; &gt; =\r\n&gt; &gt; &gt; organisms, is associated \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; with providing opportunity to=\r\n accumulate\n&gt; &gt; &gt; &gt; &gt; &gt; sufficient neighborhood \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; evaluations =\r\nto &quot;discover&quot; the same local minimia\n&gt; &gt; &gt; &gt; &gt; &gt; over multiple \n&gt; &gt; &gt; &gt; &gt; &gt;=\r\n &gt; generations, that a local search may discover in\n&gt; &gt; &gt; &gt; &gt; &gt; one generat=\r\nion. \n&gt; &gt; &gt; &gt; &gt; &gt; And \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; maintaining multiple species in hope t=\r\nhat one of\n&gt; &gt; &gt; &gt; &gt; &gt; the local minimia \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; will in fact also b=\r\ne the global minimia.\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; --- In neat@yahoogroups.=\r\ncom, &quot;afcarl2&quot; &lt;a.carl@&gt;\n&gt; &gt; &gt; &gt; &gt; &gt; wrote:\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;=\r\n If &quot;most individuals in a species represented by\n&gt; &gt; &gt; &gt; &gt; &gt; a given \n&gt; &gt; =\r\n&gt; &gt; &gt; &gt; topology&quot; \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; ended up in &quot;the same local minimia&quot;, on=\r\ne could\n&gt; &gt; &gt; &gt; &gt; &gt; argue that the \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; subject specie&#39;s logica=\r\nl end point was the same\n&gt; &gt; &gt; &gt; &gt; &gt; local minimia, \n&gt; &gt; &gt; &gt; &gt; &gt; and \n&gt; &gt; &gt;=\r\n &gt; &gt; &gt; &gt; &gt; that the cost of maintaining more than one\n&gt; &gt; &gt; &gt; &gt; &gt; organism =\r\nwas \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; computationally wasteful. Better to know sooner\n&gt; &gt; &gt; =\r\n&gt; &gt; &gt; and breed \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; additional \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; organisms of dif=\r\nfering topology so as to\n&gt; &gt; &gt; &gt; &gt; &gt; maintain the population \n&gt; &gt; &gt; &gt; &gt; &gt; &gt;=\r\n size \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; and maximize the population&#39;s &quot;effective&quot;\n&gt; &gt; &gt; &gt; &gt; =\r\n&gt; diversity.\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; Paying more for the same answ=\r\ner does not make it\n&gt; &gt; &gt; &gt; &gt; &gt; a better answer.\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt;=\r\n &gt; &gt; &gt; --- In neat@yahoogroups.com, &quot;petar_chervenski&quot; \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &lt;pe=\r\ntar_chervenski@&gt; wrote:\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; Well I think th=\r\nat encoding the resulting\n&gt; &gt; &gt; &gt; &gt; &gt; weights back to the \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; ge=\r\nnome \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; would somehow hurt the population weight\n&gt; &gt; &gt; &gt; &gt; =\r\n&gt; diversity, since most \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; individuals in a species represe=\r\nnted by a\n&gt; &gt; &gt; &gt; &gt; &gt; given topology can \n&gt; &gt; &gt; &gt; &gt; &gt; end \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; up=\r\n \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; in the same local minima, thus leaving out a\n&gt; &gt; &gt; &gt; &gt; =\r\n&gt; species with the \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; nearly \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; same individu=\r\nals, i.e. clones. \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; This is why I think that backprop shou=\r\nld be\n&gt; &gt; &gt; &gt; &gt; &gt; applied occasionaly \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; after \n&gt; &gt; &gt; &gt; &gt; &gt; &gt;=\r\n &gt; &gt; long periods of stagnation, for example the\n&gt; &gt; &gt; &gt; &gt; &gt; cases where de=\r\nlta-\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; coding \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; kicks in, when it focuses th=\r\ne search in the\n&gt; &gt; &gt; &gt; &gt; &gt; most promising \n&gt; &gt; &gt; &gt; &gt; &gt; areas \n&gt; &gt; &gt; &gt; &gt; &gt; =\r\n&gt; of \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; the search space. \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; I am still try=\r\ning to re-implement RTRL myself,\n&gt; &gt; &gt; &gt; &gt; &gt; though.. Then \n&gt; &gt; &gt; &gt; &gt; &gt; I&#39;l=\r\nl \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; see \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; if it is going to actually enhanc=\r\ne\n&gt; &gt; &gt; &gt; &gt; &gt; performance. \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; Peter\n&gt; &gt; =\r\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; --- In neat@yahoogroups.com, &quot;Kenneth Stan=\r\nley&quot;\n&gt; &gt; &gt; &gt; &gt; &gt; &lt;kstanley@&gt; \n&gt; &gt; &gt; &gt; &gt; &gt; wrote:\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; =\r\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; Rafael, thank you for pointing out the\n&gt; &gt; &gt; &gt; &gt; &gt; connection=\r\n to memetic \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; algorithms.  That is good to point out tha=\r\nt\n&gt; &gt; &gt; &gt; &gt; &gt; such a \n&gt; &gt; &gt; &gt; &gt; &gt; combination \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; falls \n&gt; &gt;=\r\n &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; under that category.\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; =\r\n&gt; &gt; However, there are still those who would\n&gt; &gt; &gt; &gt; &gt; &gt; argue that the loc=\r\nal \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; search \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; method should not be enco=\r\nded back into the\n&gt; &gt; &gt; &gt; &gt; &gt; genome, that is, \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; that \n&gt; &gt; &gt; &gt;=\r\n &gt; &gt; &gt; &gt; &gt; &gt; evolution should simply search for the best\n&gt; &gt; &gt; &gt; &gt; &gt; starti=\r\nng point \n&gt; &gt; &gt; &gt; &gt; &gt; from \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; which \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; a =\r\nlocal search would depart.  Because of the\n&gt; &gt; &gt; &gt; &gt; &gt; Baldwin Effect, \n&gt; &gt;=\r\n &gt; &gt; &gt; &gt; &gt; that \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; may \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; even work bette=\r\nr.\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; Personally, I do not know which=\r\n approach\n&gt; &gt; &gt; &gt; &gt; &gt; would work better \n&gt; &gt; &gt; &gt; &gt; &gt; but \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; b=\r\noth \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; are viable and it is probably domain\n&gt; &gt; &gt; &gt; &gt; &gt; d=\r\nependent.\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; ken\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; =\r\n\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; --- In neat@yahoogroups.com, &quot;Rafael C.P.&quot;\n&gt; &gt; &gt; &gt; &gt; &gt;=\r\n &lt;kurama.youko.br@&gt; \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; wrote:\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt;=\r\n &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; Ken, it doesn&#39;t fit pure evolution but it\n&gt; &gt; &gt; &gt; &gt; &gt; fits=\r\n memetic \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; algorithms, \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; that\n&gt; &gt; &gt; &gt; &gt;=\r\n &gt; &gt; &gt; &gt; &gt; &gt; consists exactly of evolution alternated\n&gt; &gt; &gt; &gt; &gt; &gt; with loca=\r\nl search \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; methods \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; for fine\n&gt; &gt; &gt; &gt; &gt;=\r\n &gt; &gt; &gt; &gt; &gt; &gt; tunning (just few steps). NEAT+BP may\n&gt; &gt; &gt; &gt; &gt; &gt; become a goo=\r\nd memetic \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; algorithm for\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; neural n=\r\networks.\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; On Mon, Mar 10, 2008 =\r\nat 2:19 PM, Kenneth\n&gt; &gt; &gt; &gt; &gt; &gt; Stanley &lt;kstanley@&gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; w=\r\nrote:\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;   Peter, I believe tha=\r\nt backprop can\n&gt; &gt; &gt; &gt; &gt; &gt; potentially improve \n&gt; &gt; &gt; &gt; &gt; &gt; the\n&gt; &gt; &gt; &gt; &gt; &gt;=\r\n &gt; &gt; &gt; &gt; &gt; &gt; accuracy. It has been shown to work\n&gt; &gt; &gt; &gt; &gt; &gt; effectively wi=\r\nth \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; neurevolution\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; in classificati=\r\non tasks in the past. So\n&gt; &gt; &gt; &gt; &gt; &gt; in principle it \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; could\n&gt;=\r\n &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; help. Of course, there is always the\n&gt; &gt; &gt; &gt; &gt; &gt; cha=\r\nnce that it will \n&gt; &gt; &gt; &gt; &gt; &gt; not\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; enhance performan=\r\nce as well.\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; One issue I wou=\r\nld also consider is that\n&gt; &gt; &gt; &gt; &gt; &gt; some people \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; disagree \n&gt;=\r\n &gt; &gt; &gt; &gt; &gt; &gt; &gt; on\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; whether the changes to weights fr=\r\nom\n&gt; &gt; &gt; &gt; &gt; &gt; backprop should be \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; encoded \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;=\r\n &gt; &gt; back\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; into the genome or not. If it is\n&gt; &gt; &gt; &gt; =\r\n&gt; &gt; actually encoded back \n&gt; &gt; &gt; &gt; &gt; &gt; into \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; the\n&gt; &gt; &gt; &gt; &gt; =\r\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; genome, that is &quot;Lamarckian&quot; evolution\n&gt; &gt; &gt; &gt; &gt; &gt; because in=\r\n effect \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; what \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; the\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; =\r\norganism learned over its lifetime is\n&gt; &gt; &gt; &gt; &gt; &gt; encoded into its own\n&gt; &gt; =\r\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; =3D=3D=3D message truncated =3D=3D=3D\n&gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt;=\r\n &gt; \n&gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt;       \n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; _________________________=\r\n_____________________________________________\n&gt; &gt; &gt; &gt; ______________\n&gt; &gt; &gt; =\r\n&gt; &gt; Never miss a thing.  Make Yahoo your home page. \n&gt; &gt; &gt; &gt; &gt; http://www.y=\r\nahoo.com/r/hs\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt;\n&gt;\n\n\n\n"}}