{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":211599040,"authorName":"Jeff Clune","from":"Jeff Clune &lt;jclune@...&gt;","profile":"jeffreyclune","replyTo":"LIST","senderId":"OavIqifLwE0-UBNncz-fvcXVMmnuUhKWKdWKYk-o4zT_REQehXUIGnnfme9mqO7rCp7wD7keWy_xJKMV59jh2NpS","spamInfo":{"isSpam":false,"reason":"12"},"subject":"FW: [neat] Re: Machine Learning and the Long View of AI","postDate":"1209513804","msgId":4012,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PEM0M0QyRjhDLjIyODc0JWpjbHVuZUBtc3UuZWR1Pg==","inReplyToHeader":"PGZ2NjE3bCtna2hkQGVHcm91cHMuY29tPg=="},"prevInTopic":4011,"nextInTopic":4013,"prevInTime":4011,"nextInTime":4013,"topicId":3955,"numMessagesInTopic":49,"msgSnippet":"Hello Ken- You make a very strong defense of your position. I have been entirely persuaded that in the multi-agent paper it was appropriate to use the r(x) ","rawEmail":"Return-Path: &lt;jclune@...&gt;\r\nX-Sender: jclune@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 86905 invoked from network); 30 Apr 2008 00:03:29 -0000\r\nX-Received: from unknown (66.218.67.95)\n  by m49.grp.scd.yahoo.com with QMQP; 30 Apr 2008 00:03:29 -0000\r\nX-Received: from unknown (HELO py-out-1112.google.com) (64.233.166.181)\n  by mta16.grp.scd.yahoo.com with SMTP; 30 Apr 2008 00:03:29 -0000\r\nX-Received: by py-out-1112.google.com with SMTP id f47so232000pye.14\n        for &lt;neat@yahoogroups.com&gt;; Tue, 29 Apr 2008 17:03:28 -0700 (PDT)\r\nX-Received: by 10.35.92.18 with SMTP id u18mr107098pyl.33.1209513808438;\n        Tue, 29 Apr 2008 17:03:28 -0700 (PDT)\r\nReturn-Path: &lt;jclune@...&gt;\r\nX-Received: from ?192.168.2.2? ( [67.167.130.112])\n        by mx.google.com with ESMTPS id f57sm1577854pyh.6.2008.04.29.17.03.25\n        (version=TLSv1/SSLv3 cipher=OTHER);\n        Tue, 29 Apr 2008 17:03:27 -0700 (PDT)\r\nUser-Agent: Microsoft-Entourage/12.1.0.080305\r\nDate: Tue, 29 Apr 2008 20:03:24 -0400\r\nTo: &quot;neat@yahoogroups.com&quot; &lt;neat@yahoogroups.com&gt;\r\nMessage-ID: &lt;C43D2F8C.22874%jclune@...&gt;\r\nThread-Topic: [neat] Re: Machine Learning and the Long View of AI\r\nThread-Index: AciqVZug2WapBeXhCUa8ymf2zE6jeQ==\r\nIn-Reply-To: &lt;fv617l+gkhd@...&gt;\r\nMime-version: 1.0\r\nContent-type: text/plain;\n\tcharset=&quot;US-ASCII&quot;\r\nContent-transfer-encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Jeff Clune &lt;jclune@...&gt;\r\nSubject: FW: [neat] Re: Machine Learning and the Long View of AI\r\nX-Yahoo-Group-Post: member; u=211599040; y=dmhV1sHHTtsSMavv8_ffYQMu5GYBkQ1BGXrhKkD-SeX0w_pc5QRw\r\nX-Yahoo-Profile: jeffreyclune\r\n\r\nHello Ken-\n\nYou make a very strong defense of your position. I have been entirely\npersuaded that in the multi-agent paper it was appropriate to use the r(x)\nstrategy. However, I continue to think that the issue comes down to what\none&#39;s goal is. Your goal here was to evolve sophisticated multi-agent\ncontrollers. Given that goal, you are right that adding the extra challenge\nof figuring out where one brain begins and ends is not necessary. However,\nwere one&#39;s goal to test how good HyperNEAT is at discovering and exploiting\nregularities in the problem space, in order to know how well it will perform\nwhen it encounters unanticipated regularities, it could be worthwhile to see\nhow it does without the injected information.\n\nWhile I do think that evolution is cool, and like to show its prowess, I too\nam mainly interested in using evolution to produce general AI. My\nsupposition was that figuring out to what extent our algorithms are able to\nexploit problem regularities may facilitate improvements on that font. I\nalso think such improvements will be necessary steps in the path to\nproducing evolutionary algorithms capable of creating general AI.\n\nNevertheless, I benefited from hearing your perspective and appreciate the\nexchange. \n\n\nCheers,\nJeff Clune\n\nDigital Evolution Lab, Michigan State University\n\njclune@...\n\n\n\n------ Forwarded Message\n&gt; From: Kenneth Stanley &lt;kstanley@...&gt;\n&gt; Reply-To: &quot;neat@yahoogroups.com&quot; &lt;neat@yahoogroups.com&gt;\n&gt; Date: Tue, 29 Apr 2008 02:30:13 -0000\n&gt; To: &quot;neat@yahoogroups.com&quot; &lt;neat@yahoogroups.com&gt;\n&gt; Subject: [neat] Re: Machine Learning and the Long View of AI\n&gt; \n&gt; Jeff,\n&gt; \n&gt; I think it would be interesting to step back and look at the\n&gt; assumptions that underly your straw man argument.  In particular, why\n&gt; are we using evolution in the first place?\n&gt; \n&gt; Your straw man implies a world view wherein we are using evolution\n&gt; because we like evolution and want it to succeed.  Under that\n&gt; philosophy, then indeed, creating a super-powerful neural network\n&gt; (DeepNet) by hand and then creating a faux-mutation operator that\n&gt; simply makes a neural network turn into DeepNet would be a\n&gt; disappointment, because it would mean that evolution didn&#39;t really do\n&gt; what we wanted, and we would be being disingenuous (and unimpressed).\n&gt; \n&gt; However, my world view is different.  I am not using evolution because\n&gt; I like evolution and want to prove that it is impressive. (Note that I\n&gt; *do* like evolution, but that&#39;s not the reason I use it.)  Rather, I\n&gt; am using evolution because I genuinely believe that full-fledged AI\n&gt; likely *cannot* be constructed by hand, and that evolution is the best\n&gt; alternative.  If someone went ahead and built a general-AI neural\n&gt; network by hand, it would simply prove me wrong.  But it would mean\n&gt; nothing with respect to how we should go about doing the same with\n&gt; evolution (which would however be a moot point at that point anyway\n&gt; because why bother when someone figured out a better way?).  So as\n&gt; soon as someone did what you said, evolution would be thrown out\n&gt; anyway, at least in terms of being the best path to general AI.\n&gt; \n&gt; So I am looking at things in a kind of reverse perspective from you.\n&gt; To me, the point is not to bolster up evolution and show how powerful\n&gt; it is.  Rather, the point is that I believe it *is* powerful and\n&gt; therefore I am using it.  If I can do something to boost it further,\n&gt; good.  What else is there to prove?\n&gt; \n&gt; However, I am a realist and I doubt that evolution alone will get the\n&gt; job done in the long view.  It&#39;s just too gigantic a search space and\n&gt; the problem is too poorly specified.  Therefore, I think there will be\n&gt; a lot of biases and manipulations along the way.\n&gt; \n&gt; I&#39;m way off speculating about the far off future here, but my guess is\n&gt; that those manipulations will come mostly at the genetic level rather\n&gt; than the ANN level.  In other words, the kinds of hacks that you are\n&gt; talking about (&quot;building blocks&quot; provided a priori) generally seem to\n&gt; be kind of neural &quot;modules&quot; that are built a priori and just dumped\n&gt; into the network en masse.  Those are indeed a bit cringe-inducing.\n&gt; However, my problem with them is not that they are cringe-inducing.\n&gt; Rather, again, I doubt they will really be a big help in the long run.\n&gt;  The reason I doubt their utility is because I believe that a massive\n&gt; brain needs to be also massively interwoven, such that each internal\n&gt; area of each part is entirely accessible and &quot;speaks the language&quot; of\n&gt; any other part.  Some ad hoc module thrown in the mix, while perhaps\n&gt; helpful in the short run, will never be able to fill that role because\n&gt; it was not built along with the rest of the infrastructure.  So that&#39;s\n&gt; why I&#39;m against it: Not because it&#39;s cheating, but because it won&#39;t work.\n&gt; \n&gt; So I think you have to distinguish between that type of hack and the\n&gt; kind of thing where we provide sort of &quot;genetically-engineered&quot;\n&gt; information, i.e. at the genetic level.  That I do believe will be\n&gt; useful, and should be exploited, because those are knobs and\n&gt; coordinate frames upon which a castle can be built.  So providing\n&gt; coordinate systems that are useful seems to me likely *long-run*\n&gt; useful.  It is not the same as telling it how to connect up, and the\n&gt; substrate that pops out in the end is going to be as pure as any:\n&gt; completely ANN through and through and totally a product of the\n&gt; indirect encoding.\n&gt; \n&gt; Finally, I think you are inferring too much about how much prior\n&gt; information I am advocating based on just Multiagent HyperNEAT.  There\n&gt; is no animal on earth that has to develop five disconnected brains on\n&gt; a single sheet with five compartments.  Expecting evolution to just\n&gt; figure out where one brain begins and the other ends seems to me very\n&gt; unnatural and bizarre, and also uninteresting.  Statistically\n&gt; speaking, it is evident that *any* intelligence would take longer to\n&gt; figure that out and solve the problem on average than one that was\n&gt; provided such information a priori.  So there&#39;s no surprise in that.\n&gt; \n&gt; So of course HyperNEAT performs worse without knowing the divisions\n&gt; between separate brains on a substrate than when it knows them up\n&gt; front.  That doesn&#39;t imply  that HyperNEAT cannot figure it out on its\n&gt; own, or that I think it doesn&#39;t matter if HyperNEAT can find\n&gt; regularities on its own.  It&#39;s just, it would take a while longer and\n&gt; would be less reliable, so why bother waiting?  The spatial divisions\n&gt; among the brains is ad hoc (something we simply decided a priori by\n&gt; fiat) and thus is not the interesting issue.\n&gt; \n&gt; I don&#39;t think it will be the same in a lot of non-multiagent tasks\n&gt; because this unnatural issue of multiple brains and their positions\n&gt; does not come up, and I do believe that HyperNEAT often does discover\n&gt; regularities on its own, and that&#39;s a good thing.\n&gt; \n&gt; Anyway, the broader point is that I will stick to my strong position:\n&gt; I do not believe that finding  middle ground or a sweet spot in terms\n&gt; of biases and constraints is the important issue in the long view of\n&gt; achieving general AI through evolution *unless* you are only doing it\n&gt; to prove how cool evolution is.  In contrast, I&#39;m using evolution\n&gt; because I think it is the best hope. The funny thing is that we will\n&gt; regardless end up agreeing on a lot, because I too don&#39;t like to\n&gt; provide big building blocks.  But my reason is that they will end up\n&gt; being incapable of building a general AI.  So I think a lot of things\n&gt; that look bad also won&#39;t work, so someone who is trying to make\n&gt; evolution look good will often see me as sharing their assumptions.\n&gt; \n&gt; ken\n&gt; \n&gt; \n&gt; --- In neat@yahoogroups.com, Jeff Clune &lt;jclune@...&gt; wrote:\n&gt;&gt; \n&gt;&gt; Ken-\n&gt;&gt; \n&gt;&gt; Thank you for explaining these arguments at length. I always learn a lot\n&gt;&gt; when we have these sorts of discussions, and there is no exception\n&gt; in this\n&gt;&gt; case. I agree with most of what you write below, especially that\n&gt;&gt; constraining/biasing evolution is very important. I guess the only\n&gt; place we\n&gt;&gt; disagree is that I believe there is a middle ground of constraint\n&gt; that we\n&gt;&gt; should shoot for, whereas you seem to feel &#39;the more the better&#39;.\n&gt;&gt; \n&gt;&gt; You write:\n&gt;&gt; \n&gt;&gt;&gt;  Therefore, progress is NE should in part be measured with respect to\n&gt;&gt;&gt; progress in constraining the problem to make such a discovery more\n&gt;&gt;&gt; likely.  When an NE algorithm is improved to allow us to tell it more\n&gt;&gt;&gt; about the world in which its output will be situated, that is good\n&gt;&gt;&gt; news for the long view.  In short, we don&#39;t care at all how NE\n&gt;&gt;&gt; produced a brain as long as it really does.\n&gt;&gt; \n&gt;&gt; This reminds me of something that Hod Lipson says repeatedly. Whenever\n&gt;&gt; someone evolves something impressive the first question to ask is,\n&gt; &quot;How big\n&gt;&gt; are your building blocks?&quot; I am going to provide a straw man of your\n&gt;&gt; argument. Hopefully the fact that I admit that up front will make it\n&gt; less\n&gt;&gt; objectionable. Imagine that Kasparov and a neural net engineer\n&gt; teamed up and\n&gt;&gt; hand-created a neural  net (call it &#39;DeepNet&#39;)  that played chess at a\n&gt;&gt; grandmaster level. Now imagine that we create an NE algorithm for\n&gt; learning\n&gt;&gt; chess playing that was otherwise identical to NEAT, but had one extra\n&gt;&gt; mutation operator, which was &#39;clear out the current phenotype and\n&gt; replace it\n&gt;&gt; with DeepNet&#39;. In this case we would have highly constrained the\n&gt; problem to\n&gt;&gt; find a good chess playing solution. We would have also successfully\n&gt; injected\n&gt;&gt; our a priori knowledge of the problem. However, it would be very\n&gt;&gt; unimpressive as an accomplishment in the field of evolutionary\n&gt; computation.\n&gt;&gt; The credit goes to the humans that designed DeepNet, not for the\n&gt;&gt; evolutionary algorithm that recreated it.\n&gt;&gt; \n&gt;&gt; As I said, this is an unfair caricature of your view. However, I\n&gt; think it\n&gt;&gt; might reveal what I have been trying to say. In my mind, the goal is to\n&gt;&gt; provide smaller and smaller building blocks because then we know it is\n&gt;&gt; evolution that is doing the work, and not us. There is a sweet spot\n&gt; in the\n&gt;&gt; middle. If we humans don&#39;t do any work in biasing the search, then\n&gt; evolution\n&gt;&gt; will perform terribly. But if we provide building blocks that are\n&gt; too large,\n&gt;&gt; then evolution did not really do the heavy lifting. So, as opposed\n&gt; to saying\n&gt;&gt; &#39;the more constraint the better,&#39; I think it is interesting to try to\n&gt;&gt; provide smaller building blocks while still gaining high levels of\n&gt;&gt; performance. As I have said before, I also think that if we make\n&gt; progress on\n&gt;&gt; this front, the evolutionary algorithm (not its product) will be\n&gt; more likely\n&gt;&gt; to generalize to solving other problems. The long term goal, of\n&gt; course, is\n&gt;&gt; to have our algorithms solve problems and create things where we either\n&gt;&gt; don&#39;t know how to solve the problems, or can&#39;t be bothered to do so. For\n&gt;&gt; example, the NE that produced DeepNet would not do very well at race car\n&gt;&gt; driving. But an algorithm that was constrained in a more abstract way to\n&gt;&gt; exploit regularities in its environment might do better on both car\n&gt; racing\n&gt;&gt; and chess. \n&gt;&gt; \n&gt;&gt; I guess I start from the recognition that evolution produced humans\n&gt; without\n&gt;&gt; any bias from a conscious entity. How it did that is one of the most\n&gt;&gt; fascinating and open questions both in our field and in biology. We\n&gt; agree\n&gt;&gt; that trying to emulate ways in which natural evolution did things\n&gt; like bias\n&gt;&gt; itself, and thus allow the evolution of modularity, is the way\n&gt; forward for\n&gt;&gt; our field. HyperNEAT represents such amazing progress because it\n&gt; employed\n&gt;&gt; this strategy. But it strikes me that nature was not told a priori\n&gt; how many\n&gt;&gt; leg modules it should make or learn to control. Nor was it told how many\n&gt;&gt; neural modules it should create in the brain. It figured that stuff\n&gt; out on\n&gt;&gt; its own, and probably performed better as a result because it could\n&gt; learn to\n&gt;&gt; tailor the number of modules it needed to the regularity of the\n&gt; problems it\n&gt;&gt; faced. I guess I don&#39;t think we will make it very far towards evolving\n&gt;&gt; brains that are generally intelligent if our evolutionary algorithms\n&gt; cannot\n&gt;&gt; do likewise. It seems that something is majorly lacking if we have\n&gt; to tell\n&gt;&gt; it each time what the regularities are in the environment, and how to go\n&gt;&gt; about exploiting them.\n&gt;&gt; \n&gt;&gt; Apologies for the straw man argument. I do think there is a lot of\n&gt; merit to\n&gt;&gt; the general thrust of what you say. I may be overreacting in\n&gt; focusing on the\n&gt;&gt; extremes\n&gt;&gt; \n&gt;&gt; \n&gt;&gt; \n&gt;&gt; Cheers,\n&gt;&gt; Jeff Clune\n&gt;&gt; \n&gt;&gt; Digital Evolution Lab, Michigan State University\n&gt;&gt; \n&gt;&gt; jclune@...\n&gt;&gt; \n&gt;&gt; \n&gt;&gt; \n&gt;&gt; \n&gt;&gt;&gt; From: Kenneth Stanley &lt;kstanley@...&gt;\n&gt;&gt;&gt; Reply-To: &quot;neat@yahoogroups.com&quot; &lt;neat@yahoogroups.com&gt;\n&gt;&gt;&gt; Date: Sun, 27 Apr 2008 21:36:33 -0000\n&gt;&gt;&gt; To: &quot;neat@yahoogroups.com&quot; &lt;neat@yahoogroups.com&gt;\n&gt;&gt;&gt; Subject: [neat] Re: Machine Learning and the Long View of AI\n&gt;&gt;&gt; \n&gt;&gt;&gt; --- In neat@yahoogroups.com, &quot;Derek James&quot; &lt;djames@&gt; wrote:\n&gt;&gt;&gt; \n&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt;  In RL, in contrast, the long view is almost the opposite: They\n&gt;&gt;&gt; want to\n&gt;&gt;&gt;&gt;&gt;  remove all constraints and still learn nevertheless.\n&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt; I&#39;m not sure what you mean by this, Ken. Could you elaborate a\n&gt; little?\n&gt;&gt;&gt;&gt; \n&gt;&gt;&gt; \n&gt;&gt;&gt; Sure.  I think the problem is that I can&#39;t find a way to explain my\n&gt;&gt;&gt; point concisely.  As I try to explain it, it starts taking up too much\n&gt;&gt;&gt; text so I shorten it and then it loses its meaning.  Let me give it a\n&gt;&gt;&gt; try again...\n&gt;&gt;&gt; \n&gt;&gt;&gt; I think the difference between the goals of RL and NE is an\n&gt;&gt;&gt; interesting topic because they are almost always conflated, as if they\n&gt;&gt;&gt; are trying to solve the same problem.\n&gt;&gt;&gt; \n&gt;&gt;&gt; The RL community (e.g. value-function approaches) is trying to build\n&gt;&gt;&gt; something that learns like a natural brain.  They are saying, through\n&gt;&gt;&gt; analytic means we can deduce how a brain can learn from sparse\n&gt;&gt;&gt; reinforcement and formalize that process in an algorithm.  The hope, I\n&gt;&gt;&gt; would think, is to eventually build the &quot;general intelligence&quot; that\n&gt;&gt;&gt; aligns with the holy grail of AI.  So each step along the way is an\n&gt;&gt;&gt; improvement in that general ability.\n&gt;&gt;&gt; \n&gt;&gt;&gt; So if that is your goal, then the benchmarks you choose have to be\n&gt;&gt;&gt; designed to measure progress to that goal.  So what they need to do is\n&gt;&gt;&gt; show that their designed intelligence can work largely independently\n&gt;&gt;&gt; of a priori &quot;cheats&quot; that provide the meat of the solution.  Because,\n&gt;&gt;&gt; after all, how can it be a general intelligence if it needs you to\n&gt;&gt;&gt; tell it something that it is supposed to be able to figure out?  This\n&gt;&gt;&gt; perspective, I believe, is aligned with Jeff&#39;s view.\n&gt;&gt;&gt; \n&gt;&gt;&gt; However, NE as a long-term pursuit is involved in something different,\n&gt;&gt;&gt; even though it can be applied to the same problems.  NE is not an\n&gt;&gt;&gt; attempt to formalize how people learn with sparse reinforcement.\n&gt;&gt;&gt; Rather, it is an attempt to formalize how evolution can build a brain.\n&gt;&gt;&gt;  So RL is formalizing the brain itself and NE is formalizing how\n&gt;&gt;&gt; evolution succeeds in creating a brain.  NE is therefore one step\n&gt; removed.\n&gt;&gt;&gt; \n&gt;&gt;&gt; This difference is ultimately a philosophical difference on the best\n&gt;&gt;&gt; approach to creating a full-blown AI.  The instrumental issue is\n&gt;&gt;&gt; whether you think it&#39;s easier to build it yourself or to design an\n&gt;&gt;&gt; algorithm that can build it.  The confusion and hence conflation of\n&gt;&gt;&gt; the two approaches arises in part because they do indeed both aim at\n&gt;&gt;&gt; the same long view goal: a general AI.  But they are coming at it from\n&gt;&gt;&gt; very different angles.\n&gt;&gt;&gt; \n&gt;&gt;&gt; And because of this stark difference, the *metric* of progress should\n&gt;&gt;&gt; be quite different.  We cannot measure our progress in building a\n&gt;&gt;&gt; general intelligence directly in the same way that we measure our\n&gt;&gt;&gt; progress in creating an evolutionary algorithm that itself will\n&gt;&gt;&gt; someday output one.\n&gt;&gt;&gt; \n&gt;&gt;&gt; This distinction is potentially subtle and confusing so let me try to\n&gt;&gt;&gt; make it clearer:  Human brains aren&#39;t designed to build yet more human\n&gt;&gt;&gt; brains.  We are good at a lot of things, and we learn generally, but\n&gt;&gt;&gt; we do not build 100-trillion part devices that are more complex than\n&gt;&gt;&gt; any known object in the universe.  I&#39;m not saying we won&#39;t ever be\n&gt;&gt;&gt; able to do it, but if you want to simulate a human brain, your first\n&gt;&gt;&gt; thought would not be that it needs to be capable of designing yet\n&gt;&gt;&gt; another brain by itself.  Your first thought is about things like\n&gt;&gt;&gt; object recognition or pursuit and evasion.\n&gt;&gt;&gt; \n&gt;&gt;&gt; In contrast, building brains is exactly what natural evolution did,\n&gt;&gt;&gt; and it did it quite well.  Natural evolution does not perform object\n&gt;&gt;&gt; recognition; it does not communicate with language; it does not run\n&gt;&gt;&gt; away from predators or hunt for prey.  Yet it does build brains that\n&gt;&gt;&gt; themselves do those things.  And that is the aspect of it we wish to\n&gt;&gt;&gt; harness- a very specific niche kind of skill (though radically\n&gt;&gt;&gt; impressive)- not a general skill.\n&gt;&gt;&gt; \n&gt;&gt;&gt; So the two pursuits are really quite different.  And therefore they\n&gt;&gt;&gt; deserve different metrics to judge their progress with respect to the\n&gt;&gt;&gt; long term goal.  That is, unless we conflate them to be the same\n&gt;&gt;&gt; thing, which we often do without thinking about it.\n&gt;&gt;&gt; \n&gt;&gt;&gt; For example, we could just say, well, both NE and RL are learning\n&gt;&gt;&gt; techniques, and after all, we can apply them to the same problems, so\n&gt;&gt;&gt; why make a big distinction in how we judge them?  Let&#39;s just compare\n&gt;&gt;&gt; them directly on the same benchmarks and get on with it.\n&gt;&gt;&gt; \n&gt;&gt;&gt; That&#39;s fine for the short-term view, i.e. let&#39;s just improve our\n&gt;&gt;&gt; ability to tackle practical problems, but for the long view, they\n&gt;&gt;&gt; cannot be judged in the same way.  If I improve at my ability to\n&gt;&gt;&gt; balance on one foot is that a sign that I will be able to build a\n&gt;&gt;&gt; brain someday?  If evolution evolves a brain that plays checkers, is\n&gt;&gt;&gt; that a sign that evolution *itself* is on the road to performing\n&gt;&gt;&gt; object recognition?  These are totally different pursuits.\n&gt;&gt;&gt; \n&gt;&gt;&gt; So in that context, how should they be judged with respect to long\n&gt;&gt;&gt; term goals?  Well, I think RL deserves to be judged based on its\n&gt;&gt;&gt; increasing ability to learn more generally.  And in that sense,\n&gt;&gt;&gt; exactly Jeff&#39;s criteria should apply to it: We should be interested in\n&gt;&gt;&gt; whether it &quot;needs&quot; a priori information to learn.  In other words, the\n&gt;&gt;&gt; less we need to constrain the problem for the learner, the more\n&gt;&gt;&gt; impressed we deserve to be.  That shows progress towards more and more\n&gt;&gt;&gt; general AI and ML.\n&gt;&gt;&gt; \n&gt;&gt;&gt; But if evolution is not *itself* supposed to be a general learner\n&gt;&gt;&gt; (rather, we just want it to concentrate on one very specific skill:\n&gt;&gt;&gt; brain building), then those considerations are orthogonal to its\n&gt;&gt;&gt; greatest promise.  Its promise is to evolve a brain itself, and as\n&gt;&gt;&gt; such, neuroevolutionary algorithms deserve to be judged on our ability\n&gt;&gt;&gt; to *constrain* the problem so that they can accomplish exactly that.\n&gt;&gt;&gt; In other words, the problem NE *algorithms* face is leaps and bounds\n&gt;&gt;&gt; beyond what RL algorithms face.  RL algorithms just need to be able to\n&gt;&gt;&gt; do as well as brains; NE has to be able to discover brains themselves.\n&gt;&gt;&gt;  Therefore, progress is NE should in part be measured with respect to\n&gt;&gt;&gt; progress in constraining the problem to make such a discovery more\n&gt;&gt;&gt; likely.  When an NE algorithm is improved to allow us to tell it more\n&gt;&gt;&gt; about the world in which its output will be situated, that is good\n&gt;&gt;&gt; news for the long view.  In short, we don&#39;t care at all how NE\n&gt;&gt;&gt; produced a brain as long as it really does.  Will anyone complain if a\n&gt;&gt;&gt; human brain pops out of a system that was a priori given the concept\n&gt;&gt;&gt; of symmetry?  Rather, we should be glad that such a priori context was\n&gt;&gt;&gt; possible to provide in the first place, because it may have saved us a\n&gt;&gt;&gt; year of wasted computation in figuring it out needlessly.\n&gt;&gt;&gt; \n&gt;&gt;&gt; This distinction is almost completely ignored when NE and RL are\n&gt;&gt;&gt; compared directly.  Therefore, the implications of any such comparison\n&gt;&gt;&gt; are fuzzy and lacking context with respect to the long view.  I am not\n&gt;&gt;&gt; sure if I should care or not if RL solves something better than NE, or\n&gt;&gt;&gt; vice versa, because the author doesn&#39;t explain how the result aligns\n&gt;&gt;&gt; with the long-term goals of the fields.  Long term goals seem like\n&gt;&gt;&gt; unwelcome guests these days in AI, which is why I probably won&#39;t be\n&gt;&gt;&gt; writing about any of this in a publication any time soon.\n&gt;&gt;&gt; \n&gt;&gt;&gt; ...\n&gt;&gt;&gt; \n&gt;&gt;&gt; So Derek what you are saying about NE being good at &quot;hard-wired&quot;\n&gt;&gt;&gt; solutions and RL being appropriate for ontogenetic lifetime learning,\n&gt;&gt;&gt; while true, is not what I think of as the primary long-view issue.\n&gt;&gt;&gt; \n&gt;&gt;&gt; In the long view, NE will be used to evolve structures that do learn\n&gt;&gt;&gt; over their lifetime, i.e. not hardwired at all.  The only reason that\n&gt;&gt;&gt; it tends to be used to evolve hardwired solutions today is because we\n&gt;&gt;&gt; are trying to get a foothold on how to evolve certain types of complex\n&gt;&gt;&gt; structures.   Once we get very good at it, focus will naturally shift\n&gt;&gt;&gt; to evolving dynamic brains (and of course there is already work along\n&gt;&gt;&gt; these lines today, much from Floreano).  I do not even think that we\n&gt;&gt;&gt; will need to include stock learning algorithms like Hebbian learning.\n&gt;&gt;&gt;  When we achieve our long-term goals, those *themselves* will be left\n&gt;&gt;&gt; up to evolution because after all there may be something even better.\n&gt;&gt;&gt;  \n&gt;&gt;&gt;&gt;&gt; My aim is to design an\n&gt;&gt;&gt;&gt;&gt;  algorithm that will output a brain, not to design the brain itself.\n&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt; But what kind of brain are you wanting to output?\n&gt;&gt;&gt;&gt; \n&gt;&gt;&gt; \n&gt;&gt;&gt; Note that I&#39;m speaking purely about the long view for these different\n&gt;&gt;&gt; fields here.  Of course on a day-to-day basis I am not solely focused\n&gt;&gt;&gt; on what will happen 100 years from now.  On a practical day-to-day\n&gt;&gt;&gt; basis, of course I want to make NE better capable to tackle problems\n&gt;&gt;&gt; that e.g. RL tackles.  So in the short-term context, I just want to\n&gt;&gt;&gt; output something that works for the problem at hand.\n&gt;&gt;&gt; \n&gt;&gt;&gt; But in the long view, which we were talking about, I think the\n&gt;&gt;&gt; ultimate goal would be to output a full-fledged adaptive system with\n&gt;&gt;&gt; astronomical complexity and the power and subtlety of human reasoning.\n&gt;&gt;&gt;  On that path, constraint is the only hope, unless you want to wait\n&gt;&gt;&gt; three billion years and just hope in the meantime that the initial\n&gt;&gt;&gt; conditions were set up correctly.  Therefore, demonstrations of the\n&gt;&gt;&gt; power of constraint deserve to be judged as evidence of the promise of\n&gt;&gt;&gt; and progress towards the long term goal in NE.\n&gt;&gt;&gt; \n&gt;&gt;&gt; ken\n&gt;&gt;&gt; \n&gt;&gt;&gt; \n&gt;&gt; \n&gt; \n&gt; \n\n------ End of Forwarded Message\n\n\n\n"}}