{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":226666827,"authorName":"Joshua Auerbach","from":"Joshua Auerbach &lt;joshua.auerbach@...&gt;","profile":"phareyouwell","replyTo":"LIST","senderId":"pyh4Mj0NyykPuqjIp1vLM1Gi7GBgWitZ9eJQHDswsKuWivnk1Hu5B5R9QWEKhHXESpy61klR2F3oXzvnTJd9j71o9y-4hNfd3sKnZsAYwlHFk9KnxMW6","spamInfo":{"isSpam":false,"reason":"3"},"subject":"Re: [neat] New E-print: Neuroevolution Offers New Approach to Deep Learning","postDate":"1405606282","msgId":6378,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDUzQzdEOThBLjIwMjAxQG1haWwubWNnaWxsLmNhPg==","inReplyToHeader":"PGxuOGdnZCszbWhwMmVAWWFob29Hcm91cHMuY29tPg==","referencesHeader":"PGxuOGdnZCszbWhwMmVAWWFob29Hcm91cHMuY29tPg=="},"prevInTopic":6364,"nextInTopic":6380,"prevInTime":6377,"nextInTime":6379,"topicId":6364,"numMessagesInTopic":5,"msgSnippet":"Hi Ken, Thank you for sharing.  This is a very interesting idea, and I look forward to speaking with you about it more at ALife. I have a paper at ALife that","rawEmail":"Return-Path: &lt;joshua.auerbach@...&gt;\r\nX-Sender: joshua.auerbach@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 93375 invoked by uid 102); 17 Jul 2014 14:11:25 -0000\r\nX-Received: from unknown (HELO mtaq2.grp.bf1.yahoo.com) (10.193.84.33)\n  by m13.grp.bf1.yahoo.com with SMTP; 17 Jul 2014 14:11:25 -0000\r\nX-Received: (qmail 9787 invoked from network); 17 Jul 2014 14:11:24 -0000\r\nX-Received: from unknown (HELO smtp5.epfl.ch) (128.178.224.8)\n  by mtaq2.grp.bf1.yahoo.com with SMTP; 17 Jul 2014 14:11:24 -0000\r\nX-Received: (qmail 30082 invoked by uid 107); 17 Jul 2014 14:11:22 -0000\r\nX-Virus-Scanned: ClamAV\r\nX-Received: from stidhcp-2-025.epfl.ch (HELO [128.178.39.25]) (128.178.39.25) (TLS, DHE-RSA-AES128-SHA cipher) (authenticated)\n  by smtp5.epfl.ch (AngelmatoPhylax SMTP proxy) with ESMTPSA; Thu, 17 Jul 2014 16:11:22 +0200\r\nMessage-ID: &lt;53C7D98A.20201@...&gt;\r\nDate: Thu, 17 Jul 2014 16:11:22 +0200\r\nUser-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:24.0) Gecko/20100101 Thunderbird/24.5.0\r\nMIME-Version: 1.0\r\nTo: neat@yahoogroups.com\r\nReferences: &lt;ln8ggd+3mhp2e@...&gt;\r\nIn-Reply-To: &lt;ln8ggd+3mhp2e@...&gt;\r\nContent-Type: multipart/alternative;\n boundary=&quot;------------060407030000070603090306&quot;\r\nX-eGroups-Msg-Info: 2:3:4:0:0\r\nSubject: Re: [neat] New E-print: Neuroevolution Offers New Approach to Deep\n Learning\r\nX-Yahoo-Group-Post: member; u=226666827; y=AuRlpsykvoQTWNnlA0DGU7SYWlHE0UK_hForo24hxHuAyuC68hNs\r\nX-Yahoo-Profile: phareyouwell\r\nFrom: Joshua Auerbach &lt;joshua.auerbach@...&gt;\r\n\r\n\r\n--------------060407030000070603090306\r\nContent-Type: text/plain; charset=UTF-8; format=flowed\r\nContent-Transfer-Encoding: 8bit\r\n\r\nHi Ken,\n\nThank you for sharing.  This is a very interesting idea, and I look \nforward to speaking with you about it more at ALife.\n\nI have a paper at ALife that is also about evolving features, but my \napproach is geared toward an online learning framework where selection \nis informed by the current features&#39; &quot;utility&quot; at solving the given \nlearning problem.  Since we start with an extreme learning machine and \nthen evolve the features online, we call this approach an Online Extreme \nEvolutionary Learning Machine (OEELM):\n\nOnline Extreme Evolutionary Learning Machines.  J. Auerbach, C. Fernando \nand D. Floreano.\nArtificial Life 14: International Conference on the Synthesis and \nSimulation of Living Systems, New York, NY, USA, July 30-August 2, 2014.\nhttp://dx.doi.org/10.7551/978-0-262-32621-6-ch076\n\nI am quite interested in how our approach might be augmented by \nadditionally searching for divergent features as you do here, especially \nbecause I see a major limitation of OEELMs, as they currently stand, to \nbe the lack of feature diversity in more challenging problems.\n\n---\n\nOn another note, I just (this week) came across some recent papers from \nKoutnik, Schmidhuber and Gomez that seems to be doing something very \nsimilar to your DDFA (evolving diverse features):\n\n 1. Jan Koutnik, Juergen Schmidhuber and Faustino Gomez, Online\n    Evolution of Deep Convolutional Network for Vision-Based\n    Reinforcement Learning\n    &lt;http://www.idsia.ch/%7Ekoutnik/papers/koutnik2014sab.pdf&gt;\n    Proceedings of the Simulation of Adaptive Behavior Conference (SAB,\n    Castellon, ES), 2014\n 2. Jan Koutnik, Juergen Schmidhuber and Faustino Gomez, Evolving Deep\n    Unsupervised Convolutional Networks for Vision-Based Reinforcement\n    Learning\n    &lt;http://www.idsia.ch/%7Ekoutnik/papers/koutnik2014gecco.pdf&gt;,\n    Proceedings of the Genetic and Evolutionary Computation Conference\n    (GECCO, Vancouver, CA), 2014\n\nBasically they evolve the parameters of a convolutional net to be spread \nout in feature space.  They then use the compressed representation \ncreated by this net as the input to an evolving recurrent neural network \ncontroller.\n\nAre you familiar with this work?  Can you comment on how it compares to \nyour approach?   One thing is that they are intentionally using this \nfeature space as a compressed representation, whereas you use it to \naccumulate a growing set of features.  Forming a compressed \nrepresentation seems more useful to me if you want to use this as the \ninput to a controller (policy) network as they do, but perhaps for other \nuses (such as classification) having novel dimensionality expansions is \nuseful.\n\nI am very interested to see what you have to say about any of this!\n\nCheers,\nJosh\n\n\n\nOn 06/11/2014 04:56 AM, kstanley@... [neat] wrote:\n&gt;\n&gt; My coauthors Paul Szerlip, Greg Morse, Justin Pugh, and myself are \n&gt; excited to announce our new arXiv e-print, &quot;Unsupervised Feature \n&gt; Learning through Divergent Discriminative Feature Accumulation&quot;:\n&gt;\n&gt;\n&gt; eplex link: http://eplex.cs.ucf.edu/papers/szerlip_arxiv1406.1833v2.pdf\n&gt;\n&gt;\n&gt; arXiv link: http://arxiv.org/abs/1406.1833\n&gt;\n&gt;\n&gt; While this paper is not yet published in a journal or conference, \n&gt; because we think its implications are broad for the neuroevolution \n&gt; community and beyond, we decided it&#39;s important to share it now.  \n&gt; &quot;Unsupervised feature learning&quot; has become a very popular research \n&gt; area in recent years with the rise of &quot;deep learni ng.&quot; In fact, the \n&gt; field of deep learning has a whole subfield dedicated to this topic, \n&gt; usually centered on the idea of pre-training layers of a future \n&gt; classifier network, often through an autoencoder or related \n&gt; technique.  The autoencoder is often viewed as the key piece of \n&gt; unsupervised apparatus for learning features without the need for \n&gt; labelled data.  Often it is argued that pretraining a network in this \n&gt; way sets it up for increased success later on when training more \n&gt; conventionally (e.g. with backprop) on a classification problem.\n&gt;\n&gt;\n&gt; We realized recently that there is an appealing alternative to \n&gt; autoencoders that derives much of its power from recent progress in \n&gt; the field neuroevolution. This alternative, called &quot;divergent \n&gt; discriminative feature accumulation&quot; (DDFA), basically uses novelty \n&gt; search to accumulate a continual stream of novel discriminative \n&gt; features.  In other words , novelty search is the feature learning \n&gt; algorithm (and in the paper, HyperNEAT represents the features).  This \n&gt; setup provides an entirely new perspective on feature learning that is \n&gt; quite different from autoencoders.  For example, it can run \n&gt; indefinitely and keep accumulating new features, which means you don&#39;t \n&gt; need to know how many features are needed when you start the search.  \n&gt; It also does not converge because novelty search is divergent, so it \n&gt; just keeps on going.  It further benefits from being non-objective, so \n&gt; the representations of features you get out of it are likely more \n&gt; evolvable (i.e. better representations).   On top of all that, it \n&gt; benefits from the geometric capabilities of HyperNEAT.  I think it \n&gt; also offers an interesting new way to think about learning creatively \n&gt; through a divergent process. After all, divergent thought is often \n&gt; attributed to the most creative people.  This algorithm literally \n&gt; accumulates new per spectives on the world divergently.\n&gt;\n&gt;\n&gt; We tried running DDFA on MNIST by generating a bunch of features \n&gt; (3,000 in the larger case) and then training a classifier on top of \n&gt; them with simple backprop  (similarly to procedures with autoencoders \n&gt; in deep learning).  With only a simple one-hidden-layer network with \n&gt; none of the usual tricks used in deep learning (i.e. no preprocessing, \n&gt; regularization, special activation functions, dropout, etc.) DDFA was \n&gt; able to achieve 1.25% error on MNIST.  To give perspective on that, \n&gt; Hinton&#39;s original 4-layer deep network achieved 1.2% with a much \n&gt; deeper network.  One big conclusion for this group is that \n&gt; neuroevolution can contribute meaningfully to deep learning and has a \n&gt; lot to offer, and that we can indeed achieve serious competitive \n&gt; results.  More broadly, the results raise interesting questions for \n&gt; the broader field of machine learning, like whether optimization (i.e. \n&gt; mi nimizing error) is really always the best way to think about \n&gt; learning, and whether sometimes divergence is more powerful than \n&gt; convergence.\n&gt;\n&gt;\n&gt; There are a lot of interesting future possibilities for DDFA and we \n&gt; are happy to hear your thoughts!\n&gt;\n&gt;\n&gt; Best,\n&gt;\n&gt;\n&gt; ken\n&gt;\n&gt; \n\n\r\n--------------060407030000070603090306\r\nContent-Type: text/html; charset=UTF-8\r\nContent-Transfer-Encoding: 8bit\r\n\r\n&lt;html&gt;\n  &lt;head&gt;\n    &lt;meta content=&quot;text/html; charset=UTF-8&quot; http-equiv=&quot;Content-Type&quot;&gt;\n  &lt;/head&gt;\n  &lt;body bgcolor=&quot;#FFFFFF&quot; text=&quot;#000000&quot;&gt;\n    &lt;div class=&quot;moz-cite-prefix&quot;&gt;Hi Ken,&lt;br&gt;\n      &lt;br&gt;\n      Thank you for sharing.  This is a very interesting idea, and I\n      look forward to speaking with you about it more at ALife.&lt;br&gt;\n      &lt;br&gt;\n      I have a paper at ALife that is also about evolving features, but\n      my approach is geared toward an online learning framework where\n      selection is informed by the current features&#39; &quot;utility&quot; at\n      solving the given learning problem.  Since we start with an\n      extreme learning machine and then evolve the features online, we\n      call this approach an Online Extreme Evolutionary Learning Machine\n      (OEELM):&lt;br&gt;\n      &lt;br&gt;\n      Online Extreme Evolutionary Learning Machines.  J. Auerbach, C.\n      Fernando and D. Floreano.&lt;br&gt;\n      Artificial Life 14: International Conference on the Synthesis and\n      Simulation of Living Systems, New York, NY, USA, July 30-August 2,\n      2014.&lt;br&gt;\n      &lt;a href=&quot;http://dx.doi.org/10.7551/978-0-262-32621-6-ch076&quot;&gt;http://dx.doi.org/10.7551/978-0-262-32621-6-ch076&lt;/a&gt;&lt;br&gt;\n      &lt;br&gt;\n      I am quite interested in how our approach might be augmented by\n      additionally searching for divergent features as you do here,\n      especially because I see a major limitation of OEELMs, as they\n      currently stand, to be the lack of feature diversity in more\n      challenging problems.&lt;br&gt;\n      &lt;br&gt;\n      ---&lt;br&gt;\n      &lt;br&gt;\n      On another note, I just (this week) came across some recent papers\n      from Koutnik, Schmidhuber and Gomez that seems to be doing\n      something very similar to your DDFA (evolving diverse features):&lt;br&gt;\n      &lt;ol class=&quot;publ&quot;&gt;\n        &lt;li&gt;\n          Jan Koutnik, Juergen Schmidhuber and Faustino Gomez, &lt;a\n            href=&quot;http://www.idsia.ch/%7Ekoutnik/papers/koutnik2014sab.pdf&quot;&gt;\n            Online Evolution of Deep Convolutional Network for\n            Vision-Based Reinforcement Learning&lt;/a&gt;\n          &lt;it&gt;Proceedings of the Simulation of Adaptive Behavior\n            Conference (SAB, Castellon, ES)&lt;/it&gt;, 2014 &lt;/li&gt;\n        &lt;li&gt;\n          Jan Koutnik, Juergen Schmidhuber and Faustino Gomez, &lt;a\n            href=&quot;http://www.idsia.ch/%7Ekoutnik/papers/koutnik2014gecco.pdf&quot;&gt;\n            Evolving Deep Unsupervised Convolutional Networks for\n            Vision-Based Reinforcement Learning&lt;/a&gt;, &lt;it&gt;Proceedings of\n            the Genetic and Evolutionary Computation Conference (GECCO,\n            Vancouver, CA)&lt;/it&gt;, 2014\n        &lt;/li&gt;\n      &lt;/ol&gt;\n      Basically they evolve the parameters of a convolutional net to be\n      spread out in feature space.  They then use the compressed\n      representation created by this net as the input to an evolving\n      recurrent neural network controller.&lt;br&gt;\n      &lt;br&gt;\n      Are you familiar with this work?  Can you comment on how it\n      compares to your approach?   One thing is that they are\n      intentionally using this feature space as a compressed\n      representation, whereas you use it to accumulate a growing set of\n      features.  Forming a compressed representation seems more useful\n      to me if you want to use this as the input to a controller\n      (policy) network as they do, but perhaps for other uses (such as\n      classification) having novel dimensionality expansions is useful.&lt;br&gt;\n      &lt;br&gt;\n      I am very interested to see what you have to say about any of\n      this!&lt;br&gt;\n      &lt;br&gt;\n      Cheers,&lt;br&gt;\n      Josh&lt;br&gt;\n      &lt;br&gt;\n      &lt;br&gt;\n      &lt;br&gt;\n      On 06/11/2014 04:56 AM, &lt;a class=&quot;moz-txt-link-abbreviated&quot; href=&quot;mailto:kstanley@...&quot;&gt;kstanley@...&lt;/a&gt; [neat] wrote:&lt;br&gt;\n    &lt;/div&gt;\n    &lt;blockquote cite=&quot;mid:ln8ggd+3mhp2e@...&quot; type=&quot;cite&quot;&gt;\n      &lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8&quot;&gt;\n      &lt;span style=&quot;display:none&quot;&gt; &lt;/span&gt;\n      \n          &lt;div id=&quot;ygrp-text&quot;&gt;\n            &lt;p&gt;&lt;span&gt;My coauthors Paul Szerlip, Greg Morse, Justin Pugh,\n                and myself are excited to announce our new arXiv\n                e-print, &quot;Unsupervised Feature Learning through\n                Divergent Discriminative Feature Accumulation&quot;:&lt;/span&gt;&lt;/p&gt;\n            &lt;p&gt;&lt;br&gt;\n              &lt;span&gt;&lt;/span&gt;&lt;/p&gt;\n            &lt;p&gt;&lt;span&gt;eplex link: &lt;a moz-do-not-send=&quot;true&quot;\n                  rel=&quot;nofollow&quot; target=&quot;_blank&quot;\n                  href=&quot;http://eplex.cs.ucf.edu/papers/szerlip_arxiv1406.1833v2.pdf&quot;&gt;http://eplex.cs.ucf.edu/papers/szerlip_arxiv1406.1833v2.pdf&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;\n            &lt;p&gt;&lt;span&gt;&lt;br&gt;\n              &lt;/span&gt;&lt;/p&gt;\n            &lt;p&gt;&lt;span&gt;arXiv link: &lt;a moz-do-not-send=&quot;true&quot;\n                  rel=&quot;nofollow&quot; target=&quot;_blank&quot;\n                  href=&quot;http://arxiv.org/abs/1406.1833&quot;&gt;http://arxiv.org/abs/1406.1833&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;\n            &lt;p&gt;&lt;span&gt;&lt;br&gt;\n              &lt;/span&gt;&lt;/p&gt;\n            &lt;p&gt;&lt;span&gt;While this paper is not yet published in a journal\n                or conference, because we think its implications are\n                broad for the neuroevolution community and beyond, we\n                decided it&#39;s important to share it now.  &quot;Unsupervised\n                feature learning&quot; has become a very popular research\n                area in recent years with the rise of &quot;deep learni ng.&quot; \n                In fact, the field of deep learning has a whole subfield\n                dedicated to this topic, usually centered on the idea of\n                pre-training layers of a future classifier network,\n                often through an autoencoder or related technique.  The\n                autoencoder is often viewed as the key piece of\n                unsupervised apparatus for learning features without the\n                need for labelled data.  Often it is argued that\n                pretraining a network in this way sets it up for\n                increased success later on when training more\n                conventionally (e.g. with backprop) on a classification\n                problem.&lt;/span&gt;&lt;/p&gt;\n            &lt;p&gt;&lt;br&gt;\n              &lt;span&gt;&lt;/span&gt;&lt;/p&gt;\n            &lt;p&gt;&lt;span&gt;We realized recently that there is an appealing\n                alternative to autoencoders that derives much of its\n                power from recent progress in the field neuroevolution. \n                This alternative, called &quot;&lt;/span&gt;&lt;span&gt;divergent\n                discriminative feature accumulation&quot; (DDFA), basically\n                uses novelty search to accumulate a continual stream of\n                novel discriminative features.  In other words , novelty\n                search is the feature learning algorithm (and in the\n                paper, HyperNEAT represents the features).  This setup\n                provides an entirely new perspective on feature learning\n                that is quite different from autoencoders.  For example,\n                it can run indefinitely and keep accumulating new\n                features, which means you don&#39;t need to know how many\n                features are needed when you start the search.  It also\n                does not converge because novelty search is divergent,\n                so it just keeps on going.  It further benefits from\n                being non-objective, so the representations of features\n                you get out of it are likely more evolvable (i.e. better\n                representations).   On top of all that, it benefits from\n                the geometric capabilities of HyperNEAT.  I think it\n                also offers an interesting new way to think about\n                learning creatively through a divergent process. After\n                all, divergent thought is often attributed to the most\n                creative people.  This algorithm literally accumulates\n                new per spectives on the world divergently.&lt;/span&gt;&lt;/p&gt;\n            &lt;p&gt;&lt;br&gt;\n              &lt;span&gt;&lt;/span&gt;&lt;/p&gt;\n            &lt;p&gt;&lt;span&gt;We tried running DDFA on MNIST by generating a\n                bunch of features (3,000 in the larger case) and then\n                training a classifier on top of them with simple\n                backprop  (similarly to procedures with autoencoders in\n                deep learning).  With only a simple one-hidden-layer\n                network with none of the usual tricks used in deep\n                learning (i.e. no preprocessing, regularization, special\n                activation functions, dropout, etc.) DDFA was able to\n                achieve 1.25% error on MNIST.  To give perspective on\n                that, Hinton&#39;s original 4-layer deep network achieved\n                1.2% with a much deeper network.  One big conclusion for\n                this group is that neuroevolution can contribute\n                meaningfully to deep learning and has a lot to offer,\n                and that we can indeed achieve serious competitive\n                results.  More broadly, the results raise interesting\n                questions for the broader field of machine learning,\n                like whether optimization (i.e. mi nimizing error) is\n                really always the best way to think about learning, and\n                whether sometimes divergence is more powerful than\n                convergence.  &lt;br&gt;\n              &lt;/span&gt;&lt;/p&gt;\n            &lt;p&gt;&lt;br&gt;\n              &lt;span&gt;&lt;/span&gt;&lt;/p&gt;\n            &lt;p&gt;&lt;span&gt;There are a lot of interesting future possibilities\n                for DDFA and we are happy to hear your thoughts!&lt;/span&gt;&lt;/p&gt;\n            &lt;p&gt;&lt;br&gt;\n              &lt;span&gt;&lt;/span&gt;&lt;/p&gt;\n            &lt;p&gt;&lt;span&gt;Best,&lt;/span&gt;&lt;/p&gt;\n            &lt;p&gt;&lt;br&gt;\n              &lt;span&gt;&lt;/span&gt;&lt;/p&gt;\n            &lt;p&gt;&lt;span&gt;ken&lt;br&gt;\n              &lt;/span&gt;&lt;/p&gt;\n            &lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;\n          &lt;/div&gt;\n          \n      \n      &lt;!-- end group email --&gt;\n    &lt;/blockquote&gt;\n    &lt;br&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n\r\n--------------060407030000070603090306--\r\n\n"}}