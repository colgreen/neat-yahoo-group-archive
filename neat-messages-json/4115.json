{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"2AOF-v6wAOgMeju3FB8vVJs64S36QJGYEbGkCVwOunyiT76tjNn_zxlEWOv6pWM_Tt3cHm1E-BQm-UuxS2LafyE1uLV6OlIy4dugV3fCwhau","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Can novelty search be treated as an optimization technique?","postDate":"1211745802","msgId":4115,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGcxY2dtYStubHY2QGVHcm91cHMuY29tPg==","inReplyToHeader":"PGcxYnEwZytubTlsQGVHcm91cHMuY29tPg=="},"prevInTopic":4113,"nextInTopic":4127,"prevInTime":4114,"nextInTime":4116,"topicId":4113,"numMessagesInTopic":6,"msgSnippet":"Peter, I also think NEAT could work with a CMA-ES (or some kind of EDA, which seems to do something similar).  While EANT cycles through phases of topology and","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 21809 invoked from network); 25 May 2008 20:03:24 -0000\r\nX-Received: from unknown (66.218.67.94)\n  by m46.grp.scd.yahoo.com with QMQP; 25 May 2008 20:03:24 -0000\r\nX-Received: from unknown (HELO n26c.bullet.scd.yahoo.com) (66.218.67.218)\n  by mta15.grp.scd.yahoo.com with SMTP; 25 May 2008 20:03:24 -0000\r\nX-Received: from [66.218.69.4] by n26.bullet.scd.yahoo.com with NNFMP; 25 May 2008 20:03:24 -0000\r\nX-Received: from [66.218.67.199] by t4.bullet.scd.yahoo.com with NNFMP; 25 May 2008 20:03:24 -0000\r\nDate: Sun, 25 May 2008 20:03:22 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;g1cgma+nlv6@...&gt;\r\nIn-Reply-To: &lt;g1bq0g+nm9l@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: Re: Can novelty search be treated as an optimization technique?\r\nX-Yahoo-Group-Post: member; u=54567749; y=FrB1PK1p5_kglBY6BxoydCf-1HFPxznVYwrjWZzIEnIyieVIxCMZ\r\nX-Yahoo-Profile: kenstanley01\r\n\r\nPeter, \n\nI also think NEAT could work with a CMA-ES (or some kind of EDA, w=\r\nhich\nseems to do something similar).  While EANT cycles through phases of\nt=\r\nopology and weight optimization, I think maybe there is a more\nelegant way =\r\nto just combine the two by dealing properly with variable\nnumbers of dimens=\r\nions in the CMA-ES.  \n\nAnyway, that&#39;s not the main point you&#39;re making.  Yo=\r\nur main question\nis whether something like CMA-ES, which focuses on optimiz=\r\nation, can\nbe used in a novelty search.  Of course, novelty search is\nphilo=\r\nsophically almost the antithesis of optimization.  However, in\npractice the=\r\n search for novelty itself could be viewed as the\nobjective, and then, in p=\r\nrinciple, an optimization method might be\nable to optimize to that end.\n\nIt=\r\n is true that from one generation to the next, the scores change and\ntheref=\r\nore you cannot rely on a constant gradient.  However, isn&#39;t it\ntrue that CM=\r\nA-ES (and EDAs) computes its covariance matrix from only\nthe current genera=\r\ntion (i.e. current population).  If so, it could\ncompute how genetic parame=\r\nters correlate to novelty scores, and\nthereby try to optimize novelty for t=\r\nhat particular population.  In\nother words, in a fixed population, I don&#39;t =\r\nsee a reason it couldn&#39;t\nbe applied (and then reapplied for the next popula=\r\ntion all over\nagain).  However, I guess you are saying that you can&#39;t just =\r\nlaunch a\n&quot;phase&quot; of CMA-ES on its own since it would not optimize topologie=\r\ns. \nBut I don&#39;t think that would necessarily matter.  Still, even better\nwo=\r\nuld be to seamlessly combine the CMA-ES with NEAT instead of\nsplitting them=\r\n into phases.  \n\nWhether or not CMA-ES works better than stochastic mutatio=\r\nn over the\nlong run is a different question, but it&#39;s a question even in re=\r\ngular\nobjective-based optimization.  I actually think that CMA-ES and the\nl=\r\nike are greedy, although you say it is not.  It computes the most\npromising=\r\n vector based exclusively on the current distribution.  Thus\nit is in a sen=\r\nse overly focused on the initial population\ndistribution, which may be high=\r\nly misleading with respect to the\nultimate objective.  Basically, it&#39;s the =\r\nproblem of deception as\nusual, but I think CMA-ES is even more susceptible =\r\nto it because it\nactively seeks out a model of the deceptive distribution. =\r\n That is\npretty greedy in my view.\n\nNow, if the problem is not deceptive, t=\r\nhen that is great, because the\nmodel will be way better than random perturb=\r\nations.  So you will find\nsome problems where it will be fantastic.  But in=\r\n highly deceptive\nproblems I am not sure.  Some of the results for CMA-ES r=\r\neported to\ndate may be misleading because they do not focus on deceptive\npr=\r\noblems.   One way to see this fact is that they use extremely small\npopulat=\r\nions.  That simply cannot work well in a highly deceptive\ndomain.  In fact,=\r\n as you can see in my dissertation, regular NEAT is\nalso extremely fast at =\r\npole balancing problems with a tiny population.\n So that tells you more abo=\r\nut pole balancing having a really large\nbasin of attraction than about eith=\r\ner method in general.\n\nAnyway, I think ultimately that it can be combined w=\r\nith NEAT and that\nwould be interesting, and such a system could even work w=\r\nith novelty\nsearch.  Whether it could work better is an open question.\n\nken=\r\n \n\n--- In neat@yahoogroups.com, &quot;peterberrington&quot; &lt;peterberrington@...&gt;\nwro=\r\nte:\n&gt;\n&gt; I&#39;ve been tinkering with many optimization techniques and consideri=\r\nng\n&gt; the possibility of integrating some with the mechanics of neat, in the=\r\n\n&gt; vein of EANT, which sadly has no open source implementation. \n&gt; \n&gt; While=\r\n I&#39;m trying to add that functionality to the neat-python\n&gt; implementation I=\r\n use, I&#39;m really wondering if its at all applicable to\n&gt; novelty search. In=\r\n novelty search, there is a specific pressure to do\n&gt; something new and ind=\r\need we do assign a numeric novelty value to each\n&gt; individual behaviour at =\r\nthe time of its evaluation for addition to the\n&gt; archive. However, the scor=\r\ne an individual receives depends on who its\n&gt; up against, so its not object=\r\nive. Without a function to minimize, the\n&gt; dynamics which allow optimizatio=\r\nn techniques to work may be damaged to\n&gt; the point of rendering it no bette=\r\nr than random search. \n&gt; \n&gt; I&#39;m still puzzled on the discussion Peter C rai=\r\nsed about reevaluating\n&gt; the novelty of behaviours in the archive at future=\r\n intervals.\n&gt; Shouldn&#39;t the novelty score assigned to each point be useless=\r\n after\n&gt; we&#39;ve decided whether or not to archive the behaviour? I&#39;m not sur=\r\ne\n&gt; that joint angles over every timestep is really the best way the\n&gt; char=\r\nacterize behaviour for a 3d ragdoll rig, as you are essentially\n&gt; providing=\r\n no clear way to distinguish a good behaviour from a bad\n&gt; behaviour. Altho=\r\nugh I haven&#39;t been able to use optimized libraries\n&gt; with my current simula=\r\ntion configuration (this is hopefully changing\n&gt; very soon), I have noticed=\r\n a significant boost in speed with my\n&gt; ragdoll evolution by defining behav=\r\niour simply as the final x y z\n&gt; triplet for the center of mass. In this wa=\r\ny the search quickly\n&gt; exhausts all the easy ways of falling close to its o=\r\nrigin and pressure\n&gt; mounts for it explore end locations successively farth=\r\ner and farther\n&gt; away from its origin; in essence the objective function is=\r\n realized in\n&gt; that characterization of behavioural distance. \n&gt; \n&gt; In any =\r\ncase, we are explicitly trying to reward novelty by selecting\n&gt; for further=\r\n evaluation any individuals which satisfy a minimum\n&gt; threshold of how diff=\r\nerent their behaviour is. Something about that\n&gt; gives me a feeling that ma=\r\nybe optimization techniques are applicable,\n&gt; but only if the dynamics of o=\r\nptimization can be adapted to the\n&gt; unwieldy workings of novelty search.\n&gt; =\r\n\n&gt; Rather than maximizing fitness, optimization techniques usually\n&gt; minimi=\r\nze an objective function so the fitness scale is simply reversed\n&gt; with a p=\r\nerfect score being 0.0\n&gt; Since in this domain theres no such thing as perfe=\r\nct novelty, can\n&gt; anyone provide any insights as to a framework where optim=\r\nization\n&gt; techniques can be harnessed to maximize novelty? I feel this is a=\r\n\n&gt; really important area to focus attention on, as it could potentially\n&gt; l=\r\nead to a dramatic increase in the fitness per number of evaluations.\n&gt; \n&gt; A=\r\ns some background: I&#39;ve long been tempted after reading papers on\n&gt; EANT to=\r\n try dropping in a more advanced optimization function into\n&gt; NEAT, like so=\r\nme of the new variations on the covariance matrix\n&gt; adaption evolution stra=\r\ntegy. The way this was implemented in EANT was\n&gt; by splitting NEATs main lo=\r\nop into a &quot;structural exploration&quot; loop for\n&gt; building networks, and optimi=\r\nzing the weight connection values\n&gt; separately within a nested loop, using =\r\nCMA-ES. This division of work\n&gt; permits the net to be treated as an n-dimen=\r\nsional equation where n is\n&gt; the number of connection or node weights (or o=\r\nther properties) we wish\n&gt; to optimize (i.e. everything that isn&#39;t defining=\r\n the topology of the\n&gt; net). I think it can be argued that standard fitness=\r\n based neat is a\n&gt; greedier approach because it cartwheels through topologi=\r\ncal space and\n&gt; weight parameter space at the same time. I want to plug in =\r\nan\n&gt; optimization function, but I can&#39;t think of how anymore, or even if\n&gt; =\r\nthe two search techniques are reconcilable. \n&gt; \n&gt; Theres no point optimizin=\r\ng within a specific net topology in novelty\n&gt; search because you want to ki=\r\nnd of push out and fill behavioural space\n&gt; as evenly as possible, so there=\r\ns no nook or crack which escapes the\n&gt; poking and prodding of your search; =\r\nwithout the\n&gt; competition/coevolutionary aspect of defining fitness against=\r\n the\n&gt; backdrop of current rivals and ancestors, I doubt you&#39;d see anything=\r\n\n&gt; more effective than random selection. If you can&#39;t optimize each net\n&gt; t=\r\nopology in a vacuum, is there another way fitness-based searches and\n&gt; nove=\r\nlty-based searches can reach a compromise where each complements\n&gt; the othe=\r\nr?\n&gt; \n&gt; I&#39;ve given some thought to the idea of phased searching, but you wo=\r\nuld\n&gt; have to find some way of defining stopping critera for when you&#39;d wan=\r\nt\n&gt; to optimize an objective function and when you&#39;d want to diversify\n&gt; fr=\r\nom bottom up complexity wise. \n&gt; \n&gt; \n&gt; As an interesting side note, to anyo=\r\nne whos had a sneak peak at Peter\n&gt; C&#39;s maze navigation novelty search prog=\r\nram, isn&#39;t it uncanny, the\n&gt; resemblance between the past behaviours that h=\r\nave accumulated, and the\n&gt; growth of a plant?  As more organisms fill up th=\r\ne space of possible\n&gt; behaviours its tempting to imagine it as molasses whi=\r\nch slowly oozes\n&gt; its way into every crack and crevice it can reach. Perhap=\r\ns there is a\n&gt; useful insight to be drawn from that, I have no idea, I just=\r\n thought\n&gt; it looked very &quot;organic&quot;.\n&gt;\n\n\n\n"}}