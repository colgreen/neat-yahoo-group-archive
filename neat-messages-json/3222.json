{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"u6gKKn-CBM-_aNrZGcMgsohJEgOJveSvNMk9AR2OTEwZH7jiOymRz852g1zh8SGCTryV3-Z4HZ3AZtvzJSYPmfOrTrm_OVFhkjUJS94-jCc9","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: HyperNEAT and No Free Lunch","postDate":"1177922102","msgId":3222,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGYxNDlubSthOXRwQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDExOEIyRDVFLTFENkEtNDgwQy05MkI0LTlEMUQwNzAyNDdERUBjcy51dGV4YXMuZWR1Pg=="},"prevInTopic":3221,"nextInTopic":3223,"prevInTime":3221,"nextInTime":3223,"topicId":3214,"numMessagesInTopic":27,"msgSnippet":"... here ... all ... letter, ... any ... really ... layman s ... original ... I ... You ... it ... that ... difference ... the ... Instead ... Hmm, yes, that","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 96450 invoked from network); 30 Apr 2007 08:35:15 -0000\r\nReceived: from unknown (66.218.66.71)\n  by m40.grp.scd.yahoo.com with QMQP; 30 Apr 2007 08:35:15 -0000\r\nReceived: from unknown (HELO n26c.bullet.scd.yahoo.com) (66.218.67.218)\n  by mta13.grp.scd.yahoo.com with SMTP; 30 Apr 2007 08:35:15 -0000\r\nReceived: from [66.218.69.2] by n26.bullet.scd.yahoo.com with NNFMP; 30 Apr 2007 08:35:05 -0000\r\nReceived: from [66.218.66.76] by t2.bullet.scd.yahoo.com with NNFMP; 30 Apr 2007 08:35:04 -0000\r\nDate: Mon, 30 Apr 2007 08:35:02 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;f149nm+a9tp@...&gt;\r\nIn-Reply-To: &lt;118B2D5E-1D6A-480C-92B4-9D1D070247DE@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: Re: HyperNEAT and No Free Lunch\r\nX-Yahoo-Group-Post: member; u=54567749; y=6WFRr8XEs6TVobxuGJcKpcBBoTguzv3olpozAthDMJSTjSRYCKEl\r\nX-Yahoo-Profile: kenstanley01\r\n\r\n--- In neat@yahoogroups.com, Joseph Reisinger &lt;joeraii@...&gt; wrote:\n&gt;\n&gt; \n&gt; &gt;=\r\n Joe, I am willing to go with this:\n&gt; &gt;\n&gt; &gt;&gt; At the very least, you should =\r\nlimit\n&gt; &gt;&gt; your statement just to problems where the geometry is known /to\n=\r\n&gt; &gt;&gt; the experimenter.  In the case where the experimenter knows the\n&gt; &gt;&gt; g=\r\neometry a priori, and he is correct in that knowledge, then yes,\n&gt; &gt;&gt; Hyper=\r\nNEAT would outperform other Non-Hyper algorithms in the NFL\n&gt; &gt;&gt; sense.\n&gt; &gt;=\r\n\n&gt; &gt; However, I think we&#39;re kind of splitting our theoretical hairs \nhere\n&gt;=\r\n &gt; more than necessary.  Sure, if the issue is that we need to take \nall\n&gt; =\r\n&gt; possible precaution to convey that we understand NFL to the \nletter,\n&gt; &gt; =\r\nthen we can go down this road and optimize our statement to avoid \nany\n&gt; &gt; =\r\npossible misunderstanding.  And that may be important in some\n&gt; &gt; contexts.=\r\n  Yet I feel it is a bit of a distraction from what&#39;s \nreally\n&gt; &gt; important=\r\n about what I&#39;m saying.  So let me try it in more \nlayman&#39;s\n&gt; &gt; terms, avoi=\r\nding mention of NFL:\n&gt; \n&gt; Sure, but I&#39;m still not sure if I&#39;ve convinced yo=\r\nu that your \noriginal  \n&gt; application of NFL was incorrect. I only bring th=\r\ne issue up because \nI  \n&gt; think that we /do/ need to formalize some of what=\r\n we&#39;re talking  \n&gt; about, at least so we can get a better handle on it conc=\r\neptually. \nYou  \n&gt; can see why just from this conversation we&#39;ve had: You h=\r\nave some  \n&gt; intuition about why HyperNEAT is useful, but when you try to p=\r\nin \nit  \n&gt; down more theoretically, things get really complicated.  I feel =\r\n\nthat  \n&gt; if we do spend some time &quot;splitting hairs&quot; theoretically, we migh=\r\nt  \n&gt; actually get to something really interesting. Like a more flexible  \n=\r\n&gt; framework than NFL that describes why HyperNEAT could possibly be  \n&gt; bet=\r\nter than NEAT. The way you were trying to describe the \ndifference  \n&gt; is a=\r\nn excellent point! Its quite novel, but too subtle to fit in \nthe  \n&gt; old N=\r\nFL framework. Thats what I&#39;m trying to convince you of. \nInstead  \n&gt; of try=\r\ning to shoehorn the subject into old formalisms, it might be  \n&gt; better if =\r\nwe just came up with a new formalism ourselves.\n&gt; \n\nHmm, yes, that could be=\r\n interesting.  I see where you&#39;re going.  \nMaybe there is a theoretically c=\r\nlean way to make the point.  It could \nbe nice.  I would definitely not rul=\r\ne it out.\n\nBut I need more guidance toward a statement more concrete than w=\r\nhat \nI&#39;ve already said.  You suggest that I misapplied NFL, but I don&#39;t \nth=\r\nink I was really applying NFL at all.  I was just saying something \nthat is=\r\n already well-accepted about NFL, which is that if you provide \na priori bi=\r\nas you are no longer in an NFL situation vs. something \nthat lacks such bia=\r\ns.  I was not so much making a new assertion as \njust bringing it up as a r=\r\neminder of a generally accepted concept.\n\nOf course the more interesting is=\r\nsue is the nature of the bias, which \nmay be what you are getting at, since=\r\n my statement doesn&#39;t really \ncapture that theoretically.  Perhaps you envi=\r\nsion a way to \ncharacterize it more formally, so that it could even be conv=\r\nincing \noutisde of empirical evidence.  That would definitely be interestin=\r\ng.\n\n&gt; &gt; That&#39;s the important point.\n&gt; \n&gt; Definitely! And I agree that it is=\r\n under appreciated in ML at \nlarge.  \n&gt; Although it may not be as under-app=\r\nreciated as you think. There \nare  \n&gt; few examples I can think of from RL, =\r\nmainly Sherstov and Stone&#39;s  \n&gt; knowledge transfer work, and also Shimon&#39;s =\r\nadaptive tile coding \nwork.\n&gt; \n\nI don&#39;t know the details on Sherstov and St=\r\none; I&#39;ll check that out. \nBut tile coding is not a good example.  It actua=\r\nlly does the exact \nopposite of what HyperNEAT does.  HyperNEAT encodes geo=\r\nmetric \nrelationships among inputs and outputs as variations along Cartesia=\r\nn \ndimensions.  Tile coding, on the other hand, willfully destroys \ngeometr=\r\nic relationships by breaking their respective dimensions into \ntiny little =\r\npieces that no longer have anything to tie them \ntogether.  The idea that l=\r\neft is in opposition to right is only \npossible to leverage if left and rig=\r\nht are variants of a single \ndimension (i.e. something that HyperNEAT stive=\r\ns to capture).  In \nconstrast, tile coding purposefully separates such rela=\r\ntionships into \nbroken bits that no longer vary along a dimension. \n\nIn fac=\r\nt, I think it&#39;s a great example of why it&#39;s important to \ncharacterize what=\r\n is new about HyperNEAT.  On the face of it, one \nmight think (among other =\r\nthings), oh it&#39;s just another way to \noptimize the input representation lik=\r\ne tile coding.  But that \nsuperficial connection hides the fact that these =\r\nare perfectly \nopposite ideas.   Tile coding could be the most aggregious v=\r\niolation \nof the integrity of input geometry that there is, even worse than=\r\n \nregular ANN input encoding.  It basically says, let&#39;s throw up our \nhands=\r\n in the air and just give up on exploiting regularities \nentirely.  Instead=\r\n we&#39;ll just try to solve every problem in tiny \nlittle bits.  It&#39;s actually=\r\n a good indicator of how impotent some \napproaches are that their cleverest=\r\n means to make problems easier is \nto break up all their inherent relations=\r\nhips so that the learning \nalgorithm becomes even more blind than it alread=\r\ny is.  \n\nken\n\n&gt; -- Joe\n&gt; \n&gt; &gt;\n&gt; &gt; ken\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; --- In neat@yahoogrou=\r\nps.com, Joseph Reisinger &lt;joeraii@&gt; wrote:\n&gt; &gt;&gt;\n&gt; &gt;&gt; (I&#39;ve changed the thre=\r\nad title so that this post will be easier \nto\n&gt; &gt;&gt; ignore.)\n&gt; &gt;&gt;\n&gt; &gt;&gt; Sorry=\r\n for dragging us deeper into a discussion of NFL, but I \nthink\n&gt; &gt;&gt; this po=\r\nint is really important, especially if you are going to be\n&gt; &gt;&gt; making thes=\r\ne claims in front of a broader audience. Also I think\n&gt; &gt; it\n&gt; &gt;&gt; is helpfu=\r\nl for the audience here to be crystal clear on the NFL\n&gt; &gt;&gt; issue. Its a ve=\r\nry important theorem and it has been quite often\n&gt; &gt;&gt; misapplied (for insta=\r\nnce, there is some evidence that Wolpert\n&gt; &gt;&gt; initially came up with NFL sp=\r\necifically to critique GAs).\n&gt; &gt;&gt;\n&gt; &gt;&gt;&gt; That&#39;s why I said it &quot;may&quot; be bette=\r\nr for evolving very large\n&gt; &gt; scale\n&gt; &gt;&gt;&gt; brains.\n&gt; &gt;&gt;&gt; The &quot;may&quot; hinges on=\r\n the issues you bring up and others.  \nHowever,\n&gt; &gt;&gt;&gt; even the opportunity =\r\nto be better is an advantage over having no\n&gt; &gt; such\n&gt; &gt;&gt;&gt; opportunity.  Th=\r\ne opportunity of course can be squandered with \nthe\n&gt; &gt;&gt;&gt; wrong a priori kn=\r\nowledge.   Yet part of my point is that it will\n&gt; &gt;&gt;&gt; often be the case tha=\r\nt the natural geometry of a task is all you\n&gt; &gt; need\n&gt; &gt;&gt;&gt; to provide a pow=\r\nerful bias (or at least a bias that is better \nthan\n&gt; &gt;&gt;&gt; nothing), so it w=\r\nill often be possible to seize the opportunity\n&gt; &gt;&gt;&gt; without a great deal o=\r\nf effort.\n&gt; &gt;&gt;\n&gt; &gt;&gt; Ok, note your use of the word &quot;often&quot; instead of the\n&gt; =\r\n&gt; word &quot;always.&quot;\n&gt; &gt;&gt; Just that word substitution means that you aren&#39;t wit=\r\nhin the \nrealm\n&gt; &gt; of\n&gt; &gt;&gt; situations that NFL covers. I totally agree with=\r\n you that the\n&gt; &gt;&gt; application of such geometry can be useful. I just don&#39;t=\r\n think \nyou\n&gt; &gt;&gt; can leverage NFL to back up the argument you are making.\n&gt;=\r\n &gt;&gt;\n&gt; &gt;&gt;&gt; It is true too that NEAT and other neural network algorithms do\n&gt;=\r\n &gt;&gt;&gt; indeed allow for some inclusion of prior knowledge through their\n&gt; &gt;&gt;&gt;=\r\n input/output encoding.  However, note how I phrased my\n&gt; &gt;&gt;&gt; claim: &quot;Hyper=\r\nNEAT is not subject to the No Free Lunch theorem \nwhen\n&gt; &gt;&gt;&gt; comparing to a=\r\nlgorithms that do not allow injecting such a \npriori\n&gt; &gt;&gt;&gt; knowledge.&quot;  Tha=\r\nt is, among algorithms that allow you to decide\n&gt; &gt; on an\n&gt; &gt;&gt;&gt; input encod=\r\ning in the traditional way, the provision of such\n&gt; &gt; encoding\n&gt; &gt;&gt;&gt; does n=\r\not give one algorithm a leg up over another since they all\n&gt; &gt;&gt;&gt; allow for =\r\nsuch knowledge to be included.  HyperNEAT, on the \nother\n&gt; &gt;&gt;&gt; hand, allows=\r\n a new kind of knowledge (i.e. geometry) to be\n&gt; &gt; included\n&gt; &gt;&gt;&gt; and there=\r\nfore does have a potential leg up on that class of\n&gt; &gt;&gt;&gt; algorithms.\n&gt; &gt;&gt;\n&gt;=\r\n &gt;&gt; But this leg up would necessarily be different for each problem,\n&gt; &gt;&gt; h=\r\nence my original critique. You can&#39;t build the experimenter into\n&gt; &gt; the\n&gt; =\r\n&gt;&gt; system and still talk about NFL. Its just not applicable because\n&gt; &gt; you=\r\n\n&gt; &gt;&gt; are no longer doing any generalization across classes of \nproblems.\n&gt;=\r\n &gt;&gt; Therefore you can&#39;t use it to build your case.\n&gt; &gt;&gt;\n&gt; &gt;&gt; Your argument =\r\nruns something like: &quot;HyperNEAT has more parameters\n&gt; &gt;&gt; than the experimen=\r\nter can set than other Non-Hyper NEATs that\n&gt; &gt; allow\n&gt; &gt;&gt; him/her to build=\r\n in even better prior knowledge.&quot; But you are\n&gt; &gt;&gt; neglecting the fact that=\r\n the experimenter now has more possible\n&gt; &gt;&gt; parameters to set incorrectly!=\r\n At the very least, you should \nlimit\n&gt; &gt;&gt; your statement just to problems =\r\nwhere the geometry is known /to\n&gt; &gt; the\n&gt; &gt;&gt; experimenter/.  In the case wh=\r\nere the experimenter knows the\n&gt; &gt; geometry\n&gt; &gt;&gt; a priori, and he is correc=\r\nt in that knowledge, then yes, \nHyperNEAT\n&gt; &gt;&gt; would outperform other Non-H=\r\nyper algorithms in the NFL sense.\n&gt; &gt;&gt;\n&gt; &gt;&gt;&gt; Of course it depends on how we=\r\nll the user takes\n&gt; &gt;&gt;&gt; advantage of the opportunity, but the opportunity i=\r\ns now there.\n&gt; &gt; This\n&gt; &gt;&gt;&gt; fact does indeed mean that statements about Hyp=\r\nerNEAT vs. other\n&gt; &gt;&gt;&gt; neuroevolution (or even machine learning) algorithms=\r\n can cite an\n&gt; &gt;&gt;&gt; opportunity to genuinely be better on average, which in =\r\neffect\n&gt; &gt; brings\n&gt; &gt;&gt;&gt; it outside NFL in one particular sense.\n&gt; &gt;&gt;\n&gt; &gt;&gt; I=\r\nt can be better on average, if the experimenter always makes the\n&gt; &gt;&gt; corre=\r\nct choices w.r.t. the geometry. But I don&#39;t think you can \nuse\n&gt; &gt;&gt; such &quot;o=\r\nracle&quot; experimenters as a basis for comparison. And \nanyway,\n&gt; &gt;&gt; NFL still=\r\n holds, in a broader sense: For example, imagine the\n&gt; &gt; class\n&gt; &gt;&gt; of prob=\r\nlems that /seem/ to have an underlying geometry feature,\n&gt; &gt; lets\n&gt; &gt;&gt; call=\r\n it &quot;A.&quot; There could possibly be instances of this class of\n&gt; &gt;&gt; problems w=\r\nhere naively exploiting A would actually cause the\n&gt; &gt; learning\n&gt; &gt;&gt; algori=\r\nthm to perform worse than not exploiting it. And in fact we\n&gt; &gt;&gt; could prob=\r\nably construct such a class.\n&gt; &gt;&gt;\n&gt; &gt;&gt; Again, to sum up, I think the way yo=\r\nu are framing the benefits of\n&gt; &gt;&gt; HyperNEAT&#39;s geometry exploiting features=\r\n rely solely on an\n&gt; &gt;&gt; intelligent experimenter, and in that sense do sort=\r\n of bypass \nNFL.\n&gt; &gt;&gt; But making such a statement is meaningless, because i=\r\nt basically\n&gt; &gt;&gt; assumes that the experimenter always guesses right. And if=\r\n I were\n&gt; &gt;&gt; that experimenter, then I wouldn&#39;t be here talking to you, I \n=\r\nwould\n&gt; &gt;&gt; have already solved strong AI :)\n&gt; &gt;&gt;\n&gt; &gt;&gt; Humbly,\n&gt; &gt;&gt;\n&gt; &gt;&gt; -- =\r\nJoe\n&gt; &gt;&gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; Yahoo! Groups Links\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt;\n\n\n\n"}}