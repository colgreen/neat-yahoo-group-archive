{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":283334584,"authorName":"petar_chervenski","from":"&quot;petar_chervenski&quot; &lt;petar_chervenski@...&gt;","profile":"petar_chervenski","replyTo":"LIST","senderId":"NYF1igp0KjMtNoUASmlnmvDRUbm5apAzMkOgjbny4t0ro57CG9UxghmPgVXzrhI0pf4yHdImcGY9tQnSdZ5_D5Pq_fL4JaDuRgmuESEpCxJEfLoi5bg","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: Does NEAT always lead to higher fitness value?","postDate":"1192637502","msgId":3599,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGZmNWM3dStpYjk0QGVHcm91cHMuY29tPg==","inReplyToHeader":"PGZmNHRqYytuZWxiQGVHcm91cHMuY29tPg=="},"prevInTopic":3598,"nextInTopic":3612,"prevInTime":3598,"nextInTime":3600,"topicId":3598,"numMessagesInTopic":22,"msgSnippet":"Check your speciation parameters and the importance factors for speciation. Sometimes there are too many species, sometimes there are too few. The dropoff age","rawEmail":"Return-Path: &lt;petar_chervenski@...&gt;\r\nX-Sender: petar_chervenski@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 48589 invoked from network); 17 Oct 2007 16:11:44 -0000\r\nReceived: from unknown (66.218.67.94)\n  by m53.grp.scd.yahoo.com with QMQP; 17 Oct 2007 16:11:44 -0000\r\nReceived: from unknown (HELO n16b.bullet.sp1.yahoo.com) (69.147.64.122)\n  by mta15.grp.scd.yahoo.com with SMTP; 17 Oct 2007 16:11:44 -0000\r\nReceived: from [216.252.122.218] by n16.bullet.sp1.yahoo.com with NNFMP; 17 Oct 2007 16:11:42 -0000\r\nReceived: from [66.218.69.4] by t3.bullet.sp1.yahoo.com with NNFMP; 17 Oct 2007 16:11:42 -0000\r\nReceived: from [66.218.66.89] by t4.bullet.scd.yahoo.com with NNFMP; 17 Oct 2007 16:11:42 -0000\r\nDate: Wed, 17 Oct 2007 16:11:42 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;ff5c7u+ib94@...&gt;\r\nIn-Reply-To: &lt;ff4tjc+nelb@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: &quot;petar_chervenski&quot; &lt;petar_chervenski@...&gt;\r\nSubject: Re: Does NEAT always lead to higher fitness value?\r\nX-Yahoo-Group-Post: member; u=283334584; y=OVRDey8RVXZ7RdE_YNdKJhCFwbx8YQ-UqKkC8okv1IoBJFOOcbHoWVlN9w\r\nX-Yahoo-Profile: petar_chervenski\r\n\r\nCheck your speciation parameters and the importance factors for \nspeciation=\r\n. Sometimes there are too many species, sometimes there are \ntoo few. The d=\r\nropoff age is important, too. The overall slow down is \nbecause the network=\r\ns grow in size and the simpler species reach their \ndropoff age and get kil=\r\nled of by NEAT. This is the explanation why \nthe fitness drops, too. You ca=\r\nn have an absolute elitism where the \nbest species never get killed, no mat=\r\nter how old it is. It seems that \na fitness plateau is being reached and NE=\r\nAT finds it difficult to \noptimize in a high-dimentional space, the smaller=\r\n species already \nbeing dead and the more complex are left.\nA possible thin=\r\ng to try is to implement a pruning mechanism. The \nphased pruning techniqe =\r\nby Colin Green (found in SharpNEAT) is the \none I recommend. This will impr=\r\nove the speed as well as the quality \nof the overall search. With phased pr=\r\nuning, the lower dimentional \nspace is searched better. This helps finding =\r\nthe best starting point \nfor future generations. \n\nPeter\n\n--- In neat@yahoo=\r\ngroups.com, &quot;psyphiroth.nebu&quot; &lt;psyphiroth.nebu@...&gt; \nwrote:\n&gt;\n&gt; \n&gt; I have a=\r\nn input pattern and an output pattern both with random data \n&gt; set, my goal=\r\n then is to form an ANN which can reponse to the input \n&gt; pattern by yieldi=\r\nng the output pattern as precisely as possible. Of \n&gt; course this is done b=\r\ny NEAT.\n&gt; When the error limit I preset was somewhat bigger say 0.1, the \n&gt;=\r\n program worked well, and gave results very quickly. But when I \n&gt; improved=\r\n the limit to be 0.01 or even smaller, the program became \n&gt; unbearably slo=\r\nw, while the fitness curve was not as I had expected \n&gt; but to go down and =\r\ndown through, and finally fluctuated near a \nvalue \n&gt; far from optimum.\n&gt; I=\r\n am using MATLAB_NEAT, by modifying the XOR_EXPERIMENT. I have \n&gt; changed t=\r\nhe fitness function to 1/(1+rmse)(Thanks to Emyr James), \n&gt; where rmse is t=\r\nhe root mean square error which measures the \nagreement \n&gt; of output with o=\r\nutput pattern, it is computed by concerning every \nrow \n&gt; of the output, ju=\r\nst as the original program does. The program uses \n&gt; sigmoidal function as =\r\nits activation function. In my first several \n&gt; experiments, I forgot to mo=\r\ndify the gain scale factor, which was \n4.9 \n&gt; originally, surely to be in f=\r\navour of classification, but not \nproper \n&gt; in an output pattern randomly d=\r\nistributed between -1 and 1. After \n&gt; having studied the possible weights, =\r\nI used 0.84 as the gain scale \n&gt; factor. Of course I had scaled the sigmoid=\r\nal function outputs to \nbe  \n&gt; [-1,1](Thanks to Stephen Waits).\n&gt; Have I fo=\r\nrgotten something? Until now I have mainly experimented \n&gt; under one-column=\r\n output, even this will take hundreds of \ngenerations, \n&gt; but only to find =\r\nstuck in the end, and this happens when I have \njust \n&gt; changed the error l=\r\nimit from 0.1 to 0.01. Is that common? I have \nalso \n&gt; tried the intended t=\r\nhree-column output pattern several times, but \nthe \n&gt; results were terrible=\r\n, maybe require thousands of generations to \nget \n&gt; an answer, or maybe not=\r\n.\n&gt; I have read through Ken&#39;s doctoral paper, but it&#39;s too abstract, \nand \n=\r\n&gt; I found no solution to what I have encountered. Actually the \nproblem \n&gt; =\r\nI am working on has already been resolved by BP network, with 28 \n&gt; hidden =\r\nnodes. I only want to find a simpler and more efficient \n&gt; network, so I tu=\r\nrned to NEAT.\n&gt; But now I can go no further, could you tell me why? Maybe t=\r\nhe \n&gt; original code of XOR_EXPERIMENT is basically unable to solve a \n&gt; pro=\r\nblem other than classification? Or have I messed up something?\n&gt;\n\n\n"}}