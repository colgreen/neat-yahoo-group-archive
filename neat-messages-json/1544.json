{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"_I0XMsufoG6fgUX9AzjHuI2FwANHG1HU0ANrTPRsyGOFvzp-ASd1oRtXHqE5BIGh68j4xoa_pwCy6RXzCec5NMiaLz4l9zA7cFlRX_ZVr1dE","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: More thoughts on IEX","postDate":"1095453001","msgId":1544,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGNpZmhnOSt1cDhuQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDUxN2ZhNmYxMDQwOTE3MTI0NjEyZDYyMzZlQG1haWwuZ21haWwuY29tPg=="},"prevInTopic":1542,"nextInTopic":1545,"prevInTime":1542,"nextInTime":1545,"topicId":1542,"numMessagesInTopic":14,"msgSnippet":"John, I have long thought  that it would be really cool to apply the NEAT methodology to Genetic Programming (GP), and your suggestion below sounds very","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 36510 invoked from network); 17 Sep 2004 20:31:24 -0000\r\nReceived: from unknown (66.218.66.217)\n  by m4.grp.scd.yahoo.com with QMQP; 17 Sep 2004 20:31:24 -0000\r\nReceived: from unknown (HELO n12.grp.scd.yahoo.com) (66.218.66.67)\n  by mta2.grp.scd.yahoo.com with SMTP; 17 Sep 2004 20:31:22 -0000\r\nReceived: from [66.218.67.170] by n12.grp.scd.yahoo.com with NNFMP; 17 Sep 2004 20:30:02 -0000\r\nDate: Fri, 17 Sep 2004 20:30:01 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;cifhg9+up8n@...&gt;\r\nIn-Reply-To: &lt;517fa6f1040917124612d6236e@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Length: 2789\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Remote-IP: 66.218.66.67\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: Re: More thoughts on IEX\r\nX-Yahoo-Group-Post: member; u=54567749\r\nX-Yahoo-Profile: kenstanley01\r\n\r\nJohn, I have long thought  that it would be really cool to apply the\nNEAT methodology to Genetic Programming (GP), and your suggestion\nbelow sounds very similar to this idea.  In GP,they evolve program\ntrees, but they cross them over in ad-hoc ways that don&#39;t use\nhistorical marking, and they do not speciate.  I have a feeling that\nadding things like historical markings, speciation, and starting\nminimally to genetic programming could majorly boost its performance.\n The right way to do this would take a little thought, and it might be\nsimilar to what you are suggesting here or it could be a bit different\n(For example, should &quot;weights&quot; be on connections...in normal GP the\nconnections don&#39;t contain values).  In any case, I think the general\nidea is a great thing to look into and definitely promising.\n\nken\n\n--- In neat@yahoogroups.com, John Arrowwood &lt;jarrowwx@g...&gt; wrote:\n&gt; Okay, so after I finish making a fixed-enlargement neural network, I\n&gt; have another thought...\n&gt; \n&gt; The way I had originally envisioned the solution to the problem of\n&gt; image enlargement was a complex 2-dimensional formula that uses the\n&gt; original pixel inputs as &#39;constants&#39; in the formula.  The formula\n&gt; creates a &#39;curve surface&#39; that roughly approximates the original\n&gt; surface of which the pixel inputs are a representation of.  The\n&gt; outputs then are calculated by taking the average height of the\n&gt; surface within an area.\n&gt; \n&gt; A neural network is a classifier more than a calculator.  But what\n&gt; about using the NEAT methodology to build a formula?  Use it to\nevolve\n&gt; what in compiler terminology (if I&#39;m not mistaken) is a &#39;parse tree&#39;\n&gt; of the formula in a manner that is similar to a network.  Each node\nis\n&gt; either a value or an operator.  Values have no connections, they are\n&gt; always the leaf nodes.  Operator nodes have connections to either\n&gt; value nodes or other operator nodes.  The connection has a weight\n&gt; which is a multiplier.  So if &#39;+&#39; is connected to &#39;a&#39; with a weight\nof\n&gt; 0.5 and to &#39;b&#39; with a weight of 0.75, then it is the same thing as\n&gt; 0.5a + 0.75b.  A constant input of 1 serves like a bias, so you can\n&gt; (by means of the weight) build any constant value you need.  And\n&gt; operators can be any mathematical function.\n&gt; \n&gt; So, you start with a minimal function.  It just adds all the inputs\n&gt; together or something.  Then you evolve the parse tree for the\n&gt; function.  To evaluate the function, you simply take the average of\n&gt; the values within an (x1,y1,x2,y2) region and see if it corresponds\nto\n&gt; the expected pixel value.\n&gt; \n&gt; Admittedly, the search space for such a thing would be enormous. \nAnd\n&gt; I doubt that there is a smooth ramp-up that it could follow to help\nit\n&gt; evolve.  But I don&#39;t know...has anybody read anything useful\nregarding\n&gt; evolution of formulas?\n\n\n"}}