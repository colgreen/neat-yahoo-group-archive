{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":283334584,"authorName":"petar_chervenski","from":"&quot;petar_chervenski&quot; &lt;petar_chervenski@...&gt;","profile":"petar_chervenski","replyTo":"LIST","senderId":"u5NJXKCbCrppPkwKSaeqG0yP2OwsS6lid8sfH3rjDBF5fmqmmsnlCMbzX5wx3oNCI89gZzf9UD0Zu6mueIVDK7Zum4_o-k6SDNXOfv9PYThnhSYA998","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Machine Learning and the Long View of AI","postDate":"1209354296","msgId":4000,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGZ2M2g3byt2N2s4QGVHcm91cHMuY29tPg==","inReplyToHeader":"PGZ2MnJsMSs5cWdjQGVHcm91cHMuY29tPg=="},"prevInTopic":3999,"nextInTopic":4001,"prevInTime":3999,"nextInTime":4001,"topicId":3955,"numMessagesInTopic":49,"msgSnippet":"Great post, Ken! I really enjoyed reading it. It is just all true. There is really a difference between building a brain and evolving one. No matter if the","rawEmail":"Return-Path: &lt;petar_chervenski@...&gt;\r\nX-Sender: petar_chervenski@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 10372 invoked from network); 28 Apr 2008 03:44:58 -0000\r\nX-Received: from unknown (66.218.67.97)\n  by m55.grp.scd.yahoo.com with QMQP; 28 Apr 2008 03:44:58 -0000\r\nX-Received: from unknown (HELO n27c.bullet.scd.yahoo.com) (66.218.67.220)\n  by mta18.grp.scd.yahoo.com with SMTP; 28 Apr 2008 03:44:58 -0000\r\nX-Received: from [209.73.164.83] by n27.bullet.scd.yahoo.com with NNFMP; 28 Apr 2008 03:44:58 -0000\r\nX-Received: from [66.218.67.195] by t7.bullet.scd.yahoo.com with NNFMP; 28 Apr 2008 03:44:58 -0000\r\nDate: Mon, 28 Apr 2008 03:44:56 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;fv3h7o+v7k8@...&gt;\r\nIn-Reply-To: &lt;fv2rl1+9qgc@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;petar_chervenski&quot; &lt;petar_chervenski@...&gt;\r\nSubject: Re: Machine Learning and the Long View of AI\r\nX-Yahoo-Group-Post: member; u=283334584; y=rHFJSpXs5GtEnoUw0Ou93L7L5_O4ck4ldmvwxBw7p68K7L_4q9vYU233eQ\r\nX-Yahoo-Profile: petar_chervenski\r\n\r\nGreat post, Ken! I really enjoyed reading it. It is just all true. \n\nThere =\r\nis really a difference between building a brain and evolving \none. No matte=\r\nr if the brain learns in its lifetime or not. In the \ncase you build a brai=\r\nn yourself that doesn&#39;t learn, it is just \nconventional programming at all.=\r\n \n\nI think the constraints for evolution should be very sharp, so to \nsay, =\r\nbecause in EC in general the fitness function is the most \nimportant thing =\r\nas well as the representation/mapping. You can&#39;t just \nsay &quot;be smart!&quot; to a=\r\nn EC algorithm. You have to model \nits &quot;environment&quot; as well, and the proce=\r\nss of evaluation usually \ntakes a lot of computation time for the most inte=\r\nresting problems. \n\nThere is a kind of.. Hm I guess I can&#39;t express myself =\r\nin english \nwell. The more complex the task is, the more computation time i=\r\ns \nrequired for a proper evaluation. \nI am maybe not saying anything new to=\r\n you, Ken, but I just mention I \nunderstand it. \n\nHyperNEAT and CPPNs in ge=\r\nneral opened up an entire new field of \nresearch, that is, the evolution of=\r\n mathematical compositions \ndescribing phenotypes of any kind. What I think=\r\n about it is, that \nmutations are mostly destructive to the networks, while=\r\n in a robotics \nexperiments with direct representation, one weight change i=\r\ns not that \nbad, so to say. But change one weight of a CPPN and you get a t=\r\notally \ndifferent thing. In HyperNEAT this is not just a minor change, but =\r\na \ntotal change of the network&#39;s behaviour. So there is a great deal of \nco=\r\nmputation time required to discover some concepts. In fact the \nfitness lan=\r\ndscape in CPPN-based evolution is totally different than \nother approaches =\r\nto the same problem. \n\nAnother thing is that the geometry itself does not p=\r\nrovide \ninformation about the phenotype complexity at all. I mean that even=\r\n a \nnetwork of 1000000000 connections can be generated by a connective \nCPP=\r\nN but the bias is usually towards minimal solutions. I know that \ncomplexif=\r\nication is a property of the genotype space, but why to \nwaste computation =\r\ntime evaluating individuals with millions of \nconnections that actually are=\r\n bad solutions? \n\nSo, you may provide the geometry to the search, but you s=\r\ntill can&#39;t \nprovide the complexity. You need a priori that the complexity o=\r\nf the \nsubstrate is big enough. \n\nThat 0.2 treshold is like a hard-coded ha=\r\nck to me. It may be able to \nrepresent any kind of connectivity, but I thin=\r\nk the effort for \ndiscovering it is bigger than discovering the actual regu=\r\nlarities at \nall. \n\nThere should be a way to map complexity of the genotype=\r\n to the \nphenotype, but not in such a constrained way. It should be \nincrea=\r\nsing. Did you ever see an animal as simple as a worm but big as \na whale? O=\r\nK size doesn&#39;t matter. :) This comparison was not a good \none. \n\nI know the=\r\nre is an option that HyperNEAT can evolve the substrate by \nitself, but how=\r\n to control it? The dynamics of the neural networks \nhas to be taken into a=\r\nccount. \n\nSorry about my scattered around thoughts. That was just a stream =\r\nof \nconciosness. \n\nPeter\n\n--- In neat@yahoogroups.com, &quot;Kenneth Stanley&quot; &lt;k=\r\nstanley@...&gt; wrote:\n&gt;\n&gt; --- In neat@yahoogroups.com, &quot;Derek James&quot; &lt;djames@=\r\n&gt; wrote:\n&gt; \n&gt; &gt; \n&gt; &gt; &gt;  In RL, in contrast, the long view is almost the opp=\r\nosite: They\n&gt; want to\n&gt; &gt; &gt;  remove all constraints and still learn neverth=\r\neless.\n&gt; &gt; \n&gt; &gt; I&#39;m not sure what you mean by this, Ken. Could you elaborat=\r\ne a \nlittle?\n&gt; &gt;\n&gt; \n&gt; Sure.  I think the problem is that I can&#39;t find a way=\r\n to explain my\n&gt; point concisely.  As I try to explain it, it starts taking=\r\n up too \nmuch\n&gt; text so I shorten it and then it loses its meaning.  Let me=\r\n give it \na\n&gt; try again...\n&gt; \n&gt; I think the difference between the goals of=\r\n RL and NE is an\n&gt; interesting topic because they are almost always conflat=\r\ned, as if \nthey\n&gt; are trying to solve the same problem.\n&gt; \n&gt; The RL communi=\r\nty (e.g. value-function approaches) is trying to build\n&gt; something that lea=\r\nrns like a natural brain.  They are saying, \nthrough\n&gt; analytic means we ca=\r\nn deduce how a brain can learn from sparse\n&gt; reinforcement and formalize th=\r\nat process in an algorithm.  The \nhope, I\n&gt; would think, is to eventually b=\r\nuild the &quot;general intelligence&quot; that\n&gt; aligns with the holy grail of AI.  S=\r\no each step along the way is an\n&gt; improvement in that general ability.\n&gt; \n&gt;=\r\n So if that is your goal, then the benchmarks you choose have to be\n&gt; desig=\r\nned to measure progress to that goal.  So what they need to do \nis\n&gt; show t=\r\nhat their designed intelligence can work largely independently\n&gt; of a prior=\r\ni &quot;cheats&quot; that provide the meat of the solution.  \nBecause,\n&gt; after all, h=\r\now can it be a general intelligence if it needs you to\n&gt; tell it something =\r\nthat it is supposed to be able to figure out?  \nThis\n&gt; perspective, I belie=\r\nve, is aligned with Jeff&#39;s view.\n&gt; \n&gt; However, NE as a long-term pursuit is=\r\n involved in something \ndifferent,\n&gt; even though it can be applied to the s=\r\name problems.  NE is not an\n&gt; attempt to formalize how people learn with sp=\r\narse reinforcement. \n&gt; Rather, it is an attempt to formalize how evolution =\r\ncan build a \nbrain.\n&gt;  So RL is formalizing the brain itself and NE is form=\r\nalizing how\n&gt; evolution succeeds in creating a brain.  NE is therefore one =\r\nstep \nremoved.\n&gt; \n&gt; This difference is ultimately a philosophical differenc=\r\ne on the best\n&gt; approach to creating a full-blown AI.  The instrumental iss=\r\nue is\n&gt; whether you think it&#39;s easier to build it yourself or to design an\n=\r\n&gt; algorithm that can build it.  The confusion and hence conflation of\n&gt; the=\r\n two approaches arises in part because they do indeed both aim at\n&gt; the sam=\r\ne long view goal: a general AI.  But they are coming at it \nfrom\n&gt; very dif=\r\nferent angles.\n&gt; \n&gt; And because of this stark difference, the *metric* of p=\r\nrogress \nshould\n&gt; be quite different.  We cannot measure our progress in bu=\r\nilding a\n&gt; general intelligence directly in the same way that we measure ou=\r\nr\n&gt; progress in creating an evolutionary algorithm that itself will\n&gt; somed=\r\nay output one.  \n&gt; \n&gt; This distinction is potentially subtle and confusing =\r\nso let me try \nto\n&gt; make it clearer:  Human brains aren&#39;t designed to build=\r\n yet more \nhuman\n&gt; brains.  We are good at a lot of things, and we learn ge=\r\nnerally, but\n&gt; we do not build 100-trillion part devices that are more comp=\r\nlex than\n&gt; any known object in the universe.  I&#39;m not saying we won&#39;t ever =\r\nbe\n&gt; able to do it, but if you want to simulate a human brain, your first\n&gt;=\r\n thought would not be that it needs to be capable of designing yet\n&gt; anothe=\r\nr brain by itself.  Your first thought is about things like\n&gt; object recogn=\r\nition or pursuit and evasion.\n&gt; \n&gt; In contrast, building brains is exactly =\r\nwhat natural evolution did,\n&gt; and it did it quite well.  Natural evolution =\r\ndoes not perform object\n&gt; recognition; it does not communicate with languag=\r\ne; it does not run\n&gt; away from predators or hunt for prey.  Yet it does bui=\r\nld brains that\n&gt; themselves do those things.  And that is the aspect of it =\r\nwe wish to\n&gt; harness- a very specific niche kind of skill (though radically=\r\n\n&gt; impressive)- not a general skill.\n&gt; \n&gt; So the two pursuits are really qu=\r\nite different.  And therefore they\n&gt; deserve different metrics to judge the=\r\nir progress with respect to \nthe\n&gt; long term goal.  That is, unless we conf=\r\nlate them to be the same\n&gt; thing, which we often do without thinking about =\r\nit.\n&gt; \n&gt; For example, we could just say, well, both NE and RL are learning\n=\r\n&gt; techniques, and after all, we can apply them to the same problems, \nso\n&gt; =\r\nwhy make a big distinction in how we judge them?  Let&#39;s just compare\n&gt; them=\r\n directly on the same benchmarks and get on with it.\n&gt; \n&gt; That&#39;s fine for t=\r\nhe short-term view, i.e. let&#39;s just improve our\n&gt; ability to tackle practic=\r\nal problems, but for the long view, they\n&gt; cannot be judged in the same way=\r\n.  If I improve at my ability to\n&gt; balance on one foot is that a sign that =\r\nI will be able to build a\n&gt; brain someday?  If evolution evolves a brain th=\r\nat plays checkers, is\n&gt; that a sign that evolution *itself* is on the road =\r\nto performing\n&gt; object recognition?  These are totally different pursuits.\n=\r\n&gt; \n&gt; So in that context, how should they be judged with respect to long\n&gt; t=\r\nerm goals?  Well, I think RL deserves to be judged based on its\n&gt; increasin=\r\ng ability to learn more generally.  And in that sense,\n&gt; exactly Jeff&#39;s cri=\r\nteria should apply to it: We should be interested \nin\n&gt; whether it &quot;needs&quot; =\r\na priori information to learn.  In other words, \nthe\n&gt; less we need to cons=\r\ntrain the problem for the learner, the more\n&gt; impressed we deserve to be.  =\r\nThat shows progress towards more and \nmore\n&gt; general AI and ML.\n&gt; \n&gt; But if=\r\n evolution is not *itself* supposed to be a general learner\n&gt; (rather, we j=\r\nust want it to concentrate on one very specific skill:\n&gt; brain building), t=\r\nhen those considerations are orthogonal to its\n&gt; greatest promise.  Its pro=\r\nmise is to evolve a brain itself, and as\n&gt; such, neuroevolutionary algorith=\r\nms deserve to be judged on our \nability\n&gt; to *constrain* the problem so tha=\r\nt they can accomplish exactly \nthat. \n&gt; In other words, the problem NE *alg=\r\norithms* face is leaps and bounds\n&gt; beyond what RL algorithms face.  RL alg=\r\norithms just need to be able \nto\n&gt; do as well as brains; NE has to be able =\r\nto discover brains \nthemselves.\n&gt;  Therefore, progress is NE should in part=\r\n be measured with respect \nto\n&gt; progress in constraining the problem to mak=\r\ne such a discovery more\n&gt; likely.  When an NE algorithm is improved to allo=\r\nw us to tell it \nmore\n&gt; about the world in which its output will be situate=\r\nd, that is good\n&gt; news for the long view.  In short, we don&#39;t care at all h=\r\now NE\n&gt; produced a brain as long as it really does.  Will anyone complain \n=\r\nif a\n&gt; human brain pops out of a system that was a priori given the concept=\r\n\n&gt; of symmetry?  Rather, we should be glad that such a priori context \nwas\n=\r\n&gt; possible to provide in the first place, because it may have saved \nus a\n&gt;=\r\n year of wasted computation in figuring it out needlessly.\n&gt; \n&gt; This distin=\r\nction is almost completely ignored when NE and RL are\n&gt; compared directly. =\r\n Therefore, the implications of any such \ncomparison\n&gt; are fuzzy and lackin=\r\ng context with respect to the long view.  I am \nnot\n&gt; sure if I should care=\r\n or not if RL solves something better than NE, \nor\n&gt; vice versa, because th=\r\ne author doesn&#39;t explain how the result aligns\n&gt; with the long-term goals o=\r\nf the fields.  Long term goals seem like\n&gt; unwelcome guests these days in A=\r\nI, which is why I probably won&#39;t be\n&gt; writing about any of this in a public=\r\nation any time soon.  \n&gt; \n&gt; ...\n&gt; \n&gt; So Derek what you are saying about NE =\r\nbeing good at &quot;hard-wired&quot;\n&gt; solutions and RL being appropriate for ontogen=\r\netic lifetime \nlearning,\n&gt; while true, is not what I think of as the primar=\r\ny long-view issue.\n&gt; \n&gt; In the long view, NE will be used to evolve structu=\r\nres that do learn\n&gt; over their lifetime, i.e. not hardwired at all.  The on=\r\nly reason \nthat\n&gt; it tends to be used to evolve hardwired solutions today i=\r\ns because \nwe\n&gt; are trying to get a foothold on how to evolve certain types=\r\n of \ncomplex\n&gt; structures.   Once we get very good at it, focus will natura=\r\nlly \nshift\n&gt; to evolving dynamic brains (and of course there is already wor=\r\nk \nalong\n&gt; these lines today, much from Floreano).  I do not even think tha=\r\nt we\n&gt; will need to include stock learning algorithms like Hebbian \nlearnin=\r\ng.\n&gt;  When we achieve our long-term goals, those *themselves* will be \nleft=\r\n\n&gt; up to evolution because after all there may be something even \nbetter.\n&gt;=\r\n  \n&gt; &gt; &gt; My aim is to design an\n&gt; &gt; &gt;  algorithm that will output a brain, =\r\nnot to design the brain \nitself.\n&gt; &gt; \n&gt; &gt; But what kind of brain are you wa=\r\nnting to output?\n&gt; &gt; \n&gt; \n&gt; Note that I&#39;m speaking purely about the long vie=\r\nw for these \ndifferent\n&gt; fields here.  Of course on a day-to-day basis I am=\r\n not solely \nfocused\n&gt; on what will happen 100 years from now.  On a practi=\r\ncal day-to-day\n&gt; basis, of course I want to make NE better capable to tackl=\r\ne problems\n&gt; that e.g. RL tackles.  So in the short-term context, I just wa=\r\nnt to\n&gt; output something that works for the problem at hand.\n&gt; \n&gt; But in th=\r\ne long view, which we were talking about, I think the\n&gt; ultimate goal would=\r\n be to output a full-fledged adaptive system with\n&gt; astronomical complexity=\r\n and the power and subtlety of human \nreasoning.\n&gt;  On that path, constrain=\r\nt is the only hope, unless you want to wait\n&gt; three billion years and just =\r\nhope in the meantime that the initial\n&gt; conditions were set up correctly.  =\r\nTherefore, demonstrations of the\n&gt; power of constraint deserve to be judged=\r\n as evidence of the promise \nof\n&gt; and progress towards the long term goal i=\r\nn NE.\n&gt; \n&gt; ken\n&gt;\n\n\n\n"}}