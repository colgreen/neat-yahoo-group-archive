{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":7192225,"authorName":"Ian Badcoe","from":"Ian Badcoe &lt;ian_badcoe@...&gt;","profile":"ian_badcoe","replyTo":"LIST","senderId":"vJN7iIdJL8tJZpmdtB8IuGfHWgb1_-KYJ3zw8Ifl-1-eQeJGKyaNVf4TqRQ-sZpmWW0gPvtRenbJTmkEe923YlRi3UlLaGfW8Bw","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] Re: Computation Time","postDate":"1087380967","msgId":1086,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDYuMS4wLjYuMC4yMDA0MDYxNjA5NDIzOC4wMjRmZTFmMEBwb3AubWFpbC55YWhvby5jby51az4=","inReplyToHeader":"PDQwQ0Y3RDk2LjYwNDA4MDFAZHNsLnBpcGV4LmNvbT4=","referencesHeader":"PGNhMzl2ays0djFhQGVHcm91cHMuY29tPiA8NDBDNjM4RDAuMTA2MDQwMkBkc2wucGlwZXguY29tPiA8Ni4xLjAuNi4wLjIwMDQwNjE1MTAxMDUxLjAyNTAwOWQwQHBvcC5tYWlsLnlhaG9vLmNvLnVrPiA8NDBDRjdEOTYuNjA0MDgwMUBkc2wucGlwZXguY29tPg=="},"prevInTopic":1081,"nextInTopic":1091,"prevInTime":1085,"nextInTime":1087,"topicId":845,"numMessagesInTopic":99,"msgSnippet":"... Right!  Then I think your results make sense except for the difference in sigmoid and tanh. - Inv-abs is fastest, because it uses no expensive fpu instns","rawEmail":"Return-Path: &lt;ian_badcoe@...&gt;\r\nX-Sender: ian_badcoe@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 35911 invoked from network); 16 Jun 2004 23:18:39 -0000\r\nReceived: from unknown (66.218.66.167)\n  by m14.grp.scd.yahoo.com with QMQP; 16 Jun 2004 23:18:39 -0000\r\nReceived: from unknown (HELO smtp001.mail.ukl.yahoo.com) (217.12.11.32)\n  by mta6.grp.scd.yahoo.com with SMTP; 16 Jun 2004 23:18:38 -0000\r\nReceived: from unknown (HELO ian2k.yahoo.co.uk) (ian?badcoe@212.159.73.108 with login)\n  by smtp001.mail.ukl.yahoo.com with SMTP; 16 Jun 2004 12:21:27 -0000\r\nMessage-Id: &lt;6.1.0.6.0.20040616094238.024fe1f0@...&gt;\r\nX-Sender: ian_badcoe@...\r\nX-Mailer: QUALCOMM Windows Eudora Version 6.1.0.6\r\nDate: Wed, 16 Jun 2004 11:16:07 +0100\r\nTo: neat@yahoogroups.com\r\nIn-Reply-To: &lt;40CF7D96.6040801@...&gt;\r\nReferences: &lt;ca39vk+4v1a@...&gt;\n &lt;40C638D0.1060402@...&gt;\n &lt;6.1.0.6.0.20040615101051.025009d0@...&gt;\n &lt;40CF7D96.6040801@...&gt;\r\nMime-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;us-ascii&quot;; format=flowed\r\nX-eGroups-Remote-IP: 217.12.11.32\r\nFrom: Ian Badcoe &lt;ian_badcoe@...&gt;\r\nSubject: Re: [neat] Re: Computation Time\r\nX-Yahoo-Group-Post: member; u=7192225\r\nX-Yahoo-Profile: ian_badcoe\r\n\r\nAt 23:52 15/06/2004, you wrote:\n&gt;Ian Badcoe wrote:\n&gt;\n&gt; &gt;&gt;Hi Philip,\n&gt; &gt;&gt;\n&gt; &gt;&gt;My curiosity got the better of me :) I tried the above functions using\n&gt; &gt;&gt;optimized C# on an AMD Athlon 2400+ (actually 2.17Ghz). The results are\n&gt; &gt;&gt;slightly bizarre,\n&gt; &gt;&gt; oh BTW I think you quoted the tanh function wrong, so I used y =\n&gt; &gt;&gt;tanh(0.9*x) which gives a nice sigmoid. Firstly I had to use 100 million\n&gt; &gt;&gt;(10^8) loops to get readable results, the approx. 50x difference is\n&gt; &gt;&gt;partly due to the CPU (obviously!) but maybe the rest is due to my\n&gt; &gt;&gt;oversimplistic implementation whereby I used the same value for x every\n&gt; &gt;&gt;time - did you generate random numbers perhaps? Also I know that Java\n&gt; &gt;&gt;has JIT compilers but sometime only optimize in code hot-spots during\n&gt; &gt;&gt;code execution, they can also run in interpreter mode - my run was with\n&gt; &gt;&gt;JITed code.\n&gt; &gt;&gt;\n&gt; &gt;&gt;Here are the figures:\n&gt; &gt;&gt;\n&gt; &gt;&gt;sigmoid:  3625ms\n&gt; &gt;&gt;evsail:    2359ms\n&gt; &gt;&gt;inv-abs:  188ms\n&gt; &gt;&gt;\n&gt; &gt;&gt;\n&gt; &gt;\n&gt; &gt;By my calculations, this makes just over 3 cycles per complete\n&gt; &gt;calculation.  That&#39;s not impossible.  e.g. ISRT on the K7 (Athlon\n&gt; &gt;predecessor) a floating-divide took 3 cycles but that the chip was able to\n&gt; &gt;have 2 fdivs and 2fadds and some integer instructions running \n&gt; simulatneously.\n&gt; &gt;\n&gt; &gt;\n&gt;Yep, via the various  instruction pipelines. Assuming the a 2.17Ghz\n&gt;clock I translate the above figures as follows:\n&gt;\n&gt;sigmoid: 78.66 cycles\n&gt;evsail:     51.00\n&gt;inv-abs:    4.08\n&gt;tanh:      269.00\n&gt;\n&gt;certainly interesting.\n&gt;\n&gt; &gt;It does sound suspiciously good, however.  I don&#39;t know much about C# but\n&gt; &gt;presumably it&#39;s inlining the function, and maybe unrolling the loop a\n&gt; &gt;little.  OTOH, if it did all that, then it should be able to see that you\n&gt; &gt;are making the same call every time and that the function has no side\n&gt; &gt;effects, so did it need to run the function at all?\n&gt; &gt;\n&gt; &gt;\n&gt;That particular test was a loop, no methods calls involved.\n\nRight!  Then I think your results make sense except for the difference in \nsigmoid and tanh.\n\n- Inv-abs is fastest, because it uses no expensive fpu instns and also the \nones it does use can be executed in parallel.\n- evSail is next because it also avoids expensive fpu instructions, but it \nhas 3 conditional jumps, which slow it down a bit\n- then the sigmoid and tanh because they use slow fpu instructions\n\nThe difference between sigmoid and tanh I don&#39;t get, however, since they \nuse near-identical sets of instructions:\n\nsigmoid:  y = 1 / ( 1 + EXP( -( x * 4.924273 ) ) )\n         : fmul, fexp, fadd, fdiv\ntanh:     y = -1 + ( 2 / ( 1 + EXP( -2 * ( x ) ) ) )\n         : fmul, fexp, fadd, fdiv, fadd\n\nUnless you used the equivalent call to tanh?  In which case maybe the \ncompiler is making a library call and even doing some range checking...?\n\n&gt; &gt;The thing about the more exotic instructions, like tan, is that not only do\n&gt; &gt;they take a lot of cycles, but the chip only has one processor for\n&gt; &gt;them.  Also slow instructions have a disproportionate effect on throughput\n&gt; &gt;because all the shorter instructions, which could run in parallel, can only\n&gt; &gt;go so far before they hit a dependency on the result of the long\n&gt; &gt;instruction and have to stop.  Thus effectively the whole chip hangs on the\n&gt; &gt;result of the tan.\n&gt; &gt;\n&gt; &gt;\n&gt;I think modern cpu&#39;s have more than one fpu pipeline - but yes, the\n&gt;principle still holds.\n\nLast time I looked (and this was three years ago) they have multiple \npipelines for the common fpu instructions (fdiv, fmul, fadd) but the more \nexotic instructions were generally only included in one of those pipelines.\n\n&gt; &gt;You easily can do a better analysis than that.  Run the timing a few times\n&gt; &gt;with different sizes of network (number of Ops) then plot the line of\n&gt; &gt;number of ops (x) vs time (y).  You should get an +ve intercept on the\n&gt; &gt;y-axis which is the constant cost of your program and a +ve sloping line,\n&gt; &gt;which is the cost per op...\n&gt; &gt;\n&gt; &gt;\n&gt;OK I&#39;ve made 3 measurements, the same network as before but with 104,\n&gt;207, and 413 connections. The times are:\n&gt;\n&gt;104: 2481ms\n&gt;207: 3343ms\n&gt;413:  4678ms\n&gt;\n&gt;If you plot these on a graph it is slightly non-linear, the line is\n&gt;curving upwards - which is what you might expect if, say, the cache is\n&gt;becoming less efficient with the accessing of more data. Assuming a\n&gt;straight line between the first and last reading, this then gives:\n&gt;\n&gt;secs per connection: 7.11 * 10^-8\n&gt;connections/sec : 14,064,633\n&gt;clock cycles/connection: 154\n&gt;\n&gt;\n&gt;These times don&#39;t include calculating the activation fn, this is all\n&gt;time spent executing loops to fetch a neuron output value, multiply the\n&gt;value by a weight and then add that to a total  ready to be put through\n&gt;the activation fn. So perhaps there is room for improvment there, a\n&gt;multiply an add and a couple of memory accesses taking 154 cycles is a\n&gt;little bit sloppy, but then this is .NET remember - and as such there is\n&gt;also a single type cast in there because .NET does not yet support\n&gt;templates (to be called Generics I believe), this could well be the bulk\n&gt;of the 154 cycles!\n\nI don&#39;t know much about .NET I assumed (from the result of 3 cycles for \ninv-abs activation that we got before) that it was an optimising compiler \nand makes native code.  Am I right in that?\n\n154 for a couple of accesses might be a bit slow, but what exactly is involved?\n\nI imagine code something like...\n\nnode[node_count] = node[node_count] + node[connect_from[connection_count]] \n* connect_weight[connection_count];\n\nWhich is 4 memory accesses:\n\nnode 1\nconnect_from\nnode 2\nconnect_weight\n\nA uncached memory access can take tens of cycles, but I&#39;d guess your \nexample was completely cached.\n\nNote that the access of node2 cannot begin until the access of connect_from \nis completed.  This causes a stall in the instruction pipeline because \nnormally two successive instructions would execute partially \noverlapped.  Also, if you had any conditionals in the loop (say for \ndisabled connections) then those can add a lot to execution time.\n\n         Ian Badcoe\n\n\n\nLiving@Home - Open Source Evolving Organisms - \nhttp://livingathome.sourceforge.net/\n\n\n\n"}}