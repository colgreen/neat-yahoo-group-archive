{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":7192225,"authorName":"Ian Badcoe","from":"Ian Badcoe &lt;ian_badcoe@...&gt;","profile":"ian_badcoe","replyTo":"LIST","senderId":"Zv_-hikk3FvPFqnplK3qWhba91cPcDv6q_zayVzOyRgTRlnRtebgXqF3ZDj7Y9roiXPM2ZbMCQ5Ld-yOhGy9WoIIDt4B9nELTmQ","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] Computation Time","postDate":"1087476960","msgId":1091,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDYuMS4wLjYuMC4yMDA0MDYxNzEzNTQyNi4wMjUyZWQ4OEBwb3AubWFpbC55YWhvby5jby51az4=","inReplyToHeader":"PEJBWTItRjg4eFE4U1RJMXJnUnAwMDA2OTFlMUBob3RtYWlsLmNvbT4=","referencesHeader":"PEJBWTItRjg4eFE4U1RJMXJnUnAwMDA2OTFlMUBob3RtYWlsLmNvbT4="},"prevInTopic":1086,"nextInTopic":1093,"prevInTime":1090,"nextInTime":1092,"topicId":845,"numMessagesInTopic":99,"msgSnippet":"Hi, I can t quite get the details of your 4 approaches from these brief descriptions, can you post some pseudo-code.  Also networks was small, were you","rawEmail":"Return-Path: &lt;ian_badcoe@...&gt;\r\nX-Sender: ian_badcoe@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 7640 invoked from network); 17 Jun 2004 16:02:51 -0000\r\nReceived: from unknown (66.218.66.167)\n  by m25.grp.scd.yahoo.com with QMQP; 17 Jun 2004 16:02:51 -0000\r\nReceived: from unknown (HELO smtp001.mail.ukl.yahoo.com) (217.12.11.32)\n  by mta6.grp.scd.yahoo.com with SMTP; 17 Jun 2004 16:02:51 -0000\r\nReceived: from unknown (HELO ian2k.yahoo.co.uk) (ian?badcoe@212.159.73.108 with login)\n  by smtp001.mail.ukl.yahoo.com with SMTP; 17 Jun 2004 16:02:48 -0000\r\nMessage-Id: &lt;6.1.0.6.0.20040617135426.0252ed88@...&gt;\r\nX-Sender: ian_badcoe@...\r\nX-Mailer: QUALCOMM Windows Eudora Version 6.1.0.6\r\nDate: Thu, 17 Jun 2004 13:56:00 +0100\r\nTo: neat@yahoogroups.com\r\nIn-Reply-To: &lt;BAY2-F88xQ8STI1rgRp000691e1@...&gt;\r\nReferences: &lt;BAY2-F88xQ8STI1rgRp000691e1@...&gt;\r\nMime-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;us-ascii&quot;; format=flowed\r\nX-eGroups-Remote-IP: 217.12.11.32\r\nFrom: Ian Badcoe &lt;ian_badcoe@...&gt;\r\nSubject: Re: [neat] Computation Time\r\nX-Yahoo-Group-Post: member; u=7192225\r\nX-Yahoo-Profile: ian_badcoe\r\n\r\nHi,\n         I can&#39;t quite get the details of your 4 approaches from these \nbrief descriptions, can you post some pseudo-code.  Also networks was \nsmall, were you evaluating it over and over with different data?\n\n         Ian Badcoe\n\nAt 15:30 16/06/2004, you wrote:\n&gt;I did some tests.\n&gt;\n&gt;I created a simulated (i.e. meaningless) network activation function.  The\n&gt;simulated network had 10 input nodes, 10 hidden nodes, and one output node.\n&gt;The network was fully connected, with random weights.\n&gt;\n&gt;I then tried four variations of the code, to see the effect on the speed:\n&gt;\n&gt;1. declare an array, and store the intermediate values (node * weight) in\n&gt;the array.  Then perform all the additions, then all of the sigmoids.  Idea\n&gt;was to allow as much parallelism as the chip could muster during the\n&gt;multiplication phase...\n&gt;\n&gt;2. alter the order of processing so that a single node value was being\n&gt;multiplied by each of its different weights, such that memory access was\n&gt;streamlined\n&gt;\n&gt;3. intersperse the additions and sigmoids among the multiplications.\n&gt;\n&gt;4. compose single expressions that perform all the multiplications,\n&gt;additions, and sigmoid all at once\n&gt;\n&gt;The results were:\n&gt;\n&gt;1. 377k/sec\n&gt;2. 380k/sec  (so the memory order did make a difference here)\n&gt;3. 430k/sec  (made a big difference to give the CPU something to do while it\n&gt;waited for the sigmoid to finish)\n&gt;4. 453k/sec  (letting the compiler do the optimization works well)\n&gt;\n&gt;The one configuration that I did NOT try was the traditional &#39;loop&#39;\n&gt;approach, where node and weights are fetched from memory.  I can not see how\n&gt;that approach could ever compare to the unrolled version.  Since there are\n&gt;no conditionals, there is no chance of a branch-prediction miss.  So the\n&gt;values of weights will always be available as soon as they are needed.   And\n&gt;since the only thing stored outside the code stream are node weights, the\n&gt;whole thing should have no trouble fitting in the cache, unless you have a\n&gt;really, REALLY large network.\n&gt;\n&gt;Thoughts?\n&gt;\n&gt;-- John\n&gt;\n&gt;\n&gt; &gt;From: Colin Green &lt;cgreen@...&gt;\n&gt; &gt;Reply-To: neat@yahoogroups.com\n&gt; &gt;To: neat@yahoogroups.com\n&gt; &gt;Subject: Re: [neat] Re: Computation Time\n&gt; &gt;Date: Tue, 15 Jun 2004 23:52:06 +0100\n&gt; &gt;\n&gt; &gt;Ian Badcoe wrote:\n&gt; &gt;\n&gt; &gt; &gt;&gt;Hi Philip,\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt;My curiosity got the better of me :) I tried the above functions using\n&gt; &gt; &gt;&gt;optimized C# on an AMD Athlon 2400+ (actually 2.17Ghz). The results are\n&gt; &gt; &gt;&gt;slightly bizarre,\n&gt; &gt; &gt;&gt; oh BTW I think you quoted the tanh function wrong, so I used y =\n&gt; &gt; &gt;&gt;tanh(0.9*x) which gives a nice sigmoid. Firstly I had to use 100 million\n&gt; &gt; &gt;&gt;(10^8) loops to get readable results, the approx. 50x difference is\n&gt; &gt; &gt;&gt;partly due to the CPU (obviously!) but maybe the rest is due to my\n&gt; &gt; &gt;&gt;oversimplistic implementation whereby I used the same value for x every\n&gt; &gt; &gt;&gt;time - did you generate random numbers perhaps? Also I know that Java\n&gt; &gt; &gt;&gt;has JIT compilers but sometime only optimize in code hot-spots during\n&gt; &gt; &gt;&gt;code execution, they can also run in interpreter mode - my run was with\n&gt; &gt; &gt;&gt;JITed code.\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt;Here are the figures:\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt;sigmoid:  3625ms\n&gt; &gt; &gt;&gt;evsail:    2359ms\n&gt; &gt; &gt;&gt;inv-abs:  188ms\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;By my calculations, this makes just over 3 cycles per complete\n&gt; &gt; &gt;calculation.  That&#39;s not impossible.  e.g. ISRT on the K7 (Athlon\n&gt; &gt; &gt;predecessor) a floating-divide took 3 cycles but that the chip was able\n&gt; &gt;to\n&gt; &gt; &gt;have 2 fdivs and 2fadds and some integer instructions running\n&gt; &gt;simulatneously.\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt;Yep, via the various  instruction pipelines. Assuming the a 2.17Ghz\n&gt; &gt;clock I translate the above figures as follows:\n&gt; &gt;\n&gt; &gt;sigmoid: 78.66 cycles\n&gt; &gt;evsail:     51.00\n&gt; &gt;inv-abs:    4.08\n&gt; &gt;tanh:      269.00\n&gt; &gt;\n&gt; &gt;certainly interesting.\n&gt; &gt;\n&gt; &gt; &gt;It does sound suspiciously good, however.  I don&#39;t know much about C# but\n&gt; &gt; &gt;presumably it&#39;s inlining the function, and maybe unrolling the loop a\n&gt; &gt; &gt;little.  OTOH, if it did all that, then it should be able to see that you\n&gt; &gt; &gt;are making the same call every time and that the function has no side\n&gt; &gt; &gt;effects, so did it need to run the function at all?\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt;That particular test was a loop, no methods calls involved. But yes the\n&gt; &gt;.Net compiler does do inlining, although there is no inline hint keyword\n&gt; &gt;as in some C++ compilers - as I understood it the keyword was largely\n&gt; &gt;ignored in later compilers anyway - based on the idea that the compiler\n&gt; &gt;knows best.\n&gt; &gt;\n&gt; &gt; &gt;Compilers can be blind to that sort of thing, however, like I mentioned\n&gt; &gt;before.\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;&gt;tanh:     12,400ms\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt;weird huh.  The tanh loop took 66x longer then the ins-abs one!\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;All trig, exp and log are very expensive.\n&gt; &gt; &gt;Sqrt is expensive but maybe not so bad.\n&gt; &gt; &gt;Divide is releatively cheap nowadays.\n&gt; &gt; &gt;\n&gt; &gt; &gt;The thing about the more exotic instructions, like tan, is that not only\n&gt; &gt;do\n&gt; &gt; &gt;they take a lot of cycles, but the chip only has one processor for\n&gt; &gt; &gt;them.  Also slow instructions have a disproportionate effect on\n&gt; &gt;throughput\n&gt; &gt; &gt;because all the shorter instructions, which could run in parallel, can\n&gt; &gt;only\n&gt; &gt; &gt;go so far before they hit a dependency on the result of the long\n&gt; &gt; &gt;instruction and have to stop.  Thus effectively the whole chip hangs on\n&gt; &gt;the\n&gt; &gt; &gt;result of the tan.\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt;I think modern cpu&#39;s have more than one fpu pipeline - but yes, the\n&gt; &gt;principle still holds.\n&gt; &gt;\n&gt; &gt; &gt;&gt;  I wonder though if the technqiue of trying to do\n&gt; &gt; &gt;&gt;many sequentail ops in order wll only become beneficial when the\n&gt; &gt; &gt;&gt;networks get *really* big, simply because the memory caches in modern\n&gt; &gt; &gt;&gt;CPU&#39;s are so large. So there may be some network size at which we would\n&gt; &gt; &gt;&gt;see a dramatic slow down of our code if it&#39;s not optimized in such a\n&gt; &gt;way.\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;That&#39;s what I would expect, not just the network size, however, also the\n&gt; &gt; &gt;total size of the data you want to pass through.  If you run many copies\n&gt; &gt;of\n&gt; &gt; &gt;the same small network on different data then memory-access may be your\n&gt; &gt; &gt;bottleneck.\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt;\n&gt; &gt;Ok but the input data is copied into the input nodes and then the bulk\n&gt; &gt;of the network CPU time is in activating the whole network several times\n&gt; &gt;over. So this really depends on how many network epochs you run on each\n&gt; &gt;set of input data.\n&gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;&gt;Another way of estimating how efficient my code is is to caclulate the\n&gt; &gt; &gt;&gt;average number of clock cycles that it requires per neuron and\n&gt; &gt; &gt;&gt;connection. So e.g. My 53 neuron / 413 connection network performs 413\n&gt; &gt; &gt;&gt;additions and 53 activations per epoch. So that&#39;s 466 necessary\n&gt; &gt; &gt;&gt;operations in all, this is an absolute minimum. ok, plus a couple\n&gt; &gt; &gt;&gt;because the activation fn is several operations (but this is just a\n&gt; &gt; &gt;&gt;rough bit of maths). Using a simple bit of maths I can then determine:\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt;ops per epoch = 466\n&gt; &gt; &gt;&gt;ops per test run = 466 * 100,000 (loops) = 46,600,000\n&gt; &gt; &gt;&gt;ops per second = 46,600,000 / 5000ms(approx) = 9,320,000\n&gt; &gt; &gt;&gt;CPU clock cycles per op = 2.17Ghz / 9,320,000 = 232.\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt;Now 232 isn&#39;t all that bad when you consider this doesn&#39;t take into\n&gt; &gt; &gt;&gt;account the extra code that is required to do the looping/indexing\n&gt; &gt; &gt;&gt;through all of the neurons and connections. So perhaps hand optimized\n&gt; &gt; &gt;&gt;assembler could get this down to 100 cylcles or maybe 50, but this is in\n&gt; &gt; &gt;&gt;the same ball park as optimum - and therefore I wouldn&#39;t expect any\n&gt; &gt; &gt;&gt;massive improvements. Well, not unless you start using SIMD\n&gt; &gt; &gt;&gt;instructions, which I&#39;m definitely NOT! :)\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;You easily can do a better analysis than that.  Run the timing a few\n&gt; &gt;times\n&gt; &gt; &gt;with different sizes of network (number of Ops) then plot the line of\n&gt; &gt; &gt;number of ops (x) vs time (y).  You should get an +ve intercept on the\n&gt; &gt; &gt;y-axis which is the constant cost of your program and a +ve sloping line,\n&gt; &gt; &gt;which is the cost per op...\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt;OK I&#39;ve made 3 measurements, the same network as before but with 104,\n&gt; &gt;207, and 413 connections. The times are:\n&gt; &gt;\n&gt; &gt;104: 2481ms\n&gt; &gt;207: 3343ms\n&gt; &gt;413:  4678ms\n&gt; &gt;\n&gt; &gt;If you plot these on a graph it is slightly non-linear, the line is\n&gt; &gt;curving upwards - which is what you might expect if, say, the cache is\n&gt; &gt;becoming less efficient with the accessing of more data. Assuming a\n&gt; &gt;straight line between the first and last reading, this then gives:\n&gt; &gt;\n&gt; &gt;secs per connection: 7.11 * 10^-8\n&gt; &gt;connections/sec : 14,064,633\n&gt; &gt;clock cycles/connection: 154\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;These times don&#39;t include calculating the activation fn, this is all\n&gt; &gt;time spent executing loops to fetch a neuron output value, multiply the\n&gt; &gt;value by a weight and then add that to a total  ready to be put through\n&gt; &gt;the activation fn. So perhaps there is room for improvment there, a\n&gt; &gt;multiply an add and a couple of memory accesses taking 154 cycles is a\n&gt; &gt;little bit sloppy, but then this is .NET remember - and as such there is\n&gt; &gt;also a single type cast in there because .NET does not yet support\n&gt; &gt;templates (to be called Generics I believe), this could well be the bulk\n&gt; &gt;of the 154 cycles!\n&gt; &gt;\n&gt; &gt;Colin\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;Yahoo! Groups Links\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;Yahoo! Groups Links\n&gt;\n&gt;\n&gt;\n&gt;\n\n\n\nLiving@Home - Open Source Evolving Organisms - \nhttp://livingathome.sourceforge.net/\n\n\n\n"}}