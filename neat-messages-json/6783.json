{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":344770077,"authorName":"Colin Green","from":"Colin Green &lt;colin.green1@...&gt;","profile":"alienseedpod","replyTo":"LIST","senderId":"sCHIfUbO1G5dUpDX-2rqVgsjh9gGV_0ys8zGwflNDD1JKFQeics8KAzITPQH70F6JE4fEmIt6Dewj6YJWPmhIM8K3anG4C2PU_h4","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] Batching the population on GPU?","postDate":"1493461166","msgId":6783,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PENBRTBNK1llekI3K3lSM2RLXzFkZmVtb044VWFqd0x2MDhna0oxQVpFSkZHQXU1UmV5UUBtYWlsLmdtYWlsLmNvbT4=","inReplyToHeader":"PDE5ODA5NjkxODMuMTMzMjI1NDAuMTQ5MzM0OTczMTM4NUBtYWlsLnlhaG9vLmNvbT4=","referencesHeader":"PG9kNDNrOCszZTdiNDRAWWFob29Hcm91cHMuY29tPiA8Q0FFME0rWWVWNDJDSlBWVXhMa2VkOHl2MEVYdXhuQnU5Qk55NmRhN2EyTnFOMko4QnhnQG1haWwuZ21haWwuY29tPiA8MTE5ODgzOTI5MS43NDY4NTE3LjE0OTI4NTU3NzQ0MzlAbWFpbC55YWhvby5jb20+IDxDQUUwTStZZWptdmdqYjFNTnlSRmEtNkRvVDR3Vmo5RTZQd1BibWNMYmFpbWI4VDFmK3dAbWFpbC5nbWFpbC5jb20+IDwxOTgwOTY5MTgzLjEzMzIyNTQwLjE0OTMzNDk3MzEzODVAbWFpbC55YWhvby5jb20+"},"prevInTopic":6782,"nextInTopic":0,"prevInTime":6782,"nextInTime":6784,"topicId":6770,"numMessagesInTopic":7,"msgSnippet":"Hi Fred, On 28 April 2017 at 04:22, Fred Mitchell lordalveric@yahoo.com [neat] ... Sure, however, the neural net code we use within NEAT (when searching) can","rawEmail":"Return-Path: &lt;colin.green1@...&gt;\r\nX-Sender: colin.green1@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 58955 invoked by uid 102); 29 Apr 2017 10:20:08 -0000\r\nX-Received: from unknown (HELO mtaq6.grp.bf1.yahoo.com) (10.193.84.37)\n  by m15.grp.bf1.yahoo.com with SMTP; 29 Apr 2017 10:20:08 -0000\r\nX-Received: (qmail 25511 invoked from network); 29 Apr 2017 10:20:08 -0000\r\nX-Received: from unknown (HELO mta1002.groups.mail.bf1.yahoo.com) (98.139.170.166)\n  by mtaq6.grp.bf1.yahoo.com with SMTP; 29 Apr 2017 10:20:08 -0000\r\nX-Original-Return-Path: &lt;colin.green1@...&gt;\r\nX-Received-SPF: pass (domain of gmail.com designates 209.85.218.53 as permitted sender)\r\nX-YMailISG: o4HatTwWLDsR_Daqk8cFmOtnQ..HVivvY8iruCzKXXhesEmZ\n YwY21PAqCf5OVIPEoOB85fTHbh4poDcSLMrx54aKCwzTdKKotU4JuZuEhPrA\n U4J2DqeLqf8Wy33d2R98jalIlopdMCG5sSlH9o7P0NWJoVFchy2DEUCPuzk3\n 3gnRBe8eh9gZT.wSM7FRLLt6tERG3feSMvmlYkIMcpjYxDfgGdjBPy8sqtRP\n HnLbiAv.DYekNgF9aEHmzyPrYLilcuuUvMWNRO0phCSdXJEpe.AyLl_KkwtU\n 1uatO2QzP9v2QGamYBNLSVRDz9UN.DFbK2NBc9IGvmOGHxMlSTp63lx0BLy5\n fEDxHFWIkdLlrSuAc5aLYttnIRs_VWCFZqW1AcFcze78hKo9QsAssQyGmZRM\n fyN.bd9RVip7lG6XXsO91g8cO7tlcw_87FFbo0aRBvDVa_1KX6Jy47E_UaD0\n 4ZYIEThku.WGXNv3XNSDIQBdKIP7JOQDco.1f_PF4BCcfCEVIm7UTsQDHWeC\n K_RRVcYi1P7_fUdi5tWmRCcdZvB_yFY20P_YoNtsELCcNXe__coYeXMWKDpw\n yt8h0ea3lYRqS1xBNjmKODYRQLduxEW0YbMojMxs8UTD53NnViEeDMsBOQ..\n W1abaeR5OKM8YbJphG16Vwa887lJwz0FGpyRZ0f5j18KPp52BgOABRR0UHa3\n rsgL9waIL5gwVsapcflqB2A70UMhWdF9mJ0__6uOOK.umoit7ViAmPnAgxjT\n .ykE5ii3AZj9MTb9kRmnPcJZ.r8e4su85dt_d7UG0kzXZcqTGHTM1Nb5RmYj\n KzkdibJOAVTXuqS1ajZv2x6U.z8o1UkUx6xXzpfwkap_tHest1FJyIgo4jP3\n DxwuaaYnFK5su1aKsLr16Er4aydZ7P.p5k4kajyaaTT.PUWG_9E2u66Lfb9p\n E9BBysXJg2bjR3IgW6kVlTsFSenauEAXBNswLioHpNqNIR_xpfoODz7_Ey5_\n 0MK6XoFJbKNsp5E6fnS0PVsSiCv51O89rUVtLtAtjlm4wmM8dlLKLK3VIjw7\n AK6GkLVN.SbpcAfVWmvvOBGeyOGbMuZWm0MLR.tn37fZ6zI_6lmHwOaF0JQK\n J3ZOVyQyiH8ldEc9CbTfUAP0ucvzACHmFq.H6vnzFSUgEc_dFjNF9DrfFatR\n ZfOWWYOX2tflmrW0mZI5KVh03VYYv1wjkSTlzYabLnTRX_WrxD1RLxqQnv0W\n 1qrocw_CVAx5z9D7LfXfRVfkHJcpv_lEQEVC6dfE2WguBETIRobVsrdwbAh5\n 5u35Xg--\r\nAuthentication-Results: mta1002.groups.mail.bf1.yahoo.com  from=gmail.com; domainkeys=neutral (no sig);  from=gmail.com; dkim=pass (ok)\r\nX-Received: from 127.0.0.1  (EHLO mail-oi0-f53.google.com) (209.85.218.53)\n  by mta1002.groups.mail.bf1.yahoo.com with SMTPS; Sat, 29 Apr 2017 10:20:08 +0000\r\nX-Received: by mail-oi0-f53.google.com with SMTP id x184so50674339oia.1\n        for &lt;neat@yahoogroups.com&gt;; Sat, 29 Apr 2017 03:20:07 -0700 (PDT)\r\nX-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;\n        d=1e100.net; s=20161025;\n        h=x-gm-message-state:mime-version:in-reply-to:references:from:date\n         :message-id:subject:to:content-transfer-encoding;\n        bh=+eFy799w5K+HNzDGT+j2pUgDcDqCa6Csz4QU3pgmjqg=;\n        b=AvOfZ3hLAEHC9QLgQ4wBxWGwZTYmBEqC3yR+wznEEPBIak8HXcYnqhSn+aBFj99S7M\n         j1lwRWhaM0SLguAyJzWYQ7mYcEhoCLdoTJy6DcGu9hVZXzVjaakGeB20MoU5DPPXr2+U\n         e3AV31EhREJOMBcYGqrk9s+lH3nElHoWSNrL6VfhC47/hBkU9TM3Ma0KiGOIEEn5oCZb\n         qnZyaM0KIp0lpUkrfsrDabp6+rFYDq2u9Hy1qH+mWp1TURs30jAgZbCs5mt3ryFb6YxE\n         vTgu07AfV+oezIrjoC224rNMNeBwvjq1BiRgIRFCiEz2vNyRE/O6bKIUPgRY8wm+AhU5\n         qDrQ==\r\nX-Gm-Message-State: AN3rC/6aBrZ7weylcxqDLiQCup8B89zehGWiXpXToI01IPdXB4eWY7yt\n\tQ0kDS4cH8CWCYgY/NnEW7jSJ/6uznw==\r\nX-Received: by 10.157.17.143 with SMTP id v15mr6402934otf.140.1493461207229;\n Sat, 29 Apr 2017 03:20:07 -0700 (PDT)\r\nMIME-Version: 1.0\r\nX-Received: by 10.157.48.92 with HTTP; Sat, 29 Apr 2017 03:19:26 -0700 (PDT)\r\nIn-Reply-To: &lt;1980969183.13322540.1493349731385@...&gt;\r\nReferences: &lt;od43k8+3e7b44@...&gt; &lt;CAE0M+YeV42CJPVUxLked8yv0EXuxnBu9BNy6da7a2NqN2J8Bxg@...&gt;\n &lt;1198839291.7468517.1492855774439@...&gt; &lt;CAE0M+Yejmvgjb1MNyRFa-6DoT4wVj9E6PwPbmcLbaimb8T1f+w@...&gt;\n &lt;1980969183.13322540.1493349731385@...&gt;\r\nDate: Sat, 29 Apr 2017 11:19:26 +0100\r\nMessage-ID: &lt;CAE0M+YezB7+yR3dK_1dfemoN8UajwLv08gkJ1AZEJFGAu5ReyQ@...&gt;\r\nTo: neat@yahoogroups.com\r\nContent-Type: text/plain; charset=UTF-8\r\nContent-Transfer-Encoding: quoted-printable\r\nSubject: Re: [neat] Batching the population on GPU?\r\nX-Yahoo-Group-Post: member; u=344770077; y=8UgQbSrGThRXsWoU8imq22aZjRP7qHn2eJtGC_nADaRmE3nb_4h_\r\nX-Yahoo-Profile: alienseedpod\r\nFrom: Colin Green &lt;colin.green1@...&gt;\r\n\r\nHi Fred,\n\nOn 28 April 2017 at 04:22, Fred Mitchell lordalveric@... [n=\r\neat]\n&lt;neat@yahoogroups.com&gt; wrote:\n&gt;\n&gt; There is definitely something to be =\r\nsaid for leveraging hardware-supported\n&gt; vector matrix processing. But perh=\r\naps one might want to use the results of\n&gt; NEAT in places where such hardwa=\r\nre support might not be present, for\n&gt; instance, embedded systems and the l=\r\nike.\n\nSure, however, the neural net code we use within NEAT (when searching=\r\n)\ncan be different to the implementation we use to run individual\nnetworks =\r\nin some practical application e.g. on an android phone.\n\nBoth environments =\r\n(NEAT and practical use of net) have the requirement\nto invoke a neural net=\r\n, but there are other requirements that may\ndiffer. Maybe on a smartphone y=\r\nou may give up speed to reduce memory\nfootprint, or maybe the network is sm=\r\nall enough that speed isn&#39;t an\nissue, just use plain old serial non-vectori=\r\nzed code. Or maybe\nsmartphone have specialised hardware (hardware accelerat=\r\nion) that\nrequired different neural net code to use than when we evolve the=\r\n nets\nin NEAT (I&#39;m guessing this situation already exists, and if not it\nmo=\r\nst likely will do soon).\n\n\n&gt; Also, correct me if I&#39;m wrong, but most are no=\r\nt going to be doing vector\n&gt; math on each individual neuron&#39;s inputs, but t=\r\nreat the entire neural network\n&gt; as a sparse matrix.\n\nYes, but I&#39;m not sure=\r\n what distinction you are making here. The net\ninputs are a vector, the wei=\r\nghts are a matrix, the output layer is\nanother vector. We multiple the inpu=\r\nt vector by the weight matrix; the\nresult is another vector (the output lay=\r\ner). So yes, it&#39;s dealing with\nwhole layers, or whole networks in a single =\r\n(conceptually) operation\n(vector-matrix mult), rather than looping through =\r\nnodes in series and\nindividually propagating signals to other nodes, and tr=\r\nying to\noptimise that approach.\n\n\n&gt; I have not had much time to look at the=\r\n current state of affairs with GPUs\n&gt; and the like, and sparse matrix algor=\r\nithms -- and even the efficient\n&gt; representation of sparse matrices in RAM =\r\nsuitable for the hardware matrix\n&gt; support. So I have some homework to do f=\r\nor sure.\n\nThe nice thing is that CSR is a standard format, and most librari=\r\nes\nshould have a means of worign that format without knowing what the\nstruc=\r\nture actually is. I.e. typiaclly you might just instantiate a\nmatrix type, =\r\nset elements of that matrix, and call a multiply method.\nFrom there a frame=\r\nwork will have plug-in support to use CUDA, Intel\nMKL or whatever without h=\r\naving to learn the low level details of what\nis actually going on at the ha=\r\nrdware level. Those matrix multiplcation\nroutines will typically be written=\r\n and fine tuned by hand by the\npeople working closely with the hardware peo=\r\nple, and supplied as a\ncompiled binary.\n\n\n&gt; Back during the times of Koza, =\r\nI had gotten interested in Genetic\n&gt; Programming. It seems to not have gain=\r\ned any traction at all over the years.\n&gt; My current implementation as funct=\r\nional programming basically IS Genetic\n&gt; Programming in a structured manner=\r\n.\n\nI have a hunch that interest will return to this area some day. That&#39;s\na=\r\nnother discussion. Gradient following isn&#39;t the ultimate solution to\nevery =\r\nAI problem.\n\n\n&gt; Eventually I want to leverage LLVM to represent the neural =\r\nnetworks using my\n&gt; approach, and I do wonder if doing the vector math &quot;on =\r\nthe stack&quot; as\n&gt; compiled code would not approach the speed of the sparse ma=\r\ntrix approach, at\n&gt; least in some situations. And that&#39;s not to say the LLV=\r\nM can&#39;t also leverage\n&gt; hardware-assisted matrix operations as well.\n\nIt ca=\r\nn however leverage vector CPU instructions. I.e. if you right a\ntight loop =\r\nsuch as:\n\n   double[] arr =3D // create an array of values.\n   double total=\r\n =3D 0;\n   for(int i=3D0; i&lt;arr.Length; i++) {\n      total +=3D arr[i]\n   }=\r\n\n\n\nthen it will vectorize that loop, i.e. e.g. if the available\nvector/SIMD=\r\n registers can hold say 8 doubles, then LLVM will load 8\nvalues into a regi=\r\nster at a time, do a single &#39;vector add&#39;, and add\n*that* result to &#39;total&#39;.=\r\n So the number of add operations is one\neighth what it would be for the non=\r\n vectorized version.\n\nHowever I don&#39;t know if LLVM can ever detect a code p=\r\nattern in which\nit can use the fma instructions (fused multiple and add) th=\r\nat are\ncentral to speedup of matrix multiplication code.\n\nColin\n\nOn 28 Apri=\r\nl 2017 at 04:22, Fred Mitchell lordalveric@... [neat]\n&lt;neat@yahoogrou=\r\nps.com&gt; wrote:\n&gt;\n&gt;\n&gt; Thanks, Colin for your response.\n&gt;\n&gt; There is definite=\r\nly something to be said for leveraging hardware-supported\n&gt; vector matrix p=\r\nrocessing. But perhaps one might want to use the results of\n&gt; NEAT in place=\r\ns where such hardware support might not be present, for\n&gt; instance, embedde=\r\nd systems and the like.\n&gt;\n&gt; Also, correct me if I&#39;m wrong, but most are not=\r\n going to be doing vector\n&gt; math on each individual neuron&#39;s inputs, but tr=\r\neat the entire neural network\n&gt; as a sparse matrix.\n&gt;\n&gt; I have not had much=\r\n time to look at the current state of affairs with GPUs\n&gt; and the like, and=\r\n sparse matrix algorithms -- and even the efficient\n&gt; representation of spa=\r\nrse matrices in RAM suitable for the hardware matrix\n&gt; support. So I have s=\r\nome homework to do for sure. Also, that&#39;s not to say I\n&gt; can&#39;t also leverag=\r\ne the same in RubyNEAT, for my design will allow that kind\n&gt; of representat=\r\nion without too much fuss, and I&#39;m considering more direct\n&gt; matrix support=\r\n anyway.\n&gt;\n&gt; Back during the times of Koza, I had gotten interested in Gene=\r\ntic\n&gt; Programming. It seems to not have gained any traction at all over the=\r\n years.\n&gt; My current implementation as functional programming basically IS =\r\nGenetic\n&gt; Programming in a structured manner.\n&gt;\n&gt; Eventually I want to leve=\r\nrage LLVM to represent the neural networks using my\n&gt; approach, and I do wo=\r\nnder if doing the vector math &quot;on the stack&quot; as\n&gt; compiled code would not a=\r\npproach the speed of the sparse matrix approach, at\n&gt; least in some situati=\r\nons. And that&#39;s not to say the LLVM can&#39;t also leverage\n&gt; hardware-assisted=\r\n matrix operations as well.\n&gt;\n&gt; Kind Regards,\n&gt;\n&gt; Fred Mitchell,\n&gt;\n&gt;\n&gt;  Sky=\r\npe: flajann\n&gt;\n&gt;\n&gt;\n&gt; On Saturday, 22 April 2017, 12:36, &quot;Colin Green colin.g=\r\nreen1@...\n&gt; [neat]&quot; &lt;neat@yahoogroups.com&gt; wrote:\n&gt;\n&gt;\n&gt;\n&gt; On 22 April=\r\n 2017 at 11:09, Fred Mitchell lordalveric@... [neat]\n&gt; &lt;neat@yahoogro=\r\nups.com&gt; wrote:\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; Has anyone else taken this functional approach? Or=\r\n are most\n&gt;&gt; implementations\n&gt;&gt; relying on matrices? Any thoughts as to whi=\r\nch approach is better?\n&gt;&gt;\n&gt;\n&gt; Hi Fred,\n&gt;\n&gt; The first thing to note is that =\r\nthe propagation of an array of input\n&gt; signals through a layer of weights, =\r\nand summing to produce an output\n&gt; array - this is conceptually vector-matr=\r\nix multiplication. How we\n&gt; choose to do this is an implementation detail, =\r\nbut conceptually we&#39;re\n&gt; all doing vector-matrix multiplication.\n&gt;\n&gt; In ter=\r\nms of performance, the fastest approach is always going to be to\n&gt; use hard=\r\nware acceleration that is designed to do matrix\n&gt; multiplication. (Or rathe=\r\nr, this is always going to be the most\n&gt; efficient approach, since you coul=\r\nd make things very fast by\n&gt; distributing work out to a great many servers =\r\ndoing slow matrix\n&gt; multiplication in software instead of hardware.) This i=\r\ns why most AI\n&gt; type workloads have migrated to GPUs, and why google create=\r\nd the TPU -\n&gt; at their heart these hardware platforms are providing faster =\r\nand more\n&gt; efficient means of performing matrix multiplication.\n&gt;\n&gt; The har=\r\ndware matrix multiplication support fundamentally relies on the\n&gt; data bein=\r\ng operated on being in contiguous chucks of memory, since\n&gt; this allows all=\r\n of the data to be operated on to be loaded\n&gt; efficiently, e.g in CPUs this=\r\n gives good utilisation of the CPU\n&gt; caches, whereas out of order RAM acces=\r\nses will cause many cache\n&gt; misses, and access to RAM is orders or magnitud=\r\ne slower than the\n&gt; various CPU caches (the L1 to L3 or even level 4 caches=\r\n).\n&gt;\n&gt; I&#39;m don&#39;t think this necessarily conflicts with the functional\n&gt; pro=\r\ngramming approach, but as with non-functional (procedural)\n&gt; programming mo=\r\ndels it probably does influence how you arrange data\n&gt; structures in memory=\r\n and how you structure the algorithms to operate\n&gt; on those data structures=\r\n. Fortunately, matrix multiplication is such a\n&gt; fundamental operation ther=\r\ne is much support out there that can be\n&gt; leveraged. And we we now have at =\r\nleast three large corporations\n&gt; throwing lots of money primarily at making=\r\n matrix multiplication\n&gt; faster (Nvidia, google, Intel), so things might ge=\r\nt even more\n&gt; interesting in the coming years.\n&gt;\n&gt; Colin\n&gt;\n&gt;\n&gt; \n\n"}}