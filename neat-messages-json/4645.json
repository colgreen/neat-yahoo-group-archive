{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":211599040,"authorName":"Jeff Clune","from":"Jeff Clune &lt;jclune@...&gt;","profile":"jeffreyclune","replyTo":"LIST","senderId":"yj_d4EpkckRXyjF3uq9YNX4h1yQIS188gITYpA_afBg4sNT3U_m8J4wHCaSEwLLEkVKw4_lM3AGX70A_Ij5wLDXv","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [neat] New Paper on Novelty Search and Adaptive Neural Networks","postDate":"1240849981","msgId":4645,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PEM2MUI1NjdELjJBNzU3JWpjbHVuZUBtc3UuZWR1Pg==","inReplyToHeader":"PGdyajYwaCs0YWpjQGVHcm91cHMuY29tPg=="},"prevInTopic":4619,"nextInTopic":4649,"prevInTime":4644,"nextInTime":4646,"topicId":4619,"numMessagesInTopic":8,"msgSnippet":"Hello all- Congrats on the best paper nomination, and thanks for posting this very interesting work. My favorite thing about it was said in the conclusion, ","rawEmail":"Return-Path: &lt;jclune@...&gt;\r\nX-Sender: jclune@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 40405 invoked from network); 27 Apr 2009 16:34:13 -0000\r\nX-Received: from unknown (69.147.108.202)\n  by m6.grp.re1.yahoo.com with QMQP; 27 Apr 2009 16:34:13 -0000\r\nX-Received: from unknown (HELO mail-qy0-f124.google.com) (209.85.221.124)\n  by mta3.grp.re1.yahoo.com with SMTP; 27 Apr 2009 16:34:13 -0000\r\nX-Received: by qyk30 with SMTP id 30so29124qyk.24\n        for &lt;neat@yahoogroups.com&gt;; Mon, 27 Apr 2009 09:33:13 -0700 (PDT)\r\nX-Received: by 10.229.82.9 with SMTP id z9mr2466314qck.79.1240849988451;\n        Mon, 27 Apr 2009 09:33:08 -0700 (PDT)\r\nReturn-Path: &lt;jclune@...&gt;\r\nX-Received: from ?10.0.1.200? (c-76-20-191-220.hsd1.mi.comcast.net [76.20.191.220])\n        by mx.google.com with ESMTPS id 9sm10835374ywf.19.2009.04.27.09.33.04\n        (version=TLSv1/SSLv3 cipher=RC4-MD5);\n        Mon, 27 Apr 2009 09:33:05 -0700 (PDT)\r\nUser-Agent: Microsoft-Entourage/12.13.0.080930\r\nDate: Mon, 27 Apr 2009 12:33:01 -0400\r\nTo: &quot;neat@yahoogroups.com&quot; &lt;neat@yahoogroups.com&gt;\r\nMessage-ID: &lt;C61B567D.2A757%jclune@...&gt;\r\nThread-Topic: [neat] New Paper on Novelty Search and Adaptive Neural Networks\r\nThread-Index: AcnHVdSdU4fdil6p+EaaCy5Ls6mhvw==\r\nIn-Reply-To: &lt;grj60h+4ajc@...&gt;\r\nMime-version: 1.0\r\nContent-type: text/plain;\n\tcharset=&quot;US-ASCII&quot;\r\nContent-transfer-encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Jeff Clune &lt;jclune@...&gt;\r\nSubject: Re: [neat] New Paper on Novelty Search and Adaptive Neural Networks\r\nX-Yahoo-Group-Post: member; u=211599040; y=dZ7uMhk31V76YExr975q4ZXw3DeAkmgPJnWEYpQW-bnLxPBZhrfQ\r\nX-Yahoo-Profile: jeffreyclune\r\n\r\nHello all-\n\nCongrats on the best paper nomination, and thanks for posting this very\ninteresting work. My favorite thing about it was said in the conclusion,\nwhich is that novelty search offers a new tool in the toolbox that one can\ntry when encountering a difficult problem. As I have said previously, I\nthink there are situations in which novelty search will not work very well\n(have you guys run into any yet?), but this paper demonstrates that there\nare problems in which it works quite well, and even better than fitness\nbased search.\n\nA few questions/comments;\n\nI get the impression that the paper suggests that problems that require\nadaptive-heuristics are *always* deceptive. Is that right? This line in\nparticular triggers this comment: &quot;This paper argues that domains that\nrequire adaptation are inherently deceptive.&quot;\n\nI certainly agree that some such problems are deceptive, and could be\npersuaded that many are, but doesn&#39;t it seem too strong to say that all are?\n\nThe paper seems to further suggest that the reason many of these problems\nare deceptive is because (a) it is easier to learn a fixed-heuristic, and\nthen (b) there is a fitness valley between that fixed-heuristic and the\nadaptive strategy. \n\nI agree with (a) (in most cases), but why assume (b)?\n\nIn practice, I have seen that evolution does seem to really want to find a\nfixed-heuristic instead of developing phenotypic plasticity. I actually have\nan ECAL paper about this wherein I describe how digital organisms in a\ncycling two-season environment that required them to behave differently in\neach season figured out a way to run the exact same code, but have that code\nstill work perfectly in the two different seasons. Once I tweaked the\nenvironment a bit, though, they switched over to learning the adaptive\nstrategy.  \n\nSo, I do agree that in many situations evolution learns a fixed strategy and\nthen cannot find the adaptive one. Part of the reason I ask my question is\nbecause I am interested in *why* it is typically the case that it is hard to\ngo from a fixed heuristic that gets 50% of the reward to an adaptive\nheuristic that gets 100% of the reward?  One reason you suggest in the paper\nis that the adaptive strategy simply does not pay off that much as a percent\nof overall fitness. But if that is the only problem, then switching to an\nexponential fitness function would solve it (which I am sure many\nresearchers have tried). My instincts tell me there is an additional\nexplanation. \n\nA final question. You write:&quot; novelty search removes the need to carefully\ndesign a domain that fosters the emergence of learning...the only\nprerequisite is that the novelty metric is constructed such that learning\nand non-learning agents are separable, which is not easy, but is worth the\neffort if objective-based search would otherwise fail.&quot;\n\nAs you mention, you had to spend the time to design a domain (and novelty\nmetric) that fosters the emergence of learning by delineating those that\nlearn from those that don&#39;t. Couldn&#39;t it be argued that you might have\nsuccess if you did the same thing for objective-based search? For example,\ndid you try any runs that gave a huge fitness boost to organisms that\nsuccessfully went the other way after they discovered that the high reward\nhad moved? It seems like an equal amount of energy (to that which you spent\ndesigning the novelty metric) should arguably be applied to the\nfitness-based search to see if you can coax it into evolving learners, no?\n\nIn any case, it was a great paper, partly because it raises such questions.\nThanks!\n\n\n\nCheers,\nJeff Clune\n\nDigital Evolution Lab, Michigan State University\n\njclune@...\n\n\n\n\n\n\n&gt; From: Kenneth Stanley &lt;kstanley@...&gt;\n&gt; Reply-To: &quot;neat@yahoogroups.com&quot; &lt;neat@yahoogroups.com&gt;\n&gt; Date: Wed, 08 Apr 2009 21:46:57 -0000\n&gt; To: &quot;neat@yahoogroups.com&quot; &lt;neat@yahoogroups.com&gt;\n&gt; Subject: [neat] New Paper on Novelty Search and Adaptive Neural Networks\n&gt; \n&gt; Our new paper, which will appear at GECCO this summer, explores the idea that\n&gt; novelty search is a potentially effective tool for evolving neural networks\n&gt; that adapt during their lifetime (i.e. neural networks with plastic synapses):\n&gt; \n&gt; http://eplex.cs.ucf.edu/publications/2009/risi.gecco09.html\n&gt; \n&gt; direct link:\n&gt; http://eplex.cs.ucf.edu/papers/risi_gecco09.pdf\n&gt; \n&gt; This idea is motivated by the intuition that it&#39;s difficult to evolve adaptive\n&gt; neural networks because evolution often finds strategies that improve\n&gt; performance at first without having to adapt.  In other words, evolving for\n&gt; adaptive behavior and synaptic plasticity is inherently deceptive.  Because\n&gt; novelty search is hypothesized to effectively avoid deception, the potential\n&gt; is there for a powerful combination.  The results indeed show that novelty\n&gt; search is immune to deception in the target domain (with respect to desired\n&gt; performance).  \n&gt; \n&gt; More generally, this paper increases the body of evidence supporting the\n&gt; counter-intuitive hypothesis behind novelty search that it is often more\n&gt; effective to ignore improvement in objective fitness than to select for it.\n&gt; \n&gt; We are hoping that this work inspires more research in evolving adaptive (i.e.\n&gt; plastic) neural networks by offering a new tool to make it work more\n&gt; effectively without worrying about non-adaptive (i.e. static) behavior\n&gt; deceiving the fitness function.\n&gt; \n&gt; ken\n&gt; \n\n\n\n"}}