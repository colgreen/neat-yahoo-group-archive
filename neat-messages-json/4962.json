{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":200957992,"authorName":"jgmath2000","from":"&quot;jgmath2000&quot; &lt;jgmath2000@...&gt;","profile":"jgmath2000","replyTo":"LIST","senderId":"eepLZYwvoUWMk0t7Ud0V7Ytz8ij_cgY4VAG7i3JnHSpvt_71F5KdcOVI52C8AFbWwCi9kUBN_fIWdNFsmx0sDm_hoaym7gDEHBU","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: HyperNEAT Tutorial?","postDate":"1259629773","msgId":4962,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGhmMXFjZCt1dGJ2QGVHcm91cHMuY29tPg==","inReplyToHeader":"PEE5ODRCM0YxNUFEMDU0NDlBQTNDRDMxNEY2QkQ1Q0NFRUNCQTJBQGJuZXhjaDAyLmJuZTJrLmxvYz4="},"prevInTopic":4961,"nextInTopic":4963,"prevInTime":4961,"nextInTime":4963,"topicId":4884,"numMessagesInTopic":21,"msgSnippet":"Hey Anthony, I believe that the current HyperNEAT C++ distributable contains the Checkers experiment.  I ve been spending the past few days packaging up a new","rawEmail":"Return-Path: &lt;jgmath2000@...&gt;\r\nX-Sender: jgmath2000@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 13359 invoked from network); 1 Dec 2009 01:12:29 -0000\r\nX-Received: from unknown (98.137.34.46)\n  by m3.grp.sp2.yahoo.com with QMQP; 1 Dec 2009 01:12:29 -0000\r\nX-Received: from unknown (HELO n45b.bullet.mail.sp1.yahoo.com) (66.163.168.159)\n  by mta3.grp.sp2.yahoo.com with SMTP; 1 Dec 2009 01:12:29 -0000\r\nX-Received: from [69.147.65.171] by n45.bullet.mail.sp1.yahoo.com with NNFMP; 01 Dec 2009 01:09:33 -0000\r\nX-Received: from [98.137.34.72] by t13.bullet.mail.sp1.yahoo.com with NNFMP; 01 Dec 2009 01:09:33 -0000\r\nDate: Tue, 01 Dec 2009 01:09:33 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;hf1qcd+utbv@...&gt;\r\nIn-Reply-To: &lt;A984B3F15AD05449AA3CD314F6BD5CCEECBA2A@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;jgmath2000&quot; &lt;jgmath2000@...&gt;\r\nSubject: Re: HyperNEAT Tutorial?\r\nX-Yahoo-Group-Post: member; u=200957992; y=M4EqQEs38Yon83s0lDspKJ0JM1X5zUipd3AQHm0iikIkTIrSsQ\r\nX-Yahoo-Profile: jgmath2000\r\n\r\nHey Anthony,\n\nI believe that the current HyperNEAT C++ distributable contai=\r\nns the Checkers experiment.  I&#39;ve been spending the past few days packaging=\r\n up a new version of HyperNEAT C++ that includes a visualizer and a much mo=\r\nre streamlined (and documented) CPPN implementation that should clear up a =\r\nlot of loose ends with respect to the algorithm.  When the new version is o=\r\nut, I will definitely post an announcement to the group.\n\nIn the meantime, =\r\nhopefully I can clear up some of your questions regarding HyperNEAT:\n\nThe &quot;=\r\nHyper&quot; in HyperNEAT is tied to the CPPN.  In the same way that PicBreeder a=\r\nllows users to evolve 2-D images, a CPPN is an evolved 4-D image. Instead o=\r\nf drawing the pixels to the screen or something like that, the pixels are t=\r\nranslated into connection weights.  Thus, in the same way that PicBreeder p=\r\naints a square (2-D) by querying the CPPN, HyperNEAT paints a Hypercube (4-=\r\nD) of connection weights (hence the term HyperNEAT). Once the substrates ar=\r\ne created and evaluated, the CPPNs are modified using NEAT and the process =\r\nrepeats.  Hopefully this explains where the &quot;Hyper&quot; ends and the &quot;NEAT&quot; beg=\r\nins. \n\nThere are two ways a CPPN can encode (represent) a substrate.  Take =\r\nthe checkers experiment for example, I chose to represent the substrate as =\r\nthree sheets.  As you noted correctly, the &quot;A&quot; sheet is populated based on =\r\nthe current board being evaluated.  However, the &quot;B&quot; sheet is a hidden laye=\r\nr used for computation, not the substrate.  The substrate is actually the e=\r\nntire structure with all sheets.  The &quot;C&quot; sheet contains a single node beca=\r\nuse I want a single output to decide how winning the board is for black.  Y=\r\nou can think of the &quot;A&quot; sheet as the sensors and the &quot;C&quot; sheet as the outpu=\r\nt.  It is easily possible to have an array of outputs, just as the checkers=\r\n domain has an array of inputs.  Using this multi-sheet model, the CPPN has=\r\n multiple outputs, one for every sheet.  In other words, you input a source=\r\n (x1,y1) and destination (x2,y2) position to the CPPN, and the CPPN outputs=\r\n the link weights for every connection layer (hence the AB and BC outputs o=\r\nn the CPPN).\n\nThere is a second way to encode a substrate, and that is to u=\r\nse a single sheet.  In the case of checkers, you could use a single 3-D she=\r\net to represent the entire substrate, and define (x,y,-1) to be the inputs =\r\nand (0,0,1) to be the output node.  Doing this, one could have a CPPN which=\r\n takes source (x1,y1,z1) and destination (x2,y2,z2) position to the CPPN, a=\r\nnd output the connection weight.  The end result is a &quot;soup&quot; of connectivit=\r\ny between the inputs and outputs.\n\nI hope this clarifies the CPPN -&gt; substr=\r\nate process.\n\nJason G.\n\n--- In neat@yahoogroups.com, &quot;Anthony Ison&quot; &lt;anthon=\r\ny.ison@...&gt; wrote:\n&gt;\n&gt; Thanks for the reply Ken.\n&gt; \n&gt;  \n&gt; \n&gt; I have indeed =\r\nbeen over the HyperNEAT Users page a number of times.  I&#39;ve read many of th=\r\ne papers that are linked, but there&#39;s still something that&#39;s not quite jump=\r\ning out at me.  I&#39;ve been trying to determine exactly what it is.\n&gt; \n&gt;  \n&gt; =\r\n\n&gt; The checkers game seems to be the ideal example for HyperNEAT.  (and to =\r\nbe honest, I still don&#39;t understand where Hyper ends and NEAT begins)\n&gt; \n&gt; =\r\nI think it&#39;s the 2 images at the top of the page.  From the first, I can te=\r\nll there is something called a Substrate that covers an area from (-1,-1) t=\r\no (1, 1).  I assume the number of rows and columns is customizable (dependi=\r\nng on chosen geometry) and each &quot;input node&quot; has an implied connection to e=\r\nvery other &quot;input node&quot;.  In the case of the checkers game, there are 64 in=\r\nputs with a total of 64x64 (4096) connections (assuming -1,-1 -&gt; -1,-1 make=\r\ns sense).\n&gt; \n&gt;  \n&gt; \n&gt; The CPPN (which is the learning part) and the Substra=\r\nte (the designed part) together produce a network (somehow).  Is it always =\r\na single output node?  Where does the network structure come from?  Is it t=\r\nhe CPPN?\n&gt; \n&gt;  \n&gt; \n&gt; I must admit, I have no idea what the second image is =\r\ntrying to tell me.  I can see that we are applying HyperNEAT to the checker=\r\ns problem.  I can see that we have an 8x8 Substrate to match the layout of =\r\nthe board.  A is the checkers board, B is the Substrate and C, I assume is =\r\na roving eye that can query each location on the board (though I suspect I =\r\nonly know that from a paper I&#39;ve read at some point).  Is C another Substra=\r\nte?\n&gt; \n&gt;  \n&gt; \n&gt; I don&#39;t understand the part in the middle of the second ima=\r\nge:  Inputs: Coordinates and Outputs: Connection Weights.\n&gt; \n&gt;  \n&gt; \n&gt; I gue=\r\nss at the end of the day, I&#39;m expecting some output that is a network with =\r\na predefined number of inputs and outputs.  I&#39;m also assuming that the numb=\r\ner of inputs will match the &quot;input nodes&quot; defined in the Substrate and the =\r\nnumber of output nodes will be ...  I&#39;m not sure.  One?\n&gt; \n&gt;  \n&gt; \n&gt; Anyways=\r\n, sorry about the wall of text.  I&#39;m hoping that by explaining what I&#39;m see=\r\ning, you may be able to point out what part I&#39;m muddling up.\n&gt; \n&gt;  \n&gt; \n&gt; Is=\r\n there perhaps some code available that solves the checkers problem?\n&gt; \n&gt;  =\r\n\n&gt; \n&gt; Thanks in advance,\n&gt; \n&gt; Anthony\n&gt; \n&gt;  \n&gt; \n&gt;  \n&gt; \n&gt; From: Ken [mailto:=\r\nkstanley@...] \n&gt; Sent: Sunday, 29 November 2009 8:00 AM\n&gt; To: neat@yahoogro=\r\nups.com\n&gt; Subject: [neat] Re: HyperNEAT Tutorial?\n&gt; \n&gt;  \n&gt; \n&gt;   \n&gt; \n&gt; \n&gt; \n&gt;=\r\n Anthony,\n&gt; \n&gt; We are hoping to make the HyperNEAT Users Page at http://epl=\r\nex.cs.ucf.edu/hyperNEATpage/HyperNEAT.html into a place where people can ha=\r\nve questions like yours answered. The &quot;Introduction / What is HyperNEAT?&quot; s=\r\nection on that page is intended to provide some general answers to beginner=\r\ns without having to read a research paper. Did that section help you at all=\r\n? I&#39;d like to make the page as useful as possible and we will continue to i=\r\nmprove it.\n&gt; \n&gt; To answer your question, HyperNEAT is a significant step be=\r\nyond NEAT so it involves a lot of new ideas that aren&#39;t part of the origina=\r\nl NEAT. Some of those concepts can theoretically be applied on top of non-N=\r\nEAT methods. For example, in the following paper, NEAT is substituted with =\r\nGP to create a &quot;HyperGP,&quot; in which GP evolves the CPPN:\n&gt; \n&gt; Buk Z., Koutn=\r\n=EF=BF=BDk J., =EF=BF=BDnorek M., NEAT in HyperNEAT Substituted with Geneti=\r\nc Programming, In: ICANNGA 2009. \n&gt; http://cig.felk.cvut.cz/research/public=\r\nations/hypergp.pdf\n&gt; \n&gt; That said, HyperNEAT addresses a limitation of NEAT=\r\n, which is a limitation of all direct encodings: In NEAT, there is one gene=\r\n for every connection in the network. Even with complexification, that kind=\r\n of representation cannot hope to scale to networks with millions or more c=\r\nonnections, because such networks would have millions or more genes, which =\r\nis an astronomical search space. \n&gt; \n&gt; However, there are in fact 100 trill=\r\nion connections in the human brain, which means that in principle it is pos=\r\nsible to evolve such structures. Yet there are only about 30,000 genes in t=\r\nhe human genome, which suggests that any evolutionary approach to evolving =\r\nlarge-scale neural networks must encode the connection weights in a compres=\r\nsed description, which is called an indirect encoding.\n&gt; \n&gt; In HyperNEAT, t=\r\nhe indirect encoding is the CPPN, which encodes the connectivity of a netwo=\r\nrk as a pattern across its geometry. HyperNEAT combines the idea of indirec=\r\nt encoding with a strong notion of geometry and builds on our understanding=\r\n of encoding patterns to produce an algorithm that encodes large-scale topo=\r\ngraphies (i.e. connection weights across a geometry). Thus it extends NEAT =\r\nby giving it the power of indirect encoding, thereby greatly expanding the =\r\nscope of networks it can evolve.\n&gt; \n&gt; Of course NEAT is still there under t=\r\nhe hood. NEAT is evolving the CPPNs, which in turn encode neural networks (=\r\ncalled substrates in HyperNEAT). The CPPNs themselves are still complexifyi=\r\nng. However, that complexification is no longer literally adding one connec=\r\ntion at a time to a neural network. Rather it is adding *information* to th=\r\ne encoding, so that it can encode more complex *holistic* connectivity patt=\r\nerns. In other words, HyperNEAT means that evolution is no longer limited b=\r\ny the dimensionality of the inputs and outputs but rather can search for th=\r\ne correct implicit problem complexity, whatever that may be, inside the CPP=\r\nN. The substrate (which the CPPN encodes) will then have as many connection=\r\ns as it needs, in principle up to millions or even trillions (with enough C=\r\nPU power).\n&gt; \n&gt; It is true that geometry may be vague or difficult to decid=\r\ne in some problems. Those may be more difficult for users to approach with =\r\nHyperNEAT. Yet I think most if not all problems can ultimately be posed wit=\r\nhin some geometry, even if it is abstract or conceptual. As Jeff Clune has =\r\nshown (https://www.msu.edu/~jclune/webfiles/publications/Clune-HyperNEATSen=\r\nsitivityToGeometry.pdf), geometry does not need to be perfect, or necessari=\r\nly even close to perfect, for HyperNEAT to still find some regularities to =\r\nexploit, so while it may be an imperfect art, geometry is still ultimately =\r\na useful tool for conveying exploitable information about a domain.\n&gt; \n&gt; I =\r\nhope that provides some insight,\n&gt; \n&gt; ken\n&gt; \n&gt; --- In neat@yahoogroups.com =\r\n&lt;mailto:neat%40yahoogroups.com&gt; , &quot;Anthony Ison&quot; &lt;anthony.ison@&gt; wrote:\n&gt; &gt;=\r\n\n&gt; &gt; I&#39;d like to add my vote to some kind of non-research style tutorial -\n=\r\n&gt; &gt; especially towards the HyperNEAT methodology.\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; Is the=\r\nre somewhere I can get an overview of how the CPPN is actually\n&gt; &gt; used and=\r\n what it does? I&#39;ve looked through a number of papers on the\n&gt; &gt; main Hyper=\r\nNEAT site, but I feel like I&#39;m missing something. I have read\n&gt; &gt; that Hype=\r\nrNEAT is the future of NEAT - does this apply to problems where\n&gt; &gt; there i=\r\ns no useful input geometry? I understand how NEAT grows a\n&gt; &gt; network by ad=\r\nding nodes and connections and overall I love the concept.\n&gt; &gt; It makes sen=\r\nse that a learning network can adjust itself to improve its\n&gt; &gt; performance=\r\n. What I don&#39;t really understand is how a CPPN is involved\n&gt; &gt; in this proc=\r\ness. \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; Is anyone able to give a short overview on what Hy=\r\nperNEAT offers over\n&gt; &gt; NEAT?\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; Cheers,\n&gt; &gt; \n&gt; &gt; Anthony\n&gt;=\r\n &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; From: dkuppitz [mailto:daniel_kuppitz@] \n&gt; &gt; Se=\r\nnt: Monday, 23 November 2009 9:08 AM\n&gt; &gt; To: neat@yahoogroups.com &lt;mailto:n=\r\neat%40yahoogroups.com&gt; \n&gt; &gt; Subject: [neat] Re: HyperNEAT Tutorial?\n&gt; &gt; \n&gt; =\r\n&gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; Hello Ken,\n&gt; &gt; \n&gt; &gt; here&#39;s just another vote for a tu=\r\ntorial, with the difference that I\n&gt; &gt; would prefer it for HyperSharpNEAT.\n=\r\n&gt; &gt; \n&gt; &gt; It would be great to see something like a HOL (Hands on Labs) wher=\r\ne a\n&gt; &gt; new experiment is created from the scratch. Parameters should be\n&gt; =\r\n&gt; explained in detail, for example: Which impact has the parameter\n&gt; &gt; Tres=\r\nhold, which impact has WeightRange, etc.? How are the values for\n&gt; &gt; each p=\r\narameter determined, what is taken into account when you set the\n&gt; &gt; values=\r\n? There are so many unanswered questions for those who are new to\n&gt; &gt; Hyper=\r\nNEAT and I think most people (including me) have a really great\n&gt; &gt; interes=\r\nt in this topic, but not the time to read (and understand) all\n&gt; &gt; the tech=\r\nnical papers. So any tutorial should target beginners and\n&gt; &gt; explain thing=\r\ns that have become self-evident for intermediates. \n&gt; &gt; \n&gt; &gt; I think one su=\r\nch &quot;official&quot; tutorial that explains every step in detail\n&gt; &gt; should be eno=\r\nugh, more will surely follow from the growing community.\n&gt; &gt; \n&gt; &gt; Cheers,\n&gt;=\r\n &gt; Daniel\n&gt; &gt; \n&gt; &gt; --- In neat@yahoogroups.com &lt;mailto:neat%40yahoogroups.c=\r\nom&gt;  &lt;mailto:neat%40yahoogroups.com&gt; , &quot;Ken&quot;\n&gt; &gt; &lt;kstanley@&gt; wrote:\n&gt; &gt; &gt;\n&gt;=\r\n &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; &gt; Andrei, which version of HyperNEAT are you interested in=\r\n and what\n&gt; &gt; references have you looked at so far? We can potentially impr=\r\nove the\n&gt; &gt; documentation based on your comments (and a tutorial is a good =\r\nidea),\n&gt; &gt; but first I want to understand which &quot;comment-less examples&quot; you=\r\n are\n&gt; &gt; referring to.\n&gt; &gt; &gt; \n&gt; &gt; &gt; Note that several experiments with comp=\r\nlete source code are available\n&gt; &gt; in two existing HyperNEAT releases of wh=\r\nich I am aware. These and a\n&gt; &gt; variety of publications from several groups=\r\n are linked from the\n&gt; &gt; HyperNEAT Users Page, which also provides a brief =\r\nintroduction:\n&gt; &gt; &gt; \n&gt; &gt; &gt; http://eplex.cs.ucf.edu/hyperNEATpage/HyperNEAT.=\r\nhtml\n&gt; &gt; &gt; \n&gt; &gt; &gt; I understand you may have already been through this site =\r\nand its\n&gt; &gt; associated software and papers, but I wanted to point it out in=\r\n case you\n&gt; &gt; had not been aware of it.\n&gt; &gt; &gt; \n&gt; &gt; &gt; We want to make the al=\r\ngorithm as accessible as possible so your\n&gt; &gt; comments are appreciated.\n&gt; &gt;=\r\n &gt; \n&gt; &gt; &gt; ken\n&gt; &gt; &gt; \n&gt; &gt; &gt; --- In neat@yahoogroups.com &lt;mailto:neat%40yahoo=\r\ngroups.com&gt;  &lt;mailto:neat%40yahoogroups.com&gt; , &quot;Andrei&quot;\n&gt; &gt; &lt;andrei.rusu@&gt; =\r\nwrote:\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; Can anyone please recommend some HyperNEAT documenta=\r\ntion, a\n&gt; &gt; tutorial, diagram, some clue, or anything that does not mean re=\r\nverse\n&gt; &gt; engineering the comment-less examples ?\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; Thanks! =\r\nAndrei.\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt;\n&gt;\n\n\n\n"}}