{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":118900757,"authorName":"c f","from":"c f &lt;christofer_fransson@...&gt;","profile":"christofer_fransson","replyTo":"LIST","senderId":"1JjE1K3Dkfzx065n4V8TTBoXm2sUXI8_QjKkVVPeKadechjYIOsxXtHL2SYaUvAsHuVNGOXqp633TmSpDLpLULojymu8kU1EYzo","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [neat] Re: Backpropagation and NEAT","postDate":"1205495705","msgId":3868,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ2MDQyNS44MzgxOC5xbUB3ZWI1NjEwOC5tYWlsLnJlMy55YWhvby5jb20+","inReplyToHeader":"PGZyY203OCtncDQ0QGVHcm91cHMuY29tPg=="},"prevInTopic":3867,"nextInTopic":3869,"prevInTime":3867,"nextInTime":3869,"topicId":3846,"numMessagesInTopic":41,"msgSnippet":"In dynamic programming the idea is to divide the solution in steps and then for each step present a fixed number of possible solutions. Collin Greens idea is","rawEmail":"Return-Path: &lt;christofer_fransson@...&gt;\r\nX-Sender: christofer_fransson@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 30947 invoked from network); 14 Mar 2008 11:55:09 -0000\r\nX-Received: from unknown (66.218.67.97)\n  by m49.grp.scd.yahoo.com with QMQP; 14 Mar 2008 11:55:09 -0000\r\nX-Received: from unknown (HELO web56108.mail.re3.yahoo.com) (216.252.110.202)\n  by mta18.grp.scd.yahoo.com with SMTP; 14 Mar 2008 11:55:09 -0000\r\nX-Received: (qmail 83847 invoked by uid 60001); 14 Mar 2008 11:55:05 -0000\r\nX-YMail-OSG: voyReWQVM1kXG.ZAOHOTCU1ZSzAu6KvrGXzegJa3_iMwJzeOWlyVRBmnCx82Qz7.Ij1zdzHDKWlk3d7WvxtAwqfe9c0HNTXl5CnGuxBRQA7g9wIjoonKLwo9sBD.2WGzXSGASnTvVeI76.k-\r\nX-Received: from [192.36.190.2] by web56108.mail.re3.yahoo.com via HTTP; Fri, 14 Mar 2008 04:55:05 PDT\r\nDate: Fri, 14 Mar 2008 04:55:05 -0700 (PDT)\r\nTo: neat@yahoogroups.com\r\nIn-Reply-To: &lt;frcm78+gp44@...&gt;\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=iso-8859-1\r\nContent-Transfer-Encoding: 8bit\r\nMessage-ID: &lt;460425.83818.qm@...&gt;\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: c f &lt;christofer_fransson@...&gt;\r\nSubject: Re: [neat] Re: Backpropagation and NEAT\r\nX-Yahoo-Group-Post: member; u=118900757; y=08CuFB_orJpG5shLDHjK_de2ZLEmJWE_C1Byr1jlLUnlsvDTy-FycMoosglzDA\r\nX-Yahoo-Profile: christofer_fransson\r\n\r\nIn dynamic programming the idea is to divide the\nsolution in steps and then for each step present a\nfixed number of possible solutions.\n\nCollin Greens idea is that speculative nodes are added\nto the solutions but it might take time/generations\nbefore an added node are shown to be useful. \n\nIs it possible to apply dynamic programming approach\nto this area, to evolve NEAT driven networks?\n\nTo combine local optimization and dynamic programming\nideas?\n\nBr,\nChristofer\n\n\n\n\n\n\n\n\n\n\n\n\n--- petar_chervenski &lt;petar_chervenski@...&gt;\nwrote:\n\n&gt; Given the simplest topology (a perceptron\n&gt; structure), the local \n&gt; minima is just one. Perceptrons are always\n&gt; guaranteed to converge on \n&gt; correct weights. But increasing the dimentionality\n&gt; of the solution \n&gt; increases the error surface&#39;s curvature as well. So\n&gt; more dimentions \n&gt; means more complex error surface. The coolest thing\n&gt; in NEAT is that \n&gt; when it increases the dimentionality of the\n&gt; solution, the individuals \n&gt; are already located in a promising area of the new\n&gt; space. In fact \n&gt; those spaces are related to each other - you don&#39;t\n&gt; know how the error \n&gt; surface is going to look like when you enter the new\n&gt; space with more \n&gt; dimentions. There are unlimited possibilities. \n&gt; So what local gradient search will do in essence is\n&gt; pushing the \n&gt; weights towards the *local* minumim.. It is not\n&gt; guaranteed that this \n&gt; is the *solution*! It is simply because you don&#39;t\n&gt; know the solution&#39;s \n&gt; dimentionality at first. It may require 3 or\n&gt; 21342532 dimentions. \n&gt; Don&#39;t forget that NEAT complexifies solutions\n&gt; incrementaly. \n&gt; \n&gt; Peter\n&gt; \n&gt; --- In neat@yahoogroups.com, &quot;afcarl2&quot; &lt;a.carl@...&gt;\n&gt; wrote:\n&gt; &gt;\n&gt; &gt; In fact, it may be that a substancial portion of\n&gt; the value-added of \n&gt; &gt; speciation and niche protection of infant\n&gt; organisms, is associated \n&gt; &gt; with providing opportunity to accumulate\n&gt; sufficient neighborhood \n&gt; &gt; evaluations to &quot;discover&quot; the same local minimia\n&gt; over multiple \n&gt; &gt; generations, that a local search may discover in\n&gt; one generation. \n&gt; And \n&gt; &gt; maintaining multiple species in hope that one of\n&gt; the local minimia \n&gt; &gt; will in fact also be the global minimia.\n&gt; &gt; \n&gt; &gt; --- In neat@yahoogroups.com, &quot;afcarl2&quot; &lt;a.carl@&gt;\n&gt; wrote:\n&gt; &gt; &gt;\n&gt; &gt; &gt; If &quot;most individuals in a species represented by\n&gt; a given \n&gt; topology&quot; \n&gt; &gt; &gt; ended up in &quot;the same local minimia&quot;, one could\n&gt; argue that the \n&gt; &gt; &gt; subject specie&#39;s logical end point was the same\n&gt; local minimia, \n&gt; and \n&gt; &gt; &gt; that the cost of maintaining more than one\n&gt; organism was \n&gt; &gt; &gt; computationally wasteful. Better to know sooner\n&gt; and breed \n&gt; &gt; additional \n&gt; &gt; &gt; organisms of differing topology so as to\n&gt; maintain the population \n&gt; &gt; size \n&gt; &gt; &gt; and maximize the population&#39;s &quot;effective&quot;\n&gt; diversity.\n&gt; &gt; &gt; \n&gt; &gt; &gt; Paying more for the same answer does not make it\n&gt; a better answer.\n&gt; &gt; &gt; \n&gt; &gt; &gt; --- In neat@yahoogroups.com, &quot;petar_chervenski&quot; \n&gt; &gt; &gt; &lt;petar_chervenski@&gt; wrote:\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; Well I think that encoding the resulting\n&gt; weights back to the \n&gt; &gt; genome \n&gt; &gt; &gt; &gt; would somehow hurt the population weight\n&gt; diversity, since most \n&gt; &gt; &gt; &gt; individuals in a species represented by a\n&gt; given topology can \n&gt; end \n&gt; &gt; up \n&gt; &gt; &gt; &gt; in the same local minima, thus leaving out a\n&gt; species with the \n&gt; &gt; &gt; nearly \n&gt; &gt; &gt; &gt; same individuals, i.e. clones. \n&gt; &gt; &gt; &gt; This is why I think that backprop should be\n&gt; applied occasionaly \n&gt; &gt; &gt; after \n&gt; &gt; &gt; &gt; long periods of stagnation, for example the\n&gt; cases where delta-\n&gt; &gt; &gt; coding \n&gt; &gt; &gt; &gt; kicks in, when it focuses the search in the\n&gt; most promising \n&gt; areas \n&gt; &gt; of \n&gt; &gt; &gt; &gt; the search space. \n&gt; &gt; &gt; &gt; I am still trying to re-implement RTRL myself,\n&gt; though.. Then \n&gt; I&#39;ll \n&gt; &gt; &gt; see \n&gt; &gt; &gt; &gt; if it is going to actually enhance\n&gt; performance. \n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; Peter\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; --- In neat@yahoogroups.com, &quot;Kenneth Stanley&quot;\n&gt; &lt;kstanley@&gt; \n&gt; wrote:\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; Rafael, thank you for pointing out the\n&gt; connection to memetic \n&gt; &gt; &gt; &gt; &gt; algorithms.  That is good to point out that\n&gt; such a \n&gt; combination \n&gt; &gt; &gt; &gt; falls \n&gt; &gt; &gt; &gt; &gt; under that category.\n&gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; However, there are still those who would\n&gt; argue that the local \n&gt; &gt; &gt; &gt; search \n&gt; &gt; &gt; &gt; &gt; method should not be encoded back into the\n&gt; genome, that is, \n&gt; &gt; that \n&gt; &gt; &gt; &gt; &gt; evolution should simply search for the best\n&gt; starting point \n&gt; from \n&gt; &gt; &gt; &gt; which \n&gt; &gt; &gt; &gt; &gt; a local search would depart.  Because of the\n&gt; Baldwin Effect, \n&gt; &gt; that \n&gt; &gt; &gt; &gt; may \n&gt; &gt; &gt; &gt; &gt; even work better.\n&gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; Personally, I do not know which approach\n&gt; would work better \n&gt; but \n&gt; &gt; &gt; both \n&gt; &gt; &gt; &gt; &gt; are viable and it is probably domain\n&gt; dependent.\n&gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; ken\n&gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; --- In neat@yahoogroups.com, &quot;Rafael C.P.&quot;\n&gt; &lt;kurama.youko.br@&gt; \n&gt; &gt; &gt; &gt; &gt; wrote:\n&gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; Ken, it doesn&#39;t fit pure evolution but it\n&gt; fits memetic \n&gt; &gt; &gt; &gt; algorithms, \n&gt; &gt; &gt; &gt; &gt; that\n&gt; &gt; &gt; &gt; &gt; &gt; consists exactly of evolution alternated\n&gt; with local search \n&gt; &gt; &gt; &gt; methods \n&gt; &gt; &gt; &gt; &gt; for fine\n&gt; &gt; &gt; &gt; &gt; &gt; tunning (just few steps). NEAT+BP may\n&gt; become a good memetic \n&gt; &gt; &gt; &gt; &gt; algorithm for\n&gt; &gt; &gt; &gt; &gt; &gt; neural networks.\n&gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; On Mon, Mar 10, 2008 at 2:19 PM, Kenneth\n&gt; Stanley &lt;kstanley@&gt;\n&gt; &gt; &gt; &gt; &gt; &gt; wrote:\n&gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; &gt;   Peter, I believe that backprop can\n&gt; potentially improve \n&gt; the\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; accuracy. It has been shown to work\n&gt; effectively with \n&gt; &gt; &gt; &gt; neurevolution\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; in classification tasks in the past. So\n&gt; in principle it \n&gt; &gt; could\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; help. Of course, there is always the\n&gt; chance that it will \n&gt; not\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; enhance performance as well.\n&gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; One issue I would also consider is that\n&gt; some people \n&gt; &gt; disagree \n&gt; &gt; &gt; on\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; whether the changes to weights from\n&gt; backprop should be \n&gt; &gt; &gt; encoded \n&gt; &gt; &gt; &gt; &gt; back\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; into the genome or not. If it is\n&gt; actually encoded back \n&gt; into \n&gt; &gt; &gt; the\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; genome, that is &quot;Lamarckian&quot; evolution\n&gt; because in effect \n&gt; &gt; what \n&gt; &gt; &gt; &gt; the\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; organism learned over its lifetime is\n&gt; encoded into its own\n&gt; \n=== message truncated ===\n\n\n\n      ____________________________________________________________________________________\nNever miss a thing.  Make Yahoo your home page. \nhttp://www.yahoo.com/r/hs\n\n"}}