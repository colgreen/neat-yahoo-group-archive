{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":127853030,"authorName":"Colin Green","from":"Colin Green &lt;cgreen@...&gt;","profile":"alienseedpod","replyTo":"LIST","senderId":"sjAsE0aMIjpHTK3QEAeK4Sv6pumoqf9d_2CA6gP3thS2uVXfDhRn-awVwWa5yxdgEmukioKz_EYchkhNVzve408OopR9ba175A","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Evolving a Trading Strategy: Update","postDate":"1157580859","msgId":2733,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ0RkY0ODNCLjMwNDA0MDZAZHNsLnBpcGV4LmNvbT4="},"prevInTopic":0,"nextInTopic":2734,"prevInTime":2732,"nextInTime":2734,"topicId":2733,"numMessagesInTopic":13,"msgSnippet":"Hi, OK so over the weekend I modified my experiments to use Z-Score inputs along with continuous means and standard deviations. I ve been running an experiment","rawEmail":"Return-Path: &lt;cgreen@...&gt;\r\nX-Sender: cgreen@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 97250 invoked from network); 6 Sep 2006 22:14:52 -0000\r\nReceived: from unknown (66.218.66.216)\n  by m22.grp.scd.yahoo.com with QMQP; 6 Sep 2006 22:14:52 -0000\r\nReceived: from unknown (HELO astro.systems.pipex.net) (62.241.163.6)\n  by mta1.grp.scd.yahoo.com with SMTP; 6 Sep 2006 22:14:49 -0000\r\nReceived: from [10.0.0.11] (81-86-161-87.dsl.pipex.com [81.86.161.87])\n\tby astro.systems.pipex.net (Postfix) with ESMTP id 0FE75E0000E4\n\tfor &lt;neat@yahoogroups.com&gt;; Wed,  6 Sep 2006 23:14:14 +0100 (BST)\r\nMessage-ID: &lt;44FF483B.3040406@...&gt;\r\nDate: Wed, 06 Sep 2006 23:14:19 +0100\r\nUser-Agent: Mozilla Thunderbird 1.0.7 (Windows/20050923)\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: neat@yahoogroups.com\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Colin Green &lt;cgreen@...&gt;\r\nSubject: Evolving a Trading Strategy: Update\r\nX-Yahoo-Group-Post: member; u=127853030; y=VvvwwgvfL42OQP5AN1dW7fMYWcEp71wPJli1JT2SfHrnfZr1Rw_0\r\nX-Yahoo-Profile: alienseedpod\r\n\r\nHi,\n\nOK so over the weekend I modified my experiments to use Z-Score inputs \nalong with continuous means and standard deviations. I&#39;ve been running \nan experiment for a few days now over 163 days worth of data for 521 of \nthe most traded companies on the LSE, and in terms of profit/fitness the \nresult so far is about 40% higher than my previous best strategy. \n\nSo in terms of presenting the data in a way that an ANN and NEAT can \ncapitalise upon this approach would seem to be much better, but as far \nas evolving a useable trading strategy is concerned this has just \nhighighted once again that the data set is far too small and that \noverfitting is therefore a serious problem. Now I don&#39;t know how much of \nthis high score is due to overfitting, but from previous experience and \ncommon sense I estimate it to be close to 100% !  To try and answer this \nquestion I&#39;ll be running the experiment again over the next few days \nwith a subset of that 163 day data set, plus I&#39;ll have an extra 10 days \nworth of data at the end of this week which will also give some clues - \nactually I already have 5 days of data it wasn&#39;t trained against and it \nmakes two losing trades on that :(\n\nPart of the problem is that although I have data for 521 companies, \nthese aren&#39;t 521 independent data sets because the price graph for a lot \nof these companies is very similar. Mostly the champ trader is \ncapitalising on a sharp fall in the markets in May and the subsequent \npartial recovery. So a strategy that detects and capitalises on that \nscenario matches a lot of those 521 companies and therefore scores well.\n\nSo going forward it looks like I&#39;ll be putting my &#39;high quality&#39; data \n(with buy/sell volume, buy/sell trades and bid/offer spread) to one side \nand using a data set with a far longer history but probably less \nquality, e.g. just the closing [mid] price and volume (actually I use \nmoney flow which = volume * share price) simply on the basis that high \nquality data costs a lot of money. Although initially I might look at \ntrading the major indices (FTSE, DJIA, S&P etc.) or perhaps currency \nmarkets if the data is available.\n\nOne problem I envisage with longer histories is that older data may be \nfairly useless in evolving a trading strategy for the present, since no \ndoubt automated trading and semi-automatic analysis/trading has probably \nbeen progressing in leaps and bounds in the last 5 years or so, thus \nfundamentally altering the short term dynamics of the markets. However \nif that is the case it might actually show up in experiments - e.g. if a \nstrategy evolves that does really well for several years in a row and \nthen starts doing badly for a long period.\n\nSo on the whole it&#39;s very interesting that NEAT has found a strategy \nthat performs really well over the provided data set, but all this does \nfor now is provide food for thought for the next round of experimentation.\n\nRegards,\n\nColin\n\n\n\n"}}