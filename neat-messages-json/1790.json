{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":199382914,"authorName":"mike woodhouse","from":"mike woodhouse &lt;mikewoodhouse@...&gt;","profile":"mikewoodhouse","replyTo":"LIST","senderId":"BeV-uvtzLbDPT6skVXUSQMMXA-fB0fUvMesbbomMMIPJ_eqAcbpcrC2AU9R2s2QulZnuff_4NGXnw8n8bE-Lyi-X4_LSAy6AyXr_eyGGww","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] Stock Market Revisited","postDate":"1105531241","msgId":1790,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGI1NDQ3MzBlMDUwMTEyMDQwMDM3ZjM0NGExQG1haWwuZ21haWwuY29tPg==","inReplyToHeader":"PFdvcmxkQ2xpZW50LUYyMDA1MDExMTIzMjAuQUEyMDU0MDEwOUBvY3RhZ2F0ZS5jb20+","referencesHeader":"PFdvcmxkQ2xpZW50LUYyMDA1MDExMTExMDcuQUEwNzIzMDA5NEBvY3RhZ2F0ZS5jb20+CSA8YjU0NDczMGUwNTAxMTEwNTExNjliYjY5NmZAbWFpbC5nbWFpbC5jb20+CSA8V29ybGRDbGllbnQtRjIwMDUwMTExMjMyMC5BQTIwNTQwMTA5QG9jdGFnYXRlLmNvbT4="},"prevInTopic":1789,"nextInTopic":1792,"prevInTime":1789,"nextInTime":1791,"topicId":1779,"numMessagesInTopic":14,"msgSnippet":"I ll cut-and-comment as necessary to keep this as brief as possible... ... I d us price1 / price14. I might consider normalising by dividing by the standard","rawEmail":"Return-Path: &lt;mikewoodhouse@...&gt;\r\nX-Sender: mikewoodhouse@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 21939 invoked from network); 12 Jan 2005 12:00:42 -0000\r\nReceived: from unknown (66.218.66.167)\n  by m3.grp.scd.yahoo.com with QMQP; 12 Jan 2005 12:00:42 -0000\r\nReceived: from unknown (HELO rproxy.gmail.com) (64.233.170.204)\n  by mta6.grp.scd.yahoo.com with SMTP; 12 Jan 2005 12:00:41 -0000\r\nReceived: by rproxy.gmail.com with SMTP id y7so9222rne\n        for &lt;neat@yahoogroups.com&gt;; Wed, 12 Jan 2005 04:00:41 -0800 (PST)\r\nDomainKey-Signature: a=rsa-sha1; q=dns; c=nofws;\n        s=beta; d=gmail.com;\n        h=received:message-id:date:from:reply-to:to:subject:in-reply-to:mime-version:content-type:content-transfer-encoding:references;\n        b=Ans2fiRoURDHxpe1jbcsNkdBkFc9wbtWrGBIiEh7l7w9TocTPNGiR/g4auAgkEVV76Wo02iKDNDQVu3HDpd296NUS5weX0lyoP3XswWrFTaM3XkPcEtwmEgBnJsgTFlnBwDlh3pALZL0gaBxTrF0lYisVoy5GHRSpfaLlHwZYps=\r\nReceived: by 10.38.15.71 with SMTP id 71mr240034rno;\n        Wed, 12 Jan 2005 04:00:41 -0800 (PST)\r\nReceived: by 10.38.65.45 with HTTP; Wed, 12 Jan 2005 04:00:41 -0800 (PST)\r\nMessage-ID: &lt;b544730e050112040037f344a1@...&gt;\r\nDate: Wed, 12 Jan 2005 12:00:41 +0000\r\nTo: neat@yahoogroups.com\r\nIn-Reply-To: &lt;WorldClient-F200501112320.AA20540109@...&gt;\r\nMime-Version: 1.0\r\nContent-Type: text/plain; charset=US-ASCII\r\nContent-Transfer-Encoding: 7bit\r\nReferences: &lt;WorldClient-F200501111107.AA07230094@...&gt;\n\t &lt;b544730e050111051169bb696f@...&gt;\n\t &lt;WorldClient-F200501112320.AA20540109@...&gt;\r\nX-eGroups-Remote-IP: 64.233.170.204\r\nFrom: mike woodhouse &lt;mikewoodhouse@...&gt;\r\nReply-To: mike woodhouse &lt;mikewoodhouse@...&gt;\r\nSubject: Re: [neat] Stock Market Revisited\r\nX-Yahoo-Group-Post: member; u=199382914\r\nX-Yahoo-Profile: mikewoodhouse\r\n\r\nI&#39;ll cut-and-comment as necessary to keep this as brief as possible...\n\n&gt; That&#39;s interesting, I&#39;ll consider that. Hm... how do you calculate the\n&gt; return over a 14 day period, the price of day 1 - the price of price 14?\n\nI&#39;d us price1 / price14. I might consider normalising by dividing by\nthe standard deviation if the same network is to be appied to all\nstocks [see below].\n\n&gt; Buy/Sell indications. &gt;0.5 means wants to own &lt;0.5 doesn&#39;t want to own. So\n&gt; if it&#39;s owning and decides it doesn&#39;t want to own any more, it sells.\n\nI&#39;d suggest at least one additional band in the middle to indicate\n&quot;not interested at the moment&quot; (or maybe &quot;exit  long&quot;?). Or do you\npost-process the results in some way to avoid getting whipsawed by\ncontinual 0.49 - 0.51 results?\n\n&gt; * Long positions have a downside limited to 100% but an upside that&#39;s\n&gt; unlimited. I prefer long positions.\n\nEasier to trade, too, unless you use derivatives (e.g. Contracts For\nDifference or finanicla betting sites)\n\n&gt; One input is &quot;is owning stock now&quot; which encourages longterm investments\n&gt; because my fitness is somewhat profitability based.\n\nAh. This probably helps to filter the potential whipsaw.\n\n&gt; I find that infrequent trading is more of a problem in my current\n&gt; simulations than frequent trading. And it&#39;s a big problem, because on a\n&gt; long time series (10 years), it&#39;s easy to pick 10 good buy spots and 10\n&gt; good sell spots and earn fantastic profits, but it&#39;s harder to pick 100\n&gt; such spots.\n\nYes. Much too easy for the network to come up with a lucky\n(=overfitted) topology/weight configuration.\n\n&gt; Currently, I severely punish infrequent trading because I view it as a\n&gt; form of overfitting.\n\nBad network, bad. Daddy spank. :)\n\n&gt; my initial attempts (years back) presented each strategy with dozens of\n&gt; indicators, including: Forecast using linear regression, Momentum,\n&gt; Relative Strength Index, Money Flow Index, Accumulation / Distribution...\n&gt; (there were a lot more). My thinking was that more data is better.\n&gt; \n&gt; My conclusion after extensive testing is that more data tends to lead to\n&gt; overfitting. I was using Genetic Programming at the time, but I think the\n&gt; argument holds for NEAT also;\n\nI just remembered a discussion in a mighty technical analysis tome (by\nWilliam F Eng, can&#39;t remember the title) of a horoscope-based system.\nThe book (to its credit, I think) analyses the system seriously. I\ndon&#39;t think the conclusion was particularly enthusiastic/\n\nMight NEAT with a super-sparse initial configuration (one random\nconnection) do a decent job of filtering out the chaff? Or would some\nhorrendous decorrelation exercise be useful?\n\n&gt; * Less data makes each situation less-unique (ooh, an oxymoron!), which\n&gt; makes strategies more generic.\n\nI take it that English is not your first language? I am truly humbled.\n\n&gt; The question is - which indicators to use and what fitness function to\n&gt; use. I tried a dozen different fitness functions also. Max drawdown, max\n&gt; profit, max profit / max drawdown etc.\n\nAh, the fitness function, there indeed is the rub. Could some sort of\nChi-squared function be applied to determine statistical significance\nof the results?\n\n&gt; Each day at three I&#39;d perform trades as the system instructed me (that was\n&gt; 9 NY time). One day, just before I was about to place my orders, DATEK\n&gt; went off the air for no apparant reason, it was very weird. That was\n&gt; september 11:th 2001, you all know what happened next. The next day that\n&gt; the marked was open I took the money of the market and the general marked\n&gt; downturn meant that I had to use the money for other things. Haven&#39;t\n&gt; traded the system since. But I&#39;d like to get into it again.\n\nFollowing on from that unfortunate timing is something that&#39;s always\nworried me: major events such as 9/11, perhaps the ERM crisis in &#39;92\nand plenty of others that come to mind. Hell, even legislation\n(particulrly tax or corporate governance) can have an effect oin the\nway cmopany shares are priced. I think these may have a permanent\neffect on markets, such that rules that were applicable before are\npotentially lethal (in a financial sense) after. Using daily data then\nseverely restricts the amount of history that can safely be used.\nPerhaps a candidate input might be the time since a major shock was\nexperienced, which might help the model decide when it&#39;s learned\nenough about the way the market has changed. I&#39;d expect that recurrent\nnetworks would be a big help here.\n\nWould you expect the same model to be able to assess, say, an\ninsurance company against, oh, I don&#39;t know, a wholesale food\nmanufacturer or a high-tech pharmaceutical? I&#39;ve speculated that the\nsame network could work, but a separate instance be kept for each\ncompany so that the recurrent node &quot;memory&quot; values to that company&#39;s\ndata. Or is that already obvious? I think to get the full benefit of\nthis the network shouldn&#39;t be iterated for each set of inputs until it\nreaches some kind of &quot;steady state&quot;, rather that each set of inputs\nconsitututes one &quot;tick&quot; in the evaluation.\n\n&gt; My experience is this; it&#39;s near impossible to come up with a single\n&gt; strategy that can trade a single stock profitable, out of bounds.\n\nBear in mind that these guys have access to several things that are\nstill out of our reach: vastly greater supplies of market data,\nsignificantly lower trading costs, huge computer resources to name\nthree. Of course, they don&#39;t have the smartest people, &#39;cos they don&#39;t\nhave us...\n\n&gt; I&#39;m not saying that AI trading isn&#39;t possible though...\n\nI&#39;m utterly convinced that it is - I&#39;ve worked with too many traders\nto believe that most of them couldn&#39;t be replaced with a reasonably\nwell-written program!\n\nMike\n\n"}}