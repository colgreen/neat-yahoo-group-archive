{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":7192225,"authorName":"Ian Badcoe","from":"Ian Badcoe &lt;ian_badcoe@...&gt;","profile":"ian_badcoe","replyTo":"LIST","senderId":"9lEFm2Ou-Hq8qAbIgiJtx72DBpMRKXXaxiimgln_xpIkDyq67BeYUj_M-Rs3y9Mg4TpWLsf9scxPI6jo-f5cceCbuu1e-MFoCJk","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] Neural network optimization details","postDate":"1087912954","msgId":1119,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDYuMS4wLjYuMC4yMDA0MDYyMjE0NTMwMS4wMjUxNTFiOEBwb3AubWFpbC55YWhvby5jby51az4=","inReplyToHeader":"PDQwRDVBODkzLjMwODA0MDZAZHNsLnBpcGV4LmNvbT4=","referencesHeader":"PDQwRDVBODkzLjMwODA0MDZAZHNsLnBpcGV4LmNvbT4="},"prevInTopic":1110,"nextInTopic":1123,"prevInTime":1118,"nextInTime":1120,"topicId":1106,"numMessagesInTopic":7,"msgSnippet":"... Hi, That s very cool! floats vs doubles: 1) I would think float precision is easily enough for NN purposes 2) if it isn t, won t they evolve to compensate ","rawEmail":"Return-Path: &lt;ian_badcoe@...&gt;\r\nX-Sender: ian_badcoe@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 60742 invoked from network); 22 Jun 2004 14:50:57 -0000\r\nReceived: from unknown (66.218.66.167)\n  by m15.grp.scd.yahoo.com with QMQP; 22 Jun 2004 14:50:57 -0000\r\nReceived: from unknown (HELO smtp003.mail.ukl.yahoo.com) (217.12.11.34)\n  by mta6.grp.scd.yahoo.com with SMTP; 22 Jun 2004 14:50:57 -0000\r\nReceived: from unknown (HELO ian2k.yahoo.co.uk) (ian?badcoe@212.159.73.108 with login)\n  by smtp003.mail.ukl.yahoo.com with SMTP; 22 Jun 2004 14:50:55 -0000\r\nMessage-Id: &lt;6.1.0.6.0.20040622145301.025151b8@...&gt;\r\nX-Sender: ian_badcoe@...\r\nX-Mailer: QUALCOMM Windows Eudora Version 6.1.0.6\r\nDate: Tue, 22 Jun 2004 15:02:34 +0100\r\nTo: neat@yahoogroups.com\r\nIn-Reply-To: &lt;40D5A893.3080406@...&gt;\r\nReferences: &lt;40D5A893.3080406@...&gt;\r\nMime-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;us-ascii&quot;; format=flowed\r\nX-eGroups-Remote-IP: 217.12.11.34\r\nFrom: Ian Badcoe &lt;ian_badcoe@...&gt;\r\nSubject: Re: [neat] Neural network optimization details\r\nX-Yahoo-Group-Post: member; u=7192225\r\nX-Yahoo-Profile: ian_badcoe\r\n\r\nAt 16:09 20/06/2004, you wrote:\n&gt;For those following the &#39;computation time&#39; thread I have written up the\n&gt;details of the optimizations I have performed on my network code along\n&gt;with some benchmarks. The details are at\n&gt;http://www.cgreen.dsl.pipex.com/network_optimization.htm\n\nHi,\n\nThat&#39;s very cool!\n\nfloats vs doubles:\n\n1) I would think float precision is easily enough for NN purposes\n2) if it isn&#39;t, won&#39;t they evolve to compensate\n\nw.r.t the casts, there is another consideration, values kept in the FPU \nregister stack are (almost) always 80bit values, and this is converted down \nto 64bits or 32bits when it needs writing to memory.  (This can lead to \nlow-bit differences and C compilers have flags to prioritise speed or \nprecision.)\n\nIf your compiler will keep an intermediate value on the FPU (I don&#39;t know \nabout Java), then for a complex expression, the best option would be to do \nall the maths in double, on the FPU, and just finally convert one value \nback at the end.  That&#39;s not your situation, however, and I don&#39;t see a lot \nyou could do to better your current case.  I don&#39;t think casts on reals are \nespecially slow -- UNLESS, java does a load of range-checking?\n\np.s. Your tanh would be faster if you used the equivalent exp-based \nexpression, I forget what it was but it&#39;s been on the list already.\n\n         Ian Badcoe\n\n\n\n\nLiving@Home - Open Source Evolving Organisms - \nhttp://livingathome.sourceforge.net/\n\n\n\n"}}