{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":115403844,"authorName":"John Arrowwood","from":"&quot;John Arrowwood&quot; &lt;jarrowwx@...&gt;","profile":"jarrowwx","replyTo":"LIST","senderId":"z8uzmX9tONxytny67oGS2Ad7mYTAvpimPnT_PIYsl5GyfbS--RDR-Rliznw2qU6sBXBd3yXkkGtfyBV2dwqv72so5ZvwLP7FrppF3ZeY","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] IEX update","postDate":"1091739449","msgId":1336,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PEJBWTItRjZxV24zR0NJRm9Qa0cwMDAwMDdjNEBob3RtYWlsLmNvbT4="},"prevInTopic":1335,"nextInTopic":1361,"prevInTime":1335,"nextInTime":1337,"topicId":1176,"numMessagesInTopic":38,"msgSnippet":"... That s what I *WAS* doing, but there was a drawback in this case.  It is so much easier to do the reduction than the enlargement that it was optimizing for","rawEmail":"Return-Path: &lt;jarrowwx@...&gt;\r\nX-Sender: jarrowwx@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 28192 invoked from network); 5 Aug 2004 20:57:30 -0000\r\nReceived: from unknown (66.218.66.216)\n  by m17.grp.scd.yahoo.com with QMQP; 5 Aug 2004 20:57:30 -0000\r\nReceived: from unknown (HELO hotmail.com) (65.54.247.6)\n  by mta1.grp.scd.yahoo.com with SMTP; 5 Aug 2004 20:57:30 -0000\r\nReceived: from mail pickup service by hotmail.com with Microsoft SMTPSVC;\n\t Thu, 5 Aug 2004 13:57:29 -0700\r\nReceived: from 64.122.44.102 by by2fd.bay2.hotmail.msn.com with HTTP;\n\tThu, 05 Aug 2004 20:57:29 GMT\r\nX-Originating-Email: [jarrowwx@...]\r\nX-Sender: jarrowwx@...\r\nTo: neat@yahoogroups.com\r\nBcc: \r\nDate: Thu, 05 Aug 2004 13:57:29 -0700\r\nMime-Version: 1.0\r\nContent-Type: text/plain; format=flowed\r\nMessage-ID: &lt;BAY2-F6qWn3GCIFoPkG000007c4@...&gt;\r\nX-OriginalArrivalTime: 05 Aug 2004 20:57:29.0628 (UTC) FILETIME=[D21155C0:01C47B2E]\r\nX-eGroups-Remote-IP: 65.54.247.6\r\nFrom: &quot;John Arrowwood&quot; &lt;jarrowwx@...&gt;\r\nReply-To: john@...\r\nSubject: Re: [neat] IEX update\r\nX-Yahoo-Group-Post: member; u=115403844\r\nX-Yahoo-Profile: jarrowwx\r\n\r\n&gt;From: Colin Green &lt;cgreen@...&gt;\n&gt;John Arrowwood wrote:\n&gt;\n&gt; &gt;I also took a long hard look at how MUCH enlargement I was asking of it.\n&gt; &gt;16x is too much to ask it to try to learn on.  So I made a minor\n&gt; &gt;modification that lets me first train it to do  2x.  Once it appears to \n&gt;be\n&gt; &gt;able to do 2x, I can up the expected amount of enlargement to 3x, 4x, 5x,\n&gt; &gt;and so on, until I max out forward progress.  That should make things\n&gt; &gt;progress much faster.  But we&#39;ll have to wait and see if theory meets\n&gt; &gt;reality. :)\n&gt; &gt;\n&gt;I suppose if you take the logic that we applied to Chad&#39;s cross product\n&gt;problem then it would make more sense to test against a spread of\n&gt;enlargement/reduction factors on each evaluation. Otherwise you might\n&gt;see overfitting at the enlargement end of the scale similar to the\n&gt;signal averaging you observed with reduction.  I guess that would slow\n&gt;your experiment down quite a lot though huh :(\n\nThat&#39;s what I *WAS* doing, but there was a drawback in this case.  It is so \nmuch easier to do the reduction than the enlargement that it was optimizing \nfor the easier problem without making any progress towards the harder \nproblem.\n\nSo now, to ensure that it does solve the harder problem, I will only expose \nit to the harder problem until it actually gets it right.  Then, to \ndifferentiate between two networks that both solve the harder problem, both \nwill be exposed to the easier problem, and whichever one does better overall \nis the more fit network.\n\nI do this all at once, though, not in phases.  That is, I will evaluate a \nnetwork at a particular scaling factor (initially 2x).  The fitness is 1 - \navg_error, or a value between 0 and 1.  If it is greater than a threshold \n(0.9), then it evaluates it again at a slightly smaller scaling factor, and \nthe fitness range is 1..2.  As long as the avg_error is less than 0.1, it \nkeeps decreasing the scaling factor and increasing the fitness value range.  \nOnce it either falls outside the range or runs out of expected outputs to \ncompare it to, it stops, and the result is that network&#39;s fitnss value.\n\nSo a fitness value of 1 means it does a good job of scaling up to 2x.  A \nfitness value higher than that means that it also has such an accurate model \nof the underlying image, and it&#39;s query mechanism is so accurate that I am \nable to query that model and get good results for more than just 2x \nenlargement.  The biggest hurdle will be getting past a fitness of 1.  \nEverything after that is just &#39;optimization&#39;.\n\nAs for over-fitting, that basically can&#39;t happen.  Over fitting requires you \nto expose the network to the same data, over and over again.  I have 3,332 \nimages, 10 copies of each at different scaling factors \n(1,1/2,1/3,1/4,...1/10) for a total of 33,320 images.  Each image has a very \nlarge number of samples from which to choose.  I wouldn&#39;t live long enough \nto expose the network to all possible testing samples enough times for it to \nover-fit.  :)\n\nNow, I am running into a small issue that somebody might want to comment on:\n\nInitially, I start with one topology.  Next generation, I have two .  Next \ngeneration I&#39;ll have three, and so on up to 10.  BUT:  Initially, fitness \nvalues are very, very low.  If there is one network that happens to get \nlucky and produce a well above average fitness evaluation, it takes so much \nof the fitness as a whole that the other topologies are allocated less than \n1 offspring during fitness sharing.  This squeezes those networks into \nextenction.  OOPS!  Has anybody else encountered this kind of problem?  If \nso, how have you solved it, or did you bother?\n\n\n\n"}}