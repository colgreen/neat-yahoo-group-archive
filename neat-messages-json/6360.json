{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":434634266,"authorName":"Vassilis Vassiliades","from":"Vassilis Vassiliades &lt;vassilisvas@...&gt;","profile":"v.vassiliades","replyTo":"LIST","senderId":"BBuSiTvr3PNNI4ChMBwV_ZFxNpTIpE9Gu-VV0jXo1rlCU1XTaovP83pQ2z4BYrHELoIShbQcxuKysNyrog51MMbR13IHjX-rzunXaDMWWnY4GwY","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [neat] New Paper on RAAHN, a New Kind of Neural Plasticity","postDate":"1402181704","msgId":6360,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PENBTnRYaG11dGpiWm9ldTJMT2dNOUJRTXlYTTJXVFFvVTArK1R1NDFXZV8wR0tETmNDUUBtYWlsLmdtYWlsLmNvbT4=","inReplyToHeader":"PGxuMDI4bCtudjc1dnRAWWFob29Hcm91cHMuY29tPg==","referencesHeader":"PGxuMDI4bCtudjc1dnRAWWFob29Hcm91cHMuY29tPg=="},"prevInTopic":6359,"nextInTopic":0,"prevInTime":6359,"nextInTime":6361,"topicId":6359,"numMessagesInTopic":2,"msgSnippet":"Hello Ken, Justin and Andrea, Congrats on the new paper! Looks fascinating! Can t wait to read it. Best, Vassilis On Jun 8, 2014 1:04 AM,","rawEmail":"Return-Path: &lt;vassilisvas@...&gt;\r\nX-Sender: vassilisvas@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 62063 invoked by uid 102); 7 Jun 2014 22:55:05 -0000\r\nX-Received: from unknown (HELO mtaq1.grp.bf1.yahoo.com) (10.193.84.32)\n  by m17.grp.bf1.yahoo.com with SMTP; 7 Jun 2014 22:55:05 -0000\r\nX-Received: (qmail 27297 invoked from network); 7 Jun 2014 22:55:05 -0000\r\nX-Received: from unknown (HELO mail-pb0-f46.google.com) (209.85.160.46)\n  by mtaq1.grp.bf1.yahoo.com with SMTP; 7 Jun 2014 22:55:05 -0000\r\nX-Received: by mail-pb0-f46.google.com with SMTP id rq2so3891221pbb.5\n        for &lt;neat@yahoogroups.com&gt;; Sat, 07 Jun 2014 15:55:05 -0700 (PDT)\r\nMIME-Version: 1.0\r\nX-Received: by 10.68.186.33 with SMTP id fh1mr2829382pbc.140.1402181705130;\n Sat, 07 Jun 2014 15:55:05 -0700 (PDT)\r\nX-Received: by 10.70.50.103 with HTTP; Sat, 7 Jun 2014 15:55:04 -0700 (PDT)\r\nX-Received: by 10.70.50.103 with HTTP; Sat, 7 Jun 2014 15:55:04 -0700 (PDT)\r\nIn-Reply-To: &lt;ln028l+nv75vt@...&gt;\r\nReferences: &lt;ln028l+nv75vt@...&gt;\r\nDate: Sun, 8 Jun 2014 01:55:04 +0300\r\nMessage-ID: &lt;CANtXhmutjbZoeu2LOgM9BQMyXM2WTQoU0++Tu41We_0GKDNcCQ@...&gt;\r\nTo: neat@yahoogroups.com\r\nContent-Type: multipart/alternative; boundary=047d7bd7642c71e8ee04fb46de4d\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nSubject: Re: [neat] New Paper on RAAHN, a New Kind of Neural Plasticity\r\nX-Yahoo-Group-Post: member; u=434634266; y=e-i_We78QQGbGLJWz7r0w6HTqhK4Z2DJelFUsWoAmRajJ9JhXL1fbw\r\nX-Yahoo-Profile: v.vassiliades\r\nFrom: Vassilis Vassiliades &lt;vassilisvas@...&gt;\r\n\r\n\r\n--047d7bd7642c71e8ee04fb46de4d\r\nContent-Type: text/plain; charset=UTF-8\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\nHello Ken, Justin and Andrea,\n\nCongrats on the new paper! Looks fascinating=\r\n! Can&#39;t wait to read it.\n\nBest,\nVassilis\nOn Jun 8, 2014 1:04 AM, &quot;kstanley@=\r\ncs.utexas.edu [neat]&quot; &lt;\nneat@yahoogroups.com&gt; wrote:\n\n&gt;\n&gt;\n&gt; I am pleased to=\r\n announce with my coauthors Justin Pugh and Andrea\n&gt; Soltoggio our new ALIF=\r\nE conference paper, &quot;Real-time Hebbian Learning from\n&gt; Autoencoder Features=\r\n for Control Tasks.&quot;\n&gt;\n&gt;\n&gt; http://eplex.cs.ucf.edu/papers/pugh_alife14.pdf\n=\r\n&gt;\n&gt;\n&gt; While the first thing you&#39;ll probably notice about this paper is that=\r\n\n&gt; there is no evolution, the idea was still conceived very much with\n&gt; evo=\r\nlution in mind.  It&#39;s based on the insight that one of the things\n&gt; holding=\r\n back neuroevolution research that involves plasticity is that local\n&gt; plas=\r\nticity rules, while they can be helpful for learning correlations, do\n&gt; not=\r\n tend to build up new representations (i.e. features) of the world over\n&gt; t=\r\nhe agent&#39;s lifetime.  We (myself and coauthors) think that&#39;s a big\n&gt; roadbl=\r\nock for evolving much more interesting kinds of plastic brains.\n&gt; Basically=\r\n, we&#39;d like to see ANNs that build up new representations of the\n&gt; world at=\r\n the same time as they learn to control themselves based on those\n&gt; new rep=\r\nresentations.\n&gt;\n&gt;\n&gt; That&#39;s why we introduce the RAAHN in this paper, which =\r\nstands for a\n&gt; Real-time Autoencoder-Augmented Hebbian Network.  It&#39;s a pre=\r\ntty\n&gt; straightforward idea:  an autoencoder running in real-time picks up\n&gt;=\r\n features while neuromodulated Hebbian connections projecting from the\n&gt; au=\r\ntoencoder learn control policies from those developing features based on\n&gt; =\r\nrewards and penalties (i.e. neuromodulation).  The paper shows a proof of\n&gt;=\r\n concept where RAAHN works even without evolution, but I believe a big part=\r\n\n&gt; of the future for RAAHN is to embed it within evolved networks (even to\n=\r\n&gt; evolve its embedding), and there are numerous exciting possibilities for\n=\r\n&gt; that.\n&gt;\n&gt;\n&gt; It&#39;s also interesting for its potential to unify ideas from d=\r\neep learning\n&gt; (where autoencoders made their splash) with neuroevolution. =\r\n After all, the\n&gt; latest unsupervised feature learning technology from deep=\r\n learning (like\n&gt; present autoencoders) can easily be swapped into RAAHN in=\r\n the future and\n&gt; then evolved into ANNs through neuroevolution.  I think i=\r\nt also suggests\n&gt; that deep learning has missed some of the more interestin=\r\ng implications of\n&gt; its own work by focusing so much on classification task=\r\ns, which tend to\n&gt; obfuscate how cool it is to be able to sit there and gen=\r\nerate unsupervised\n&gt; reinterpretations of the world on the fly as you go ab=\r\nout your business,\n&gt; which if you think about, is a big part of life.\n&gt;\n&gt;\n&gt;=\r\n Best,\n&gt;\n&gt;\n&gt; ken\n&gt;\n&gt;\n&gt;  \n&gt;\n\r\n--047d7bd7642c71e8ee04fb46de4d\r\nContent-Type: text/html; charset=UTF-8\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\n&lt;p dir=3D&quot;ltr&quot;&gt;Hello Ken, Justin and Andrea,&lt;/p&gt;\n&lt;p dir=3D&quot;ltr&quot;&gt;Congrats on=\r\n the new paper! Looks fascinating! Can&#39;t wait to read it.&lt;/p&gt;\n&lt;p dir=3D=\r\n&quot;ltr&quot;&gt;Best,&lt;br&gt;\nVassilis&lt;/p&gt;\n&lt;div class=3D&quot;gmail_quote&quot;&gt;On Jun 8, 2014 1:04=\r\n AM, &quot;&lt;a href=3D&quot;mailto:kstanley@...&quot;&gt;kstanley@...=\r\n&lt;/a&gt; [neat]&quot; &lt;&lt;a href=3D&quot;mailto:neat@yahoogroups.com&quot;&gt;neat@yahoogro=\r\nups.com&lt;/a&gt;&gt; wrote:&lt;br type=3D&quot;attribution&quot;&gt;\n&lt;blockquote class=3D&quot;gmail_=\r\nquote&quot; style=3D&quot;margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1=\r\nex&quot;&gt;\n\n\n&lt;u&gt;&lt;/u&gt;\n\n\n\n\n\n\n\n\n\n \n&lt;div style=3D&quot;background-color:#fff&quot;&gt;\n&lt;span&gt;=C2=\r\n=A0&lt;/span&gt;\n\n\n&lt;div&gt;\n  &lt;div&gt;\n\n\n    &lt;div&gt;\n      \n      \n      &lt;p&gt;&lt;p&gt;&lt;span&gt;I am=\r\n pleased to announce with my coauthors Justin Pugh and Andrea Soltoggio our=\r\n new ALIFE conference paper, &quot;Real-time Hebbian Learning from Autoenco=\r\nder Features for Control Tasks.&quot;&lt;/span&gt;&lt;/p&gt;\n&lt;p&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p=\r\n&gt;&lt;span&gt;&lt;a rel=3D&quot;nofollow&quot; href=3D&quot;http://eplex.cs.ucf.edu/papers/pugh_alif=\r\ne14.pdf&quot; target=3D&quot;_blank&quot;&gt;http://eplex.cs.ucf.edu/papers/pugh_alife14.pdf&lt;=\r\n/a&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;While the first thing yo=\r\nu&#39;ll probably notice about this paper is that there is no evolution, th=\r\ne idea was still conceived very much with evolution in mind.=C2=A0 It&#39;s=\r\n based on the insight that one of the things holding back neuroevolution re=\r\nsearch that involves plasticity is that local plasticity rules, while they =\r\ncan be helpful for learning correlations, do not tend to build up new repre=\r\nsentations (i.e. features) of the world over the agent&#39;s lifetime.=C2=\r\n=A0 We (myself and coauthors) think that&#39;s a big roadblock for evolving=\r\n much more interesting kinds of plastic brains.=C2=A0 Basically, we&#39;d l=\r\nike to see ANNs that build up new representations of the world at the same =\r\ntime as they learn to control themselves based on those new representations=\r\n.&lt;/span&gt;&lt;/p&gt;\n&lt;p&gt;&lt;span&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;That&#39;s why we introduce t=\r\nhe RAAHN in this paper, which stands for a Real-time Autoencoder-Augmented =\r\nHebbian Network.=C2=A0 It&#39;s a pretty straightforward idea:=C2=A0 an aut=\r\noencoder running in real-time picks up features while neuromodulated Hebbia=\r\nn connections projecting from the autoencoder learn control policies from t=\r\nhose developing features based on rewards and penalties (i.e. neuromodulati=\r\non).=C2=A0 The paper shows a proof of concept where RAAHN works even withou=\r\nt evolution, but I believe a big part of the future for RAAHN is to embed i=\r\nt within evolved networks (even to evolve its embedding), and there are num=\r\nerous exciting possibilities for that.=C2=A0=C2=A0&lt;/span&gt;&lt;/p&gt;\n&lt;p&gt;&lt;br&gt;&lt;span&gt;=\r\n&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;It&#39;s also interesting for its potential to unify id=\r\neas from deep learning (where autoencoders made their splash) with neuroevo=\r\nlution.=C2=A0 After all, the latest unsupervised feature learning technolog=\r\ny from deep learning (like present autoencoders) can easily be swapped into=\r\n RAAHN in the future and then evolved into ANNs through neuroevolution.=C2=\r\n=A0 I think it also suggests that deep learning has missed some of the more=\r\n interesting implications of its own work by focusing so much on classifica=\r\ntion tasks, which tend to obfuscate how cool it is to be able to sit there =\r\nand generate unsupervised reinterpretations of the world on the fly as you =\r\ngo about your business, which if you think about, is a big part of life.&lt;br=\r\n&gt;\n&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Best,&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;span&gt;=\r\n&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;ken&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;=C2=A0&lt;/span&gt;&lt;/p&gt;&lt;/p&gt;\n\n    &lt;=\r\n/div&gt;\n     \n\n    \n    &lt;div style=3D&quot;color:#fff;min-height:0&quot;&gt;&lt;/div&gt;\n\n\n&lt;/div=\r\n&gt;\n\n\n\n  \n\n\n\n\n\n\n&lt;/blockquote&gt;&lt;/div&gt;\n\r\n--047d7bd7642c71e8ee04fb46de4d--\r\n\n"}}