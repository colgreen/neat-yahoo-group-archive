{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"kenstanley01","from":"&quot;kenstanley01&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"dgIm4m9k8I5f9uKhAk2T3iSJZtqrkb5lZIL4WeLd_hAxTMhh7p91fXbk6OpnVDqtk-2V3oIE3oL6DomZulDiBvC4kSWcGMSzmmX88TQy","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Substrate Evolution","postDate":"1252997944","msgId":4853,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGg4bmR2byttYnZtQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGg4OGdpaysyc2M4QGVHcm91cHMuY29tPg=="},"prevInTopic":4848,"nextInTopic":4854,"prevInTime":4852,"nextInTime":4854,"topicId":4848,"numMessagesInTopic":5,"msgSnippet":"Paul, these are good thoughts and I m glad that you re thinking about substrate evolution, since it is clearly an interesting future step.  Note that the","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 30516 invoked from network); 15 Sep 2009 06:59:10 -0000\r\nX-Received: from unknown (98.137.34.45)\n  by m2.grp.sp2.yahoo.com with QMQP; 15 Sep 2009 06:59:10 -0000\r\nX-Received: from unknown (HELO n37b.bullet.mail.sp1.yahoo.com) (66.163.168.151)\n  by mta2.grp.sp2.yahoo.com with SMTP; 15 Sep 2009 06:59:10 -0000\r\nX-Received: from [69.147.65.149] by n37.bullet.mail.sp1.yahoo.com with NNFMP; 15 Sep 2009 06:59:06 -0000\r\nX-Received: from [98.137.34.72] by t9.bullet.mail.sp1.yahoo.com with NNFMP; 15 Sep 2009 06:59:06 -0000\r\nDate: Tue, 15 Sep 2009 06:59:04 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;h8ndvo+mbvm@...&gt;\r\nIn-Reply-To: &lt;h88gik+2sc8@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;kenstanley01&quot; &lt;kstanley@...&gt;\r\nSubject: Re: Substrate Evolution\r\nX-Yahoo-Group-Post: member; u=54567749; y=zWjIFFZEi4D1xzSFyPfGERYVSCdpiPBeGjaQOGnOBQa6lksGFbDy\r\nX-Yahoo-Profile: kenstanley01\r\n\r\nPaul, these are good thoughts and I&#39;m glad that you&#39;re thinking about subst=\r\nrate evolution, since it is clearly an interesting future step.  Note that =\r\nthe substrate in HyperNEAT is to some extent more about topography than top=\r\nology.  In other words, it has to do with the geometry to the topology, rat=\r\nher than just with what is connected to what.  In any case, lifetime adapta=\r\ntions clearly does influence the ultimate topology/topography of our brains=\r\n.  Connections arise or die off as a result of events during life.  Yet I a=\r\nlso believe that this observation cannot be the whole solution to substrate=\r\n evolution, because it still leaves the question open of not only with what=\r\n topography to start, but also how the rules are originally created that di=\r\nctate which connections survive and which are eliminated.  So I think there=\r\n is a still a fundamental issue, which is how to start the brain, that come=\r\ns before a neural-Darwinism type process kicks in.\n\nAs far as whether neura=\r\nl Darwinism can play a role in evolved neural networks, I think it certainl=\r\ny can.  However, the details would need to be worked out as you point out, =\r\nand the theory is possibly an oversimplification of a more subtle and diver=\r\nse set of processes that prune and add connections in real brains.  So ther=\r\ne are a lot of questions left to answer.\n\nHowever, I would not worry about =\r\nthe issue of CPU load too much.  My view is that if something really useful=\r\n can result from an expensive process (such as deciding what to do with 9 m=\r\nillion connections), as long as it&#39;s not possible in another way, then bein=\r\ng computationally expensive is okay and the purchase of 100 new processors =\r\nmight be justified if what would come out is really phenomenal.\n\nAnyway,  I=\r\n hope to hear more of your thoughts on future HyperNEAT directions as they =\r\ndevelop.\n\nken\n\n--- In neat@yahoogroups.com, &quot;spoonsx21&quot; &lt;spoonsx21@...&gt; wro=\r\nte:\n&gt;\n&gt; Hello everyone,\n&gt; \n&gt; I&#39;m new to the Neat group. My name is Paul, I&#39;=\r\nm an undergraduate interested in evolutionary computation, and obviously Hy=\r\nperNEAT/ NEAT. I have been thinking a lot about substrate evolution, and it=\r\ns parallel in biology. One of the fundamental questions I&#39;ve been trying to=\r\n answer is, how did brain topology evolve? And it is clearly fundamental to=\r\n the problem of substrate evolution. Our sensory information is pretty well=\r\n segregated in the brain. For instance, visual areas are broken down into s=\r\neparate processing locations (V1-V5), each area dealing with different aspe=\r\ncts of visualization like object movement, or pattern recognition. There ar=\r\ne plenty of studies and papers on current brain topology, but it&#39;s difficul=\r\nt to find ideas on how brain topology evolved. \n&gt; \n&gt; However, I did stumble=\r\n upon one theory I enjoyed. It was Gerald Edelman&#39;s theory on what he calls=\r\n neural Darwinism, or the theory of neuronal group selection (TNGS). And wh=\r\nile I attempt to truly get my head around the theory, what I have drawn fro=\r\nm his theory seems applicable to HyperNEAT&#39;s extensions. The theory states =\r\nthat within the brain there is first a process of selection in creating the=\r\n brain&#39;s anatomy, with small epigenetic changes occurring in development (H=\r\nere you can imagine that the anatomy in HyperNEAT is our substrate). Then i=\r\nn the postnatal stage, there is a time of neuron selection, where some syna=\r\nptic connections are strengthened, and others simply disappear altogether t=\r\nhrough neuron death (something with little or no parallel in HyperNEAT). He=\r\n gives as an example a chicken, which is born with 20,000 neurons. At the a=\r\ndult stage, the chicken has 12,000 neurons, keeping only 60% of the origina=\r\nl neurons. There is in fact much more to this theory, but I am in no way ab=\r\nle to communicate it effectively. I encourage you to read any of his papers=\r\n of books (or a quick Wikipedia scan). My interest was in HyperNEAT&#39;s possi=\r\nble abstraction of the idea. \n&gt; \n&gt; Something HyperNEAT has yet to incorpora=\r\nte is the idea of intra-life learning. I read a few of the other posts, and=\r\n I think this might be in some ways related to the HybrID conversation abou=\r\nt irregularities. HybrID attempts to make up for this lack of intra-life le=\r\narning through the use of NEAT. At some point in the algorithm, HyperNEAT i=\r\ns stopped in favor of using NEAT to more accurately pinpoint irregularities=\r\n. But this is not really the &quot;job&quot; of evolution, rather this is an intralif=\r\ne task. Evolution can provide the framework (i.e. a species), but the more =\r\nfit individual is able to adapt to the irregularities of life (i.e. through=\r\n intralife learning). \n&gt; \n&gt; The point I&#39;m laboriously trying to bring you t=\r\no is that I believe substrate evolution and intralife learning are related.=\r\n And perhaps you could take out two birds with one stone using some ideas f=\r\nrom neural Darwinism. My idea is a bit crude, and the details aren&#39;t ironed=\r\n out, but these were some thoughts. Speaking strictly about HyperNEAT, what=\r\n I thought would be helpful would be to generate more points then necessary=\r\n within the substrate. This would happen during mutation/crossover. Perhaps=\r\n duplicating inputs in more than one place on the substrate (within a certa=\r\nin distance from each other), and adding additional layers in the hidden no=\r\ndes (if they exist). Through the evaluation of the new population, essentia=\r\nlly neuron&#39;s that fire together wire together (as Edelman loves to say in h=\r\nis books) and weights can be modified slowly during the evaluation, additio=\r\nnally allowing for the removal of less fit neurons. What&#39;s left is an indiv=\r\nidual whose substrate isn&#39;t strictly identical to the original, and weight =\r\nconnections that might slightly differ from the original CPPN. \n&gt; \n&gt; Now pr=\r\noblems. There are a host of them, and this is what currently makes this ide=\r\na a bit clunky and inelegant. At a very basic level, I&#39;m still uncertain ho=\r\nw one could reconcile the difference between the resulting substrate and th=\r\ne original. Also the resulting CPPN and the original. Also scaling issues, =\r\nwhen trying to modify neuron connections, making changes to 1 connection we=\r\night at a time makes this impossible when examining a neural net of 9 milli=\r\non connections. This makes it difficult to convince anyone that this is the=\r\n direction that HyperNEAT should head in. Rather, it was my idea to ping so=\r\nme ideas off of you guys. And I do believe that the solutions to substrate =\r\nevolution and intralife learning are linked, whether or not this is the bes=\r\nt way to do it (most likely not). \n&gt; \n&gt; Let me know what you guys think,\n&gt; =\r\n-Paul\n&gt;\n\n\n\n"}}