{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":464818732,"authorName":"Jeff Clune","from":"Jeff Clune &lt;jclune@...&gt;","profile":"jeffreyclune","replyTo":"LIST","senderId":"NyuiyekzFcKXtNvYPjZ3lRMkE6qXOqzIJSH55YWJwfM0QnKpKAil5mByEf38a9BPR_mtOVJnWP9xoXHbzIrwO4TMqhY","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [neat] New paper on why modules evolve, and how to evolve modular artificial neural networks","postDate":"1360305961","msgId":5980,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDgwMkJBOEVGLUI3RjItNEIyNS05RDg3LTM5QTUyMjlEMDk3MEBjb3JuZWxsLmVkdT4=","inReplyToHeader":"PGtmMWNhdis1anJvQGVHcm91cHMuY29tPg==","referencesHeader":"PGtmMWNhdis1anJvQGVHcm91cHMuY29tPg=="},"prevInTopic":5979,"nextInTopic":5981,"prevInTime":5979,"nextInTime":5981,"topicId":5976,"numMessagesInTopic":30,"msgSnippet":"Hello Martin, That is an excellent question. My shortest answer is I don t know, but I expect so. The reason I expect modularity will still emerge is because","rawEmail":"Return-Path: &lt;jclune@...&gt;\r\nX-Sender: jclune@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 40977 invoked from network); 8 Feb 2013 06:46:10 -0000\r\nX-Received: from unknown (10.193.84.163)\n  by m3.grp.bf1.yahoo.com with QMQP; 8 Feb 2013 06:46:10 -0000\r\nX-Received: from unknown (HELO mail-ia0-f172.google.com) (209.85.210.172)\n  by mta3.grp.bf1.yahoo.com with SMTP; 8 Feb 2013 06:46:10 -0000\r\nX-Received: by mail-ia0-f172.google.com with SMTP id u8so3823150iag.17\n        for &lt;neat@yahoogroups.com&gt;; Thu, 07 Feb 2013 22:46:10 -0800 (PST)\r\nX-Received: by 10.50.193.200 with SMTP id hq8mr323492igc.101.1360305970032;\n        Thu, 07 Feb 2013 22:46:10 -0800 (PST)\r\nReturn-Path: &lt;jclune@...&gt;\r\nX-Received: from [10.0.1.3] (host-69-146-94-113.lar-wy.client.bresnan.net. [69.146.94.113])\n        by mx.google.com with ESMTPS id bg10sm13135093igc.6.2013.02.07.22.46.02\n        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);\n        Thu, 07 Feb 2013 22:46:07 -0800 (PST)\r\nContent-Type: multipart/alternative; boundary=&quot;Apple-Mail=_5A44ADC6-1778-4AF7-8B5C-DFEBFD937FCF&quot;\r\nMessage-Id: &lt;802BA8EF-B7F2-4B25-9D87-39A5229D0970@...&gt;\r\nMime-Version: 1.0 (Mac OS X Mail 6.2 &#92;(1499&#92;))\r\nDate: Thu, 7 Feb 2013 23:46:01 -0700\r\nReferences: &lt;kf1cav+5jro@...&gt;\r\nTo: neat@yahoogroups.com\r\nIn-Reply-To: &lt;kf1cav+5jro@...&gt;\r\nX-Mailer: Apple Mail (2.1499)\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nX-eGroups-From: Jeff Clune &lt;jeffclune@...&gt;\r\nFrom: Jeff Clune &lt;jclune@...&gt;\r\nSubject: Re: [neat] New paper on why modules evolve, and how to evolve modular artificial neural networks\r\nX-Yahoo-Group-Post: member; u=464818732; y=Q0nEw4Lvi4VhUGk61tZjojV4ahsvwOt1cSOylO83m_uToymGYS0Y\r\nX-Yahoo-Profile: jeffreyclune\r\n\r\n\r\n--Apple-Mail=_5A44ADC6-1778-4AF7-8B5C-DFEBFD937FCF\r\nContent-Transfer-Encoding: quoted-printable\r\nContent-Type: text/plain;\n\tcharset=windows-1252\r\n\r\nHello Martin,\n\nThat is an excellent question. My shortest answer is &quot;I don&#39;=\r\nt know, but I expect so.&quot;\n\nThe reason I expect modularity will still emerge=\r\n is because a connection cost encourages low-cost networks, and low-cost ne=\r\ntworks tend to be modular. As I just wrote to Alexandre, we show in our pap=\r\ner that even before any selection is applied, there is an inverse correlati=\r\non between the cost of a network (e.g the number of connections) and its mo=\r\ndularity (Fig. S12 here: http://goo.gl/8aHIc). As such, it should not matte=\r\nr what the particular task we are selecting for is: a connection cost will =\r\nstill encourage modularity. \n\nAdditionally, we tested whether the geometric=\r\n separation of the problem mattered by randomizing the inputs, and modulari=\r\nty still evolved. That test indicates that functional modularity can evolve=\r\n  even without geometric separation in the problem. Moreover, the treatment=\r\ns that used the number of connections (instead of their length) is agnostic=\r\n to the geometric locations of the neurons, and modularity evolved there to=\r\no. So we can rule out that geometric separation matters. \n\nHowever, I would=\r\n love to see experiments done on this front to demonstrate that modularity =\r\nstill forms in temporal, as opposed to parallel, problems. If you (or anyon=\r\ne) conducts such experiments, please let me know. \n\nA minor technical point=\r\n: I agree with you that some temporal problems might not have spatial paral=\r\nlelism, but the first task you sketched out &quot;(e.g. low-feature-analysis of =\r\nretinal input =3D&gt; detection of an object =3D&gt; associating the object with =\r\na learnt item)&quot; does seem to have spatial parallelism, and is in my opinion=\r\n very similar to the retina problem (although scaled up). In most deep lear=\r\nning networks, for example, you have separate, parallel edge detection, and=\r\n then separate, parallel joining of edges into corners, and then you separa=\r\ntely join corners into shapes, and finally shapes into the object=85then yo=\r\nu can learn something about the object=85all of that involves a lot of spat=\r\nial parallelism. This discussion raises the larger question of whether you =\r\ncould have truly temporal problems that did not require spatially distinct =\r\nprocessing streams: I&#39;m sure you could for very simple problems, but probab=\r\nly not for complex problems. As such, even if our technique did not encoura=\r\nge modularity for simple problems where no separate processing streams are =\r\nrequired, it probably is still important for most (all?) complex problems. =\r\n \n\nThanks for asking such an interesting question. I enjoyed thinking about=\r\n it. \n\n\nBest regards,\nJeff Clune\n\nAssistant Professor\nComputer Science\nUniv=\r\nersity of Wyoming\njclune@...\njeffclune.com\n\nOn Feb 7, 2013, at 4:11 PM=\r\n, martin_pyka &lt;martin.pyka@...&gt; wrote:\n\n&gt; Hello Jeff,\n&gt; \n&gt; first of a=\r\nll, congratulations to this very nice article! Here are my thoughts (obviou=\r\nsly influenced by the fact that I work in a group that tries to model the h=\r\nippocampus ;))\n&gt; \n&gt; My impression is that you showed the emergence of modul=\r\narity in the sense of parallel processing but I was wondering whether the o=\r\nbjective &quot;low connection costs&quot; would also yield to modular networks in seq=\r\nuential processes. Sequential processes could be either interpreted as a de=\r\ncomposition of the problem in sequential subproblems (e.g. low-feature-anal=\r\nysis of retinal input =3D&gt; detection of an object =3D&gt; associating the obje=\r\nct with a learnt item), or as solving a problem in the temporal domain (e.g=\r\n. the network receives two objects in a temporal order and should decide wh=\r\nether the second object corresponds to an expected object).\n&gt; \n&gt; The modula=\r\nrity observed in your paper seems to result from the fact that the problem =\r\ndomain contains already two groups of input neurons that require independen=\r\nt computations + a small number of computational steps to produce the solut=\r\nion. What if modularity does not correspond to parallel and independent pro=\r\ncessing and the number of layers is much higher? Will connection cost be st=\r\nill an important factor here?\n&gt; \n&gt; Best,\n&gt; Martin\n&gt; \n&gt; --- In neat@yahoogro=\r\nups.com, Jeff Clune wrote:\n&gt; &gt;\n&gt; &gt; Hello all,\n&gt; &gt; \n&gt; &gt; I&#39;m extremely please=\r\nd to announce a new paper on a subject that many--including myself--think i=\r\ns critical to making significant progress in our field: the evolution of mo=\r\ndularity. \n&gt; &gt; \n&gt; &gt; Jean-Baptiste Mouret, Hod Lipson and I have a new paper=\r\n that \n&gt; &gt; \n&gt; &gt; 1) sheds light on why modularity may evolve in biological n=\r\networks (e.g. neural, genetic, metabolic, protein-protein, etc.)\n&gt; &gt; \n&gt; &gt; 2=\r\n) provides a simple technique for evolving neural networks that are modular=\r\n and have increased evolvability, in that they adapt faster to new environm=\r\nents. The modules that formed solved subproblems in the domain. \n&gt; &gt; \n&gt; &gt; C=\r\nite: Clune J, Mouret J-B, Lipson H (2013) The evolutionary origins of modul=\r\narity. Proceedings of the Royal Society B. 280: 20122863. http://dx.doi.org=\r\n/10.1098/rspb.2012.2863 (pdf)\n&gt; &gt; \n&gt; &gt; Abstract: A central biological quest=\r\nion is how natural organisms are so evolvable (capable of quickly adapting =\r\nto new environments). A key driver of evolvability is the widespread modula=\r\nrity of biological networks=97their organization as functional, sparsely co=\r\nnnected subunits=97but there is no consensus regarding why modularity itsel=\r\nf evolved. Although most hypotheses assume indirect selection for evolvabil=\r\nity, here we demonstrate that the ubiquitous, direct selection pressure to =\r\nreduce the cost of connections between network nodes causes the emergence o=\r\nf modular networks. Computational evolution experiments with selection pres=\r\nsures to maximize network performance and minimize connection costs yield n=\r\networks that are significantly more modular and more evolvable than control=\r\n experiments that only select for performance. These results will catalyse =\r\nresearch in numerous disciplines, such as neuroscience and genetics, and en=\r\nhance our ability to harness evolution for engineering purposes.\n&gt; &gt; \n&gt; &gt; V=\r\nideo: http://www.youtube.com/watch?feature=3Dplayer_embedded&v=3DSG4_aW8LMn=\r\ng\n&gt; &gt; \n&gt; &gt; There has been some nice coverage of this work in the popular pr=\r\ness, in case you are interested:\n&gt; &gt; \n&gt; &gt; National Geographic: http://pheno=\r\nmena.nationalgeographic.com/2013/01/30/the-parts-of-life/\n&gt; &gt; MIT&#39;s Technol=\r\nogy Review: http://www.technologyreview.com/view/428504/computer-scientists=\r\n-reproduce-the-evolution-of-evolvability/ \n&gt; &gt; Fast Company: http://www.fas=\r\ntcompany.com/3005313/evolved-brains-robots-creep-closer-animal-learning\n&gt; &gt;=\r\n Cornell Chronicle: http://www.news.cornell.edu/stories/Jan13/modNetwork.ht=\r\nml\n&gt; &gt; ScienceDaily: http://www.sciencedaily.com/releases/2013/01/130130082=\r\n300.htm\n&gt; &gt; \n&gt; &gt; Please let me know what you think and if you have any ques=\r\ntions. I hope this work will help our field move forward!\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; =\r\n&gt; Best regards,\n&gt; &gt; Jeff Clune\n&gt; &gt; \n&gt; &gt; Assistant Professor\n&gt; &gt; Computer Sc=\r\nience\n&gt; &gt; University of Wyoming\n&gt; &gt; jclune@...\n&gt; &gt; jeffclune.com\n&gt; &gt;\n&gt; \n&gt; \n=\r\n\n\r\n--Apple-Mail=_5A44ADC6-1778-4AF7-8B5C-DFEBFD937FCF\r\nContent-Transfer-Encoding: quoted-printable\r\nContent-Type: text/html;\n\tcharset=windows-1252\r\n\r\n&lt;html&gt;&lt;head&gt;&lt;meta http-equiv=3D&quot;Content-Type&quot; content=3D&quot;text/html charset=\r\n=3Dwindows-1252&quot;&gt;&lt;/head&gt;&lt;body style=3D&quot;word-wrap: break-word; -webkit-nbsp-=\r\nmode: space; -webkit-line-break: after-white-space; &quot;&gt;Hello Martin,&lt;div&gt;&lt;br=\r\n&gt;&lt;/div&gt;&lt;div&gt;That is an excellent question. My shortest answer is &quot;I don&#39;t k=\r\nnow, but I expect so.&quot;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;The reason I expect modular=\r\nity will still emerge is because a connection cost encourages low-cost netw=\r\norks, and low-cost networks tend to be modular. As I just wrote to Alexandr=\r\ne, we show in our paper that even before any selection is applied, there is=\r\n an inverse correlation between the cost of a network (e.g the number of co=\r\nnnections) and its modularity&nbsp;(Fig. S12 here:&nbsp;&lt;a href=3D&quot;http://g=\r\noo.gl/8aHIc&quot;&gt;http://goo.gl/8aHIc&lt;/a&gt;). As such, it should not matter what t=\r\nhe particular task we are selecting for is: a connection cost will still en=\r\ncourage modularity.&nbsp;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Additionally, we tested =\r\nwhether the geometric separation of the problem mattered by randomizing the=\r\n inputs, and modularity still evolved. That test indicates that functional =\r\nmodularity can evolve &nbsp;even without geometric separation in the proble=\r\nm. Moreover, the treatments that used the number of connections (instead of=\r\n their length) is agnostic to the geometric locations of the neurons, and m=\r\nodularity evolved there too. So we can rule out that geometric separation m=\r\natters.&nbsp;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;However, I would love to see experim=\r\nents done on this front to demonstrate that modularity still forms in tempo=\r\nral, as opposed to parallel, problems. If you (or anyone) conducts such exp=\r\neriments, please let me know.&nbsp;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;A minor techni=\r\ncal point: I agree with you that some temporal problems might not have spat=\r\nial parallelism, but the first task you sketched out &quot;&lt;span style=3D&quot;backgr=\r\nound-color: rgb(255, 255, 255); &quot;&gt;(e.g. low-feature-analysis of retinal inp=\r\nut =3D&gt; detection of an object =3D&gt; associating the object with a lea=\r\nrnt item)&quot; does seem to have spatial parallelism, and is in my opinion very=\r\n similar to the retina problem (although scaled up). In most deep learning =\r\nnetworks, for example, you have separate,&nbsp;parallel&nbsp;edge detection=\r\n, and then separate, parallel joining of edges into corners, and then you s=\r\neparately join corners into shapes, and finally shapes into the object=85th=\r\nen you can learn something about the object=85all of that involves a lot of=\r\n spatial parallelism. This discussion raises the larger&nbsp;question&nbsp;=\r\nof whether you could have truly temporal problems that did not require spat=\r\nially distinct processing streams: I&#39;m sure you could for very simple probl=\r\nems, but probably not for complex problems. As such, even if our&nbsp;techn=\r\nique&nbsp;did not encourage modularity for simple problems where no separat=\r\ne processing streams are required, it probably is still important for most =\r\n(all?) complex problems. &nbsp;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=3D&quot;background-=\r\ncolor: rgb(255, 255, 255); &quot;&gt;&lt;br&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=3D&quot;backgroun=\r\nd-color: rgb(255, 255, 255); &quot;&gt;Thanks for asking such an interesting questi=\r\non. I enjoyed thinking about it.&nbsp;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;div apple-content-=\r\nedited=3D&quot;true&quot;&gt;\n&lt;div style=3D&quot;color: rgb(0, 0, 0); font-family: Times; fon=\r\nt-size: medium; font-style: normal; font-variant: normal; font-weight: norm=\r\nal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: -w=\r\nebkit-auto; text-indent: 0px; text-transform: none; white-space: normal; wi=\r\ndows: 2; word-spacing: 0px; -webkit-text-size-adjust: auto; -webkit-text-st=\r\nroke-width: 0px; word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-l=\r\nine-break: after-white-space; &quot;&gt;&lt;span class=3D&quot;Apple-style-span&quot; style=3D&quot;b=\r\norder-collapse: separate; border-spacing: 0px; &quot;&gt;&lt;div style=3D&quot;word-wrap: b=\r\nreak-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;=\r\n &quot;&gt;&lt;span class=3D&quot;Apple-style-span&quot; style=3D&quot;border-collapse: separate; col=\r\nor: rgb(0, 0, 0); font-family: Times; font-style: normal; font-variant: nor=\r\nmal; font-weight: normal; letter-spacing: normal; line-height: normal; orph=\r\nans: 2; text-align: -webkit-auto; text-indent: 0px; text-transform: none; w=\r\nhite-space: normal; widows: 2; word-spacing: 0px; border-spacing: 0px; -web=\r\nkit-text-decorations-in-effect: none; -webkit-text-size-adjust: auto; -webk=\r\nit-text-stroke-width: 0px; font-size: medium; &quot;&gt;&lt;div style=3D&quot;word-wrap: br=\r\neak-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; =\r\n&quot;&gt;&lt;span class=3D&quot;Apple-style-span&quot; style=3D&quot;border-collapse: separate; colo=\r\nr: rgb(0, 0, 0); font-variant: normal; letter-spacing: normal; line-height:=\r\n normal; orphans: 2; text-align: -webkit-auto; text-indent: 0px; text-trans=\r\nform: none; white-space: normal; widows: 2; word-spacing: 0px; border-spaci=\r\nng: 0px; -webkit-text-decorations-in-effect: none; -webkit-text-size-adjust=\r\n: auto; -webkit-text-stroke-width: 0px; &quot;&gt;&lt;div style=3D&quot;word-wrap: break-wo=\r\nrd; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; &quot;&gt;&lt;spa=\r\nn class=3D&quot;Apple-style-span&quot; style=3D&quot;border-collapse: separate; color: rgb=\r\n(0, 0, 0); font-variant: normal; letter-spacing: normal; line-height: norma=\r\nl; orphans: 2; text-align: -webkit-auto; text-indent: 0px; text-transform: =\r\nnone; white-space: normal; widows: 2; word-spacing: 0px; border-spacing: 0p=\r\nx; -webkit-text-decorations-in-effect: none; -webkit-text-size-adjust: auto=\r\n; -webkit-text-stroke-width: 0px; &quot;&gt;&lt;div style=3D&quot;word-wrap: break-word; -w=\r\nebkit-nbsp-mode: space; -webkit-line-break: after-white-space; &quot;&gt;&lt;br class=\r\n=3D&quot;Apple-interchange-newline&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div style=3D&quot;font-size: medium; f=\r\nont-weight: normal; font-style: normal; word-wrap: break-word; -webkit-nbsp=\r\n-mode: space; -webkit-line-break: after-white-space; &quot;&gt;Best regards,&lt;br&gt;&lt;fo=\r\nnt class=3D&quot;Apple-style-span&quot; color=3D&quot;#0a5d19&quot;&gt;&lt;b&gt;Jeff Clune&lt;/b&gt;&lt;/font&gt;&lt;br=\r\n&gt;&lt;br&gt;Assistant Professor&lt;br&gt;Computer Science&lt;/div&gt;&lt;div style=3D&quot;font-size: =\r\nmedium; font-weight: normal; font-style: normal; word-wrap: break-word; -we=\r\nbkit-nbsp-mode: space; -webkit-line-break: after-white-space; &quot;&gt;University =\r\nof Wyoming&lt;br&gt;&lt;a href=3D&quot;mailto:jclune@...&quot;&gt;jclune@...&lt;/a&gt;&lt;br&gt;jef=\r\nfclune.com&lt;/div&gt;&lt;/span&gt;&lt;/div&gt;&lt;/span&gt;&lt;/div&gt;&lt;/span&gt;&lt;/div&gt;&lt;/span&gt;&lt;/div&gt;\n&lt;/div&gt;=\r\n\n&lt;br&gt;&lt;div&gt;&lt;div&gt;On Feb 7, 2013, at 4:11 PM, martin_pyka &lt;&lt;a href=3D&quot;mailt=\r\no:martin.pyka@...&quot;&gt;martin.pyka@...&lt;/a&gt;&gt; wrote:&lt;/div&gt;&lt;br clas=\r\ns=3D&quot;Apple-interchange-newline&quot;&gt;&lt;blockquote type=3D&quot;cite&quot;&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;di=\r\nv style=3D&quot;background-color: rgb(255, 255, 255); position: static; z-index:=\r\n auto; &quot;&gt;\n&lt;span style=3D&quot;display:none&quot;&gt;&nbsp;&lt;/span&gt;\n\n\n\n    &lt;div id=3D&quot;ygrp=\r\n-text&quot;&gt;&lt;p&gt;Hello Jeff,&lt;br&gt;\n&lt;br&gt;\nfirst of all, congratulations to this very n=\r\nice article! Here are my thoughts (obviously influenced by the fact that I =\r\nwork in a group that tries to model the hippocampus ;))&lt;br&gt;\n&lt;br&gt;\nMy impress=\r\nion is that you showed the emergence of modularity in the sense of parallel=\r\n processing but I was wondering whether the objective &quot;low connection costs=\r\n&quot; would also yield to modular networks in sequential processes. Sequential =\r\nprocesses could be either interpreted as a decomposition of the problem in =\r\nsequential subproblems (e.g. low-feature-analysis of retinal input =3D&gt; =\r\ndetection of an object =3D&gt; associating the object with a learnt item), =\r\nor as solving a problem in the temporal domain (e.g. the network receives t=\r\nwo objects in a temporal order and should decide whether the second object =\r\ncorresponds to an expected object).&lt;br&gt;\n&lt;br&gt;\nThe modularity observed in you=\r\nr paper seems to result from the fact that the problem domain contains alre=\r\nady two groups of input neurons that require independent computations + a s=\r\nmall number of computational steps to produce the solution. What if modular=\r\nity does not correspond to parallel and independent processing and the numb=\r\ner of layers is much higher? Will connection cost be still an important fac=\r\ntor here?&lt;br&gt;\n&lt;br&gt;\nBest,&lt;br&gt;\nMartin&lt;br&gt;\n&lt;br&gt;\n--- In &lt;a href=3D&quot;mailto:neat%=\r\n40yahoogroups.com&quot;&gt;neat@yahoogroups.com&lt;/a&gt;, Jeff Clune  wrote:&lt;br&gt;\n&gt;&lt;br=\r\n&gt;\n&gt; Hello all,&lt;br&gt;\n&gt; &lt;br&gt;\n&gt; I&#39;m extremely pleased to announce a ne=\r\nw paper on a subject that many--including myself--think is critical to maki=\r\nng significant progress in our field: the evolution of modularity. &lt;br&gt;\n&gt=\r\n; &lt;br&gt;\n&gt; Jean-Baptiste Mouret, Hod Lipson and I have a new paper that &lt;b=\r\nr&gt;\n&gt; &lt;br&gt;\n&gt; 1) sheds light on why modularity may evolve in biological=\r\n networks (e.g. neural, genetic, metabolic, protein-protein, etc.)&lt;br&gt;\n&gt;=\r\n &lt;br&gt;\n&gt; 2) provides a simple technique for evolving neural networks that=\r\n are modular and have increased evolvability, in that they adapt faster to =\r\nnew environments. The modules that formed solved subproblems in the domain.=\r\n &lt;br&gt;\n&gt; &lt;br&gt;\n&gt; Cite: Clune J, Mouret J-B, Lipson H (2013) The evoluti=\r\nonary origins of modularity. Proceedings of the Royal Society B. 280: 20122=\r\n863. &lt;a href=3D&quot;http://dx.doi.org/10.1098/rspb.2012.2863&quot;&gt;http://dx.doi.org=\r\n/10.1098/rspb.2012.2863&lt;/a&gt; (pdf)&lt;br&gt;\n&gt; &lt;br&gt;\n&gt; Abstract: A central bi=\r\nological question is how natural organisms are so evolvable (capable of qui=\r\nckly adapting to new environments). A key driver of evolvability is the wid=\r\nespread modularity of biological networks=97their organization as functiona=\r\nl, sparsely connected subunits=97but there is no consensus regarding why mo=\r\ndularity itself evolved. Although most hypotheses assume indirect selection=\r\n for evolvability, here we demonstrate that the ubiquitous, direct selectio=\r\nn pressure to reduce the cost of connections between network nodes causes t=\r\nhe emergence of modular networks. Computational evolution experiments with =\r\nselection pressures to maximize network performance and minimize connection=\r\n costs yield networks that are significantly more modular and more evolvabl=\r\ne than control experiments that only select for performance. These results =\r\nwill catalyse research in numerous disciplines, such as neuroscience and ge=\r\nnetics, and enhance our ability to harness evolution for engineering purpos=\r\nes.&lt;br&gt;\n&gt; &lt;br&gt;\n&gt; Video: &lt;a href=3D&quot;http://www.youtube.com/watch?featu=\r\nre=3Dplayer_embedded&amp;v=3DSG4_aW8LMng&quot;&gt;http://www.youtube.com/watch?feat=\r\nure=3Dplayer_embedded&amp;v=3DSG4_aW8LMng&lt;/a&gt;&lt;br&gt;\n&gt; &lt;br&gt;\n&gt; There has =\r\nbeen some nice coverage of this work in the popular press, in case you are =\r\ninterested:&lt;br&gt;\n&gt; &lt;br&gt;\n&gt; National Geographic: &lt;a href=3D&quot;http://pheno=\r\nmena.nationalgeographic.com/2013/01/30/the-parts-of-life/&quot;&gt;http://phenomena=\r\n.nationalgeographic.com/2013/01/30/the-parts-of-life/&lt;/a&gt;&lt;br&gt;\n&gt; MIT&#39;s Te=\r\nchnology Review: &lt;a href=3D&quot;http://www.technologyreview.com/view/428504/com=\r\nputer-scientists-reproduce-the-evolution-of-evolvability/&quot;&gt;http://www.techn=\r\nologyreview.com/view/428504/computer-scientists-reproduce-the-evolution-of-=\r\nevolvability/&lt;/a&gt; &lt;br&gt;\n&gt; Fast Company: &lt;a href=3D&quot;http://www.fastcompany=\r\n.com/3005313/evolved-brains-robots-creep-closer-animal-learning&quot;&gt;http://www=\r\n.fastcompany.com/3005313/evolved-brains-robots-creep-closer-animal-learning=\r\n&lt;/a&gt;&lt;br&gt;\n&gt; Cornell Chronicle: &lt;a href=3D&quot;http://www.news.cornell.edu/sto=\r\nries/Jan13/modNetwork.html&quot;&gt;http://www.news.cornell.edu/stories/Jan13/modNe=\r\ntwork.html&lt;/a&gt;&lt;br&gt;\n&gt; ScienceDaily: &lt;a href=3D&quot;http://www.sciencedaily.co=\r\nm/releases/2013/01/130130082300.htm&quot;&gt;http://www.sciencedaily.com/releases/2=\r\n013/01/130130082300.htm&lt;/a&gt;&lt;br&gt;\n&gt; &lt;br&gt;\n&gt; Please let me know what you =\r\nthink and if you have any questions. I hope this work will help our field m=\r\nove forward!&lt;br&gt;\n&gt; &lt;br&gt;\n&gt; &lt;br&gt;\n&gt; &lt;br&gt;\n&gt; Best regards,&lt;br&gt;\n&gt; =\r\nJeff Clune&lt;br&gt;\n&gt; &lt;br&gt;\n&gt; Assistant Professor&lt;br&gt;\n&gt; Computer Science=\r\n&lt;br&gt;\n&gt; University of Wyoming&lt;br&gt;\n&gt; jclune@...&lt;br&gt;\n&gt; &lt;a href=3D&quot;htt=\r\np://jeffclune.com&quot;&gt;jeffclune.com&lt;/a&gt;&lt;br&gt;\n&gt;&lt;br&gt;\n&lt;br&gt;\n&lt;/p&gt;\n\n    &lt;/div&gt;\n   =\r\n  \n\n    \n\n&lt;/div&gt;\n\n\n\n&lt;!-- end group email --&gt;\n\n&lt;/blockquote&gt;&lt;/div&gt;&lt;br&gt;&lt;/div&gt;=\r\n&lt;/body&gt;&lt;/html&gt;\r\n--Apple-Mail=_5A44ADC6-1778-4AF7-8B5C-DFEBFD937FCF--\r\n\n"}}