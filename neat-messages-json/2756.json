{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"Tz-4OnwieVVQ6HIFi-Y2TXNcvrmhOKNmfIWnDbJQHMy3aRrK_e57s7ZJC9EXO2cdrKgaRHIXilHU5usjFiKW7TAQfeP2LjQk63oXt_UtnlqL","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Hebbian Learning","postDate":"1158825732","msgId":2756,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGVldGd1NCtudms2QGVHcm91cHMuY29tPg==","inReplyToHeader":"PDE5YjEwZDUxMDYwOTIwMTExN3QzNjQzOWZlZnI4OTBkN2E3Nzk1ZjM1ZTA1QG1haWwuZ21haWwuY29tPg=="},"prevInTopic":2752,"nextInTopic":2757,"prevInTime":2755,"nextInTime":2757,"topicId":2751,"numMessagesInTopic":5,"msgSnippet":"Hi Derek, nice discussion of Hebbian learning.  I wondered the same things myself when I started trying to implement local learning rules.  It does seem like","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 94494 invoked from network); 21 Sep 2006 08:02:37 -0000\r\nReceived: from unknown (66.218.67.34)\n  by m37.grp.scd.yahoo.com with QMQP; 21 Sep 2006 08:02:37 -0000\r\nReceived: from unknown (HELO n21c.bullet.sc5.yahoo.com) (66.163.187.212)\n  by mta8.grp.scd.yahoo.com with SMTP; 21 Sep 2006 08:02:37 -0000\r\nReceived: from [66.163.187.120] by n21.bullet.sc5.yahoo.com with NNFMP; 21 Sep 2006 08:02:14 -0000\r\nReceived: from [66.218.69.3] by t1.bullet.sc5.yahoo.com with NNFMP; 21 Sep 2006 08:02:14 -0000\r\nReceived: from [66.218.66.74] by t3.bullet.scd.yahoo.com with NNFMP; 21 Sep 2006 08:02:14 -0000\r\nDate: Thu, 21 Sep 2006 08:02:12 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;eetgu4+nvk6@...&gt;\r\nIn-Reply-To: &lt;19b10d510609201117t36439fefr890d7a7795f35e05@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: Re: Hebbian Learning\r\nX-Yahoo-Group-Post: member; u=54567749; y=XiI_2PstHUu3ue7Q6u59FW-kdPli_xSec2h8nR14gH_I9Na8liC-\r\nX-Yahoo-Profile: kenstanley01\r\n\r\nHi Derek, nice discussion of Hebbian learning.  I wondered the same \nthings=\r\n myself when I started trying to implement local learning \nrules.  It does =\r\nseem like the textbooks fail to address the critical \ndetails and leave you=\r\n scratching your head.  \n\nYour discussion makes a good case for using tanh =\r\n(or a sigmoid \nscaled between -1 and 1).  In fact, now I&#39;m wondering why I =\r\ndidn&#39;t \njust do that.  I think like a lot of aspects of neural networks and=\r\n \nEC, there aren&#39;t right answers here and to some extent it&#39;s more art \ntha=\r\nn science.  So if it makes good intuitive sense, it&#39;s worth \ntrying.  On th=\r\ne other hand it may turn out that these distinctions \ndon&#39;t lead to appreci=\r\nable differences in practice, even if one makes \nmore sense than another.\n\n=\r\nWhen I started working on local learning rules, I followed largely \non the =\r\nwork on Floreano, who had evolved Hebbian NNs in the past: \n\nFloreano, D., =\r\nand Urzelai, J. (2000). Evolutionary robots with online\nself-organization a=\r\nnd behavioral fitness. Neural Networks,\n13:431=964434.\n\nOur own paper on th=\r\nis subject includes my own attempts at Hebbian \nrules:\n\nhttp://nn.cs.utexas=\r\n.edu/pub-view.php?RECORD_KEY(Pubs)=3DPubID&PubID\n(Pubs)=3D131\n\nI made disti=\r\nnctions between inhibitory and excitatory connections (I \nhad a separate ru=\r\nle for each type), and also introduced a decay \nterm.  (The equations are i=\r\nn the paper)  However, your arguments \nabout how decay should work may in f=\r\nact be better motivated.  In \nfact, maybe someone should publish a paper co=\r\nmparing different \nrationales for decay.\n\nIt&#39;s worth noting also that there=\r\n are other local plasticity \ndynamics in nature aside from Hebbian:\n\n-Sensi=\r\ntization occurs when there is a neuron that becomes more \nsensitive (that i=\r\ns, ALL incoming weights becomes stronger) simply \nbecause it is getting a l=\r\not of activation coming from some neuron \nfeeding into it.  In other words,=\r\n activation levels do not need to \ncorrelate for the weights to increase.  =\r\nThis happens in Aplysia (sea \nsnails): When they get poked a lot they start=\r\n getting more jumpy in \ngeneral, even to stimulai in other areas (to simpli=\r\nfy a bit).\n\n-Habituation is kind of the opposite of sensitization, but not =\r\n\nexactly opposite.  It says that when there is a consistent low-grade \ninpu=\r\nt to a neuron, it becomes less sensitive, i.e. its incoming \nconnection wei=\r\nghts are decreased.  This concept applies e.g. when \nyou stop noticing the =\r\nsound of the air conditioner in the background.\n\nI am not aware of a lot of=\r\n work that includes these (or other) \nadditional kinds of local synaptic pl=\r\nasticity, but they are \ndefinitely big topics in neuroscience, which is whe=\r\nre I learned \nabout them.\n\nI believe the range of synaptic plasiticity dyna=\r\nmics is likely far \nwider than we appreciate in ANN research, or even perha=\r\nps in real \nneuroscience research.  To capture that range in an effective m=\r\nanner \nwould be an accomplishment.\n\nken\n\n\n\n--- In neat@yahoogroups.com, &quot;De=\r\nrek James&quot; &lt;djames@...&gt; wrote:\n&gt;\n&gt; Hello all,\n&gt; \n&gt; Philip and I are startin=\r\ng to work with unsupervised learning \nalgorithms in\n&gt; the context of NEAT, =\r\nand we had some questions about Hebbian \nlearning in\n&gt; particular that we t=\r\nhought we&#39;d throw out to the group to try to \ndiscuss.\n&gt; \n&gt; I&#39;ve read chapt=\r\ners from several books on the Hebb rule, and here&#39;s \na basic\n&gt; summary of m=\r\ny understanding of it.  In colloquial terms, Hebb&#39;s \nrule is &quot;if\n&gt; two neur=\r\nons fire together they wire together&quot;.  The simple Hebb \nrule stated\n&gt; math=\r\nematically is then:\n&gt; \n&gt; delta-w =3D n(a1)(a2)\n&gt; \n&gt; where\n&gt; delta-w =3D the=\r\n change in the connection weight betweens presynaptic \nneuron 1\n&gt; and posts=\r\nynaptic neuron 2\n&gt; n =3D the learning rate\n&gt; a1 =3D the activation of neuro=\r\nn 1\n&gt; a2 =3D the activation of neuron 2\n&gt; \n&gt; The first question is, isn&#39;t t=\r\nhe behavior of this rule dependent \non the\n&gt; activation function?  If you&#39;r=\r\ne using a sigmoid activation, the \noutput\n&gt; values are 0 &lt; x &lt; 1, right?  S=\r\no assuming you have a positive \nlearning rate\n&gt; that remains constant, the =\r\nweight can only every increase, correct?\n&gt; \n&gt; However, if you&#39;re using a ta=\r\nnh activation, -1 &lt; x &lt; 1, so now the \nlearning\n&gt; rule will behave differen=\r\ntly, since a positive and negative \nactivation for\n&gt; a1 and a2 will result =\r\nin a negative delta, so the weight will be\n&gt; decreased...something that cou=\r\nld never happen if both units were \nsigmoids.\n&gt; \n&gt; A second related questio=\r\nn is this: Theoretically the Hebb rule is \nmeant to\n&gt; strengthen the correl=\r\nation between highly-active neurons.  What \nconstitutes\n&gt; a highly-active n=\r\neuron?  A unit with sigmoid activation overloaded \nwith\n&gt; positive input wi=\r\nll produce an output near 1.  So it seems \nuncontroversial\n&gt; that two units=\r\n that are both activating near 1 are both strongly \nactive.\n&gt; \n&gt; But if the=\r\n net input into a sigmoid unit is 0, it still outputs a \nvalue of\n&gt; 0.5.  S=\r\no effectively, at rest, a sigmoid produces 0.5, and the \nHebb rule\n&gt; would =\r\nlead to an increase in weight, even if the presynaptic \nneuron was not\n&gt; &quot;f=\r\niring&quot;.\n&gt; \n&gt; What about when a sigmoid unit outputs a 0?  Is this logically=\r\n \nequivalent to\n&gt; an output of 1, only in the opposite direction?  Couldn&#39;t=\r\n you \ninterpret a\n&gt; sigmoid outputting 0 as &quot;firing strongly&quot;?  And yet, in=\r\n this case, \nthe\n&gt; change in w is going to be virtually non-existant.\n&gt; \n&gt; =\r\nHowever, with a tanh activation, 0 is the resting state, and 1 \nand -1 are =\r\nat\n&gt; the extremes of outputs.  When you apply the Hebb rule here, it \nseems=\r\n more\n&gt; intuitive.  There will be little change in w if the net input to \ne=\r\nither unit\n&gt; is 0, and a lot of change if the magnitude of the outputs of b=\r\noth \nunits are\n&gt; large.\n&gt; \n&gt; Also, a corollary to Hebb&#39;s rule is that neuro=\r\nns that fire out of \nphase have\n&gt; an inhibitory relationship increased.  Ag=\r\nain, that seems to work \nfor a tanh\n&gt; activation, where the signs of the ou=\r\ntputs will be different, but \nit seems\n&gt; like in a sigmoid activation, in p=\r\nhase firing will lead to an \nincrease in\n&gt; weight, while out of phase firin=\r\ng will just lead to a very small or\n&gt; nonexistant increase.\n&gt; \n&gt; And yet, i=\r\nn the books I&#39;m referring to for information on Hebbian \nlearning,\n&gt; I&#39;m no=\r\nt finding this discussion.  Can someone help us out here?\n&gt; \n&gt; Also, a prob=\r\nlem with the simple Hebb rule is that it leads to \nunbounded\n&gt; growth, whic=\r\nh isn&#39;t very plausible.  The most common change to the \nrule in\n&gt; order to =\r\nbound it seems to be adding a decay rate:\n&gt; \n&gt; delta-w =3D n(a1)(a2) - w(a2=\r\n)\n&gt; \n&gt; where\n&gt; delta-w =3D the change in the connection weight betweens pre=\r\nsynaptic \nneuron 1\n&gt; and postsynaptic neuron 2\n&gt; n =3D the learning rate\n&gt; =\r\na1 =3D the activation of neuron 1\n&gt; a2 =3D the activation of neuron 2\n&gt; w =\r\n=3D current connection weight\n&gt; \n&gt; This seems like a pretty good idea.  You=\r\n&#39;re bounding the growth as \na\n&gt; function of w, so the larger w gets, the le=\r\nss the weight \nincreases.  I&#39;m not\n&gt; sure what the motivation for multiplyi=\r\nng the second term by the \noutput of\n&gt; the postsynaptic neuron is, though.\n=\r\n&gt; \n&gt; It seemed more intuitive to me to bound w by making the delta \ninverse=\r\nly\n&gt; proportional the square of the weight, e.g.:\n&gt; \n&gt; delta-w =3D n(a1)(a2=\r\n) / ((w^2) + 1)\n&gt; \n&gt; That way, as the weights get larger, the change in w g=\r\nets \nsmaller.  Maybe\n&gt; the decay rate accomplishes the same thing.  It also=\r\n made sense to \nme to\n&gt; possibly bound the change as a function of time:\n&gt; =\r\n\n&gt; delta-w =3D n(a1)(a2) / ((t^2) + 1)\n&gt; \n&gt; Either of these last two rules =\r\nseem more psychologically \nplausible, since\n&gt; plasticity decreases as a fun=\r\nction of increased learning and/or as \na\n&gt; function of time.\n&gt; \n&gt; Anyway, t=\r\nhoughts are welcome. :)\n&gt; \n&gt; --Derek\n&gt;\n\n\n\n\n\n\n"}}