{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":434634266,"authorName":"Vassilis Vassiliades","from":"Vassilis Vassiliades &lt;vassilisvas@...&gt;","profile":"v.vassiliades","replyTo":"LIST","senderId":"vmL84kxXQwFLqucv3uzQ5PQQJtbks501PF3_hKwMgVqBs2OsE-Gd699Z-gUSUBxyxxD16Aj-l-_Vnmi4ZX6E9oaMywYo6KkIYlQL-gygMExZ3Ss","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [neat] Self-adaptive Mutation Rates, Novelty Search and CMA-ES","postDate":"1399212029","msgId":6304,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PENBTnRYaG11ejBFVEw2ZGF1T0dzWE95VEJySHVGMU50SnFqWDNOaE56dFYyLVp0NnNDUUBtYWlsLmdtYWlsLmNvbT4=","inReplyToHeader":"PGxrNDc4bisxODB2bmpzQFlhaG9vR3JvdXBzLmNvbT4=","referencesHeader":"PENBTnRYaG1zMXdqek9tN2hXYUNaRG5ZSFQzNE9nUEhuNVJKVXIzOEVkc19fcWlBV2trZ0BtYWlsLmdtYWlsLmNvbT4JPDUxMDBCOUQyLUFDQ0ItNDVCRi04RTc0LTJFMzY3NUE1NTQzN0BnbWFpbC5jb20+CTwwMDJmMDFjZjY2ZWQkMjI2NmFiYjAkNjczNDAzMTAkQHdhdHRzeXMuY29tPgk8Q0ErZHVpbVBHcnc1QTJqUHVoRnhEWmlPMEMrTTJpK0pTOEcwYyt3eUs9VmJvbXE9V2pBQG1haWwuZ21haWwuY29tPgk8bGs0NzhuKzE4MHZuanNAWWFob29Hcm91cHMuY29tPg=="},"prevInTopic":6303,"nextInTopic":6306,"prevInTime":6303,"nextInTime":6305,"topicId":6292,"numMessagesInTopic":19,"msgSnippet":"Hello all, Joel and Ken, I think I understand what you are saying about projecting intuitions about the objective world into novelty search and the dynamic ","rawEmail":"Return-Path: &lt;vassilisvas@...&gt;\r\nX-Sender: vassilisvas@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 90811 invoked by uid 102); 4 May 2014 14:00:29 -0000\r\nX-Received: from unknown (HELO mtaq3.grp.bf1.yahoo.com) (10.193.84.142)\n  by m17.grp.bf1.yahoo.com with SMTP; 4 May 2014 14:00:29 -0000\r\nX-Received: (qmail 16256 invoked from network); 4 May 2014 14:00:29 -0000\r\nX-Received: from unknown (HELO mail-pa0-f41.google.com) (209.85.220.41)\n  by mtaq3.grp.bf1.yahoo.com with SMTP; 4 May 2014 14:00:29 -0000\r\nX-Received: by mail-pa0-f41.google.com with SMTP id lj1so5293780pab.28\n        for &lt;neat@yahoogroups.com&gt;; Sun, 04 May 2014 07:00:29 -0700 (PDT)\r\nMIME-Version: 1.0\r\nX-Received: by 10.67.14.69 with SMTP id fe5mr60236845pad.120.1399212029377;\n Sun, 04 May 2014 07:00:29 -0700 (PDT)\r\nX-Received: by 10.70.50.103 with HTTP; Sun, 4 May 2014 07:00:29 -0700 (PDT)\r\nIn-Reply-To: &lt;lk478n+180vnjs@...&gt;\r\nReferences: &lt;CANtXhms1wjzOm7hWaCZDnYHT34OgPHn5RJUr38Eds__qiAWkkg@...&gt;\n\t&lt;5100B9D2-ACCB-45BF-8E74-2E3675A55437@...&gt;\n\t&lt;002f01cf66ed$2266abb0$67340310$@...&gt;\n\t&lt;CA+duimPGrw5A2jPuhFxDZiO0C+M2i+JS8G0c+wyK=Vbomq=WjA@...&gt;\n\t&lt;lk478n+180vnjs@...&gt;\r\nDate: Sun, 4 May 2014 17:00:29 +0300\r\nMessage-ID: &lt;CANtXhmuz0ETL6dauOGsXOyTBrHuF1NtJqjX3NhNztV2-Zt6sCQ@...&gt;\r\nTo: neat@yahoogroups.com\r\nContent-Type: multipart/alternative; boundary=047d7b15fc77fa015b04f8936f9b\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Vassilis Vassiliades &lt;vassilisvas@...&gt;\r\nSubject: Re: [neat] Self-adaptive Mutation Rates, Novelty Search and CMA-ES\r\nX-Yahoo-Group-Post: member; u=434634266; y=H3BAooBz0moFmdwfnwiS64S9RJZkceAEiN7gANcTbpMXj9vYoSf5DA\r\nX-Yahoo-Profile: v.vassiliades\r\n\r\n\r\n--047d7b15fc77fa015b04f8936f9b\r\nContent-Type: text/plain; charset=UTF-8\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\nHello all,\n\nJoel and Ken, I think I understand what you are saying about &quot;p=\r\nrojecting\nintuitions about the objective world into novelty search&quot; and the=\r\n dynamic\nand divergent nature of Novelty Search (NS)... and also why naivel=\r\ny\ncombining Covariance Matrix Adaptation - Evolution Strategies (CMA-ES)\nmi=\r\nght not work well with NS. A &quot;fitness&quot; peak in generation (g) might\ndiminis=\r\nh in generation (g+1), and generally the &quot;fitness&quot; peaks are\nconstantly mov=\r\ning.\n\nThis reminds me a bit of multiagent learning problems, where one coul=\r\nd say\nthat the &quot;target&quot; is constantly and adaptively being moved. However, =\r\nI am\nnot sure whether/how we could frame NS as a multiagent learning scenar=\r\nio. I\nam also imagining the landscape of NS as a... &quot;boiling soup&quot; where bu=\r\nbbles\n(fitness peaks) keep appearing and disappearing all the time. :) Thes=\r\ne\nbubbles could be guided by the archive, so that they would never appear a=\r\nt\nthe same place, or if we were to bound the archive (or use the\nprobabilis=\r\ntic approach), they could appear again after some time (or\nprobabilisticall=\r\ny).\n\nQuestions:\n\n1) Does anyone think that there is any relationship of NS =\r\nwith\nthermodynamics?\n\n2) Do you think there are any relationships between N=\r\nS and dynamic\noptimization?\n\n3) Joel, you often mention competitive coevolu=\r\ntion when talking about NS.\nYou also said that &quot;there exist similar fixed-p=\r\noint concepts for\ncompetitive co-evolution (like mediocre stable states and=\r\n disengagement)&quot;.\nTo be honest, I haven&#39;t read the literature on these conc=\r\nepts. I was\nthinking, however, that if we were to think about coevolution i=\r\nn game\ntheoretic terms, then competitive coevolution is like a zero-sum gam=\r\ne: the\ngains of one individual are balanced by the losses of the others. Is=\r\n this\ncorrect? Does NS behave in this manner? It seems to me that it is not=\r\n a\nzero-sum game.\n\n\n\nJeff, I agree with you that CMA-ES learns correlations=\r\n. However, allow me\nto express here my understanding as to how it works and=\r\n its relationship to\nself-adaptive mutation rates (SAMR). * I included this=\r\n part at the end of\nthe email because it got very big :) Anyone who read th=\r\nis please do correct\nme if something is wrong.\n\nJeff you wrote: &quot;my guess i=\r\ns that if you tried CMA-ES with and without a\nmutation rate on the genome, =\r\nyou=E2=80=99d get the same result I report in my\npaper&quot;. CMA-ES by construc=\r\ntion uses the mutation rates. It is an important\npart of the algorithm. I w=\r\nould be very curious to see how CMA-ES behaves in\nthe same setup used in yo=\r\nur paper.\n\nI noticed something in your paper on SAMR. I also noticed that i=\r\nn Joel&#39;s\nand Ken&#39;s paper on SAMR with NS (which I still haven&#39;t read very c=\r\narefully,\nonly skimmed though): in both papers the SAMR are updated differe=\r\nntly than\nthe SAMR of ES (see at the end of the email). Do you think that u=\r\nsing an ES\napproach for SAMR in your setups would lead to any different\nres=\r\nults/conclusions?\n\n&quot;&#39;novelty plateaus&#39;, a concept my students and I introdu=\r\nce in our last,\nnot-yet-announced GECCO paper&quot;\nSounds interesting! Congrats=\r\n by the way on all new papers!\n\n\n\nKen Lloyd, thanks for the link on Adaptiv=\r\ne Stochastic Resonance. I haven&#39;t\nread it yet, but I also noticed that ther=\r\ne are several works that have\nsimilarities with NS (e.g., intrinsic motivat=\r\nions, maximization of\npredictive information, causal entropic forces, empow=\r\nerment etc.). Does\nanyone think it would it be interesting to crowdsource a=\r\n list of similar\nworks on a different thread? I think Joel and Ken (Stanley=\r\n) probably know a\nlot of them already.\n\n\n\nOliver, you mentioned &quot;some work =\r\nhas probabilistically applied the\nobjective function&quot;. I would be intereste=\r\nd to see this work if you find the\nlink. If I remember correctly Jeff, JBM =\r\nand Hod Lipson in &quot;The evolutionary\norigins of modularity&quot; used something s=\r\nimilar, but on the secondary\nobjective (connection cost) objective.\n\n\n\nBest=\r\n,\nVassilis\n\n\n* Notes on CMA-ES:\n\nIn evolution strategies (ES) the mutation =\r\noperator is usually the primary\nsource of variation. Thus, we could say tha=\r\nt while selection exploits the\nfitness information in order to guide search=\r\n into promising regions,\nmutation (or variation) explores the search space =\r\nand should not use any\nfitness information to do that, i.e., it should be u=\r\nnbiased. This is a\ntheoretical consideration/requirement (called &quot;unbiasedn=\r\ness&quot;), which\nnaturally leads to the maximum entropy principle. In the case =\r\nof\nreal-valued search spaces this leads to gaussian distributions, while it=\r\n\nhas been also shown in the literature how to potentially handle integer an=\r\nd\ndiscrete parameters.\n\nLet&#39;s stay on real-valued spaces and let&#39;s say that=\r\n an individual &quot;a&quot;\ncomprises an object parameter vector &quot;x&quot;, and its fitnes=\r\ns function value\n&quot;F(x)&quot;: a =3D (x, F(x)). Mutations on x work like this:\n\nx=\r\n&#39; =3D x + z\n\nwith z being related to gaussian distributions.\n\nThe simplest =\r\ncase is z =3D sigma * ( N_1(0,1), N_2(0,1), ... , N_d(0,1) ),\nwhere sigma i=\r\ns the standard deviation of the normal distribution, d is the\ndimensionalit=\r\ny, and N_i(0,1) are independent random samples from the normal\ndistribution=\r\n.\n\nPeople have noticed that when sigma is constant and the very\nsimple (1+1=\r\n)-ES is used (meaning 1 parent creates 1 offspring and the\nstrongest of the=\r\nm survives), then in very simplified unimodal fitness\nfunctions (such as th=\r\ne sphere function), the ES initially displays a period\nof improvements; how=\r\never, after a while it becomes very very slow and loses\nits evolvability be=\r\ncause when approaching the minimum sigma is too big and\ntends to overshoot.=\r\n By analyzing how sigma influences the success\nprobability by which an offs=\r\npring replaces a parent, as well as the\nprogress rate, people have come up =\r\nwith something called the &quot;evolution\nwindow&quot; as well as the 1/5th control r=\r\nule that appropriately scales sigma\nperiodically.\n\nNow, the 1/5th rule is a=\r\n heuristic and very specific to (1+1)-ES and the\nfitness landscape, so peop=\r\nle needed something better. Hence the following\nidea:\n\nLet&#39;s add an endogen=\r\nous/evolvable strategy parameter vector &quot;s&quot; to the\nindividual, that contain=\r\ns any other parameters we want (such as the\nstandard deviation). So now an =\r\nindividual is: a =3D (x,s,F(x)). But how do we\nupdate sigma / the standard =\r\ndeviation? How do we mutate this mutation\nstrength/rate? The maximum entrop=\r\ny principle specifies that we should use\ngaussian distributions, but using =\r\nthese on the standard deviation could\nlead to negative values. A neat solut=\r\nion is to do it in log scale:\n\nln( sigma&#39; ) =3D ln( sigma ) + zeta\n\nwhich l=\r\neads to the multiplicative update:\n\nsigma&#39; =3D sigma * exp( zeta )\n\nwhere z=\r\neta =3D tau * N(0,1), and tau is an exogenous learning parameter which\ndete=\r\nrmines the rate and precision of self-adaptation. It is usually\nproportiona=\r\nl to 1/sqrt(d).\n\nUntil now we were talking about a single strategy paramete=\r\nr (mutation rate)\nfor the whole genotype, i.e., s =3D sigma, also known as =\r\nisotropic mutations.\nWe can extend this to the case where we have a vector =\r\nof strategy\nparameters (mutation rates), i.e., s =3D (sigma_1, sigma_2, ...=\r\n, sigma_d),\nalso known as non-isotropic mutations. This technique is more f=\r\nlexible\nespecially in high dimensional problems. However, it is still not v=\r\nery\neffective in some non-separable problems, e.g., consider a fitness\nland=\r\nscape that is not aligned with the coordinate system. How do we deal\nwith a=\r\nrbitrary rotations of the fitness landscape, which is the most\ngeneral situ=\r\nation?\n\nNow the idea of Covariance Matrix Adaptation is introduced: let&#39;s e=\r\nstimate\nthe shape of the fitness landscape and adapt a rotation matrix in o=\r\nrder to\nbe able to align our coordinate axes with the principal axes of the=\r\n fitness\nlandscape. This covariance matrix introduces correlations between =\r\nthe\ncomponents of z.\n\nSo, in:\n\n1) isotropic mutations: z =3D sigma * ( N_1(=\r\n0,1), N_2(0,1), ... , N_d(0,1) )\n=3D sigma * N(0, I), where I is the identi=\r\nty matrix\n\n2) non-isotropic mutations: z =3D ( sigma_1 * N_1(0,1), sigma_2 =\r\n* N_2(0,1),\n..., sigma_d * N_d(0,1) ) =3D D * N(0, I), where D is a diagona=\r\nl matrix\ncontaining all sigma values\n\n3) correlated mutations: z =3D M * ( =\r\nsigma_1 * N_1(0,1), sigma_2 * N_2(0,1),\n..., sigma_d * N_d(0,1) ) =3D M * D=\r\n * N(0, I), where M is the rotation matrix\nthat introduces correlations bet=\r\nween the components of z, and C =3D M^T * M\nis the covariance matrix (M^T i=\r\ns the transpose of M).\n\n\nI won&#39;t get into more detail (e.g., how to estimat=\r\ne the covariance matrix\netc.), but note that CMA-ES has been shown to work =\r\nvery well in various\nbenchmarks and with small population sizes.\n\r\n--047d7b15fc77fa015b04f8936f9b\r\nContent-Type: text/html; charset=UTF-8\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\n&lt;div dir=3D&quot;ltr&quot;&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;Hello all,&lt;/div&gt;&lt;div class=3D&quot;g=\r\nmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;Joel and Ken, I think I un=\r\nderstand what you are saying about &quot;projecting intuitions about the ob=\r\njective world into novelty search&quot; and the dynamic and divergent natur=\r\ne of Novelty Search (NS)... and also why naively combining Covariance Matri=\r\nx Adaptation - Evolution Strategies (CMA-ES) might not work well with NS. A=\r\n &quot;fitness&quot; peak in generation (g) might diminish in generation (g=\r\n+1), and generally the &quot;fitness&quot; peaks are constantly moving.&lt;/di=\r\nv&gt;\n&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;This rem=\r\ninds me a bit of multiagent learning problems, where one could say that the=\r\n &quot;target&quot; is constantly and adaptively being moved. However, I am=\r\n not sure whether/how we could frame NS as a multiagent learning scenario. =\r\nI am also imagining the landscape of NS as a... &quot;boiling soup&quot; wh=\r\nere bubbles (fitness peaks) keep appearing and disappearing all the time. :=\r\n) These bubbles could be guided by the archive, so that they would never ap=\r\npear at the same place, or if we were to bound the archive (or use the prob=\r\nabilistic approach), they could appear again after some time (or probabilis=\r\ntically).=C2=A0&lt;/div&gt;\n&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gm=\r\nail_extra&quot;&gt;Questions:&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=\r\n=3D&quot;gmail_extra&quot;&gt;1) Does anyone think that there is any relationship of NS =\r\nwith thermodynamics?&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;\n&lt;br&gt;&lt;/div&gt;&lt;div class=\r\n=3D&quot;gmail_extra&quot;&gt;2) Do you think there are any relationships between NS and=\r\n dynamic optimization?&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=\r\n=3D&quot;gmail_extra&quot;&gt;3) Joel, you often mention competitive coevolution when ta=\r\nlking about NS. You also said that &quot;&lt;span style=3D&quot;font-family:arial,s=\r\nans-serif;font-size:13px&quot;&gt;there exist similar fixed-point concepts for comp=\r\netitive co-evolution (like mediocre stable states and disengagement)&quot;.=\r\n To be honest, I haven&#39;t read the literature on these concepts. I was t=\r\nhinking, however, that if we were to think about coevolution in game theore=\r\ntic terms, then=C2=A0&lt;/span&gt;&lt;span style=3D&quot;font-family:arial,sans-serif;fon=\r\nt-size:13px&quot;&gt;competitive coevolution is like a zero-sum game: the gains of =\r\none individual are balanced by the losses of the others. Is this correct? D=\r\noes NS behave in this manner? It seems to me that it is not a zero-sum game=\r\n.&lt;/span&gt;&lt;/div&gt;\n&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_ext=\r\nra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_extr=\r\na&quot;&gt;Jeff, I agree with you that CMA-ES learns correlations. However, allow m=\r\ne to express here my understanding as to how it works and its relationship =\r\nto self-adaptive mutation rates (SAMR). * I included this part at the end o=\r\nf the email because it got very big :) Anyone who read this please do corre=\r\nct me if something is wrong.&lt;/div&gt;\n&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;di=\r\nv class=3D&quot;gmail_extra&quot;&gt;Jeff you wrote: &quot;my guess is that if you tried=\r\n CMA-ES with and without a mutation rate on the genome, you=E2=80=99d get t=\r\nhe same result I report in my paper&quot;. CMA-ES by construction uses the =\r\nmutation rates. It is an important part of the algorithm. I would be very c=\r\nurious to see how CMA-ES behaves in the same setup used in your paper.&lt;/div=\r\n&gt;\n&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;I noticed=\r\n something in your paper on SAMR. I also noticed that in Joel&#39;s and Ken=\r\n&#39;s paper on SAMR with NS (which I still haven&#39;t read very carefully=\r\n, only skimmed though): in both papers the SAMR are updated differently tha=\r\nn the SAMR of ES (see at the end of the email). Do you think that using an =\r\nES approach for SAMR in your setups would lead to any different results/con=\r\nclusions?&lt;br&gt;\n&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmai=\r\nl_extra&quot;&gt;&quot;&#39;novelty plateaus&#39;, a concept my students and I intr=\r\noduce in our last, not-yet-announced GECCO paper&quot;&lt;/div&gt;&lt;div class=3D&quot;g=\r\nmail_extra&quot;&gt;Sounds interesting! Congrats by the way on all new papers!&lt;/div=\r\n&gt;\n&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div=\r\n&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;Ken Lloyd,=\r\n thanks for the link on Adaptive Stochastic Resonance. I haven&#39;t read i=\r\nt yet, but I also noticed that there are several works that have similariti=\r\nes with NS (e.g., intrinsic motivations, maximization of predictive informa=\r\ntion, causal entropic forces, empowerment etc.). Does anyone think it would=\r\n it be interesting to crowdsource a list of similar works on a different th=\r\nread? I think Joel and Ken (Stanley) probably know a lot of them already.&lt;/=\r\ndiv&gt;\n&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/=\r\ndiv&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;Oliver,=\r\n you mentioned &quot;&lt;span style=3D&quot;font-family:arial,sans-serif;font-size:=\r\n13px&quot;&gt;some work has probabilistically applied the objective function&quot;.=\r\n I would be interested to see this work if you find the link. If I remember=\r\n correctly Jeff, JBM and Hod Lipson in &quot;The evolutionary origins of mo=\r\ndularity&quot; used something similar, but on the secondary objective (conn=\r\nection cost) objective.&lt;/span&gt;&lt;/div&gt;\n&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;=\r\ndiv class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;d=\r\niv class=3D&quot;gmail_extra&quot;&gt;Best,&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;Vassilis&lt;/di=\r\nv&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;\n&lt;br&gt;&lt;/di=\r\nv&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;* Notes on CMA-ES:&lt;/div&gt;&lt;div class=3D&quot;gmail_ex=\r\ntra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;In evo=\r\nlution strategies (ES) the mutation operator is usually the primary source =\r\nof variation. Thus, we could say that while selection exploits the fitness =\r\ninformation in order to guide search into promising regions, mutation (or v=\r\nariation) explores the search space and should not use any fitness informat=\r\nion to do that, i.e., it should be unbiased. This is a theoretical consider=\r\nation/requirement (called &quot;unbiasedness&quot;), which naturally leads =\r\nto the maximum entropy principle. In the case of real-valued search spaces =\r\nthis leads to gaussian distributions, while it has been also shown in the l=\r\niterature how to potentially handle integer and discrete parameters.&lt;/div&gt;\n=\r\n&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;Let&#39;s s=\r\ntay on real-valued spaces and let&#39;s say that an individual &quot;a&quot=\r\n; comprises an object parameter vector &quot;x&quot;, and its fitness funct=\r\nion value &quot;F(x)&quot;: a =3D (x, F(x)). Mutations on x work like this:=\r\n&lt;/div&gt;\n&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;x&#3=\r\n9; =3D x + z&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_=\r\nextra&quot;&gt;with z being related to gaussian distributions.=C2=A0&lt;/div&gt;&lt;div clas=\r\ns=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;\n&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;The simplest case i=\r\ns z =3D sigma * ( N_1(0,1), N_2(0,1), ... , N_d(0,1) ), where sigma is the =\r\nstandard deviation of the normal distribution, d is the dimensionality, and=\r\n N_i(0,1) are independent random samples from the normal distribution.&lt;/div=\r\n&gt;\n&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;People ha=\r\nve noticed that when sigma is constant and the very simple=C2=A0(1+1)-ES=C2=\r\n=A0is used (meaning 1 parent creates 1 offspring and the strongest of them =\r\nsurvives), then in very simplified unimodal fitness functions (such as the =\r\nsphere function), the=C2=A0ES initially displays a period of improvements; =\r\nhowever, after a while it becomes very very slow and loses its evolvability=\r\n because when approaching the minimum sigma is too big and tends to oversho=\r\not. By analyzing how sigma influences the success probability by which an o=\r\nffspring replaces a parent, as well as the progress rate, people have come =\r\nup with something called the &quot;evolution window&quot; as well as the 1/=\r\n5th control rule that appropriately scales sigma periodically.&lt;/div&gt;\n&lt;div c=\r\nlass=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;Now, the 1/5th ru=\r\nle is a heuristic and very specific to (1+1)-ES and the fitness landscape, =\r\nso people needed something better. Hence the following idea:=C2=A0&lt;/div&gt;&lt;di=\r\nv class=3D&quot;gmail_extra&quot;&gt;\n&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;Let&#39;s add=\r\n an endogenous/evolvable strategy parameter vector &quot;s&quot; to the ind=\r\nividual, that contains any other parameters we want (such as the standard d=\r\neviation). So now an individual is: a =3D (x,s,F(x)). But how do we update =\r\nsigma / the standard deviation? How do we mutate this mutation strength/rat=\r\ne? The maximum entropy principle specifies that we should use gaussian dist=\r\nributions, but using these on the standard deviation could lead to negative=\r\n values. A neat solution is to do it in log scale:&lt;/div&gt;\n&lt;div class=3D&quot;gmai=\r\nl_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;ln( sigma&#39; ) =3D ln( sigm=\r\na ) + zeta&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_ex=\r\ntra&quot;&gt;which leads to the multiplicative update:&lt;/div&gt;&lt;div class=3D&quot;gmail_ext=\r\nra&quot;&gt;\n&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;sigma&#39; =3D sigma * exp( zeta =\r\n)&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;wher=\r\ne zeta =3D tau * N(0,1), and tau is an exogenous learning parameter which d=\r\netermines the rate and precision of self-adaptation. It is usually proporti=\r\nonal to 1/sqrt(d).&lt;/div&gt;\n&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D=\r\n&quot;gmail_extra&quot;&gt;Until now we were talking about a single strategy parameter (=\r\nmutation rate) for the whole genotype, i.e., s =3D sigma, also known as iso=\r\ntropic mutations. We can extend this to the case where we have a vector of =\r\nstrategy parameters (mutation rates), i.e., s =3D (sigma_1, sigma_2, ..., s=\r\nigma_d), also known as non-isotropic mutations. This technique is more flex=\r\nible especially in high dimensional problems. However, it is still not very=\r\n effective in some non-separable problems, e.g., consider a fitness landsca=\r\npe that is not aligned with the coordinate system. How do we deal with arbi=\r\ntrary rotations of the fitness landscape, which is the most general situati=\r\non?&lt;/div&gt;\n&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;N=\r\now the idea of Covariance Matrix Adaptation is introduced: let&#39;s estima=\r\nte the shape of the fitness landscape and adapt a rotation matrix in order =\r\nto be able to align our coordinate axes with the principal axes of the fitn=\r\ness landscape. This covariance matrix introduces correlations between the c=\r\nomponents of z.&lt;/div&gt;\n&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gm=\r\nail_extra&quot;&gt;So, in:&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;=\r\ngmail_extra&quot;&gt;1) isotropic mutations: z =3D sigma * ( N_1(0,1), N_2(0,1), ..=\r\n. , N_d(0,1) ) =3D sigma * N(0, I), where I is the identity matrix&lt;/div&gt;\n&lt;d=\r\niv class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;2) non-isotro=\r\npic mutations: z =3D ( sigma_1 * N_1(0,1), sigma_2 * N_2(0,1), ..., sigma_d=\r\n * N_d(0,1) ) =3D D * N(0, I), where D is a diagonal matrix containing all =\r\nsigma values&lt;/div&gt;\n&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail=\r\n_extra&quot;&gt;3) correlated mutations: z =3D M * ( sigma_1 * N_1(0,1), sigma_2 * =\r\nN_2(0,1), ..., sigma_d * N_d(0,1) ) =3D M * D * N(0, I), where M is the rot=\r\nation matrix that introduces correlations between the components of z, and =\r\nC =3D M^T * M is the covariance matrix (M^T is the transpose of M).&lt;/div&gt;\n&lt;=\r\ndiv class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;d=\r\niv class=3D&quot;gmail_extra&quot;&gt;I won&#39;t get into more detail (e.g., how to est=\r\nimate the covariance matrix etc.), but note that CMA-ES has been shown to w=\r\nork very well in various benchmarks and with small population sizes.&lt;/div&gt;\n=\r\n&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;/div&gt;\n\r\n--047d7b15fc77fa015b04f8936f9b--\r\n\n"}}