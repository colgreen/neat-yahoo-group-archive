{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":283334584,"authorName":"petar_chervenski","from":"&quot;petar_chervenski&quot; &lt;petar_chervenski@...&gt;","profile":"petar_chervenski","replyTo":"LIST","senderId":"-NWmIo72PN9uoyQq58I7nRfT5ZcQxHa57tTQ3uMLDpfpsj6mi4U7SaGNkFxRRY62ajd6CBiksDMNSWWbTGnMIWBEInBNxyVZUu2EHQI6ytIDHLVtZ3g","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Mix of Identity and Sigmoid activation functions","postDate":"1176304760","msgId":3117,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGV2aXU5cCs0YzQ4QGVHcm91cHMuY29tPg==","inReplyToHeader":"PGQyNzhlM2FkMDcwNDExMDcxOXU1YWE2MjBkZWpkNDAwZDZkMWRjMDA1MmNkQG1haWwuZ21haWwuY29tPg=="},"prevInTopic":3116,"nextInTopic":3118,"prevInTime":3116,"nextInTime":3118,"topicId":3114,"numMessagesInTopic":8,"msgSnippet":"This is the Identity function? Heh, I call it linear.. So I have this already :) You re right that a simple mutation will have a significant impact, but what","rawEmail":"Return-Path: &lt;petar_chervenski@...&gt;\r\nX-Sender: petar_chervenski@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 99023 invoked from network); 11 Apr 2007 15:19:57 -0000\r\nReceived: from unknown (66.218.66.167)\n  by m25.grp.scd.yahoo.com with QMQP; 11 Apr 2007 15:19:57 -0000\r\nReceived: from unknown (HELO n32b.bullet.sp1.yahoo.com) (209.131.38.212)\n  by mta6.grp.scd.yahoo.com with SMTP; 11 Apr 2007 15:19:57 -0000\r\nReceived: from [216.252.122.219] by n32.bullet.sp1.yahoo.com with NNFMP; 11 Apr 2007 15:19:23 -0000\r\nReceived: from [66.218.69.1] by t4.bullet.sp1.yahoo.com with NNFMP; 11 Apr 2007 15:19:23 -0000\r\nReceived: from [66.218.66.80] by t1.bullet.scd.yahoo.com with NNFMP; 11 Apr 2007 15:19:23 -0000\r\nDate: Wed, 11 Apr 2007 15:19:20 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;eviu9p+4c48@...&gt;\r\nIn-Reply-To: &lt;d278e3ad0704110719u5aa620dejd400d6d1dc0052cd@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0\r\nFrom: &quot;petar_chervenski&quot; &lt;petar_chervenski@...&gt;\r\nSubject: Re: Mix of Identity and Sigmoid activation functions\r\nX-Yahoo-Group-Post: member; u=283334584; y=0jHmhNPO-TN0gPAvgMtldGBVCR2BTHEHNGGfPw0UEv7itT0bQ2aPBQcr3A\r\nX-Yahoo-Profile: petar_chervenski\r\n\r\nThis is the Identity function? Heh, I call it linear.. So I have this \nalre=\r\nady :) \nYou&#39;re right that a simple mutation will have a significant impact,=\r\n \nbut what if we adjust the importance factors for speciation? I mean \nthat=\r\n if activation function types are so important to performace, we \ncan incre=\r\nase that multiplier and every time such a mutation occurs, a \nnew species w=\r\nill be created that will further optimize its weights \nand connections, not=\r\n changing the activation functions.. \nI think the best thing to do is to su=\r\npply the system with everything, \nMIN,MAX,ADD and MUL summing functions, an=\r\nd let evolution decide what \nis best. There may be some heuristics, of cour=\r\nse.\n\n--- In neat@yahoogroups.com, &quot;Rafael C.P.&quot; &lt;kurama.youko.br@...&gt; \nwrot=\r\ne:\n&gt;\n&gt; Identity function is f(x) =3D x, simple =3D)\n&gt; I&#39;ve thought about mu=\r\nlti-activation-functions populations as you 2 \nproposed.\n&gt; My contribution =\r\nhere is to add the sine function too, allowing \nrecognition\n&gt; of periodic p=\r\natterns.\n&gt; The problem I see with that approach is that a simple mutation\n&gt;=\r\n including/modifying an activation function would cause a big change \n(with=\r\n\n&gt; undefined direction) to the outputs. There must be a way to cluster \nsim=\r\nilar\n&gt; functions in order to make always the smallest change possible and \n=\r\nto the\n&gt; right direction.\n&gt; \n&gt; PS: you could use the MIN function instead o=\r\nf AND (and MAX for OR), \nturning\n&gt; it into fuzzy logic.\n&gt; \n&gt; On 4/11/07, pe=\r\ntar_chervenski &lt;petar_chervenski@...&gt; wrote:\n&gt; &gt;\n&gt; &gt;   Hi Shane,\n&gt; &gt;\n&gt; &gt; I =\r\ndon&#39;t know what the identity function is exactly.. I need to \nlearn\n&gt; &gt; tha=\r\nt.\n&gt; &gt; But I suppose you can also try to split the neuron activation into\n&gt;=\r\n &gt; stages, introduce another function that sums the weighted inputs \nin\n&gt; &gt;=\r\n different ways, you know that this is usually addition, but\n&gt; &gt; multiplica=\r\ntion is also promising, since it is like the AND logical\n&gt; &gt; operation on t=\r\nhose inputs. When this is combined with linear and\n&gt; &gt; other activation fun=\r\nctions, it turns NEAT into actually a ..\n&gt; &gt; something like a math equation=\r\n evolving system.. I don&#39;t think \nthis\n&gt; &gt; will enhance performace in most =\r\ntasks, but interesting results \nmay be\n&gt; &gt; obtained.\n&gt; &gt; There may be also =\r\na MAX summing function, that returns the maximum\n&gt; &gt; input of all. This way=\r\n the neurons can have winner-take-all\n&gt; &gt; behaviour, I guess. I have implem=\r\nented this and I see it gives\n&gt; &gt; results.\n&gt; &gt;\n&gt; &gt; Peter\n&gt; &gt;\n&gt; &gt; --- In nea=\r\nt@yahoogroups.com &lt;neat%\n40yahoogroups.com&gt;, &quot;shanemcdonaldryan&quot;\n&gt; &gt;\n&gt; &gt; &lt;s=\r\nhanemcdonaldryan@&gt; wrote:\n&gt; &gt; &gt;\n&gt; &gt; &gt; I am sure this must have been done bu=\r\nt I can&#39;t find any papers \non\n&gt; &gt; it.\n&gt; &gt; &gt;\n&gt; &gt; &gt; Has anyone tried starting=\r\n out with an initial population that\n&gt; &gt; connects\n&gt; &gt; &gt; all the inputs to t=\r\nhe outputs with a parallel set of neurons \nusing\n&gt; &gt; &gt; identity as the acti=\r\nvation function? Then add a new type of \nmutation\n&gt; &gt; &gt; that can add new ne=\r\nurons with the identity activation function.\n&gt; &gt; &gt;\n&gt; &gt; &gt; Basically what I a=\r\nm proposing is starting with the same initial \nNEAT\n&gt; &gt; &gt; configuration of =\r\nall the inputs connected to the outputs via\n&gt; &gt; sigmoids.\n&gt; &gt; &gt; But doublin=\r\ng up this configuration with an identical net \nconnected\n&gt; &gt; &gt; via the iden=\r\ntity activation function.\n&gt; &gt; &gt;\n&gt; &gt; &gt; The whole point of this would be to c=\r\nreate a hybrid network \nthat is\n&gt; &gt; &gt; good at capturing the linear aspects =\r\n(A+B, cross product) of the\n&gt; &gt; data\n&gt; &gt; &gt; and composing it with the non-li=\r\nnear aspects.\n&gt; &gt; &gt;\n&gt; &gt; &gt; But now that I think about it. I guess a good sta=\r\nrting question\n&gt; &gt; would\n&gt; &gt; &gt; be is a Neural Net using only the Identity a=\r\nctivation function \ngood\n&gt; &gt; &gt; at approximating linear functions?\n&gt; &gt; &gt;\n&gt; &gt;=\r\n &gt; Something like this might be good at approximating time series \nwith\n&gt; &gt;=\r\n a\n&gt; &gt; &gt; simple linear trend. So you don&#39;t have to detrend the data.\n&gt; &gt; &gt;\n=\r\n&gt; &gt; &gt; Thanks,\n&gt; &gt; &gt;\n&gt; &gt; &gt; Shane\n&gt; &gt; &gt;\n&gt; &gt;\n&gt; &gt;  \n&gt; &gt;\n&gt; \n&gt; \n&gt; \n&gt; -- \n&gt; =3D=3D=\r\n=3D=3D=3D=3D=3D=3D=3D\n&gt; Rafael C.P.\n&gt; =3D=3D=3D=3D=3D=3D=3D=3D=3D\n&gt;\n\n\n\n"}}