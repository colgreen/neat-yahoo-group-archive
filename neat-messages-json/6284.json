{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":434634266,"authorName":"Vassilis Vassiliades","from":"Vassilis Vassiliades &lt;vassilisvas@...&gt;","profile":"v.vassiliades","replyTo":"LIST","senderId":"9Mefziw2Y5iJeWGTViNuDx_UcVFjwbDLRvbgD8g6XwEnoqiVq5B14WmUSTwCPkLz2ea_01OnsLF2bsgJZrFb382c7QZrqoJchZJloJPDIA0rjtA","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [neat] New paper: Automated Generation of Environments to Test the General Learning Capabilities of AI Agents","postDate":"1398857403","msgId":6284,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PENBTnRYaG10eG9oTzRSZmhVYzBCUzRhMmZXMTlKY2pEYmUtOHEwdzlNSDFXalhjbzh6UUBtYWlsLmdtYWlsLmNvbT4=","inReplyToHeader":"PENBK2R1aW1ONCtZVTMtelN4ZnV1ek9OLVAtcnI4NVBTcHMrMDFDMW5Wa2twSkxjUmR0d0BtYWlsLmdtYWlsLmNvbT4=","referencesHeader":"PENBK2R1aW1PMjRzYWtPWFNNVnVxYkVleDgremlCbVFIdmVjb1kza3dBZCt6QUI1Wmt3UUBtYWlsLmdtYWlsLmNvbT4JPENBTnRYaG12dUpHMkxkWXpSRGVXRldpU01HNW1iK3pmQkxWZ0VhQm10dHkyV0ZQaXhFd0BtYWlsLmdtYWlsLmNvbT4JPENBK2R1aW1ONCtZVTMtelN4ZnV1ek9OLVAtcnI4NVBTcHMrMDFDMW5Wa2twSkxjUmR0d0BtYWlsLmdtYWlsLmNvbT4="},"prevInTopic":6282,"nextInTopic":6285,"prevInTime":6283,"nextInTime":6285,"topicId":6279,"numMessagesInTopic":11,"msgSnippet":"Hello Oliver, Thanks for your quick reply. Perhaps that sentence should have read (additions in bold): the genome *directly ... Yes, it is clear now. I guess","rawEmail":"Return-Path: &lt;vassilisvas@...&gt;\r\nX-Sender: vassilisvas@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 21674 invoked by uid 102); 30 Apr 2014 11:30:06 -0000\r\nX-Received: from unknown (HELO mtaq5.grp.bf1.yahoo.com) (10.193.84.36)\n  by m6.grp.bf1.yahoo.com with SMTP; 30 Apr 2014 11:30:06 -0000\r\nX-Received: (qmail 22591 invoked from network); 30 Apr 2014 11:30:06 -0000\r\nX-Received: from unknown (HELO mail-pd0-f179.google.com) (209.85.192.179)\n  by mtaq5.grp.bf1.yahoo.com with SMTP; 30 Apr 2014 11:30:06 -0000\r\nX-Received: by mail-pd0-f179.google.com with SMTP id y10so1555253pdj.24\n        for &lt;neat@yahoogroups.com&gt;; Wed, 30 Apr 2014 04:30:05 -0700 (PDT)\r\nMIME-Version: 1.0\r\nX-Received: by 10.66.119.136 with SMTP id ku8mr7497500pab.121.1398857403322;\n Wed, 30 Apr 2014 04:30:03 -0700 (PDT)\r\nX-Received: by 10.70.50.103 with HTTP; Wed, 30 Apr 2014 04:30:03 -0700 (PDT)\r\nIn-Reply-To: &lt;CA+duimN4+YU3-zSxfuuzON-P-rr85PSps+01C1nVkkpJLcRdtw@...&gt;\r\nReferences: &lt;CA+duimO24sakOXSMVuqbEex8+ziBmQHvecoY3kwAd+zAB5ZkwQ@...&gt;\n\t&lt;CANtXhmvuJG2LdYzRDeWFWiSMG5mb+zfBLVgEaBmtty2WFPixEw@...&gt;\n\t&lt;CA+duimN4+YU3-zSxfuuzON-P-rr85PSps+01C1nVkkpJLcRdtw@...&gt;\r\nDate: Wed, 30 Apr 2014 14:30:03 +0300\r\nMessage-ID: &lt;CANtXhmtxohO4RfhUc0BS4a2fW19JcjDbe-8q0w9MH1WjXco8zQ@...&gt;\r\nTo: neat@yahoogroups.com\r\nContent-Type: multipart/alternative; boundary=e89a8ffbacdf9dd7b604f840def4\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Vassilis Vassiliades &lt;vassilisvas@...&gt;\r\nSubject: Re: [neat] New paper: Automated Generation of Environments to Test\n the General Learning Capabilities of AI Agents\r\nX-Yahoo-Group-Post: member; u=434634266; y=dCLtULFs2Ck-2LuWaNBdczx90u8c8fb9Xt0_Rwjx270lz2HB6DVU9A\r\nX-Yahoo-Profile: v.vassiliades\r\n\r\n\r\n--e89a8ffbacdf9dd7b604f840def4\r\nContent-Type: text/plain; charset=UTF-8\r\n\r\nHello Oliver,\n\nThanks for your quick reply.\n\n\nPerhaps that sentence should have read (additions in bold): &quot;the\ngenome *directly\n&gt; encodes* the parameter values for several plasticity rules, or classes,  *(based\n&gt; on Niv&#39;s equation)* and then *indirectly encodes, via the CPPN,* which\n&gt; rule each connection references.&quot; In HyperNEAT the genome is usually just\n&gt; the CPPN, but we&#39;ve added some additional genes (floating-point numbers) to\n&gt; directly encode parameter values for Niv&#39;s rule. Each genome encodes four\n&gt; sets of parameter values for Niv&#39;s rule, resulting in four different\n&gt; specific weight update rules (all using Niv&#39;s rule with but different\n&gt; parameter values). Then when we build the neural network with HyperNEAT we\n&gt; add extra outputs to the CPPN that determine which one of these plasticity\n&gt; rules is to be used for each connection in the network. Does that make more\n&gt; sense?\n&gt;\n\nYes, it is clear now. I guess one could also encode the mutation rates in\nthe genotype, like in Evolution Strategies, and make these parameters\nself-adaptive. If I remember correctly when reading Soltoggio&#39;s paper, he\nused some constraints when evolving the parameters of the plasticity rule\nand specifically, A-D were in the range [-1,1] and eta in the range\n[-100,100]. Did you use any similar constraints?\n\nJust out of curiosity, what activation function did you use for these n+1\noutputs that correspond to the classes? Did you use a softmax activation\nfunction to interpret the outputs as a probability distribution (and\nconsequently selected the class probabilistically) or did you just select\nthe class based on the highest output among these neurons?\n\n\nThe number of states is independent of the number of actions. Different\n&gt; actions in state A may all lead to state B but provide different reward\n&gt; values.\n&gt;\n\nSo, how many states did you use for your simulations? Is it 4 (like in\nFigure 1)? I might have missed that when reading the paper, this is why I\nasked whether the number of actions correspond to the number of states.\n\n\n\n&gt; 3) On page 3 you say that &quot;the proportion of state transitions that\n&gt;&gt; provide a reward value is 0.5&quot;. It is not clear to me, however, what the\n&gt;&gt; reward values are. Do all transitions that have a reward value have the\n&gt;&gt; *same* reward value (e.g. equal to 1), or does this value vary?\n&gt;&gt;\n&gt;\n&gt; For transitions that provide a reward, the reward is selected uniformly\n&gt; from the range [0, 1).\n&gt;\n&gt;\n&gt;&gt;\n&gt;&gt; Also, regarding the &quot;maximum possible reward maxRx&quot;, do you mean the\n&gt;&gt; &quot;return (sum of rewards) obtained by the optimal policy&quot;? If you have the\n&gt;&gt; *same* reward value on the transitions (as mentioned above) then it is easy\n&gt;&gt; to calculate maxRx; if the reward values vary then I guess you have to\n&gt;&gt; calculate maxRx using dynamic programming; the initial state and the trial\n&gt;&gt; length matters, especially in the case where you have 16 actions (states?)\n&gt;&gt; and trial length = 4.\n&gt;&gt;\n&gt;\n&gt; Because the length of trials is relatively small (and the MDPs\n&gt; deterministic) we calculate the maximum return via a simple brute force\n&gt; method that tries every possible sequence of actions for the specific trial\n&gt; length in question.\n&gt;\n\nOk, it&#39;s clear now.\n\n\n\n&gt; Yes, this is an interesting question in general, and certainly previous\n&gt; results on simple deceptive domains indicate that for delayed-reward MDP\n&gt; environments like you describe (neuro)evolution will likely get stuck if\n&gt; the search isn&#39;t aided by something like Novelty Search. In our paper we\n&gt; don&#39;t worry about this issue as we are comparing the performance of the\n&gt; different neural network models relative to each other and are not\n&gt; particularly interested in their performance relative to the maximum\n&gt; possible (we do scale the results relative to the maximum possible but this\n&gt; is simply to make aggregation of results easier).\n&gt;\n\nOne issue at a time :)\n\r\n--e89a8ffbacdf9dd7b604f840def4\r\nContent-Type: text/html; charset=UTF-8\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\n&lt;div dir=3D&quot;ltr&quot;&gt;Hello Oliver,&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Thanks for your quick rep=\r\nly.&lt;br&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;div c=\r\nlass=3D&quot;gmail_quote&quot;&gt;&lt;blockquote class=3D&quot;gmail_quote&quot; style=3D&quot;margin:0px =\r\n0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);bord=\r\ner-left-style:solid;padding-left:1ex&quot;&gt;\n\n&lt;div&gt;&lt;div dir=3D&quot;ltr&quot;&gt;&lt;div class=3D=\r\n&quot;gmail_extra&quot;&gt;&lt;div class=3D&quot;gmail_quote&quot;&gt;&lt;div&gt;&lt;div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;Perhaps=\r\n that sentence should have read (additions in bold): &quot;the genome &lt;b&gt;di=\r\nrectly encodes&lt;/b&gt; the parameter values for several plasticity rules, or cl=\r\nasses, =C2=A0&lt;b&gt;(based on Niv&#39;s equation)&lt;/b&gt;=C2=A0and then &lt;b&gt;indirect=\r\nly encodes, via the CPPN,&lt;/b&gt;=C2=A0which rule each connection references.&q=\r\nuot; In HyperNEAT the genome is usually just the CPPN, but we&#39;ve added =\r\nsome additional genes (floating-point numbers) to directly encode parameter=\r\n values for Niv&#39;s rule. Each genome encodes four sets of parameter valu=\r\nes for Niv&#39;s rule, resulting in four different specific weight update r=\r\nules (all using Niv&#39;s rule with but different parameter values). Then w=\r\nhen we build the neural network with HyperNEAT we add extra outputs to the =\r\nCPPN that determine which one of these plasticity rules is to be used for e=\r\nach connection in the network. Does that make more sense?&lt;/div&gt;\n\n&lt;/div&gt;&lt;/di=\r\nv&gt;&lt;/div&gt;&lt;/div&gt;&lt;/blockquote&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Yes, it is clear now. I gues=\r\ns one could also encode the mutation rates in the genotype, like in Evoluti=\r\non Strategies, and make these parameters self-adaptive. If I remember corre=\r\nctly when reading Soltoggio&#39;s paper, he used some constraints when evol=\r\nving the parameters of the plasticity rule and specifically, A-D were in th=\r\ne range [-1,1] and eta in the range [-100,100]. Did you use any similar con=\r\nstraints?=C2=A0&lt;/div&gt;\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Just out of curiosity, what activ=\r\nation function did you use for these n+1 outputs that correspond to the cla=\r\nsses? Did you use a softmax activation function to interpret the outputs as=\r\n a probability distribution (and consequently selected the class probabilis=\r\ntically) or did you just select the class based on the highest output among=\r\n these neurons?=C2=A0&lt;/div&gt;\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;blockquote clas=\r\ns=3D&quot;gmail_quote&quot; style=3D&quot;margin:0px 0px 0px 0.8ex;border-left-width:1px;b=\r\norder-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex&quot;=\r\n&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div dir=3D&quot;ltr&quot;&gt;&lt;div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;\n&lt;div=\r\n class=3D&quot;gmail_quote&quot;&gt;\n&lt;div&gt;The number of states is independent of the num=\r\nber of actions. Different actions in state A may all lead to state B but pr=\r\novide different reward values.&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/d=\r\niv&gt;&lt;/div&gt;&lt;/blockquote&gt;\n\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;So, how many states did you use=\r\n for your simulations? Is it 4 (like in Figure 1)? I might have missed that=\r\n when reading the paper, this is why I asked whether the number of actions =\r\ncorrespond to the number of states.&lt;/div&gt;\n\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;=C2=A0=C2=A0=\r\n&lt;/div&gt;&lt;blockquote class=3D&quot;gmail_quote&quot; style=3D&quot;margin:0px 0px 0px 0.8ex;b=\r\norder-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:s=\r\nolid;padding-left:1ex&quot;&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div dir=3D&quot;ltr&quot;&gt;\n&lt;div class=3D&quot;=\r\ngmail_extra&quot;&gt;&lt;div class=3D&quot;gmail_quote&quot;&gt;\n&lt;div&gt;&lt;blockquote class=3D&quot;gmail_qu=\r\note&quot; style=3D&quot;margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-co=\r\nlor:rgb(204,204,204);border-left-style:solid&quot;&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div dir=\r\n=3D&quot;ltr&quot;&gt;&lt;div&gt;&lt;/div&gt;&lt;div&gt;3) On page 3 you say that &quot;the proportion of =\r\nstate transitions that provide a reward value is 0.5&quot;. It is not clear=\r\n to me, however, what the reward values are. Do all transitions that have a=\r\n reward value have the *same* reward value (e.g. equal to 1), or does this =\r\nvalue vary?&lt;/div&gt;\n\n\n\n&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/blockquote&gt;&lt;div&gt;&lt;br&gt;&lt;/=\r\ndiv&gt;&lt;/div&gt;&lt;div&gt;For transitions that provide a reward, the reward is selecte=\r\nd uniformly from the range [0, 1).&lt;/div&gt;&lt;div&gt;&lt;div&gt;=C2=A0&lt;/div&gt;&lt;blockquote c=\r\nlass=3D&quot;gmail_quote&quot; style=3D&quot;margin:0px 0px 0px 0.8ex;border-left-width:1p=\r\nx;border-left-color:rgb(204,204,204);border-left-style:solid&quot;&gt;\n\n\n\n&lt;div&gt;&lt;div=\r\n&gt;&lt;div&gt;&lt;div&gt;&lt;div dir=3D&quot;ltr&quot;&gt;\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Also, regarding the &quot;=\r\nmaximum possible reward maxRx&quot;, do you mean the &quot;return (sum of r=\r\newards) obtained by the optimal policy&quot;? If you have the *same* reward=\r\n value on the transitions (as mentioned above) then it is easy to calculate=\r\n=C2=A0maxRx; if the reward=C2=A0values vary then I guess you have to calcul=\r\nate=C2=A0maxRx=C2=A0using dynamic programming; the initial state and the tr=\r\nial length=C2=A0matters, especially in the case where you have 16 actions (=\r\nstates?) and trial length =3D 4.&lt;/div&gt;\n\n\n\n&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/b=\r\nlockquote&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;Because the length of trials is relativ=\r\nely small (and the MDPs deterministic) we calculate the maximum return via =\r\na simple brute force method that tries every possible sequence of actions f=\r\nor the specific trial length in question.&lt;/div&gt;\n\n&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/=\r\ndiv&gt;&lt;/div&gt;&lt;/div&gt;&lt;/blockquote&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Ok, it&#39;s clear now.&lt;/d=\r\niv&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;=C2=A0=C2=A0&lt;/div&gt;&lt;blockquote class=3D&quot;gmail_quote&quot; =\r\nstyle=3D&quot;margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:r=\r\ngb(204,204,204);border-left-style:solid;padding-left:1ex&quot;&gt;\n\n&lt;div&gt;&lt;div&gt;&lt;div&gt;=\r\n&lt;div&gt;&lt;div dir=3D&quot;ltr&quot;&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;div class=3D&quot;gmail_quote&quot;=\r\n&gt;&lt;div&gt;Yes, this is an interesting question in general, and certainly previo=\r\nus results on simple deceptive domains indicate that for delayed-reward MDP=\r\n environments like you describe (neuro)evolution will likely get stuck if t=\r\nhe search isn&#39;t aided by something like Novelty Search. In our paper we=\r\n don&#39;t worry about this issue as we are comparing the performance of th=\r\ne different neural network models relative to each other and are not partic=\r\nularly interested in their performance relative to the maximum possible (we=\r\n do scale the results relative to the maximum possible but this is simply t=\r\no make aggregation of results easier).&lt;/div&gt;\n\n&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div=\r\n&gt;&lt;/div&gt;&lt;/div&gt;&lt;/blockquote&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;One issue at a time :)&lt;/div&gt;&lt;=\r\ndiv&gt;&lt;br&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;\n\r\n--e89a8ffbacdf9dd7b604f840def4--\r\n\n"}}