{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":236201729,"authorName":"Jean-Baptiste Mouret","from":"Jean-Baptiste Mouret &lt;mandor@...&gt;","profile":"mandor_42","replyTo":"LIST","senderId":"nPtOv814hHO9Yt1sdgg1lKwOMEYwWRFTj44PjIaM2M-8-hBxT0MIbYqax_AB5rNa_rIqQCyEmHHWyeStEEZFHXeaQbi-jhfWvPEKp2V9_vz48-yH","spamInfo":{"isSpam":false,"reason":"12"},"subject":"New paper in PLoS One: On the Relationships between Generative Encodings, Regularity, and Learning Abilities when Evolving Plastic Artificial Neural Networks.","postDate":"1389874207","msgId":6241,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PEJBMDEwRDBCLTM1QzAtNEVCNi05QjgxLUNCMkQ2N0U0MDJEMUBoYXBweWNvZGVycy5vcmc+"},"prevInTopic":0,"nextInTopic":0,"prevInTime":6240,"nextInTime":6242,"topicId":6241,"numMessagesInTopic":1,"msgSnippet":"Hello all I am very happy to announce you that our last paper is online: Tonelli, P. and Mouret, J.-B. (2013). On the Relationships between Generative","rawEmail":"Return-Path: &lt;jeanbaptiste.mouret@...&gt;\r\nX-Sender: jeanbaptiste.mouret@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 1902 invoked by uid 102); 16 Jan 2014 12:10:12 -0000\r\nX-Received: from unknown (HELO mtaq4.grp.bf1.yahoo.com) (10.193.84.143)\n  by m12.grp.bf1.yahoo.com with SMTP; 16 Jan 2014 12:10:12 -0000\r\nX-Received: (qmail 31667 invoked from network); 16 Jan 2014 12:10:12 -0000\r\nX-Received: from unknown (HELO mail-we0-f171.google.com) (74.125.82.171)\n  by mtaq4.grp.bf1.yahoo.com with SMTP; 16 Jan 2014 12:10:12 -0000\r\nX-Received: by mail-we0-f171.google.com with SMTP id w61so3035682wes.2\n        for &lt;neat@yahoogroups.com&gt;; Thu, 16 Jan 2014 04:10:11 -0800 (PST)\r\nX-Received: by 10.180.21.166 with SMTP id w6mr7605821wie.31.1389874211836;\n        Thu, 16 Jan 2014 04:10:11 -0800 (PST)\r\nReturn-Path: &lt;jeanbaptiste.mouret@...&gt;\r\nX-Received: from [134.157.18.142] ([134.157.18.142])\n        by mx.google.com with ESMTPSA id jw4sm6050190wjc.20.2014.01.16.04.10.11\n        for &lt;multiple recipients&gt;\n        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);\n        Thu, 16 Jan 2014 04:10:11 -0800 (PST)\r\nContent-Type: text/plain; charset=us-ascii\r\nContent-Transfer-Encoding: quoted-printable\r\nDate: Thu, 16 Jan 2014 13:10:07 +0100\r\nTo: neat@yahoogroups.com\r\nMessage-Id: &lt;BA010D0B-35C0-4EB6-9B81-CB2D67E402D1@...&gt;\r\nMime-Version: 1.0 (Mac OS X Mail 7.1 &#92;(1827&#92;))\r\nX-Mailer: Apple Mail (2.1827)\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Jean-Baptiste Mouret &lt;mandor@...&gt;\r\nSubject: New paper in PLoS One: On the Relationships between Generative Encodings, Regularity, and Learning Abilities when Evolving Plastic Artificial Neural Networks.\r\nX-Yahoo-Group-Post: member; u=236201729; y=TthXKSln9lwdIdv8Q-4Cv2i0mpsxeWW6nyapnwkChBQ8uFbf\r\nX-Yahoo-Profile: mandor_42\r\n\r\nHello all\n\nI am very happy to announce you that our last paper is online:\nT=\r\nonelli, P. and Mouret, J.-B. (2013). On the Relationships between Generativ=\r\ne Encodings, Regularity, and Learning Abilities when Evolving Plastic Artif=\r\nicial Neural Networks.\nPLoS One. Vol 8 No 11 Pages e79138.\n-&gt; http://www.is=\r\nir.upmc.fr/files/2013ACLI2965.pdf\n-&gt; http://dx.doi.org/10.1371/journal.pone=\r\n.0079138\n\nThis paper shows that developmental systems, synaptic plasticity,=\r\n regularity and flexibility are interleaved topics. \n\nBesides the scientifi=\r\nc contributions about encodings and neural networks, you might be intereste=\r\nd by the new technique used in this paper to evaluate the regularity of neu=\r\nral networks. This technique is based on counting the number of automorphis=\r\nms (http://en.wikipedia.org/wiki/Graph_automorphism), a problem that can be=\r\n efficiently solved by widely available libraries. Our implementation is av=\r\nailable on github (https://github.com/jbmouret/network_toolbox), but it is =\r\nmainly a call to bliss (http://www.tcs.hut.fi/Software/bliss/) and you shou=\r\nld be able to easily do the same in your framework. There are examples at t=\r\nhe end of the paper to illustrate how the number of automorphisms relate to=\r\n regularity. For those interested, there is also a theoritical paper that l=\r\ninks the number of automorphisms to the Kolmogorov complexity (http://arxiv=\r\n.org/abs/1306.0322).\n\nOverall, while this tool is not perfect, I think we s=\r\nhould, as a scientific community, work on a common set of measures to compa=\r\nre and analyze our work (and not focus on performance benchmarks, which are=\r\n too short-sighted). This measure of regularity is a first step in this dir=\r\nection.\n\nAbstract:\nA major goal of bio-inspired artificial intelligence is =\r\nto design artificial neural networks with abilities that resemble those of =\r\nanimal nervous systems. It is commonly believed that two keys for evolving =\r\nnature-like artificial neural networks are (1) the developmental process th=\r\nat links genes to nervous systems, which enables the evolution of large, re=\r\ngular neural networks, and (2) synaptic plasticity, which allows neural net=\r\nworks to change during their lifetime. So far, these two topics have been m=\r\nainly studied separately. The present paper shows that they are actually de=\r\neply connected. Using a simple operant conditioning task and a classic evol=\r\nutionary algorithm, we compare three ways to encode plastic neural networks=\r\n: a direct encoding, a developmental encoding inspired by computational neu=\r\nroscience models, and a developmental encoding inspired by morphogen gradie=\r\nnts (similar to HyperNEAT). Our results suggest that using a developmental =\r\nencoding could improve the learning abilities of evolved, plastic neural ne=\r\ntworks. Complementary experiments reveal that this result is likely the con=\r\nsequence of the bias of developmental encodings towards regular structures:=\r\n (1) in our experimental setup, encodings that tend to produce more regular=\r\n networks yield networks with better general learning abilities; (2) whatev=\r\ner the encoding is, networks that are the more regular are statistically th=\r\nose that have the best learning abilities.\n\nBest regards,\n--\nJean-Baptiste =\r\nMouret / Mandor\nhttp://pages.isir.upmc.fr/~mouret/\n\n\n"}}