{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":204774783,"authorName":"Matt Simmerson","from":"&quot;Matt Simmerson&quot; &lt;m.simmerson@...&gt;","profile":"easablade","replyTo":"LIST","senderId":"YQtEgv1Gbr2vyahYetFJWYUO3jQuIQss1ox8HocY7B3Dx1cY14_ayaW3SoHcZ89RykKHtsKd9ohFLqGYhBzLs4yUn5-iCe3p5YgTHNCzgT8ScZ0EMg","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: NEAT enhancements","postDate":"1155064904","msgId":2699,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGViYW84OCt2M2o4QGVHcm91cHMuY29tPg==","inReplyToHeader":"PDQ0RDg5OEVFLjQwMjA3MDlAdmlyZ2luLm5ldD4="},"prevInTopic":2698,"nextInTopic":2700,"prevInTime":2698,"nextInTime":2700,"topicId":2684,"numMessagesInTopic":17,"msgSnippet":"I hadn t thought about using a value of N as determined by the network - well no in this experiment.  I have done it in my chess experiment. Maybe I will do","rawEmail":"Return-Path: &lt;m.simmerson@...&gt;\r\nX-Sender: m.simmerson@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 85640 invoked from network); 8 Aug 2006 19:22:05 -0000\r\nReceived: from unknown (66.218.66.216)\n  by m25.grp.scd.yahoo.com with QMQP; 8 Aug 2006 19:22:05 -0000\r\nReceived: from unknown (HELO n17a.bullet.scd.yahoo.com) (66.94.237.46)\n  by mta1.grp.scd.yahoo.com with SMTP; 8 Aug 2006 19:22:05 -0000\r\nReceived: from [66.218.66.58] by n17.bullet.scd.yahoo.com with NNFMP; 08 Aug 2006 19:21:46 -0000\r\nReceived: from [66.218.66.91] by t7.bullet.scd.yahoo.com with NNFMP; 08 Aug 2006 19:21:46 -0000\r\nDate: Tue, 08 Aug 2006 19:21:44 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;ebao88+v3j8@...&gt;\r\nIn-Reply-To: &lt;44D898EE.4020709@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0\r\nFrom: &quot;Matt Simmerson&quot; &lt;m.simmerson@...&gt;\r\nSubject: Re: NEAT enhancements\r\nX-Yahoo-Group-Post: member; u=204774783; y=ifgSIxCNE7BfVdJ36V7VhPX4YJb4uNwtfXn9v45oSG244Zuw7C10SCil9uhHfqQrPKhWTNo7\r\nX-Yahoo-Profile: easablade\r\n\r\nI hadn&#39;t thought about using a value of N as determined by the network\n- we=\r\nll no in this experiment.  I have done it in my chess experiment.\n Maybe I =\r\nwill do that.  I have my own version of NEAT already that can\nadd extra fea=\r\ntures like that (as many as you like), and i keep meaning\nto fully document=\r\n and release it in Kens site, but time etc...  I have\nat least got a releas=\r\nebale version now, which i think is pretty much\nbug free.\n\nI think I&#39;ll try=\r\n and remove the tech indicators adn just go for raw data.\n\nTa\n\nMatt\n\nPS One=\r\n thing I have just done/doing with my NEAt is to make the\ntraining distribu=\r\ntable over n clients on a network.  done in order to\nmake up for some quite=\r\n heavy processing with large input node count\nand large data sets.  \n\n--- I=\r\nn neat@yahoogroups.com, ej &lt;e.j@...&gt; wrote:\n&gt;\n&gt; All technical indicators ar=\r\ne based on some combination of open, close, \n&gt; high, low and volume. I like=\r\n the approach taken in the book &#39;Blondie24&#39; \n&gt; - give the neural net just t=\r\nhe basic info and allow it to combine it as \n&gt; it sees fit. The net may com=\r\ne up with an amazing new indicator within \n&gt; it&#39;s weights that no-one has t=\r\nhought of before. I think it&#39;s better to \n&gt; put as little human knowledge i=\r\nn there as possible and allow it to \n&gt; discover things for itself.\n&gt; I&#39;ve b=\r\neen working on my own little project for ages now - I keep \n&gt; stopping and =\r\nrestarting and redoing things so my progress has been\nterrible!\n&gt; Again, us=\r\ning Blondie24 as my inspiration, I&#39;m intending to give the\nlast \n&gt; &#39;N&#39; days=\r\n (N to be determined - but after looking at at Sidhant&#39;s paper, \n&gt; I might =\r\nborrow the concept of adding input nodes so that neat would\ncome \n&gt; up with=\r\n an optimal value of N for itself) of open, high, low, close & \n&gt; volume as=\r\n input to the net, and getting the output to be -1 to +1 to \n&gt; represent a =\r\nstrong sell signal through to a strong buy signal.\n&gt; Assuming that I can ge=\r\nt a net that can give good buy / sell signals \n&gt; based on what the normalis=\r\ned stock graph I show it as input looks like, \n&gt; I&#39;ll then run it against e=\r\nvery stock in the FTSE All-Share say each \n&gt; night then look at the top 10 =\r\nstocks ranked by net output to see if I \n&gt; fancy buying any of them.\n&gt; I ha=\r\nve perl programs to site-scrape all the historic data off \n&gt; finance.yahoo.=\r\ncom. I&#39;m in the middle of donwloading it all - which \n&gt; according to my est=\r\nimates will take about a month of constant \n&gt; downloading to get the whole =\r\nlot!.\n&gt; All I need now is to get off my arse and finish the thing!!\n&gt; Part =\r\nof the delay is that I&#39;m trying to write my own implementation of \n&gt; neat i=\r\nn C++ which borrows heavily from SharpNEAT, but keep getting side \n&gt; tracke=\r\nd.\n&gt; I&#39;m cutting out all the recurrent stuff - I don&#39;t need it for my \n&gt; pu=\r\nrposes. Consequently the Neural Net&#39;s are Directed Acyclic Graphs,\nand \n&gt; u=\r\nsing that fact enables me to make some good optimisations over a more \n&gt; ge=\r\nneral neat that includes recurrency.\n&gt; Yes - I should be using an &#39;off-the-=\r\nshelf&#39; version of neat and just \n&gt; getting on with things, but half the fun=\r\n is writing my own version ;-)\n&gt; \n&gt; Cheers,\n&gt; Emyr\n&gt; \n&gt; Matt Simmerson wrot=\r\ne:\n&gt; &gt;\n&gt; &gt; Hi Sidhant\n&gt; &gt;\n&gt; &gt; Great paper. I have been trying to do much th=\r\ne same thing, except\n&gt; &gt; rather than value prediction, I have been creating=\r\n a trader that\n&gt; &gt; essentially decides to go short, long or take the 5th ;)=\r\n. I&#39;ve had\n&gt; &gt; mixed success, and not had the confidence to actually lay an=\r\ny money\n&gt; &gt; out yet. It would be interesting to know what input data sets h=\r\nave\n&gt; &gt; you been using for the Forex part of your paper, as I have tried al=\r\nl\n&gt; &gt; sorts of things including technical indicators etc.\n&gt; &gt;\n&gt; &gt; Cheers\n&gt; =\r\n&gt;\n&gt; &gt; Matt\n&gt; &gt;\n&gt; &gt; --- In neat@yahoogroups.com &lt;mailto:neat%40yahoogroups.c=\r\nom&gt;, Sidhant \n&gt; &gt; Dash &lt;sidhantdash@&gt; wrote:\n&gt; &gt; &gt;\n&gt; &gt; &gt; As part of an unde=\r\nrgraduate research project, I did implement\n&gt; &gt; something on those lines.\n&gt;=\r\n &gt; &gt;\n&gt; &gt; &gt; I worked on a NEAT based approach to predicting a noisy financia=\r\nl\n&gt; &gt; time series (Yen-USD exchange rate). The original NEAT algorithm was\n=\r\n&gt; &gt; modified to start with an initial population of 3 different neural\n&gt; &gt; =\r\nnetwork architectures, which included Elman networks and MLPs apart\n&gt; &gt; fro=\r\nm the normal recurrent networks that NEAT begins with. The networks\n&gt; &gt; (wi=\r\nnners) produced by NEAT were then put through a conjugate gradient\n&gt; &gt; base=\r\nd optimization process, and finally the optimized networks were\n&gt; &gt; combine=\r\nd using an ensembling technique to produce the final results.\n&gt; &gt; &gt;\n&gt; &gt; &gt; I=\r\n personally think the local optimization is necessary when we are\n&gt; &gt; using=\r\n NEAT to produce networks to accomplish tasks that require a high\n&gt; &gt; degre=\r\ne of precision. It kind of fine tunes the performance of the\n&gt; &gt; networks. =\r\nEnsembling then is one of the standard tools in the ML\n&gt; &gt; repertoire to pr=\r\noduce results that none of NEAT&#39;s &#39;winner&#39; networks\n&gt; &gt; could individually =\r\nproduce.\n&gt; &gt; &gt;\n&gt; &gt; &gt; A detailed project report is posted in the Files secti=\r\non.\n&gt; &gt; &gt;\n&gt; &gt; &gt; regards\n&gt; &gt; &gt; Sidhant\n&gt; &gt; &gt;\n&gt; &gt; &gt; aklein07 &lt;a.klein@&gt; wrote=\r\n:\n&gt; &gt; &gt; All,\n&gt; &gt; &gt;\n&gt; &gt; &gt; Ken&#39;s papers mention (at least I think that they d=\r\no) that (dynamic)\n&gt; &gt; &gt; annealing of mutation rates might be advantageous i=\r\nn order to\nconfine\n&gt; &gt; &gt; global searching after a while to a promising regi=\r\non in search\nspace.\n&gt; &gt; &gt; Having read some papers, I also found indications=\r\n that a hybrid\n&gt; &gt; &gt; weight training algorithm (global search by means of g=\r\nenetic\n&gt; &gt; &gt; algorithms and local search using some gradient descent algori=\r\nthm)\n&gt; &gt; &gt; might be able to produce better results than genetic algorithm\ns=\r\nearch\n&gt; &gt; &gt; alone. The reason is, that each type of algorithm performs well=\r\n\n&gt; &gt; &gt; mostly only in its specific field. Has anyone further insights into\n=\r\n&gt; &gt; &gt; this topic ? Has anyone tried to implement any of the approaches\n&gt; &gt; =\r\n&gt; mentioned ?\n&gt; &gt; &gt;\n&gt; &gt; &gt; Also time delay network connections might provide=\r\n improvements for\n&gt; &gt; &gt; some model estimation tasks. Has anyone tested / im=\r\nplemented yet any\n&gt; &gt; &gt; of these ideas ?\n&gt; &gt; &gt;\n&gt; &gt; &gt; Thanks for any comment=\r\ns,\n&gt; &gt; &gt; Achim\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt; Fear is only as de=\r\nep as the mind allows.\n&gt; &gt; &gt; --Japanese proverb\n&gt; &gt; &gt;\n&gt; &gt; &gt; My blog\n&gt; &gt; &gt;\n&gt;=\r\n &gt; &gt;\n&gt; &gt; &gt; ---------------------------------\n&gt; &gt; &gt; Groups are talking. We&a=\r\ncute;re listening. Check out the handy\n&gt; &gt; changes to Yahoo! Groups.\n&gt; &gt; &gt;\n=\r\n&gt; &gt;\n&gt; &gt; \n&gt; &gt;\n--------------------------------------------------------------=\r\n----------\n&gt; &gt;\n&gt; &gt; No virus found in this incoming message.\n&gt; &gt; Checked by =\r\nAVG Free Edition.\n&gt; &gt; Version: 7.1.394 / Virus Database: 268.10.7/411 - Rel=\r\nease Date:\n07/08/2006\n&gt; &gt;\n&gt;\n\n\n\n\n\n"}}