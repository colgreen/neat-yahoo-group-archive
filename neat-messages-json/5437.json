{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":344770077,"authorName":"Colin Green","from":"Colin Green &lt;colin.green1@...&gt;","profile":"alienseedpod","replyTo":"LIST","senderId":"MmoDeXzQHfjkmSCWBEslytp_tmNf0x87EzMwJViQz0J29q8plsdDZBZGe70lYMJnaiaMnh11lb6GzHS-8EXqAPTJT4X95DaH5DDY","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [neat] Some Questions Regarding SharpNEAT v2","postDate":"1293985015","msgId":5437,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PEFBTkxrVGlrcGZMOFZPOWdBdkIyanQ5M3FuWHVzUTV2S19pWnBfalhhVFQxZUBtYWlsLmdtYWlsLmNvbT4=","inReplyToHeader":"PDI1MzE1NS41MjA0My5xbUB3ZWI1NDQwOS5tYWlsLnJlMi55YWhvby5jb20+","referencesHeader":"PDI1MzE1NS41MjA0My5xbUB3ZWI1NDQwOS5tYWlsLnJlMi55YWhvby5jb20+"},"prevInTopic":5413,"nextInTopic":5759,"prevInTime":5436,"nextInTime":5438,"topicId":5413,"numMessagesInTopic":7,"msgSnippet":"... It s a good question. I m not 100% sure, all I can say is that of the learning methods I know of, generally they aren t used on recursive networks. One of","rawEmail":"Return-Path: &lt;colin.green1@...&gt;\r\nX-Sender: colin.green1@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 7547 invoked from network); 2 Jan 2011 16:16:56 -0000\r\nX-Received: from unknown (98.137.34.45)\n  by m4.grp.sp2.yahoo.com with QMQP; 2 Jan 2011 16:16:56 -0000\r\nX-Received: from unknown (HELO mail-ww0-f46.google.com) (74.125.82.46)\n  by mta2.grp.sp2.yahoo.com with SMTP; 2 Jan 2011 16:16:56 -0000\r\nX-Received: by wwj40 with SMTP id 40so14285853wwj.15\n        for &lt;neat@yahoogroups.com&gt;; Sun, 02 Jan 2011 08:16:55 -0800 (PST)\r\nMIME-Version: 1.0\r\nX-Received: by 10.227.129.17 with SMTP id m17mr11002884wbs.79.1293985015789;\n Sun, 02 Jan 2011 08:16:55 -0800 (PST)\r\nX-Received: by 10.227.61.132 with HTTP; Sun, 2 Jan 2011 08:16:55 -0800 (PST)\r\nIn-Reply-To: &lt;253155.52043.qm@...&gt;\r\nReferences: &lt;253155.52043.qm@...&gt;\r\nDate: Sun, 2 Jan 2011 16:16:55 +0000\r\nMessage-ID: &lt;AANLkTikpfL8VO9gAvB2jt93qnXusQ5vK_iZp_jXaTT1e@...&gt;\r\nTo: neat@yahoogroups.com\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Colin Green &lt;colin.green1@...&gt;\r\nSubject: Re: [neat] Some Questions Regarding SharpNEAT v2\r\nX-Yahoo-Group-Post: member; u=344770077; y=_oiHx_ipzlV9Ml1dXrhGgKUxepX5DCuPyeEalWapZDBnYSW-rfIA\r\nX-Yahoo-Profile: alienseedpod\r\n\r\nOn 2 December 2010 21:22, Sina Iravanian &lt;sina_iravanian@...&gt; wrote:\n&gt;\n&gt;\n&gt; I am using SharpNEAT v2 for some NEAT and HyperNEAT experiments, and wondered if:\n&gt;\n&gt; 1. Is it possible to train the evolved neural networks (as in Lamarckian evolution)? Are there any learning mechanisms that exist for recurrent\n&gt; networks (such as backpropagation through time) implemented for them? How about CPPNs? Are there any ways to train them in a way\n&gt; other than evolution?\n\nIt&#39;s a good question. I&#39;m not 100% sure, all I can say is that of the\nlearning methods I know of, generally they aren&#39;t used on recursive\nnetworks. One of the outstanding tasks I have is to provide support\nfor evolving feedforward only nets as they are more useful in soem\nrespects, e.g. easier to reverse engineer and to apply further\nlearning methods to as you poijt out.\n\n\n&gt; 2. Do the evolved networks contain standard feed-forward links implicitly? For example, consider a network with 5 inputs and 5 outputs, that\n&gt; the visualizer shows only one link between one of the inputs and one of the outputs. Are the other links missing or just not visualized by the\n&gt; library?\n\nWhat you see is that you get. The initial random populations have just\ninputs and ouputs and typically have some small proportion of the\ninputs and outputs connected up; Evolution works from there.\n\n\n&gt; 3. Why can&#39;t I define a CPPN with only 1 output? When I do so the library throws an instance of IndexOutOfBoundException. But whenever I\n&gt; set the number of outputs to 2, everything works just fine.\n\nSounds like a bug. As you&#39;re probabyl aware the dimensionality of the\nsubstrate is not restricted in any way, but I may have maed an\nassumption somewhere in the code that there would be at least 2\ndimensions. Let me know if this is still a problem and I&#39;ll look into\nit.\n\n\n\n&gt; 4. As far as I know the set of activation functions and their parameters are fixed, e.g., in order to create a repeating pattern the library\n&gt; contains only Sin(2 * x). But what if the regularities to be modeled in some problem requires a repetition with a higher (or lower) frequency?\n\nThe set of activation functions is defined by\nIActivationFunctionLibrary, there are standard sets defined by\nDefaultActivationFunctionLibrary.CreateLibraryNeat() and\nCreateLibraryCppn(), but this can be replaced with a custom set of\nfunctions when you instantiate the IGenomeFactory, e.g. see the\nCreateGenomeFactory() method in\nBoxesVisualDiscriminationExperiment.cs.\n\n\n&gt; In the end, a lot of thanks go to Colin Green for his nice library.\n\nThanks, hope the above helps. Apologies for the late response.\n\nColin.\n\n"}}