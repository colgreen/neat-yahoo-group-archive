{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"3hd_iFygUOmIVxXZyNl4j8N3RQfYYYDejmL43KNo24q7a8z9LWQQdFq30rAR8XbngiMVLPUjAk1QMhQho4o3rN87cU9eE3qI9aKyQbbvDTQv","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Backpropagation and NEAT","postDate":"1205634012","msgId":3872,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGZyaTA0cytyYzMwQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGZyZXU1NStuMzFiQGVHcm91cHMuY29tPg=="},"prevInTopic":3871,"nextInTopic":3873,"prevInTime":3871,"nextInTime":3873,"topicId":3846,"numMessagesInTopic":41,"msgSnippet":"Andy, I may be misunderstanding some of your points about local search. First, the term local search itself implies that we are talking about a method that","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 11486 invoked from network); 16 Mar 2008 02:20:13 -0000\r\nX-Received: from unknown (66.218.67.95)\n  by m42.grp.scd.yahoo.com with QMQP; 16 Mar 2008 02:20:13 -0000\r\nX-Received: from unknown (HELO n34b.bullet.mail.sp1.yahoo.com) (66.163.168.148)\n  by mta16.grp.scd.yahoo.com with SMTP; 16 Mar 2008 02:20:13 -0000\r\nX-Received: from [216.252.122.216] by n34.bullet.mail.sp1.yahoo.com with NNFMP; 16 Mar 2008 02:20:13 -0000\r\nX-Received: from [209.73.164.86] by t1.bullet.sp1.yahoo.com with NNFMP; 16 Mar 2008 02:20:13 -0000\r\nX-Received: from [66.218.66.79] by t8.bullet.scd.yahoo.com with NNFMP; 16 Mar 2008 02:20:13 -0000\r\nDate: Sun, 16 Mar 2008 02:20:12 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;fri04s+rc30@...&gt;\r\nIn-Reply-To: &lt;freu55+n31b@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: Re: Backpropagation and NEAT\r\nX-Yahoo-Group-Post: member; u=54567749; y=fy7jZAKR2XIUNsaFpDwIlgUYeHC2zkgcYuZ16SBrVL_VJqDhGfsL\r\nX-Yahoo-Profile: kenstanley01\r\n\r\nAndy, I may be misunderstanding some of your points about local search.\n\nFi=\r\nrst, the term &quot;local search&quot; itself implies that we are talking\nabout a met=\r\nhod that has been developed exclusively to search in one\nlocal part of the =\r\nsearch space.  But what method is really intended\nfor that purpose and why =\r\nis that a good thing?  The problem in\ndifficult tasks in general is usually=\r\n that whatever algorithm you are\nusing gets *trapped* in a local area.  The=\r\nrefore, generally speaking,\nan algorithm that is designed to stay in one ar=\r\nea is probably a bad\nthing, not a good thing.  Only the most trivial proble=\r\nms entail simply\nrunning up the first hillside that you see.  For that, all=\r\n we would\nneed is hill climbing.  \n\nIn fact, one of the liabilities of back=\r\nprop is that it tends to be\ncaught on local optima.  That is a serious prob=\r\nlem and by no means a\nreason to recommend it.  \n\nYou say, &quot;There are many t=\r\nimes in which it is orders of magnitude\nquicker for a non-GA local gradient=\r\n search method to find a local\nminima, than an equivalent GA to mutate thro=\r\nugh generations to find\nthe same local minima.&quot;\n\nGenerally speaking finding=\r\n a local minimum is easy for any algorithm.\n   A gradient technique is like=\r\nly faster than a GA, but &quot;orders of\nmagnitude?&quot;  In general, that sounds ex=\r\nagerated.  In fact, in the 90s\nbefore NEAT existed there were a number of p=\r\napers written that\ncompared neuroevolution to backprop and concluded that n=\r\neuroevolution\nwas faster.  I do not think these results are meaningful beca=\r\nuse the\nissue is highly domain-dependent, but it does show that neither\napp=\r\nroach has a massive advantage over the other in general.  In fact,\nclaiming=\r\n otherwise just winds up running into No Free Lunch.\n\nYet in most problems =\r\nwhat we really care about finding are not just\nany local optima but *partic=\r\nular* local optima (or the global optimum)\nthat satisfy a &quot;good enough&quot; cri=\r\nterion.  Thus we need to be able to\nleave the local neighborhood entirely a=\r\nnd go on a real search.  For\nthat kind of search, you need an algorithm tha=\r\nt is designed not to\nsimply rush up the nearest hill.  \n \nYou also say, &quot;Bu=\r\nt a local search method could just as easily wander\nfree in the local fitne=\r\nss landscape, un-encumbered by organism\ntopology issues, to find the local =\r\nminima.&quot;\n\nI do not understand what you mean by &quot;un-encumbered.&quot;  All search=\r\n is\nwithin the confines of a certain topology, even if that topology is\nset=\r\n by the user.  The topology is what defines the search space.  You\ncannot i=\r\nsolate search from the space being searched.  Furthermore, if\nthere is any =\r\nencumbrance, it is the inability to change the topology\nwhen it is found wa=\r\nnting.  Thus searching through different topologies\nis the opposite of an e=\r\nncumbrance; it is a liberation.  \n\nAlong the same lines, this statement als=\r\no seems misleading: &quot;The moral\nof the story is that local search does not h=\r\nave to be constrained by\norganism topology, beyond that of providing the st=\r\nart point.&quot;\n\nAgain, local search is always constrained by topology.  Topolo=\r\ngy\ndefines the dimensions of the search. \n\nWhat you may not be considering =\r\nis that changing the topology is\nchanging the search space itself.  It is a=\r\nn entirely different type of\noperation than moving *within* a particular se=\r\narch space.  Adding a\nnew connection adds a new dimension to space.  That i=\r\ns not the same as\nmoving in any particular direction along any particular d=\r\nimension,\nwhich is what you mean by &quot;local search.&quot;\n\nIn a broader picture, =\r\ngradient search is not the panacea that we need.\nIt is exactly the failures=\r\n of such search that are the reason that\nwe are not able to handle these ve=\r\nry difficult real world problems\nthat you mention.  The big challenge is th=\r\nat the gradient in a massive\nmultidimensional space on a complex problem is=\r\n almost certainly highly\ndeceptive, and gradient techniques have nothing go=\r\ning for them other\nthan the gradient!  If the gradient is all we have to go=\r\n on, how can\nthat be a good thing?  \n\nIt will certainly be necessary to cha=\r\nnge topologies as a part of any\neffective formula for success on extremely =\r\nhard problems, among other\ningredients that are are still being discovered =\r\nand invented today.\n\nken\n\n\n--- In neat@yahoogroups.com, &quot;afcarl2&quot; &lt;a.carl@.=\r\n..&gt; wrote:\n&gt;\n&gt; Peter,\n&gt; \n&gt; It appears that you are implicitly assuming that=\r\n that the only way \n&gt; to &quot;reach out and touch&quot; the fitness evaluation is vi=\r\na a NEAT \n&gt; fabricated organism topology. Local search does not have to pro=\r\nceed \n&gt; via organism topology. \n&gt; \n&gt; Usage of NEAT as a global search metho=\r\nd, as part of a hierarchical \n&gt; methodology, surely uses organism topology =\r\nas the local search start \n&gt; point. And in the case of your proposed backpr=\r\nop search on organism \n&gt; weight values, maintains the same number and compo=\r\nsition of nodes and \n&gt; associated connectivity. But a local search method c=\r\nould just as \n&gt; easily wander free in the local fitness landscape, un-encum=\r\nbered by \n&gt; organism topology issues, to find the local minima.\n&gt; \n&gt; The ve=\r\nry strong point of GA can also be its greatest weakness, in \n&gt; instances in=\r\n which the computational resource requirements of the \n&gt; fitness evaluation=\r\n, in light of the dimensionality and hyper volume \n&gt; size and complexity, b=\r\necome non-trivial.\n&gt; \n&gt; This is especially true in the case of non-adaptive=\r\n mutation \n&gt; parameters, as is currently the case with NEAT. There are many=\r\n times \n&gt; in which it is orders of magnitude quicker for a non-GA local \n&gt; =\r\ngradient search method to find a local minima, than an equivalent GA \n&gt; to =\r\nmutate through generations to find the same local minima. \n&gt; \n&gt; NEAT specia=\r\ntion and niche protection help to mitigate the problem via \n&gt; population si=\r\nze and maintaining multiple species, but at a \n&gt; computational cost. But a =\r\nhierarchical search methodology can apply \n&gt; the strengths of both GA and l=\r\nocal search, without having to use \n&gt; computational power to cover-up GA&#39;s =\r\nweak points.\n&gt; \n&gt; The remaining question as to whether to a) re-encode the =\r\nfinal \n&gt; destination of the local search back into the organism&#39;s topology,=\r\n or \n&gt; b) simply take the final/best fitness derived by the local search \n&gt;=\r\n (using the organism&#39;s original topology as the start point), and \n&gt; associ=\r\nate it with the organism and it&#39;s original topology, is up for \n&gt; debate an=\r\nd/or personal preference.\n&gt; \n&gt; The moral of the story is that local search =\r\ndoes not have to be \n&gt; constrained by organism topology, beyond that of pro=\r\nviding the start \n&gt; point. \n&gt; \n&gt; Real world problems are so complex in ligh=\r\nt of current computer \n&gt; speeds and fitness computation requirements, to re=\r\nnder GA alone to be \n&gt; computationally unpractical in many instances, even =\r\nwith the obvious \n&gt; benefits that NEAT brings to the table.\n&gt; \n&gt; \n&gt; \n&gt; --- =\r\nIn neat@yahoogroups.com, &quot;petar_chervenski&quot; \n&gt; &lt;petar_chervenski@&gt; wrote:\n&gt;=\r\n &gt;\n&gt; &gt; Well actually speciation takes care of this. Species are allowed to =\r\n\n&gt; &gt; exist until they stagnate for too long time. If some new structure \n&gt; =\r\n&gt; appears through mutations, the mutated individuals are separated in \n&gt; &gt; =\r\nanother species. Each species is a local protected competition \n&gt; among \n&gt; =\r\n&gt; individuals grouped by similarity. Consider it as a GA performed on \n&gt; &gt; =\r\nnear identical topologies. Then you can see NEAT as a algorithm \n&gt; &gt; runnin=\r\ng multiple GAs. So this is actually what you mean by dynamic \n&gt; &gt; programmi=\r\nng.. or something. In fact this scheme is far better than \n&gt; &gt; it. \n&gt; &gt; As =\r\nfor the idea of speculative structure, this is the core of NEAT \n&gt; &gt; and it=\r\n is actually Ken&#39;s idea :) \n&gt; &gt; Colin Green&#39;s idea is about phased searchin=\r\ng, as far as I know. It \n&gt; is \n&gt; &gt; that after some structure is added throu=\r\ngh complexifying, a \n&gt; &gt; simplifying phase kicks in, removing any unnecessa=\r\nry structure, \n&gt; thus \n&gt; &gt; returning the search down to a baseline low dime=\r\nntional space while \n&gt; &gt; retaining the fitness (because of elitism). \n&gt; &gt; \n=\r\n&gt; &gt; Peter\n&gt; &gt; \n&gt; &gt; --- In neat@yahoogroups.com, c f &lt;christofer_fransson@&gt; =\r\nwrote:\n&gt; &gt; &gt;\n&gt; &gt; &gt; In dynamic programming the idea is to divide the\n&gt; &gt; &gt; s=\r\nolution in steps and then for each step present a\n&gt; &gt; &gt; fixed number of pos=\r\nsible solutions.\n&gt; &gt; &gt; \n&gt; &gt; &gt; Collin Greens idea is that speculative nodes =\r\nare added\n&gt; &gt; &gt; to the solutions but it might take time/generations\n&gt; &gt; &gt; b=\r\nefore an added node are shown to be useful. \n&gt; &gt; &gt; \n&gt; &gt; &gt; Is it possible to=\r\n apply dynamic programming approach\n&gt; &gt; &gt; to this area, to evolve NEAT driv=\r\nen networks?\n&gt; &gt; &gt; \n&gt; &gt; &gt; To combine local optimization and dynamic program=\r\nming\n&gt; &gt; &gt; ideas?\n&gt; &gt; &gt; \n&gt; &gt; &gt; Br,\n&gt; &gt; &gt; Christofer\n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; =\r\n&gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; &gt; --- peta=\r\nr_chervenski &lt;petar_chervenski@&gt;\n&gt; &gt; &gt; wrote:\n&gt; &gt; &gt; \n&gt; &gt; &gt; &gt; Given the simp=\r\nlest topology (a perceptron\n&gt; &gt; &gt; &gt; structure), the local \n&gt; &gt; &gt; &gt; minima i=\r\ns just one. Perceptrons are always\n&gt; &gt; &gt; &gt; guaranteed to converge on \n&gt; &gt; &gt;=\r\n &gt; correct weights. But increasing the dimentionality\n&gt; &gt; &gt; &gt; of the soluti=\r\non \n&gt; &gt; &gt; &gt; increases the error surface&#39;s curvature as well. So\n&gt; &gt; &gt; &gt; mor=\r\ne dimentions \n&gt; &gt; &gt; &gt; means more complex error surface. The coolest thing\n&gt;=\r\n &gt; &gt; &gt; in NEAT is that \n&gt; &gt; &gt; &gt; when it increases the dimentionality of the=\r\n\n&gt; &gt; &gt; &gt; solution, the individuals \n&gt; &gt; &gt; &gt; are already located in a promis=\r\ning area of the new\n&gt; &gt; &gt; &gt; space. In fact \n&gt; &gt; &gt; &gt; those spaces are relate=\r\nd to each other - you don&#39;t\n&gt; &gt; &gt; &gt; know how the error \n&gt; &gt; &gt; &gt; surface is =\r\ngoing to look like when you enter the new\n&gt; &gt; &gt; &gt; space with more \n&gt; &gt; &gt; &gt; =\r\ndimentions. There are unlimited possibilities. \n&gt; &gt; &gt; &gt; So what local gradi=\r\nent search will do in essence is\n&gt; &gt; &gt; &gt; pushing the \n&gt; &gt; &gt; &gt; weights towar=\r\nds the *local* minumim.. It is not\n&gt; &gt; &gt; &gt; guaranteed that this \n&gt; &gt; &gt; &gt; is=\r\n the *solution*! It is simply because you don&#39;t\n&gt; &gt; &gt; &gt; know the solution&#39;s=\r\n \n&gt; &gt; &gt; &gt; dimentionality at first. It may require 3 or\n&gt; &gt; &gt; &gt; 21342532 dim=\r\nentions. \n&gt; &gt; &gt; &gt; Don&#39;t forget that NEAT complexifies solutions\n&gt; &gt; &gt; &gt; inc=\r\nrementaly. \n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; Peter\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; --- In neat@yahoogroups=\r\n.com, &quot;afcarl2&quot; &lt;a.carl@&gt;\n&gt; &gt; &gt; &gt; wrote:\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; In fact, it ma=\r\ny be that a substancial portion of\n&gt; &gt; &gt; &gt; the value-added of \n&gt; &gt; &gt; &gt; &gt; sp=\r\neciation and niche protection of infant\n&gt; &gt; &gt; &gt; organisms, is associated \n&gt;=\r\n &gt; &gt; &gt; &gt; with providing opportunity to accumulate\n&gt; &gt; &gt; &gt; sufficient neighb=\r\norhood \n&gt; &gt; &gt; &gt; &gt; evaluations to &quot;discover&quot; the same local minimia\n&gt; &gt; &gt; &gt; =\r\nover multiple \n&gt; &gt; &gt; &gt; &gt; generations, that a local search may discover in\n&gt;=\r\n &gt; &gt; &gt; one generation. \n&gt; &gt; &gt; &gt; And \n&gt; &gt; &gt; &gt; &gt; maintaining multiple species=\r\n in hope that one of\n&gt; &gt; &gt; &gt; the local minimia \n&gt; &gt; &gt; &gt; &gt; will in fact also=\r\n be the global minimia.\n&gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; --- In neat@yahoogroups.com, &quot;=\r\nafcarl2&quot; &lt;a.carl@&gt;\n&gt; &gt; &gt; &gt; wrote:\n&gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; If &quot;most individu=\r\nals in a species represented by\n&gt; &gt; &gt; &gt; a given \n&gt; &gt; &gt; &gt; topology&quot; \n&gt; &gt; &gt; &gt;=\r\n &gt; &gt; ended up in &quot;the same local minimia&quot;, one could\n&gt; &gt; &gt; &gt; argue that the=\r\n \n&gt; &gt; &gt; &gt; &gt; &gt; subject specie&#39;s logical end point was the same\n&gt; &gt; &gt; &gt; local=\r\n minimia, \n&gt; &gt; &gt; &gt; and \n&gt; &gt; &gt; &gt; &gt; &gt; that the cost of maintaining more than =\r\none\n&gt; &gt; &gt; &gt; organism was \n&gt; &gt; &gt; &gt; &gt; &gt; computationally wasteful. Better to k=\r\nnow sooner\n&gt; &gt; &gt; &gt; and breed \n&gt; &gt; &gt; &gt; &gt; additional \n&gt; &gt; &gt; &gt; &gt; &gt; organisms o=\r\nf differing topology so as to\n&gt; &gt; &gt; &gt; maintain the population \n&gt; &gt; &gt; &gt; &gt; si=\r\nze \n&gt; &gt; &gt; &gt; &gt; &gt; and maximize the population&#39;s &quot;effective&quot;\n&gt; &gt; &gt; &gt; diversity=\r\n.\n&gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; Paying more for the same answer does not make it=\r\n\n&gt; &gt; &gt; &gt; a better answer.\n&gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; --- In neat@yahoogroups.=\r\ncom, &quot;petar_chervenski&quot; \n&gt; &gt; &gt; &gt; &gt; &gt; &lt;petar_chervenski@&gt; wrote:\n&gt; &gt; &gt; &gt; &gt; &gt;=\r\n &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; Well I think that encoding the resulting\n&gt; &gt; &gt; &gt; weights b=\r\nack to the \n&gt; &gt; &gt; &gt; &gt; genome \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; would somehow hurt the populati=\r\non weight\n&gt; &gt; &gt; &gt; diversity, since most \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; individuals in a spe=\r\ncies represented by a\n&gt; &gt; &gt; &gt; given topology can \n&gt; &gt; &gt; &gt; end \n&gt; &gt; &gt; &gt; &gt; up=\r\n \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; in the same local minima, thus leaving out a\n&gt; &gt; &gt; &gt; specie=\r\ns with the \n&gt; &gt; &gt; &gt; &gt; &gt; nearly \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; same individuals, i.e. clones=\r\n. \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; This is why I think that backprop should be\n&gt; &gt; &gt; &gt; applie=\r\nd occasionaly \n&gt; &gt; &gt; &gt; &gt; &gt; after \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; long periods of stagnation,=\r\n for example the\n&gt; &gt; &gt; &gt; cases where delta-\n&gt; &gt; &gt; &gt; &gt; &gt; coding \n&gt; &gt; &gt; &gt; &gt; &gt;=\r\n &gt; kicks in, when it focuses the search in the\n&gt; &gt; &gt; &gt; most promising \n&gt; &gt; =\r\n&gt; &gt; areas \n&gt; &gt; &gt; &gt; &gt; of \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; the search space. \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; I a=\r\nm still trying to re-implement RTRL myself,\n&gt; &gt; &gt; &gt; though.. Then \n&gt; &gt; &gt; &gt; =\r\nI&#39;ll \n&gt; &gt; &gt; &gt; &gt; &gt; see \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; if it is going to actually enhance\n&gt; &gt;=\r\n &gt; &gt; performance. \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; Peter\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; =\r\n&gt; &gt; &gt; &gt; --- In neat@yahoogroups.com, &quot;Kenneth Stanley&quot;\n&gt; &gt; &gt; &gt; &lt;kstanley@&gt; =\r\n\n&gt; &gt; &gt; &gt; wrote:\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; Rafael, thank you for point=\r\ning out the\n&gt; &gt; &gt; &gt; connection to memetic \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; algorithms.  Tha=\r\nt is good to point out that\n&gt; &gt; &gt; &gt; such a \n&gt; &gt; &gt; &gt; combination \n&gt; &gt; &gt; &gt; &gt; =\r\n&gt; &gt; falls \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; under that category.\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; =\r\n&gt; &gt; &gt; However, there are still those who would\n&gt; &gt; &gt; &gt; argue that the local=\r\n \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; search \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; method should not be encoded back i=\r\nnto the\n&gt; &gt; &gt; &gt; genome, that is, \n&gt; &gt; &gt; &gt; &gt; that \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; evolution=\r\n should simply search for the best\n&gt; &gt; &gt; &gt; starting point \n&gt; &gt; &gt; &gt; from \n&gt; =\r\n&gt; &gt; &gt; &gt; &gt; &gt; which \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; a local search would depart.  Because of=\r\n the\n&gt; &gt; &gt; &gt; Baldwin Effect, \n&gt; &gt; &gt; &gt; &gt; that \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; may \n&gt; &gt; &gt; &gt; &gt; =\r\n&gt; &gt; &gt; even work better.\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; Personally, I do n=\r\not know which approach\n&gt; &gt; &gt; &gt; would work better \n&gt; &gt; &gt; &gt; but \n&gt; &gt; &gt; &gt; &gt; &gt; =\r\nboth \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; are viable and it is probably domain\n&gt; &gt; &gt; &gt; dependen=\r\nt.\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; ken\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; --=\r\n- In neat@yahoogroups.com, &quot;Rafael C.P.&quot;\n&gt; &gt; &gt; &gt; &lt;kurama.youko.br@&gt; \n&gt; &gt; &gt; =\r\n&gt; &gt; &gt; &gt; &gt; wrote:\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; Ken, it doesn&#39;t fit pu=\r\nre evolution but it\n&gt; &gt; &gt; &gt; fits memetic \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; algorithms, \n&gt; &gt; &gt; =\r\n&gt; &gt; &gt; &gt; &gt; that\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; consists exactly of evolution alternated\n&gt;=\r\n &gt; &gt; &gt; with local search \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; methods \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; for fine\n&gt;=\r\n &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; tunning (just few steps). NEAT+BP may\n&gt; &gt; &gt; &gt; become a goo=\r\nd memetic \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; algorithm for\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; neural networks.=\r\n\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; On Mon, Mar 10, 2008 at 2:19 PM, Kenn=\r\neth\n&gt; &gt; &gt; &gt; Stanley &lt;kstanley@&gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; wrote:\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; =\r\n\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;   Peter, I believe that backprop can\n&gt; &gt; &gt; &gt; potential=\r\nly improve \n&gt; &gt; &gt; &gt; the\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; accuracy. It has been shown to =\r\nwork\n&gt; &gt; &gt; &gt; effectively with \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; neurevolution\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; =\r\n&gt; &gt; in classification tasks in the past. So\n&gt; &gt; &gt; &gt; in principle it \n&gt; &gt; &gt; =\r\n&gt; &gt; could\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; help. Of course, there is always the\n&gt; &gt; &gt; &gt; =\r\nchance that it will \n&gt; &gt; &gt; &gt; not\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; enhance performance as=\r\n well.\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; One issue I would also consi=\r\nder is that\n&gt; &gt; &gt; &gt; some people \n&gt; &gt; &gt; &gt; &gt; disagree \n&gt; &gt; &gt; &gt; &gt; &gt; on\n&gt; &gt; &gt; &gt;=\r\n &gt; &gt; &gt; &gt; &gt; &gt; whether the changes to weights from\n&gt; &gt; &gt; &gt; backprop should be=\r\n \n&gt; &gt; &gt; &gt; &gt; &gt; encoded \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; back\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; into the ge=\r\nnome or not. If it is\n&gt; &gt; &gt; &gt; actually encoded back \n&gt; &gt; &gt; &gt; into \n&gt; &gt; &gt; &gt; =\r\n&gt; &gt; the\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; genome, that is &quot;Lamarckian&quot; evolution\n&gt; &gt; &gt; &gt; =\r\nbecause in effect \n&gt; &gt; &gt; &gt; &gt; what \n&gt; &gt; &gt; &gt; &gt; &gt; &gt; the\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; or=\r\nganism learned over its lifetime is\n&gt; &gt; &gt; &gt; encoded into its own\n&gt; &gt; &gt; &gt; \n&gt;=\r\n &gt; &gt; =3D=3D=3D message truncated =3D=3D=3D\n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; &gt;      =\r\n \n&gt; &gt; \n&gt; __________________________________________________________________=\r\n____\n&gt; &gt; ______________\n&gt; &gt; &gt; Never miss a thing.  Make Yahoo your home pag=\r\ne. \n&gt; &gt; &gt; http://www.yahoo.com/r/hs\n&gt; &gt; &gt;\n&gt; &gt;\n&gt;\n\n\n\n"}}