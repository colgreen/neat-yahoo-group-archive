{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"zcTp5KFCAF5swt0ZdnMr0IpNZPZamuvAO8mc3iD0slrW4euEqWZAAwxVhCcetq2hMyIYI4OLqH12BOXTYE1f8jzhbowMoKdImn_mlbSC8B-Y","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Different activation methods in NEAT4J","postDate":"1184416914","msgId":3462,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGY3YWdhaStxNGNuQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGRkZjEwMDc4MDcwNzEwMTMxM2g1NmFmZjc1Ymk0ZDllM2M3MDM0NGZjMGM2QG1haWwuZ21haWwuY29tPg=="},"prevInTopic":3461,"nextInTopic":3466,"prevInTime":3461,"nextInTime":3463,"topicId":3459,"numMessagesInTopic":6,"msgSnippet":"Cesar, I m not sure if I understand completely how your activation method works, but it sounds like you may not be flushing the network between the four trials","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 15952 invoked from network); 14 Jul 2007 12:41:57 -0000\r\nReceived: from unknown (66.218.66.70)\n  by m46.grp.scd.yahoo.com with QMQP; 14 Jul 2007 12:41:57 -0000\r\nReceived: from unknown (HELO n8b.bullet.sp1.yahoo.com) (69.147.64.169)\n  by mta12.grp.scd.yahoo.com with SMTP; 14 Jul 2007 12:41:57 -0000\r\nReceived: from [216.252.122.218] by n8.bullet.sp1.yahoo.com with NNFMP; 14 Jul 2007 12:41:54 -0000\r\nReceived: from [66.218.69.2] by t3.bullet.sp1.yahoo.com with NNFMP; 14 Jul 2007 12:41:54 -0000\r\nReceived: from [66.218.66.85] by t2.bullet.scd.yahoo.com with NNFMP; 14 Jul 2007 12:41:54 -0000\r\nDate: Sat, 14 Jul 2007 12:41:54 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;f7agai+q4cn@...&gt;\r\nIn-Reply-To: &lt;ddf100780707101313h56aff75bi4d9e3c70344fc0c6@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: Re: Different activation methods in NEAT4J\r\nX-Yahoo-Group-Post: member; u=54567749; y=aibjVx8C3Oe_KPhK_pp3RYNygrCdSBXhWiGDST7ETYdcadziRbCY\r\nX-Yahoo-Profile: kenstanley01\r\n\r\nCesar, I&#39;m not sure if I understand completely how your activation \nmethod =\r\nworks, but it sounds like you may not be flushing the network \nbetween the =\r\nfour trials of XOR.  If you don&#39;t flush the network \nbetween trials, evolut=\r\nion can actually memorize the order that you \npresent them, changing the pr=\r\noblem so that it is in effect no longer \nXOR.  Rather, the new problem is, =\r\n&quot;memorize the order of outputs to \nget a good score.&quot;  It is possible that =\r\nin your example that is what \nit did.\n\nken\n\n--- In neat@yahoogroups.com, &quot;C=\r\nesar G. Miguel&quot; &lt;cesargm@...&gt; wrote:\n&gt;\n&gt; Hi there,\n&gt; \n&gt; I&#39;ve been experimen=\r\nting with two different methods for neural \nnetwork\n&gt; update in NEAT4J.\n&gt; \n=\r\n&gt; I&#39;ve used the same settings as Matt&#39;s: tahn(x) activation for hidden\n&gt; no=\r\ndes and logistic(x) for the output node.  There are two stop\n&gt; criteria: (1=\r\n) when the error is below 0.1 or (2) the number of\n&gt; generations is greater=\r\n than 100.\n&gt; \n&gt; For a detailed parameters list, please check:\n&gt; http://neat=\r\n4j.sourceforge.net/documents/config.html\n&gt; \n&gt; NEAT4J recursively activates =\r\nthe neurons linked to the output layer.\n&gt; This is its performance for XOR:\n=\r\n&gt; \n&gt; 20 runs:\n&gt; -----------------------------------------\n&gt;               G=\r\nen.      Hidden      Connections\n&gt; Avg.:      40.9      3.55          8.75\n=\r\n&gt; Std.:       7.16      1.54          2.45\n&gt; ------------------------------=\r\n-----------\n&gt; \n&gt; This is my modified version of NEAT4J (I removed some clas=\r\nses I \ndon&#39;t\n&gt; need and implemented a synchronous updating method for the n=\r\neural\n&gt; network). The results are:\n&gt; \n&gt; -----------------------------------=\r\n------\n&gt;               Gen.      Hidden      Connections\n&gt; Avg.:      18.95=\r\n     1.6            4.2\n&gt; Std.:       4.95      0.82          1.32\n&gt; ------=\r\n-----------------------------------\n&gt; \n&gt; The only difference here is that t=\r\nhe neural network is updated using\n&gt; information from the previous step onl=\r\ny.\n&gt; \n&gt; Please note that following this methodology the network has to be\n&gt;=\r\n activated in a &quot;dynamical way&quot;, even for supervised training such as\n&gt; XOR=\r\n. The input patters are presented to the network as follows:\n&gt; \n&gt; (0,0) at =\r\ntime 1, (0,1) at time 2, (1,0) at time 3 and (1,1) at time \n4.\n&gt; \n&gt; At time=\r\n 1 the output neuron is activated using the activation from \nthe\n&gt; hidden n=\r\neuron at time t0 (which is set to zero) and the second input\n&gt; value, while=\r\n the hidden neuron only uses information from the \ninputs.\n&gt; \n&gt; At step 2 t=\r\nhe output neuron uses the activation of the hidden neuron\n&gt; at time 1 and t=\r\nhe second input.\n&gt; \n&gt; And so on..\n&gt; \n&gt; In ordinary activation methods for f=\r\need-forward networks the same \ndoes\n&gt; not happen. We first need to activate=\r\n the neuron from the first \nlayer\n&gt; and then proceed to the next layer. To =\r\nachieve the same behavior \nwe&#39;d\n&gt; need to activate the network as many time=\r\ns as the numbers of neurons\n&gt; it has.\n&gt; \n&gt; It seems that evolution can take=\r\n advantage of this method and thus,\n&gt; achieve the expected error value in f=\r\newer steps exploring the\n&gt; sequential values presented to the inputs.\n&gt; \n&gt; =\r\nThe winner is attached :-)\n&gt; \n&gt; Cesar\n&gt;\n\n\n\n"}}