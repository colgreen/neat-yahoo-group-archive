{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":127853030,"authorName":"Colin Green","from":"Colin Green &lt;cgreen@...&gt;","profile":"alienseedpod","replyTo":"LIST","senderId":"GU_HbW9MdVWKWbqVemA8TOZUzxzvx_HX-rovImpDFASUbatsEUrdf9Q15PtxAEZZLiTpkFgqq8il8lU0fQ6__a-CB8TPFDzbUw","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] Neural network optimization details","postDate":"1087934901","msgId":1123,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQwRDg5MUI1LjEwOTAzMDRAZHNsLnBpcGV4LmNvbT4=","inReplyToHeader":"PEJBWTItRjk0VmpFc0s2OTNpc3AwMDAxM2Y2OUBob3RtYWlsLmNvbT4=","referencesHeader":"PEJBWTItRjk0VmpFc0s2OTNpc3AwMDAxM2Y2OUBob3RtYWlsLmNvbT4="},"prevInTopic":1119,"nextInTopic":1124,"prevInTime":1122,"nextInTime":1124,"topicId":1106,"numMessagesInTopic":7,"msgSnippet":"Hi John, ... I accept the point that using the technique of propagating an activation wave through a network requires far less operations and is therefore ","rawEmail":"Return-Path: &lt;cgreen@...&gt;\r\nX-Sender: cgreen@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 56022 invoked from network); 22 Jun 2004 20:08:40 -0000\r\nReceived: from unknown (66.218.66.172)\n  by m23.grp.scd.yahoo.com with QMQP; 22 Jun 2004 20:08:40 -0000\r\nReceived: from unknown (HELO pengo.systems.pipex.net) (62.241.160.193)\n  by mta4.grp.scd.yahoo.com with SMTP; 22 Jun 2004 20:08:40 -0000\r\nReceived: from dsl.pipex.com (81-86-175-101.dsl.pipex.com [81.86.175.101])\n\tby pengo.systems.pipex.net (Postfix) with ESMTP id 351C34C0023A\n\tfor &lt;neat@yahoogroups.com&gt;; Tue, 22 Jun 2004 21:08:16 +0100 (BST)\r\nMessage-ID: &lt;40D891B5.1090304@...&gt;\r\nDate: Tue, 22 Jun 2004 21:08:21 +0100\r\nUser-Agent: Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.5) Gecko/20031007\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: neat@yahoogroups.com\r\nReferences: &lt;BAY2-F94VjEsK693isp00013f69@...&gt;\r\nIn-Reply-To: &lt;BAY2-F94VjEsK693isp00013f69@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Remote-IP: 62.241.160.193\r\nFrom: Colin Green &lt;cgreen@...&gt;\r\nSubject: Re: [neat] Neural network optimization details\r\nX-Yahoo-Group-Post: member; u=127853030\r\nX-Yahoo-Profile: alienseedpod\r\n\r\nHi John,\n\nJohn Arrowwood wrote:\n\n&gt;I looked it over.  Some thoughts:\n&gt;\n&gt;If I understood correctly, you are still activating in time steps.  For a \n&gt;5-layer network, you still have to call it 5 times.  Thus, it is taking 5 \n&gt;times longer than it needs to, and doing 5 times the work.  If you weren&#39;t \n&gt;going to use generated code, like I did, then you would want to build an \n&gt;in-memory data structure that you could step through quickly and easily so \n&gt;that you could activate only once.  You would only have to build that \n&gt;structure once.  How many times you need to activate the network determines \n&gt;whether or not you make a return on your investment.\n&gt;  \n&gt;\nI accept the point that using the technique of propagating an activation \n&#39;wave&#39; through a network requires far less operations and is therefore \ngoing to be faster than a time-step approach, and  I&#39;ll be honest and \nadmit I haven&#39;t put much thought into this approach yet, and what its \nimplications are. What I will say though is that I don&#39;t see one \ntechnique as being better than the other, I think they are both valid \ntechniques and each is more applicable to certain types of experiment. \ne.g. in continuous control experiments you might feed a signal into a \nnetwork on each timestep and read the output on each timestep, there \nwill be a lag if the network is deep but that doesn&#39;t matter if the \ntimestep is very small,  this is biologically inspired and this type of \nexperiment fits in better with the time-step network. Alternatively you \nmight want to apply an input signal, allow the signals to propogate all \nthe way through the net and THEN read the outputs - this is better \nsuited to the &#39;propagation wave&#39; technique.\n\nRight now I&#39;m invested in the time-step approach, (A) because it handles \nrecurrent connections very simply and (B) it is applicable to both types \nof experiment described above. So I suppose what I&#39;m saying is - I see \nthe time-step net as general purpose, and the wave-propagation net as \nspecialist. I accept the later to be more efficient though for certian \ntypes of experiment.\n\n&gt;Also, you are doing all of the sigmoids one after another.  I found a \n&gt;significant improvement if I did the sigmoid as soon as the data was \n&gt;available for it.  I got better parallelism that way.  You might give it a \n&gt;try.\n&gt;  \n&gt;\n\nyep, I&#39;m still trying to sqeeze a bit more speed out of my code so I&#39;ll \ngive it a try. I was thinking of keeping the instructions close together \nand seeing if I could invoke some SSE/SSE2 instructions (via some C++ \nextensions in .NET),  but it turns out that the SSE register is only 8 \nbytes, which means that you can only work on two floats at a time- so \nnot worth the bother of messing up my clean code with COM/C++/Assembler \nextensions I recon.\n\nHere&#39;s another interesting piece of code I unturfed on the net. Say you \nwant to add all of the integers in an array, you might do this:\n\n--------------------------------------\nint sum=0;\nfor(int i=0; i&lt;myarray.length; i++)\n   sum += myarray[i];\n--------------------------------------\n\nNow because the result of each addition is being stored in &#39;sum&#39; the CPU \ncan&#39;t use it&#39;s multiple ALU&#39;s (Arithmetic & Logic Unit&#39;s) in parallel. \nNot unless the compiler does something clever equivalent to the \nfollowing code (Assuming 3 ALU&#39;s are available as in AMD Athlon):\n--------------------------------------\nint sum=0;\nint sum0=0;\nint sum1=0;\nint sum2=0;\nfor(int i=0; i&lt;myarray.length; i+=3)\n{\n   sum0 += myarray[i];\n   sum1 += myarray[i+1];\n   sum2 += myarray[i+2];\n}\nsum = sum0+sum1+sum2;\n--------------------------------------\nNote that the array length has to be a multiple of 3 in this case to \nprevent an overrun.\n\nIt just so happens that the Athlon has 3 FPUs also, so definitely worth \nsome investigation this one.\n\nColin.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&gt;  \n&gt;\n&gt;&gt;From: Colin Green &lt;cgreen@...&gt;\n&gt;&gt;Reply-To: neat@yahoogroups.com\n&gt;&gt;To: neat@yahoogroups.com\n&gt;&gt;Subject: [neat] Neural network optimization details\n&gt;&gt;Date: Sun, 20 Jun 2004 16:09:07 +0100\n&gt;&gt;\n&gt;&gt;For those following the &#39;computation time&#39; thread I have written up the\n&gt;&gt;details of the optimizations I have performed on my network code along\n&gt;&gt;with some benchmarks. The details are at\n&gt;&gt;http://www.cgreen.dsl.pipex.com/network_optimization.htm\n&gt;&gt;\n&gt;&gt;Hopefully this will work, my ISP has a habit of randomly deleting file\n&gt;&gt;juts lately!\n&gt;&gt;\n&gt;&gt;BTW my initial 12x improvement claim has been reduced to 8x, this was\n&gt;&gt;due to my not using the code optimization consistently. This is still\n&gt;&gt;pretty good though.\n&gt;&gt;\n&gt;&gt;Colin.\n&gt;&gt;    \n&gt;&gt;\n&gt;  \n&gt;\n\n\n\n"}}