{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":403065338,"authorName":"St√©phane Doncieux","from":"=?ISO-8859-1?Q?St=E9phane_Doncieux?= &lt;stephane.doncieux@...&gt;","profile":"stephane.doncieux","replyTo":"LIST","senderId":"v8y6tE9pSgKHMlH28_eykkJCgrtjzg-AoxihIkk_kVxzyYUIVpL9LNsySXmtnK22lCpZqCHxJLAsA96uO9dRxCyCOd9WQL6sT-zy8q2V4ZIKquaVObf87SNegP3M3laye20rTkBaQmPJ","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: New paper on why modules evolve, and how to evolve modular artif","postDate":"1361956016","msgId":6011,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDUxMkRDQ0IwLjMwNTAxMDJAZ21haWwuY29tPg=="},"prevInTopic":0,"nextInTopic":6012,"prevInTime":6010,"nextInTime":6012,"topicId":6011,"numMessagesInTopic":10,"msgSnippet":"Hi Ken, Thanks for the acknowledgement on our work. I m still not sure to really catch your point. It seems to me that the main criticism you have against the","rawEmail":"Return-Path: &lt;stephane.doncieux@...&gt;\r\nX-Sender: stephane.doncieux@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 88506 invoked from network); 27 Feb 2013 09:07:00 -0000\r\nX-Received: from unknown (10.193.84.163)\n  by m7.grp.bf1.yahoo.com with QMQP; 27 Feb 2013 09:07:00 -0000\r\nX-Received: from unknown (HELO mail-ee0-f49.google.com) (74.125.83.49)\n  by mta3.grp.bf1.yahoo.com with SMTP; 27 Feb 2013 09:06:59 -0000\r\nX-Received: by mail-ee0-f49.google.com with SMTP id d41so236743eek.22\n        for &lt;neat@yahoogroups.com&gt;; Wed, 27 Feb 2013 01:06:59 -0800 (PST)\r\nX-Received: by 10.14.207.200 with SMTP id n48mr4450903eeo.4.1361956019252;\n        Wed, 27 Feb 2013 01:06:59 -0800 (PST)\r\nReturn-Path: &lt;stephane.doncieux@...&gt;\r\nX-Received: from MacBook-Pro-de-Stephane-Doncieux.local ([134.157.18.73])\n        by mx.google.com with ESMTPS id 44sm5275705eek.5.2013.02.27.01.06.57\n        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);\n        Wed, 27 Feb 2013 01:06:58 -0800 (PST)\r\nMessage-ID: &lt;512DCCB0.3050102@...&gt;\r\nDate: Wed, 27 Feb 2013 10:06:56 +0100\r\nUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv:17.0) Gecko/20130107 Thunderbird/17.0.2\r\nMIME-Version: 1.0\r\nTo: neat@yahoogroups.com\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: =?ISO-8859-1?Q?St=E9phane_Doncieux?= &lt;stephane.doncieux@...&gt;\r\nSubject: Re: New paper on why modules evolve, and how to evolve modular artif\r\nX-Yahoo-Group-Post: member; u=403065338; y=nnU3HUIZloJOqyGQbEA0X19FlHYpXcSjButhfcYpm2kFOHEMYpIWFodPJTo\r\nX-Yahoo-Profile: stephane.doncieux\r\n\r\nHi Ken,\n\nThanks for the acknowledgement on our work.\n\nI&#39;m still not sure to really catch your point. It seems to me that the\nmain criticism you have against the connexion cost objective is that it\nis (1) constant  and that (2) its optimal value is trivially\nunefficient. Both aspects seem to be a problem for you and I can\nunderstand it if you put it in an open-ended evolution perspective\n(although I can&#39;t say if you are right or not). But you argue then about\nencoding biases that do not have these drawbacks, if I get it right. But\nthis is were I don&#39;t follow you. I can easily build an encoding with a\nbias towards a low connectivity : let&#39;s say that I choose a higher\nmutation rate for removing connections than for adding one.  Would it\nhave better properties in your opinion? For me it seems, at least from\nan open-ended evolution point of view, quite equivalent. If you start\nfrom one particular network, in the connection cost objective case (and\nnot biased encoding), you will generate networks with more or less\nconnections and the fitness pressure will be in favor of networks with\nless connections. As it is not the only pressure, other networks will\nalso survive if they are different (if you have a pressure towards\ndiversity or novelty) or better (because of the goal oriented\nobjective). In the case of a biased encoding, you will directly generate\nfewer networks with more connections. The end results is then not that\ndifferent, is it?\n\nYou said: &quot; By the way, applying an objective fitness function will\nundermine the potential for canalization (with CPPNs or anything else)\nbecause of the tendency of objectives to wreck the representation.&quot;\nI don&#39;t follow you here and I don&#39;t see why you say that. Your encoding\nneed some specific properties to exhibit canalization, but I do not see\nwhy an objective fitness function would undermine it. Actually, in the\nPicBreeder experiments you mention, you don&#39;t uses a &quot;classical&quot;\nselection pressure. You uses interactive evolution. So, for me, this\nwork  do not concern only encoding, but also selection pressures.\n\nYou also said: &quot; We want to make no a priori assumptions\nabout what must be correct in all generations, and instead allow\nevolution to try out everything.&quot;\nWhat I do not understand is that you choose the bias to put in your\nencoding. You choose what will be favored and this choice will remain\nall along evolution: in both cases, there is a favored and chosed\ndirection, may it be through the encoding or through the selection pressure.\n\nIt seems to me that your comparison between encoding bias and selection\npressures is not fair. Your argument is more related to other properties\nof the encoding. I would not call it a bias, but rather the expressivity\nof the encoding (like the generative aspect). I agree that generative\nproperties are mandatory to generate complex neural networks, I don&#39;t\nthink that there is a debate about that.\n\nThe question is: will the encoding alone, be enough? I think not. With\nJean-Baptiste, we had a paper at CEC in 2009 in which we observed that a\nmodular encoding was actually less efficient than a non modular encoding\n(on a problem for which modular solutions did exist) except if there was\na selection pressure aligned with the encoding features (a pressure\ntowards having efficient modules for the problem we have considered).\nThe selection pressure alone was not efficient either. Actually, I am\nconvinced that one of the features that make NEAT efficient is that it\nincludes a specific selection pressure and it is undoubtedly the same\nfor your picbreeder experiment. If I remember right, you have shown that\na goal-oriented fitness can&#39;t generate the pictures generated by\ninteractive evolution. This is one more evidence that the selection\npressure has a strong impact.\n\nAn interesting question: is what would be the equivalent of a generative\nencoding in terms of fitness pressures ? What selection pressure would\n&quot;align&quot; with generative properties of an encoding? As you said,\nevolution has no long term objective, so why would a biased encoding\nconverge towards interesting solutions if there is no selection pressure\nthat drives it at one moment or another? If the drift is enough, it\nmeans that your bias is very strong. It also probably mean that you have\nput the knowledge of your stepping stones into the encoding. It is not\nbetter than putting this knowledge into the fitness. It would actually\nbe better not to require it at all.\n\nNovelty is interesting in this perspective, but I am not sure that it\ncompletely solves the problem. The behaviors you will find will\ncorrespond to the behavior features that you have defined and I wonder\nif it would scale indefinitely: comparing behaviors with that of the\narchive will require an increasing amount of time.\n\nBy the way, the connection cost of Jeff, JB and Hod anyway does not aim\nat that. Their goal is different. What is interesting is that the\nobjective is not targetted at modularity, but encourages it as a side\neffect. I like it because it is argued, justified (you may not agree\nwith their arguments, but they were not pulling a rabbit out of a hat)\nand that its impact has been largely studied. We can then expect that\nthe result are, relatively, general, although it requires to be\nconfirmed, of course.\n\nSo to conclude, the debate, for me, is not encoding vs fitness, but goal\noriented vs goal independent, may it be in the fitness or in the encoding.\n\nThank you Ken for this opportunity to discuss such points. I would like\nto stress that my arguments are not against your work, that I find\nalways inspiring. Working on selection pressures goes against the\nhabbits coming from optimization and machine learning communities were\nthe cost function is part of the problem definition. It has often been\nused as a trick to make runs converge, and I agree that it is not\nsatisfatory. The work on connection cost do not fit in this perspective\nin my opinion and I really think that selection pressures deserves now\ndeeper studies.\n\nBest,\n\nstephane\n\n\n[1] Mouret, J.-B. and Doncieux, S. (2009). Evolving modular\nneural-networks through exaptation.\nIEEE Congress on Evolutionary Computation, 2009 (CEC 2009). Pages\n1570--1577. \n\n\n\n"}}