{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":283334584,"authorName":"petar_chervenski","from":"&quot;petar_chervenski&quot; &lt;petar_chervenski@...&gt;","profile":"petar_chervenski","replyTo":"LIST","senderId":"A_JuCNW3073vPgHzQHec7ThxBjhist3B0hnPO2wf8dA3p2SCxgwUTomQbv-iV5VllceBM6e7wh8tHCV9WciCjE1b4IzdB32PH_djfV_JYb_PuRaIV0U","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: Python NEAT","postDate":"1310104560","msgId":5615,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGl2NjY1Zyt1MnZxQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGl2NWpxbCs1djFvQGVHcm91cHMuY29tPg=="},"prevInTopic":5614,"nextInTopic":5618,"prevInTime":5614,"nextInTime":5616,"topicId":535,"numMessagesInTopic":47,"msgSnippet":"Well, exactly. I didn t mention anything about evaluation, though, this is simply a function that takes arbitrary graph (a genome) and returns a fitness value.","rawEmail":"Return-Path: &lt;petar_chervenski@...&gt;\r\nX-Sender: petar_chervenski@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 4959 invoked from network); 8 Jul 2011 06:09:32 -0000\r\nX-Received: from unknown (66.196.94.106)\n  by m10.grp.re1.yahoo.com with QMQP; 8 Jul 2011 06:09:32 -0000\r\nX-Received: from unknown (HELO n44d.bullet.mail.sp1.yahoo.com) (66.163.169.158)\n  by mta2.grp.re1.yahoo.com with SMTP; 8 Jul 2011 06:09:32 -0000\r\nX-Received: from [69.147.65.173] by n44.bullet.mail.sp1.yahoo.com with NNFMP; 08 Jul 2011 05:56:02 -0000\r\nX-Received: from [98.137.34.34] by t15.bullet.mail.sp1.yahoo.com with NNFMP; 08 Jul 2011 05:56:02 -0000\r\nDate: Fri, 08 Jul 2011 05:56:00 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;iv665g+u2vq@...&gt;\r\nIn-Reply-To: &lt;iv5jql+5v1o@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nFrom: &quot;petar_chervenski&quot; &lt;petar_chervenski@...&gt;\r\nSubject: Re: Python NEAT\r\nX-Yahoo-Group-Post: member; u=283334584; y=mRX2nCi7zxXwjGEAm8vTgdEkSi-Kx0yP6OFqyla0M5jXUoOmUFUjN3_p2g\r\nX-Yahoo-Profile: petar_chervenski\r\n\r\nWell, exactly. I didn&#39;t mention anything about evaluation, though, this is =\r\nsimply a function that takes arbitrary graph (a genome) and returns a fitne=\r\nss value. A domain-dependent black box. Graphs are perfect for complexifica=\r\ntion. Most (or all) implementations of NEAT are designed for evolution of n=\r\neural networks, and modifying existing code to evolve any graph becomes tri=\r\ncky. For example, recurrence is treated like a special case sometimes and n=\r\no two nodes can have more than one (or 2 with recurrence) connections. Inpu=\r\nts and outputs are fixed and inputs don&#39;t get connected with links pointing=\r\n to them. Break one of these rules and NEAT becomes less effective at evolu=\r\ntion of neural networks - you may get what you want, but at the cost of dam=\r\naging the existing code that previously worked fine. It&#39;s better to start f=\r\nrom scratch and make the code as general as possible. NN evolution will be =\r\nderived from it. \n\nPeter\n\n--- In neat@yahoogroups.com, &quot;afcarl2&quot; &lt;afcarl2@.=\r\n..&gt; wrote:\n&gt;\n&gt; \n&gt; \n&gt; \n&gt; Peter,\n&gt; \n&gt; A couple of thoughts. First, IMO one of=\r\n the primary values of NEAT are the various methods of managing complexific=\r\nation of structure, not how the infrastructure is constrained to match NN e=\r\nvaluation. Many profoundly useful applications are entirely non-NN in natur=\r\ne, and the genome itself is the answer, not the NN evaluation or activation=\r\n output. The genome is passed thru a translator and the evaluation is handl=\r\ned by a separate analysis code. An example being the propulsion schematic o=\r\nf a rocket, where the genome directly represents the schematic, and the obj=\r\nective function is defined by how closely the resulting prediction matches =\r\ndesired performance on a weight and/or cost basis. The ability to seed the =\r\ninitial population with previous &quot;simular-to&quot; designs/genomes, allows the i=\r\nncorporation human expert input directly, and an obvious interpretation of =\r\nbest resulting genomes output. In this instance, the determination of direc=\r\nted/non-directed/multigraph is moot within NEAT, the appropriate interpreta=\r\ntion is made within the translator prior to execution by the external analy=\r\nsis code, given proper and adequate meta-data.\n&gt; \n&gt; In the other instance o=\r\nf direct evaluation of the network, the generality on varying number of nod=\r\ne inputs/output, evaluation functions of nodes and embedded networks within=\r\n a node can be addressed by interpretation of node type and genome definiti=\r\non nomenclature. It would seem that the explosion in size of the search spa=\r\nce would tend to be addressed via the complexification process as additiona=\r\nl dimensionality is justified as a consequence of incremental improved obje=\r\nctive function value.\n&gt; \n&gt; Andy\n&gt; \n&gt; --- In neat@yahoogroups.com, &quot;petar_ch=\r\nervenski&quot; &lt;petar_chervenski@&gt; wrote:\n&gt; &gt;\n&gt; &gt; Hi Andy, \n&gt; &gt; \n&gt; &gt; The impleme=\r\nntation is not yet complete, but things are really good so far. NetworkX is=\r\n a very good choice for the genomes, as it gives me a lot of freedom, many =\r\nbonus functions for the graphs, and saves me debugging time. Basically this=\r\n implementation allows any kind of graph to be evolved. For example, nodes =\r\nand edges can represent numeric data (integer & float) or objects from a se=\r\nt (like characters from an alphabet, class instances or functions, etc.). W=\r\nhen you define the graph type, you have to write a distance function if you=\r\n have objects from a set in the graph - like the distance between character=\r\ns in the alphabet), and also a few more functions like mutators, which, of =\r\ncourse, will change the objects randomly or the way you like. You can have =\r\nany number of properties for nodes/edges. You can also have an undirected g=\r\nraph, or a multigraph where many edges connect the same nodes. You can even=\r\n have nested genomes, where genomes are the objects from a set, which are a=\r\nttached to nodes or edges. This makes things mind-blowing and lifts NEAT to=\r\n a much broader set of domains. I even think that the N in NEAT is somehow =\r\nunnecessary here, as the primary objective in this implementation is not ne=\r\nural networks. Neural networks will be derived from a special function that=\r\n will translate the graph and then build a C++ object from it. They will be=\r\n directed graphs with sigmoid or whatever functions attached to nodes and f=\r\nloats attached to edges. (Node types also attached to nodes - to know what =\r\nis input and output). Perhaps I&#39;ll make a separate project designed for neu=\r\nral networks that will use the core module. \n&gt; &gt; I will release the first v=\r\nersion of the code soon, which will probably not have rtNEAT and novelty se=\r\narch built in. CPPNs and HyperNEAT are just special cases of graph evolutio=\r\nn and interpretation, like neural networks. The special code about them wil=\r\nl be added later as the project evolves. Perhaps the community will like it=\r\n and contribute some code. I can&#39;t promise a release date, but work is prog=\r\nressing. Any ideas to minimize the search space (which blows up as you add =\r\nmore properties to nodes and edges) are appreciated. Also I could use some =\r\nhelp about innovation numbers and crossover between undirected and multi gr=\r\naphs. I&#39;m so afraid of bugs in these cases that I haven&#39;t even started to t=\r\nhink about it. :D\n&gt; &gt; \n&gt; &gt; Peter\n&gt; &gt; \n&gt; &gt; --- In neat@yahoogroups.com, &quot;afc=\r\narl2&quot; &lt;afcarl2@&gt; wrote:\n&gt; &gt; &gt;\n&gt; &gt; &gt; Hi Peter,\n&gt; &gt; &gt; \n&gt; &gt; &gt; How is your pyth=\r\non implementation going? Took a look at the NetworkX module. It looks very =\r\ninteresting! In my C++ version, I had added variable input/output connectio=\r\nns, network within a node and seeding of the initial population with the re=\r\nquired infrastructure updates and a distributed processing backend. But wha=\r\nt you are doing goes so much farther, that I am eager to get a look at it!\n=\r\n&gt; &gt; &gt; \n&gt; &gt; &gt; Andy\n&gt; &gt; &gt; \n&gt; &gt; &gt; --- In neat@yahoogroups.com, &quot;petar_chervens=\r\nki&quot; &lt;petar_chervenski@&gt; wrote:\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; Hi all, \n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; I =\r\nam almost done with the basic code and I&#39;ll mention some of its features no=\r\nw. I decided to use NetworkX for the genomes, because this module has lots =\r\nof useful algorithms and allows any python object to be a node and edges ca=\r\nn be associated with anything. This makes the evolution of neural networks =\r\na tiny part of what&#39;s really possible. Any graph can be evolved, including =\r\nundirected graphs and nodes/edges containing discrete one-of-N values (inte=\r\ngers, lists of python objects, etc). So given that an evaluation function e=\r\nxists for any kind of graph, you can quickly setup evolution. Neural networ=\r\nks are a particular kind of graphs and the package will have built in code =\r\nnecessary to evolve neural networks - the initialization functions, mutator=\r\ns, and a C++ interface to a class that represents the phenotypes. CPPNs sup=\r\nport is trivial to make, and given that python functions themselves can be =\r\nattached to nodes, it&#39;s possible to have algorithmic nodes working with mor=\r\ne than one variable and .. well, infinite stuff. OK, I gotta go. Wish me lu=\r\nck debugging. Talk to you soon. :) \n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; Peter\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt;=\r\n --- In neat@yahoogroups.com, Jan van der Lugt &lt;janlugt@&gt; wrote:\n&gt; &gt; &gt; &gt; &gt;\n=\r\n&gt; &gt; &gt; &gt; &gt; Hi Peter,\n&gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; Sound like an ambitious and noble =\r\nplan. Good luck coding, I&#39;m looking\n&gt; &gt; &gt; &gt; &gt; forward to seeing your result=\r\ns!\n&gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; Regards,\n&gt; &gt; &gt; &gt; &gt; Jan\n&gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; On Mon,=\r\n Mar 14, 2011 at 12:43, petar_chervenski\n&gt; &gt; &gt; &gt; &gt; &lt;petar_chervenski@&gt;wrote=\r\n:\n&gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; Hi people,\n&gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt;=\r\n &gt; &gt; &gt; &gt; For about a month I&#39;ll be writing a Python implementation of NEAT,=\r\n which\n&gt; &gt; &gt; &gt; &gt; &gt; includes all advances in the recent years, including rtN=\r\nEAT, phased\n&gt; &gt; &gt; &gt; &gt; &gt; searching, leaky integrators, HyperNEAT, HyperNEAT =\r\nwith evolving substrates,\n&gt; &gt; &gt; &gt; &gt; &gt; and novelty search. Coevolution code =\r\nand visualizations will be included.\n&gt; &gt; &gt; &gt; &gt; &gt; Optimized C++ code for run=\r\nning the NNs too. This code will be free and I\n&gt; &gt; &gt; &gt; &gt; &gt; promise this wil=\r\nl be the best NEAT code I can write. No bugs, no meaningless\n&gt; &gt; &gt; &gt; &gt; &gt; NN=\r\ns, etc.\n&gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; Peter\n&gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt;  \n&gt; &gt; &gt; &gt; &gt; &gt;=\r\n\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt;\n&gt;\n\n\n\n"}}