{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":256638517,"authorName":"William Monti","from":"William Monti &lt;william.monti@...&gt;","profile":"william.monti","replyTo":"LIST","senderId":"YOgitnRNagoTxE2YGR1V2PzpQfnMe55eN5meYM-FizNjD0zDMtbV-XLvzp9V0s2z6HDzKqFsEZYAGorqhquqRRmiLu2dlDYJpL-Aol-S","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [neat] train 3 billion training data with 3000 inputs and 150 \toutputs?","postDate":"1261942762","msgId":5038,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDVlY2U1ODg3MDkxMjI3MTEzOWgzNmJiOTUzZHJhOTkwOTAxNDlmODk4NDNAbWFpbC5nbWFpbC5jb20+","inReplyToHeader":"PGhoOGFnOCs2OHYxQGVHcm91cHMuY29tPg==","referencesHeader":"PGhoOGFnOCs2OHYxQGVHcm91cHMuY29tPg=="},"prevInTopic":5037,"nextInTopic":5039,"prevInTime":5037,"nextInTime":5039,"topicId":5037,"numMessagesInTopic":9,"msgSnippet":"Hello Christian, What I ve done to deal with a lot of inputs is to make sensor neurons that chooses what to read from the input data (and the sensor itself","rawEmail":"Return-Path: &lt;william.monti@...&gt;\r\nX-Sender: william.monti@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 20096 invoked from network); 27 Dec 2009 19:39:26 -0000\r\nX-Received: from unknown (66.196.94.107)\n  by m12.grp.re1.yahoo.com with QMQP; 27 Dec 2009 19:39:26 -0000\r\nX-Received: from unknown (HELO mail-bw0-f224.google.com) (209.85.218.224)\n  by mta3.grp.re1.yahoo.com with SMTP; 27 Dec 2009 19:39:26 -0000\r\nX-Received: by bwz24 with SMTP id 24so6747231bwz.10\n        for &lt;neat@yahoogroups.com&gt;; Sun, 27 Dec 2009 11:39:22 -0800 (PST)\r\nMIME-Version: 1.0\r\nX-Received: by 10.204.6.69 with SMTP id 5mr643158bky.83.1261942762242; Sun, 27 \n\tDec 2009 11:39:22 -0800 (PST)\r\nIn-Reply-To: &lt;hh8ag8+68v1@...&gt;\r\nReferences: &lt;hh8ag8+68v1@...&gt;\r\nDate: Sun, 27 Dec 2009 14:39:22 -0500\r\nMessage-ID: &lt;5ece58870912271139h36bb953dra99090149f89843@...&gt;\r\nTo: neat@yahoogroups.com\r\nContent-Type: multipart/alternative; boundary=000e0cd1e31e120695047bbaf2f3\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: William Monti &lt;william.monti@...&gt;\r\nSubject: Re: [neat] train 3 billion training data with 3000 inputs and 150 \n\toutputs?\r\nX-Yahoo-Group-Post: member; u=256638517; y=vJ8rgddky07FczZSuqeKDDKTlLLRCahWFv6PUDWeosK508s8EU8UeQ\r\nX-Yahoo-Profile: william.monti\r\n\r\n\r\n--000e0cd1e31e120695047bbaf2f3\r\nContent-Type: text/plain; charset=windows-1252\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\nHello Christian,\n\nWhat I&#39;ve done to deal with a lot of inputs is to make &quot;s=\r\nensor neurons&quot; that\nchooses what to read from the input data (and the senso=\r\nr itself has genetic\ncode to evolve and choose the best inputs over time), =\r\ndepending on the kind\nof the input you can even add a function (genetically=\r\n encoded) to read the\naverage of several inputs or some other useful math f=\r\nunction, and let the\nevolution take care of picking the best ones. This way=\r\n you can add way less\nsensors neurons than the inputs available.\n\nRegards,\n=\r\nWilliam\n\nOn Sun, Dec 27, 2009 at 1:54 PM, chhofchhof &lt;Christian.Hofmann@gmx=\r\n.de&gt;wrote:\n\n&gt;\n&gt;\n&gt; Hello together!\n&gt;\n&gt; I want to start a new project with ve=\r\nry much training data.\n&gt;\n&gt; Doing some tests, currently I need for one gener=\r\nation about 347 days.\n&gt; That&#39;s a bit too long :-)\n&gt;\n&gt; Do you know tricks to=\r\n handle that amount of data?\n&gt; Or should I just select per random one milli=\r\non datasets and only use these?\n&gt;\n&gt; Thank you,\n&gt;\n&gt; Christian\n&gt;\n&gt;  \n&gt;\n\r\n--000e0cd1e31e120695047bbaf2f3\r\nContent-Type: text/html; charset=windows-1252\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\nHello Christian,&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;What I&#39;ve done to deal with a lot o=\r\nf inputs is to make &quot;sensor neurons&quot; that chooses what to read fr=\r\nom the input data (and the sensor itself has genetic code to evolve and cho=\r\nose the best inputs over time), depending on the kind of the input you can =\r\neven add a function (genetically encoded) to read the average of several in=\r\nputs or some other useful math function, and let the evolution take care of=\r\n picking the best ones. This way you can add way less sensors neurons than =\r\nthe inputs available.&lt;/div&gt;\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Regards,&lt;/div&gt;&lt;div&gt;William&lt;=\r\nbr&gt;&lt;br&gt;&lt;div class=3D&quot;gmail_quote&quot;&gt;On Sun, Dec 27, 2009 at 1:54 PM, chhofchh=\r\nof &lt;span dir=3D&quot;ltr&quot;&gt;&lt;&lt;a href=3D&quot;mailto:Christian.Hofmann@...&quot;&gt;Christ=\r\nian.Hofmann@...&lt;/a&gt;&gt;&lt;/span&gt; wrote:&lt;br&gt;\n&lt;blockquote class=3D&quot;gmail_quo=\r\nte&quot; style=3D&quot;margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex;=\r\n&quot;&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;div style=3D&quot;background-color:#fff&quot;&gt;\n&lt;span&gt;=A0&lt;/span&gt;\n\n\n&lt;d=\r\niv&gt;\n  &lt;div&gt;\n\n\n    &lt;div&gt;\n      \n      \n      &lt;p&gt;Hello together!&lt;br&gt;\n&lt;br&gt;\nI w=\r\nant to start a new project with very much training data. &lt;br&gt;\n&lt;br&gt;\nDoing so=\r\nme tests, currently I need for one generation about 347 days.&lt;br&gt;\nThat&#39;=\r\ns a bit too long :-)&lt;br&gt;\n&lt;br&gt;\nDo you know tricks to handle that amount of d=\r\nata?&lt;br&gt;\nOr should I just select per random one million datasets and only u=\r\nse these?&lt;br&gt;\n&lt;br&gt;\nThank you,&lt;br&gt;\n&lt;br&gt;\nChristian&lt;br&gt;\n&lt;br&gt;\n&lt;/p&gt;\n\n    &lt;/div&gt;\n=\r\n     \n\n    \n    &lt;div style=3D&quot;color:#fff;min-height:0&quot;&gt;&lt;/div&gt;\n\n\n&lt;/div&gt;\n\n\n\n =\r\n \n\n\n\n\n\n\n&lt;/blockquote&gt;&lt;/div&gt;&lt;br&gt;&lt;/div&gt;\n\r\n--000e0cd1e31e120695047bbaf2f3--\r\n\n"}}