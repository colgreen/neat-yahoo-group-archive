{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":130107745,"authorName":"Chad Bohannan","from":"&quot;Chad Bohannan&quot; &lt;chad@...&gt;","profile":"tailboom22","replyTo":"LIST","senderId":"ZEcMaRzdhwurz7fgijZmLj25-tL80SHRqztxZl_aFrOQC3ZSIbOldWq-p6DqasZPcz7OlgYsyPO0Bc5GWRLKu3uYCFIHLP7KmVk","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] memory","postDate":"1085176285","msgId":822,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDFhYjgwMWM0M2Y3ZCRjMzUyMDIxMCQ2NmNiMDEwYUBtYWlsMndvcmxkLmNvbT4="},"prevInTopic":0,"nextInTopic":0,"prevInTime":821,"nextInTime":823,"topicId":822,"numMessagesInTopic":1,"msgSnippet":"Anyway, to get back to the interesting stuff. Do you think that sequence memory is achieved by having a central sequencer which temporarily wires its first,","rawEmail":"Return-Path: &lt;chad@...&gt;\r\nX-Sender: chad@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 98622 invoked from network); 21 May 2004 21:57:26 -0000\r\nReceived: from unknown (66.218.66.218)\n  by m16.grp.scd.yahoo.com with QMQP; 21 May 2004 21:57:26 -0000\r\nReceived: from unknown (HELO mwde08la.mail2world.com) (66.28.189.182)\n  by mta3.grp.scd.yahoo.com with SMTP; 21 May 2004 21:57:26 -0000\r\nReceived: from mail pickup service by mwde08la.mail2world.com with Microsoft SMTPSVC;\n\t Fri, 21 May 2004 14:51:25 -0700\r\nauth-sender:chad@...\r\nReceived: from 10.1.203.102 unverified ([10.1.203.102]) by mwde08la.mail2world.com with Mail2World SMTP Server,\n\tFri 21 May 2004 14:51:25 -07:00\r\nReceived: from [153.90.194.179] by bohannan.net with HTTP; 5/21/2004 2:51:25 PM PST\r\nthread-index: AcQ/fcNS/AbZeZoMRSeP0nz5M2FYBA==\r\nThread-Topic: [neat] memory\r\nTo: &lt;neat@yahoogroups.com&gt;\r\nDate: Fri, 21 May 2004 14:51:25 -0700\r\nMessage-ID: &lt;1ab801c43f7d$c3520210$66cb010a@...&gt;\r\nMIME-Version: 1.0\r\nContent-Type: multipart/alternative;\n\tboundary=&quot;----=_NextPart_000_1AB9_01C43F43.16F32A10&quot;\r\nX-Mailer: Microsoft CDO for Exchange 2000\r\nContent-Class: urn:content-classes:message\r\nImportance: normal\r\nPriority: normal\r\nX-MimeOLE: Produced By Microsoft MimeOLE V6.00.2800.1165\r\nX-OriginalArrivalTime: 21 May 2004 21:51:25.0940 (UTC) FILETIME=[C3AA5B40:01C43F7D]\r\nX-eGroups-Remote-IP: 66.28.189.182\r\nFrom: &quot;Chad Bohannan&quot; &lt;chad@...&gt;\r\nSubject: Re: [neat] memory\r\nX-Yahoo-Group-Post: member; u=130107745\r\nX-Yahoo-Profile: tailboom22\r\n\r\n\r\n------=_NextPart_000_1AB9_01C43F43.16F32A10\r\nContent-Type: text/plain;\n\tcharset=&quot;iso-8859-1&quot;\r\nContent-Transfer-Encoding: 7bit\r\n\r\n\nAnyway, to get back to the interesting stuff. Do you think that sequence\nmemory is achieved by having a central &quot;sequencer&quot; which temporarily\nwires \nits &quot;first, second, third&quot; outputs to the sub-nets for the concepts in\nthe \nsequence? Or would you think that different types of sequence are \nprocessed in different ways? I know that when I learn music it feels\nvery \ndifferent from memorizing (say) a phone number. \n\nIan B \n\nI don&#39;t yet believe in a central anything in an ANN. I think\ndistribution is always key. I&#39;m pondering the implications of the &#39;small\nworld&#39; network architecture, and wondering if it would work better if it\nwere explicitly engineered, or if we let NEAT put it together. Or better\nyet, used NEAT to find a general scheme that could be implemented later,\nwhere we could say &#39;this part of the brain is involved in memory.&#39;\nThat&#39;s when we could start saying things were centralized. I don&#39;t think\nwe&#39;re there yet. \nAnyway, I&#39;m looking at an approach to memory on the individual neuron\nlevel that is more complicated than a simple sigmoid function, but\ndoesn&#39;t go so far as the integrate and fire pulse trains of some of the\nhardcore biological models. The idea is to integrate a prioritized\nhistory of previous inputs. Fractional calculous will let you build a\nfunction that uses a parameter to describe the influence of history\nwhich is an arbitrary floating point value of\nintegration/differentiation. Fractional calculous is used in industry\nfor modeling material memory from metal fatigue to capacitance in\nelectronics. I think it could also model the varied memory effects of\nsignals in a net. \nFor example: An input neuron might have very little memory, and use the\nsigmoid transfer function, which is one full integral (1.0f) of the\nnormal distribution curve. But the first hidden layer might be slightly\nhistory dependant, so transfer function might be the 0.95f integral of\nthe normal distribution. A recurrent connection deeper in the network\nmight use a 0.06f integral, in parallel with a neuron with a -0.06\nintegral, which behaves as a derivative. Any one of these non-integer\ncalculations needs a history of the inputs, which on a computer is just\na buffer. The buffer size doesn&#39;t need to be very big. I have a hunch\nthat this approach will couple with recurrence to keep memories alive\nmuch longer than recurrence alone. \nIt may well turn out that if I&#39;m right, this same framework will be able\nto remember a sequence of digits, as well as more context sensitive\ninformation. I should like to share any insights I get with you folks. \n\nchad \n\n\n\r\n------=_NextPart_000_1AB9_01C43F43.16F32A10\r\nContent-Type: text/html\r\nContent-Transfer-Encoding: 7bit\r\n\r\n&lt;HTML&gt;\n&lt;BODY&gt;\n&lt;snip&gt;    &lt;br&gt;\nAnyway, to get back to the interesting stuff.  Do you think that sequence     &lt;br&gt;\nmemory is achieved by having a central &quot;sequencer&quot; which temporarily wires     &lt;br&gt;\nits &quot;first, second, third&quot; outputs to the sub-nets for the concepts in the     &lt;br&gt;\nsequence?  Or would you think that different types of sequence are     &lt;br&gt;\nprocessed in different ways?  I know that when I learn music it feels very     &lt;br&gt;\ndifferent from memorizing (say) a phone number.    &lt;br&gt;\n    &lt;br&gt;\n\tIan B    &lt;br&gt;\n    &lt;br&gt;\nI don&#39;t yet believe in a central anything in an ANN. I think distribution is always key. I&#39;m pondering the implications of the &#39;small world&#39; network architecture, and wondering if it would work better if it were explicitly engineered, or if we let NEAT put it together. Or better yet, used NEAT to find a general scheme that could be implemented later, where we could say &#39;this part of the brain is involved in memory.&#39; That&#39;s when we could start saying things were centralized. I don&#39;t think we&#39;re there yet. &lt;br&gt;\nAnyway, I&#39;m looking at an approach to memory on the individual neuron level that is more complicated than a simple sigmoid function, but doesn&#39;t go so far as the integrate and fire pulse trains of some of the hardcore biological models. The idea is to integrate a prioritized history of previous inputs. Fractional calculous will let you build a function that uses a parameter to describe the influence of history which is an arbitrary floating point value of integration/differentiation. Fractional calculous is used in industry for modeling material memory from metal fatigue to capacitance in electronics. I think it could also model the varied memory effects of signals in a net.    &lt;br&gt;\nFor example: An input neuron might have very little memory, and use the sigmoid transfer function, which is one full integral (1.0f) of the normal distribution curve. But the first hidden layer might be slightly history dependant, so transfer function might be the 0.95f integral of the normal distribution. A recurrent connection deeper in the network might use a 0.06f integral, in parallel with a neuron with a -0.06 integral, which behaves as a derivative. Any one of these non-integer calculations needs a history of the inputs, which on a computer is just a buffer. The buffer size doesn&#39;t need to be very big. I have a hunch that this approach will couple with recurrence to keep memories alive much longer than recurrence alone.    &lt;br&gt;\nIt may well turn out that if I&#39;m right, this same framework will be able to remember a sequence of digits, as well as more context sensitive information. I should like to share any insights I get with you folks.     &lt;br&gt;\n    &lt;br&gt;\nchad    &lt;br&gt;\n    &lt;br&gt;\n\n&lt;/BODY&gt;&lt;/HTML&gt;\n\r\n------=_NextPart_000_1AB9_01C43F43.16F32A10--\r\n\n"}}