{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"8jrTZOtYnVVP_BXNs9tC5LVMsxkwhRD5uLuas6QzIefHdRE6qbz477kOa9TqMSDElvuZz5DI6Jwcd3sBMCILAuq0ZmqxE016zgr7PApgI0Xv","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: Bloat","postDate":"1086650441","msgId":1024,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGNhMnQ4OStzMXRpQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDAwOTEwMWM0NGNlMyQxNzFjOGExMCQzMjAxYThjMEBORVdBR0U+"},"prevInTopic":1023,"nextInTopic":1025,"prevInTime":1023,"nextInTime":1025,"topicId":904,"numMessagesInTopic":68,"msgSnippet":"Jim, Please do not worry about any possible interpretations of disrespect on my part.  I definitely want people to feel free to challenge any ideas whatsoever","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 63121 invoked from network); 7 Jun 2004 23:20:51 -0000\r\nReceived: from unknown (66.218.66.216)\n  by m6.grp.scd.yahoo.com with QMQP; 7 Jun 2004 23:20:51 -0000\r\nReceived: from unknown (HELO n22.grp.scd.yahoo.com) (66.218.66.78)\n  by mta1.grp.scd.yahoo.com with SMTP; 7 Jun 2004 23:20:51 -0000\r\nReceived: from [66.218.66.123] by n22.grp.scd.yahoo.com with NNFMP; 07 Jun 2004 23:20:42 -0000\r\nDate: Mon, 07 Jun 2004 23:20:41 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;ca2t89+s1ti@...&gt;\r\nIn-Reply-To: &lt;009101c44ce3$171c8a10$3201a8c0@NEWAGE&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Length: 14076\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-eGroups-Remote-IP: 66.218.66.78\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: Re: Bloat\r\nX-Yahoo-Group-Post: member; u=54567749\r\nX-Yahoo-Profile: kenstanley01\r\n\r\nJim,\n\nPlease do not worry about any possible interpretations of disrespect \non my part.  I definitely want people to feel free to challenge any \nideas whatsoever so we can have stimulating discussion.  \n\nMy response was just my attempt to articulate why I think topology \nis such an important part of search.  By all means feel free to \ndisagree anytime :)\n\nken\n\n\n--- In neat@yahoogroups.com, &quot;Jim O&#39;Flaherty, Jr.&quot; \n&lt;jim_oflaherty_jr@y...&gt; wrote:\n&gt; Ken,\n&gt; \n&gt; I meant no disrespect in my original post.  I was honestly \nconfused why huge variations in topology were desirable when finding \nthe proper weight values is just as important.  That whole argument \nis pretty much moot now.  ;^)\n&gt; \n&gt; Now that I have re-read this and I understand the &quot;mutation rate&quot; \nissues, a huge part of my concern is now alleviated.  And what \nlittle I have remaining is insignificant.  So until I have done some \nmore experiments with NEAT, I will wait to address any other issues.\n&gt; \n&gt; \n&gt; Jim\n&gt; \n&gt; \n&gt;   ----- Original Message ----- \n&gt;   From: Kenneth Stanley \n&gt;   To: neat@yahoogroups.com \n&gt;   Sent: Saturday, June 05, 2004 7:06 PM\n&gt;   Subject: [neat] Re: Bloat\n&gt; \n&gt; \n&gt;   Jim, I hope you will allow me a rather long and detailed \nresponse to \n&gt;   your point.  I feel this is the right time for me to respond \n&gt;   broadly, since you have touched on the central theme behind much \n&gt;   discussion on this group, and ultimately behind my own \nmotivations \n&gt;   for introducing NEAT.  Therefore, forgive me for a long-winded \n&gt;   response, but one I would to get on the record.\n&gt; \n&gt;   I doubt that the importance of topology can be overstated.  That \n&gt;   said, I want to concede up front that there is no question that \nmost \n&gt;   of the key steps in exploration are through weight mutation, and \n&gt;   that weight mutation will get you far.  In fact, there are very \n&gt;   sophisticated methods for altering the weight mutation \ndistribution \n&gt;   to point it in more promising directions, and these methods can \nbe \n&gt;   quite powerful.\n&gt; \n&gt;   Nevertheless, weight mutation is no more than exploring a fixed \n&gt;   space, and exploring a fixed space is well understood and tried \nand \n&gt;   tested.  In fact, it is proven that there  is only so much a \nblack \n&gt;   box method can do to explore a space.  No method can promise \nalways \n&gt;   to escape local optima, and no method ever will make such a \npromise \n&gt;   (so says the No Free Lunch Theorem).  \n&gt; \n&gt;   There are fundamental questions at the core of AI that fixed-\nspace \n&gt;   exploration can never address.  Most perplexing and fundamental \nis \n&gt;   the question of what space should we be exploring in the first \n&gt;   place?  Fogel tried 3 topologies (and probably more, off the \n&gt;   ecord).  But where did those topologies come from?  What was the \n&gt;   basis of the decisions to use them?  Isn&#39;t our mission, as \n&gt;   researchers in AI, to make *that* decision automatic?  After \nall, \n&gt;   *that* decision- the decision of what topology to search, i.e. \nwhat \n&gt;   space to search in- is really the only hard decision, the one \nthat \n&gt;   requires &quot;intelligence&quot;.  It is a relatively trivial matter, \nonce \n&gt;   you know what to search, just to go searching.  The fact that \nweight \n&gt;   mutation alone (once the correct topology has been identified) \nis \n&gt;   sufficient to solve checkers says more about checkers and human \n&gt;   intelligence (intelligence for choosing the right space to \nsearch) \n&gt;   than about the prowess of weight mutation.\n&gt; \n&gt;   Yet this is not only a philosophical argument about what AI \nshould \n&gt;   be able to do automatically.  It is also a critical practical \n&gt;   matter.  Contrary to your reasoning, the real danger is not in \n&gt;   adding a single dimension to a search space, but in beginning \nsearch \n&gt;   in a bad space in the first place.  If you are concerned that \n&gt;   addition of a single dimension has some exponential expense \n(which I \n&gt;   believe is not correct anyway), what cost then must there be in \n&gt;   searching in a topology with dozens or even hundreds of \nunnecessary \n&gt;   dimensions?  The effect on search could be catastrophic.\n&gt; \n&gt;   And yet for most difficult problems we have not the slightest \nidea \n&gt;   what the right space is to search, other than that it is large.  \nHow \n&gt;   many dimensions are in the brain of a robotic maid?  Surely at \nleast \n&gt;   thousands; maybe millions.  Should we begin search then in a \nnetwork \n&gt;   of a million connections?  Weight space exploration offers no \n&gt;   comfort: The search is intractable in million dimensional space, \n&gt;   even if the solution is somewhere within.  \n&gt; \n&gt;   Yet even as there is danger from above in the form of too-high \n&gt;   dimensional space, there is danger from below in spaces of too-\nfew \n&gt;   dimensions, where a solution may not even exist.  What if Fogel \nhad \n&gt;   chosen to search in networks with 2 fewer neurons?  5 fewer? At \nsome \n&gt;   point, the good player just doesn&#39;t exist in that space \nanymore.  \n&gt;   But how could we know this in advance?  There is no analysis \nthat \n&gt;   can tell us a priori how many dimensions we need.  And if we try \nto \n&gt;   go lean and get just the right amount, we might miss the boat \n&gt;   entirely, even by a single connection, and end up searching \nforever \n&gt;   in futility in a space without a solution.\n&gt; \n&gt;   Worse, even if we knew *exactly* the minimal number of \nconnections \n&gt;   necessary to solve a problem *and* the perfect topology, even \nthen, \n&gt;   if the space is too large, weight mutation alone is likely to \nfail.  \n&gt;   The problem is, where in a large space do you *begin* to search? \nAnd \n&gt;   that problem is impossible to address since by definition you \ndon&#39;t \n&gt;   know anything about the space before you begin searching!  \n&gt;   Therefore, in a high-dimensional space, you are highly likely to \n&gt;   begin in an unpromising part of the space; it&#39;s simply too large.\n&gt; \n&gt;   Therefore, to begin minimally and complexify into the the proper \n&gt;   space is addressing a fundamental issue and I believe is \nultimately \n&gt;   unavoidable as a critical component of any black box search for \n&gt;   complex behaviors.  Rather than adding expense as you imply, it \nis \n&gt;   reducing expense by spending most of search in lower-dimensional \n&gt;   space than the final solution.  A complexifying method only may \nbe \n&gt;   searching in the space of the final solution for 10% of the \nrun.  \n&gt;   Fixed-topology search spends 100% of the run in the high \ndimensional \n&gt;   space of the final solution, which, according to your \nformulation \n&gt;   should incur an incomprehensibly vast exponential penalty.  \n&gt; \n&gt;   I think ultimately what you are misunderstanding is that NEAT is \nnot\n&gt;   an attempt to search in high-dimensional space.  It is a method \nfor \n&gt;   spending most of your search in *lower-dimensional space* than \nthe \n&gt;   final solution, and complexifying up to the complexity of the \nfinal \n&gt;   solution.  The goal is to be able to find solutions that *exist* \nin \n&gt;   high-dimensional space.  That&#39;s not the same as a goal of \nsearching \n&gt;   directly in high-dimensional space no matter the problem.  The \n&gt;   latter goal is the antithesis of what NEAT is about.  NEAT is \n&gt;   designed to avoid searching in unnecessarily high-dimensional \nspace.\n&gt; \n&gt;   Thus, I feel strongly that the idea of searching through \ntopologies \n&gt;   must be taken seriously, and should not be viewed as merely \n&gt;   a &quot;fun&quot;, &quot;sexy,&quot; or &quot;somwhat spatially interesting&quot;  \nrecreation.  It \n&gt;   is not mere intellectual exercise.  Prior topology-evolving \nsystems \n&gt;   before NEAT were perhaps better targets for your criticism, \nsince \n&gt;   they were essentially aimed at flipping through random \ntopologies \n&gt;   unsystematically for its own sake.  However, NEAT is designed to \nto \n&gt;   use topology as a way of minimizing dimensionality in search, \nand \n&gt;   ultimately to automatically address that fundamental question of \n&gt;   what space to be searching in, a completely different endeavor. \n&gt; \n&gt;   (\n&gt; \n&gt;   A couple side notes:\n&gt; \n&gt;   I agree that structural mutation needs to be relatively rare.  \nIn \n&gt;   NEAT, it is generally 5% or lower.  Years of experimentation \nwith \n&gt;   NEAT have gone into testing different rates of structure-adding.\n&gt; \n&gt;   Finally, I believe your mathematical formulation is incorrect. \n&gt;   Adding a dimensions to an already-partially-optimized structure \n&gt;   certainly does not incur exponential expense in the search \nprocess.  \n&gt;   In fact, the effect can be quite the opposite, adding new routes \noff \n&gt;   the top of a local optimum.\n&gt; \n&gt;   Not to be picky, and this isn&#39;t really important, but here&#39;s \nwhat \n&gt;   doesn&#39;t make sense to me about your formal argument:\n&gt; \n&gt;   -&quot;What is occurring to me is that just doing weight mutation is \na \n&gt;   search at a rate X in a huge space.&quot;  How do you define &quot;search \nat \n&gt;   rate X?&quot;  This does not seem to mean anything formally \nspeaking.  \n&gt;   What are the units of search rate?  How is it derived?\n&gt; \n&gt;   -&quot;And it seems to me adding topological variation is not just a \n&gt;   multiplier, but an exponent increasing X.&quot;  If X is a rate (as \nyou \n&gt;   defined it), then increasing X means the rate becomes faster.  \nSo I \n&gt;   assume X is not a rate.  But then what is it?\n&gt; \n&gt;   -&quot;topological complexity curve for having a more fit player is \n&gt;   exponetial&quot;  What is a topological complexity curve?  Is it \nbased \n&gt;   somehow on rate X?  Complexity is usually defined as the size of \nthe \n&gt;   space or number of connections in a network.  Under that usual \n&gt;   definition, complexity goes up linearly with the addition of new\n&gt;     structure, not exponentially.\n&gt; \n&gt;   -&quot;X^T, where T = Y^Z and Z is the complexity curve&quot;  I still am \nnot \n&gt;   sure what X really means formally, but you haven&#39;t given a \n&gt;   definition for T or Y or Z either.  What are these variables?\n&gt; \n&gt;   Ultimately I think you are arguing from intuition rather than \n&gt;   formally, and intuitions can be misleading.\n&gt;   )\n&gt; \n&gt; \n&gt;   Sorry to all for the long-windedness of this response!  I hope \nit is \n&gt;   still useful!\n&gt; \n&gt;   --- In neat@yahoogroups.com, &quot;Jim O&#39;Flaherty, Jr.&quot; \n&gt;   &lt;jim_oflaherty_jr@y...&gt; wrote:\n&gt;   &gt; John, Colin, Ken and Derek,\n&gt;   &gt; \n&gt;   &gt; I am wondering if there is not a wee bit too much focus on \n&gt;   topological vairation and insignificant focus on just weight \n&gt;   mutation.  I get that NEAT is unique in the fact that it has a \nvery \n&gt;   effective search mechanism for topoligical variation, with the \n&gt;   ability to stress both additive and subtractive aspects of \nchange.  \n&gt;   And I get that it is fun to focus on the topological variation \nas it \n&gt;   is somewhat spatially interesting.\n&gt;   &gt; \n&gt;   &gt; However, I am realizing that just doing effective weight \n&gt;   mutations, sans topological changes, can end up producing \nsolutions \n&gt;   that are very &quot;fit&quot;.  I have been focused on reproducing the \n&gt;   experiments Fogel and Kumar produced which are covered in their \nbook \n&gt;   Blondie24.  In that, they had only 3 topologies they \nexperimented \n&gt;   with.  All of the GA searching was just done with weight \nmutation \n&gt;   within a step size that was both a GA parameter and nudged \ntowards \n&gt;   smaller values.  In the Fogel experiments, they arrived at an \nexpert \n&gt;   player (well, at least against human opponents) using just co-\n&gt;   evolution and a static topology.  And in my own replication of \nthe \n&gt;   experiments, something as simple as turn on/off biases had a \n&gt;   substantial effect in how long it took to arrive at a specimen \nof \n&gt;   similar fitness.\n&gt;   &gt; \n&gt;   &gt; Now, I realize that mutating weights only is not near as sexy \n&gt;   sounding as both weight mutation and topological variation.  \n&gt;   However, what I am wondering and hope to be able to evaluate \nwith \n&gt;   experimentation is whether the topological mutation rates ought \nnot \n&gt;   be very small with the focus more on trying out many weight \n&gt;   mutations within a given topology?  What is occurring to me is \nthat \n&gt;   just doing weight mutation is a search at a rate X in a huge \nspace.  \n&gt;   And it seems to me adding topological variation is not just a \n&gt;   multiplier, but an exponent increasing X.  Perhaps the space is \n&gt;   being made too large too quickly, before a search just in the \nweight \n&gt;   mutation space might demonstrate a uniquely fit individual.\n&gt;   &gt; \n&gt;   &gt; My understanding is that the &quot;search in higher dimensional \nspace&quot; \n&gt;   might produce more robust and fit players.  At what \ncomputational \n&gt;   cost?  If the topological complexity curve for having a &quot;more \nfit \n&gt;   player&quot; is exponetial, then doesn&#39;t that mean that there is a \n&gt;   threshold of diminishing returns somewhere?  Granted, it may not \n&gt;   be.  However, when it is exponential (and my intuition says it \nis \n&gt;   more of the time), we now have an exponent on an exponent of \n&gt;   complexification which massively enlarges the search space, X^T, \n&gt;   where T = Y^Z and Z is the complexity curve.  If this is true, \nthen \n&gt;   we are massive amounts of processing power away from achieving \n&gt;   result in anything but the simplest of domains, like XOR and Tic-\nTac-\n&gt;   Toe.\n&gt;   &gt; \n&gt;   &gt; Am I missing something here?  Perhaps I need to do more direct \n&gt;   experimentation and examine the results before jumping to this \nkind \n&gt;   of conclusion.  I just get the sense that simple weight mutation \n&gt;   achieved quite a bit in Checkers, a domain more complex than Tic-\nTac-\n&gt;   Toe.  It would be interesting to see how Checkers might do with \nNEAT \n&gt;   and see what kinds of mutation rates might be more/less \neffective \n&gt;   and why.\n&gt;   &gt; \n&gt;   &gt; \n&gt;   &gt; Jim\n&gt;   &gt; \n&gt;   &gt; \n&gt; \n&gt; \n&gt; \n&gt;         Yahoo! Groups Sponsor \n&gt;               ADVERTISEMENT\n&gt;              \n&gt;        \n&gt;        \n&gt; \n&gt; \n&gt; -------------------------------------------------------------------\n-----------\n&gt;   Yahoo! Groups Links\n&gt; \n&gt;     a.. To visit your group on the web, go to:\n&gt;     http://groups.yahoo.com/group/neat/\n&gt;       \n&gt;     b.. To unsubscribe from this group, send an email to:\n&gt;     neat-unsubscribe@yahoogroups.com\n&gt;       \n&gt;     c.. Your use of Yahoo! Groups is subject to the Yahoo! Terms \nof Service.\n\n\n"}}