{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":130984297,"authorName":"joel278","from":"&quot;joel278&quot; &lt;lehman.154@...&gt;","profile":"joel278","replyTo":"LIST","senderId":"AXWXopatksKLU9nIPEJLoe2-xnNkLWA1dXWH1ZCihC6yk5L2n8gWLlRcK1T9CcVJvhr5L5pGtsAhVnI6upo8OQs_6tHE","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: Novelty Search for Classification/Regression problems","postDate":"1271817065","msgId":5212,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGhxbG8xOSthaDlyQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGhxbGZvMStqN3FpQGVHcm91cHMuY29tPg=="},"prevInTopic":5211,"nextInTopic":5213,"prevInTime":5211,"nextInTime":5213,"topicId":5211,"numMessagesInTopic":3,"msgSnippet":"Hi Lior, Thanks for your interest in novelty search, you make some interesting points about overfitting. One thing I m not sure about is the explanation behind","rawEmail":"Return-Path: &lt;lehman.154@...&gt;\r\nX-Sender: lehman.154@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 465 invoked from network); 21 Apr 2010 02:35:51 -0000\r\nX-Received: from unknown (98.137.34.45)\n  by m1.grp.sp2.yahoo.com with QMQP; 21 Apr 2010 02:35:51 -0000\r\nX-Received: from unknown (HELO n4-vm6.bullet.mail.sp2.yahoo.com) (67.195.135.100)\n  by mta2.grp.sp2.yahoo.com with SMTP; 21 Apr 2010 02:35:51 -0000\r\nX-Received: from [67.195.134.49] by n4.bullet.mail.sp2.yahoo.com with NNFMP; 21 Apr 2010 02:31:07 -0000\r\nX-Received: from [69.147.65.171] by t2.bullet.mail.sp2.yahoo.com with NNFMP; 21 Apr 2010 02:31:07 -0000\r\nX-Received: from [98.137.34.72] by t13.bullet.mail.sp1.yahoo.com with NNFMP; 21 Apr 2010 02:31:07 -0000\r\nDate: Wed, 21 Apr 2010 02:31:05 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;hqlo19+ah9r@...&gt;\r\nIn-Reply-To: &lt;hqlfo1+j7qi@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nFrom: &quot;joel278&quot; &lt;lehman.154@...&gt;\r\nSubject: Re: Novelty Search for Classification/Regression problems\r\nX-Yahoo-Group-Post: member; u=130984297; y=Tnldyvnkm0iAdWbaA5ql7QVczjpOTbD34Az2zZ89zeYr6S-7Czis\r\nX-Yahoo-Profile: joel278\r\n\r\n\n\n\n\n\nHi Lior,\n\nThanks for your interest in novelty search, you make some in=\r\nteresting points about overfitting. One thing I&#39;m not sure about is the exp=\r\nlanation behind the log(n)+log(log(n)) complexity of the Nth neural network=\r\n? It seems that in order to regenerate the solution you would have to also =\r\ntake into account the complexity of the search algorithm itself which (if i=\r\nt is NEAT+novelty search) is likely a lot larger than this log(n) term? Reg=\r\nardless, it is interesting to consider if novelty search could be a good wa=\r\ny of evolving ANNs that solve classification tasks without overfitting.\n\nIt=\r\n is true that there is tentative evidence that novelty search may evolve mo=\r\nre compact solutions than objective-based search (shown in the recent GP pu=\r\nblication and also in the original Alife conference paper) and may thus be =\r\nless prone to overfitting. However, I think that classification is not an i=\r\ndeal domain for unconstrained novelty search. That is, I am not sure it wou=\r\nld always be better than an exhaustive or random search in such a scenario.=\r\n And while exhaustive or random search may be feasible for toy problems, I =\r\nwould be a bit surprised if a significant breakthrough was produced through=\r\n such means.\n\nThe main problem with *efficiently* applying raw novelty sear=\r\nch to a classification problem is that with a naive behavioral characteriza=\r\ntion (e.g. the concatenation of all the classes an ANN outputs over the tra=\r\nining set), there are no domain restrictions and a vast space of possible b=\r\nehaviors. With such a characterization, novelty search would continually fi=\r\nnd novel ways of partitioning the training examples in different classes. T=\r\nhe problem is, the number of possible partitions grows exponentially in the=\r\n number of training examples.  \n\nWhile sometimes learning a new way to part=\r\nition classification examples may reflect an important aspect of the distri=\r\nbution of the training examples, NS may also exploit noise or incidental as=\r\npects of the training examples to generate new classifications. For example=\r\n, if the training data were 10-dimensional, but only one dimension was impo=\r\nrtant to classification, and the other 9 were random, NS might exploit the =\r\nrandom differences in the unimportant dimensions to generate a huge amount =\r\nof novel classifications.\n\nThe problem is that in NS classification the &quot;be=\r\nhavior&quot; of the ANN is a direct product of its output signals. Contrast such=\r\n directness with a robot&#39;s behavior in a maze: The location at which a robo=\r\nt ends up in a maze is an indirect product of its output signals. The robot=\r\n&#39;s output signals direct its actions, but are subject to the virtual world =\r\nin which the robot lives (a robot cannot go through walls nor travel faster=\r\n than its motors will allow). \n\nImagine if a maze-robot&#39;s outputs simply te=\r\nleported it into a specific area of the maze; this is similar in spirit to =\r\na naive search for novelty in a classification domain. There is little to l=\r\nearn in a raw NS classification problem because the &quot;world&quot; in which the cl=\r\nassifying ANN lives in is so simple and completely unconstrained.\n\nConstrai=\r\nnts or domain restrictions are important because they may allow a search fo=\r\nr novelty to learn about a domain; for example, searching for novelty in a =\r\nmaze incentivizes learning about how to avoid walls. That is, by avoiding w=\r\nalls a robot may end up in a novel location in the maze. Although there are=\r\n no inherent domain restrictions in a classification problem, such restrict=\r\nions can possibly be introduced artificially.\n\nKen Stanley and I have a pub=\r\nlication on minimal criteria novelty search (http://eplex.cs.ucf.edu/public=\r\nations/2010/lehman.gecco10a.html), which provides such a way of introducing=\r\n artificial domain constraints. An individual in MCNS is considered only if=\r\n it meets all user-imposed minimal criteria, otherwise it is discarded. \n\nA=\r\nn intuitive minimal criterion for classification problems might be maintain=\r\ning at least x% accuracy (where x could be the accuracy that a simple neare=\r\nst-neighbor classification scheme achieves). This would mean that an ANN mu=\r\nst find novel classification schemes that are partially constrained by the =\r\ncorrect labels of the training set. However, I do not know if this is a goo=\r\nd approach; there may be some domains that are not amenable to novelty sear=\r\nch, just as very deceptive domains provide a bad match for objective-based =\r\nsearch.\n\nBut, I think that a MCNS approach might be the best way to approac=\r\nh interesting classification problems in a novelty-search way.\n\n\nAs for XOR=\r\n, I did try novelty search with XOR classification in my initial experiment=\r\ns and found that it was able to consistently solve the problem (perhaps bec=\r\nause there are only 4 training examples), although it did not provide an ad=\r\nvantage over objective-based search.\n\nJoel\n\n--- In neat@yahoogroups.com, &quot;l=\r\nior_fainshil&quot; &lt;lior_fainshil@...&gt; wrote:\n&gt;\n&gt; I find the idea of novelty sea=\r\nrch very interesting for the reasons\n&gt; described in the articles, but also =\r\nfor another very different reason.\n&gt; Think what happens when novelty search=\r\n is used to design a classifier.\n&gt; A notorious problem in classification/re=\r\ngression problems is that of\n&gt; overfitting. That is when the classifier lea=\r\nrns the peculiarities of the\n&gt; training set instead of the underlying rule =\r\nand its good performance on\n&gt; the training set fails to transfer to new exa=\r\nmples.\n&gt; It can be mathematically shown that overfitting can be minimized b=\r\ny\n&gt; minimizing the amount of information needed to encode the classificatio=\r\nn\n&gt; rule. If the information complexity of the rule is considerably lower\n&gt;=\r\n than the information complexity of the training set, then the\n&gt; probabilit=\r\ny of overfitting goes to zero.\n&gt; Imagine we use novelty search to find a cl=\r\nassification rule. For our\n&gt; analysis we will assume that we use a pseudo-r=\r\nandom number generator\n&gt; with a known seed(This should not affect the perfo=\r\nrmance). We will run\n&gt; novelty search until we find a classifier that reach=\r\nes the desired\n&gt; performance on the training set. This will be the classifi=\r\ner we use. One\n&gt; way to identify this classifier is by n, its index number =\r\nin the series\n&gt; of classifiers tested. We can recreate the classifier by ru=\r\nnning the\n&gt; search from the start. (We can count only classifiers with diff=\r\nerent\n&gt; classifications). In such case the information content of the class=\r\nifier\n&gt; will be log(n)+O(log(log(n)). If this number is small compared to t=\r\nhe\n&gt; information content of the training set, then we can be assured that w=\r\ne\n&gt; avoided overfitting.\n&gt; \n&gt; The same logic can be applied to any random s=\r\nearch. It does not work\n&gt; however with a classification performance directe=\r\nd search since in such\n&gt; a search the classifier can not be recreated from =\r\nits number in the\n&gt; enumeration alone. To get the classifier from its numbe=\r\nr, we also need\n&gt; the training set. This makes the overfitting bound meanin=\r\ngless.\n&gt; \n&gt; Together with the other benefits of novelty search this seems a=\r\nlmost too\n&gt; good to be true. There are some points which are not clear to m=\r\ne\n&gt; however. One is comparison to random search. Random search possesses th=\r\ne\n&gt; same property of overfitting avoidance as well as an additional\n&gt; optim=\r\nality property. In a random search it is relatively straightforward\n&gt; to or=\r\nder the hypotheses by our interpretation of their complexity - we\n&gt; can per=\r\nfectly incorporate Occam&#39;s Razor. This doesn&#39;t seem possible to\n&gt; the same =\r\ndegree in novelty search. A random(actually brute force) search\n&gt; is used f=\r\nor the theoretically optimal Solomonoff Induction(which\n&gt; unfortunately is =\r\nfar from optimal in computation time). People actually\n&gt; managed to use a b=\r\nrute force search successfully even to develop neural\n&gt; networks\n&gt; &lt;http://=\r\nagi-conf.org/2010/wp-content/uploads/2009/06/paper_57.pdf&gt; . So\n&gt; I would b=\r\ne careful about throwing away random search.\n&gt; \n&gt; I actually tried a kind o=\r\nf Novely Search with NEAT on the Xor problem.\n&gt; It did not work very well. =\r\nI know that xor is not a very good example\n&gt; for this. I am thinking about =\r\ntrying it on more interesting problems.\n&gt; \n&gt; What are your thoughts on this=\r\n?\n&gt;\n\n\n\n"}}