{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Ken","from":"&quot;Ken&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"WRiZrSGO_ompO4dh2DSmrj-UQpy6eU-esPidM8A5oSgmShaHDkaiE95igbJwuRBaUsiwnTcvO8TkQbYwY7SFY6c5-eIS","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: CPPN IO encoding for fully connected/recurrent networks using HyperNEAT","postDate":"1325802300","msgId":5750,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGplNTd2cytxdmNzQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGpkZThzYStoMWtzQGVHcm91cHMuY29tPg=="},"prevInTopic":5749,"nextInTopic":0,"prevInTime":5749,"nextInTime":5751,"topicId":5749,"numMessagesInTopic":2,"msgSnippet":"Hi Oliver, interesting topic.  I am not aware of any published work with such fully-connected recurrent grid-like structures.  Just for thinking about it, you","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 69261 invoked from network); 5 Jan 2012 22:25:08 -0000\r\nX-Received: from unknown (98.137.34.45)\n  by m8.grp.sp2.yahoo.com with QMQP; 5 Jan 2012 22:25:08 -0000\r\nX-Received: from unknown (HELO ng5-ip2.bullet.mail.bf1.yahoo.com) (98.139.165.28)\n  by mta2.grp.sp2.yahoo.com with SMTP; 5 Jan 2012 22:25:08 -0000\r\nX-Received: from [98.139.164.120] by ng5.bullet.mail.bf1.yahoo.com with NNFMP; 05 Jan 2012 22:25:01 -0000\r\nX-Received: from [69.147.65.150] by tg1.bullet.mail.bf1.yahoo.com with NNFMP; 05 Jan 2012 22:25:01 -0000\r\nX-Received: from [98.137.34.51] by t7.bullet.mail.sp1.yahoo.com with NNFMP; 05 Jan 2012 22:25:01 -0000\r\nDate: Thu, 05 Jan 2012 22:25:00 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;je57vs+qvcs@...&gt;\r\nIn-Reply-To: &lt;jde8sa+h1ks@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;Ken&quot; &lt;kstanley@...&gt;\r\nSubject: Re: CPPN IO encoding for fully connected/recurrent networks using HyperNEAT\r\nX-Yahoo-Group-Post: member; u=54567749; y=7rDZPEEyuI9336mz9_uFoMxDnX21dnC8TtWxlorBNI7m4oV7iuc9\r\nX-Yahoo-Profile: kenstanley01\r\n\r\n\n\nHi Oliver, interesting topic.  I am not aware of any published work with =\r\nsuch fully-connected recurrent grid-like structures.  Just for thinking abo=\r\nut it, you might even ask the same for a 2D recurrent grid to simplify the =\r\nissue without really changing the fundamental question.\n\nOne problem with s=\r\nuch a structure that is not exactly the same as the one you raised is the &quot;=\r\nspaghetti&quot; type of connectivity that you can get if anything can connect to=\r\n literally anything.  That can probably be reduced effectively with the Hyp=\r\nerNEAT-LEO (Link Expression Output) that we recently introduced, which help=\r\ns reduce connectivity:\n\nhttp://eplex.cs.ucf.edu/papers/verbancsics_gecco11.=\r\npdf\n\nHowever, of course, you are raising a bigger issue about how to best s=\r\ntructure a CPPN to encode such a pattern in general.  There may be a clever=\r\n way to do it, i.e. something analogous to the &quot;one output per layer&quot; conce=\r\npt, but modified to be more efficient and thereby amenable to large recurre=\r\nnt networks.  I&#39;d leave that possibility open, since I don&#39;t have an idea f=\r\nor it right now.\n\nBut my guess is that what is really instrumental in this =\r\ndiscussion is the issue of what is &quot;easier to generate&quot; with certain kinds =\r\nof CPPNs.  You point out that it seems &quot;easier&quot; to generate divergent layer=\r\n patterns if they are represented by different CPPN outputs.  But the word =\r\n&quot;easier&quot; here is relative to a particular fitness function.  What it really=\r\n is saying is that the stepping stones that might lead to divergent pattern=\r\ns are somehow not rewarded (and therefore not preserved) when there is a si=\r\nngle output.\n\nHowever, that does not really mean that such stepping stones =\r\ndon&#39;t exist, or that they are even in principle &quot;hard&quot; to discover.  It&#39;s j=\r\nust that the fitness function doesn&#39;t recognize them.  So I think the real =\r\ntrick here might be with how stepping stones are rewarded rather than with =\r\nmanipulating the encoding, though of course there&#39;s still room for that.  B=\r\nut if you look for example at CPPN-generated images on Picbreeder, you can =\r\nsee that areas of differentiation (even hard differentiation) emerge consis=\r\ntently.  You can get one part of an image doing one thing, and the other do=\r\ning something else.  Yet I think the problem is that if you look at the anc=\r\nestry of such images, you will find the ancestors hardly resemble their des=\r\ncendents (at least visually).  So we&#39;re really facing a stepping stone prob=\r\nlem, or more fundamentally, the paradox of objectives in general.\n\nSo I&#39;d g=\r\nuess that single-output (or few output) CPPNs could actually produce the ki=\r\nnd of differentiated layer structures we&#39;d want in such networks, but that =\r\nthe path to them is likely not going to be followed by a typical objective =\r\nfunction.  Some kind of non-objective search that is more open-ended is mor=\r\ne likely to accumulate the prerequisites we would need.\n\nThis perspective d=\r\noes not offer a solution so much as pose the question a different way.  Ins=\r\ntead of a question of how to structure the CPPN, perhaps the question is, w=\r\nhat kind of non-objective process can lead to such structures emerging syst=\r\nematically?\n\nken\n\n\n\n--- In neat@yahoogroups.com, &quot;olivercoleman04&quot; &lt;oliver.=\r\ncoleman@...&gt; wrote:\n&gt;\n&gt; I&#39;ve been thinking about methods for evolving recur=\r\nrent networks or network modules using HyperNEAT, and have been wondering a=\r\nbout the input and output representation used by the CPPN.\n&gt; \n&gt; Most papers=\r\n that evolve networks with multiple layers use 2D layers (and so 2 inputs f=\r\nor the CPPN) and an output in the CPPN per every pair of connected layers (=\r\ncall it a weight layer). An alternative is to use an additional input to sp=\r\necify the connection layer, or z-axis coordinates; it&#39;s just another dimens=\r\nion orthogonal to the two dimensions of the layers. Or if you&#39;re allowing c=\r\nonnections between arbitrary layers (rather than just the next layer), then=\r\n you would use two additional inputs to specify the z-axis coordinates for =\r\nsource and target neurons.\n&gt; \n&gt; An output for each weight layer, as is done=\r\n in most/all published experiments to date, seems to make sense because we =\r\ngenerally want quite different patterns of weights between different pairs =\r\nof layers (even if different connection layer patterns are geometrically co=\r\nrrelated they are still typically quite different), and it seems to be easi=\r\ner to generate CPPNs that produce very different (but correlated) patterns =\r\nfrom different outputs than it is to generate CPPNs that produce very diffe=\r\nrent patterns depending on the value of an input (at least, I tried both en=\r\ncoding schemes and this is what I found).\n&gt; \n&gt; Getting back to recurrent ne=\r\ntworks; what does the above mean for a collection of neurons arranged in so=\r\nme N-dimensional space that are potentially fully recurrent/connected? Let&#39;=\r\ns say the substrate neurons are arranged in a 3-dimensional space. We would=\r\n create CPPNS with 6 inputs (2 for each dimension for source and target neu=\r\nron coordinates) and one output. However, allowing recurrent connections es=\r\nsentially means that we have many different 2D (and 1D) layers connecting t=\r\no many other layers (both parallel and orthogonal in two ways), so accordin=\r\ng to the above discussion perhaps we need an output for each possible combi=\r\nnation of layers? This doesn&#39;t seem like a very good solution as we would q=\r\nuickly end up with vast numbers of outputs.\n&gt; \n&gt; So how could we make it ea=\r\nsier to generate CPPNs that have an input (or some constant number of input=\r\ns) for each dimension and one output for encoding fully connected/recurrent=\r\n substrate networks? If we assume the reason having multiple outputs works =\r\nis because the CPPN can easily encode largely disparate (but correlated) fu=\r\nnctions for each output, the challenge for a single output is to enable &quot;pi=\r\nping&quot; or multiplexing largely disparate functions through it. Perhaps one m=\r\nethod is to introduce a transfer function available to the CPPN that produc=\r\nes a high/on output only for a specific range of input value, and low/off o=\r\nutput for everything else. Seems plausible, except we already commonly use =\r\ntthe Gaussian function with a bias, which can already perform this function=\r\n (especially when combined serially with a step function). Perhaps introduc=\r\ning a specialised transfer function (eg akin to a Gaussian and step functio=\r\nn in series, or something more like an entire multiplexer in a node) for th=\r\nis task would help things along?\n&gt; \n&gt; Thoughts and/or experimental results,=\r\n anyone? :)\n&gt;\n\n\n\n"}}