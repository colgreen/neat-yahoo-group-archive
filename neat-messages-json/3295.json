{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":283334584,"authorName":"petar_chervenski","from":"&quot;petar_chervenski&quot; &lt;petar_chervenski@...&gt;","profile":"petar_chervenski","replyTo":"LIST","senderId":"pYUTI-E6ER6HANVFMfEfuzSY7GHxbELfYuiSP6kF9p0gVNFdds7pvhu9jAOJDI2mNy6W79vKOSkDscbGX9sBNi1eB1vWzbLNJu9nLzo7Op4VFMQLoeg","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Placement of conceptually different inputs in HyperNEAT?","postDate":"1179343197","msgId":3295,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGYyZmxndCtvbWFnQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGYyZmhsais4OWVsQGVHcm91cHMuY29tPg=="},"prevInTopic":3294,"nextInTopic":3296,"prevInTime":3294,"nextInTime":3296,"topicId":3278,"numMessagesInTopic":20,"msgSnippet":"What about the general theory behind CPPNs? The dimensionality of the CPPN input/output space is ALWAYS the same, assuming that the dimensionality of the","rawEmail":"Return-Path: &lt;petar_chervenski@...&gt;\r\nX-Sender: petar_chervenski@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 18490 invoked from network); 16 May 2007 19:20:48 -0000\r\nReceived: from unknown (66.218.67.35)\n  by m42.grp.scd.yahoo.com with QMQP; 16 May 2007 19:20:48 -0000\r\nReceived: from unknown (HELO n26c.bullet.scd.yahoo.com) (66.218.67.218)\n  by mta9.grp.scd.yahoo.com with SMTP; 16 May 2007 19:20:41 -0000\r\nReceived: from [209.73.164.86] by n26.bullet.scd.yahoo.com with NNFMP; 16 May 2007 19:19:57 -0000\r\nReceived: from [66.218.66.74] by t8.bullet.scd.yahoo.com with NNFMP; 16 May 2007 19:19:57 -0000\r\nDate: Wed, 16 May 2007 19:19:57 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;f2flgt+omag@...&gt;\r\nIn-Reply-To: &lt;f2fhlj+89el@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0\r\nFrom: &quot;petar_chervenski&quot; &lt;petar_chervenski@...&gt;\r\nSubject: Re: Placement of conceptually different inputs in HyperNEAT?\r\nX-Yahoo-Group-Post: member; u=283334584; y=ujQMashYs1sISC8ehwtAaj4gpT8MA8XO5JmpcEYdg8J6qFQgICWCr29AWg\r\nX-Yahoo-Profile: petar_chervenski\r\n\r\nWhat about the general theory behind CPPNs? The dimensionality of the \nCPPN=\r\n input/output space is ALWAYS the same, assuming that the \ndimensionality o=\r\nf the substrate space is known. If the substrate \nspace is 2D, the simplest=\r\n connective CPPN (without additional inputs \nto boost the evolution) will h=\r\nave 5 inputs (bias node (1.0) counted), \nin 3D the inputs will be 7. And th=\r\nis is enough to achieve ANY \nfunctionality within the substrate, since the =\r\nCPPNs complexify. The \nbasic coordinate frames (x1, y1, x2, y2) are passed =\r\nthrough more and \nmore coordinate frames, creating very complex connective =\r\npatterns. \nThe substrate can have any number of inputs/hidden/outputs, no m=\r\natter \nwhat the topology of the CPPN is or the CPPN&#39;s input/output \ndimensi=\r\nonality. The substrate can scale up to millions of \nnodes/connections, but =\r\nits CPPN is still the same. \nThere are 2 spaces in general - the phenotype =\r\nspace (the neural \nsubstrate) and the genotype space (CPPN). I believe it i=\r\ns the \nphenotype space, that you call &quot;design space&quot;. (Am I wrong?)\nThe inp=\r\nuts provided in the substrate should be chosen as wise as in \nordinary ANN =\r\nexperiment. But in the HyperNEAT case, you can provide \ngeometric informati=\r\non like a bonus to the evolutionary algorithm. All \ninputs/outputs are prob=\r\nlem specific, of course :) \nYour explanation looks like the CPPN can depend=\r\n on the substrate in \nsome way. I don&#39;t think this is possible. \nMaybe I di=\r\ndn&#39;t understand you correctly, I don&#39;t know. Can you give \nme some specific=\r\n example to clear out what you exactly mean? \n\nPeter\n\n\n--- In neat@yahoogro=\r\nups.com, &quot;afcarl2&quot; &lt;a.carl@...&gt; wrote:\n&gt;\n&gt; Peter,\n&gt; \n&gt; Given an arbitrary p=\r\noint &quot;P&quot; in design space, comprised of the \n&gt; following:\n&gt; (a) problem spec=\r\nific inputs (PSIs);\n&gt; (b) auxiliary additional parameters associated w/ PSI=\r\ns;\n&gt; (c) problem specific outputs (PSOs);\n&gt; (d) auxiliary additional parame=\r\nters associated w/ PSOs;\n&gt; \n&gt; Substrate Inputs: comprised of PSIs, &quot;a&quot; abov=\r\ne;\n&gt; Substrate Outputs: comprised of PSOs, &quot;c&quot; above;\n&gt; \n&gt; Hypercube Inputs=\r\n: comprised of auxiliary additional parameters \n&gt; associated w/ PSIs & PSOs=\r\n, &quot;b&quot; & &quot;d&quot; above;\n&gt; Hypercube Output: weight between current pair of subst=\r\nrate nodes\n&gt; \n&gt; If:\n&gt; (1) the dimensionality of the hypercube input &quot;from&quot; =\r\nnode is \n&gt; different in number and/or nature as the links in the substrate =\r\n\n&gt; network are looped over, or\n&gt; (2)  the dimensionality of the hypercube i=\r\nnput &quot;to&quot; node is \ndifferent \n&gt; in number and/or nature as the links in the=\r\n substrate network are \n&gt; looped over,\n&gt; \n&gt; ...then the topology of the hyp=\r\nercube inputs are different/change \nas \n&gt; you loop over the substrate links=\r\n, while computing the current link \n&gt; weight with the hypercube network.\n&gt; =\r\n\n&gt;    The examples and explanations provided to date do not address \nthis \n=\r\n&gt; more general condition.\n&gt; \n&gt;    Does this explain the issue in question b=\r\netter?\n&gt; \n&gt; Thanks,\n&gt;    Andy Carl\n&gt; \n&gt; \n&gt; --- In neat@yahoogroups.com, &quot;pe=\r\ntar_chervenski&quot; \n&gt; &lt;petar_chervenski@&gt; wrote:\n&gt; &gt;\n&gt; &gt; Hi Andy, \n&gt; &gt; \n&gt; &gt; I =\r\nam sorry, but I didn&#39;t understand what you mean by &quot;hypercube \n&gt; input \n&gt; &gt;=\r\n topology&quot;? \n&gt; &gt; As far as I know, this is simply a CPPN taking 2 points fr=\r\nom the \n&gt; &gt; substrate space (connective CPPN) instead of only one (spatial =\r\n\n&gt; CPPN). \n&gt; &gt; The coordinates of this points in the substrate space are th=\r\ne \n&gt; inputs \n&gt; &gt; to this CPPN and the output is the weight/point intensity.=\r\n \n&gt; &gt; The topology between these inputs and the output node is evolved \n&gt; w=\r\nith \n&gt; &gt; the regular NEAT method. The evolved topology and the weights are =\r\n\n&gt; the \n&gt; &gt; most important thing to consider. \n&gt; &gt; I guess this topology ca=\r\nn change? Maybe in time (leaky neurons) \nor \n&gt; &gt; with recurrent connections=\r\n?\n&gt; &gt; The idea of changing input topology for the CPPN is like \n&gt; &gt; HyperHy=\r\nperNEAT :) CPPNs that further create CPPNs..\n&gt; &gt; \n&gt; &gt; Peter\n&gt; &gt; \n&gt; &gt; --- In=\r\n neat@yahoogroups.com, &quot;afcarl2&quot; &lt;a.carl@&gt; wrote:\n&gt; &gt; &gt;\n&gt; &gt; &gt; Ken,\n&gt; &gt; &gt; \n&gt;=\r\n &gt; &gt;    It appears that you keep attempting to recast the substrate \n&gt; &gt; in=\r\nputs \n&gt; &gt; &gt; and outputs such that the hypercube input topology remains \n&gt; &gt;=\r\n constant. \n&gt; &gt; &gt; Not taking anything away from HyperNEAT&#39;s ability to expl=\r\noit \n&gt; &gt; 1d/2d/3d \n&gt; &gt; &gt; geometry in which the hypercube input topology rem=\r\nains \nconstant, \n&gt; &gt; the \n&gt; &gt; &gt; more general question of when the hypercube=\r\n input topology is \nnot \n&gt; &gt; &gt; constant, what do you do? Please address thi=\r\ns specific issue. \nThe \n&gt; &gt; &gt; answer to this question will help to identify=\r\n the \n&gt; &gt; &gt; applicability/limitations of HyperNEAT.\n&gt; &gt; &gt; \n&gt; &gt; &gt; Thanks,\n&gt; =\r\n&gt; &gt;    Andy Carl\n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; &gt; --- In neat@yahoogroups.com, &quot;Kenneth =\r\nStanley&quot; &lt;kstanley@&gt; \nwrote:\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; Thomas, this is a great questi=\r\non that highlights an important \n&gt; &gt; area \n&gt; &gt; &gt; for \n&gt; &gt; &gt; &gt; further resea=\r\nrch.  We can speculate on the right approach in \n&gt; this \n&gt; &gt; &gt; case \n&gt; &gt; &gt; =\r\n&gt; (and I have my hunches) but we won&#39;t know until we do some \n&gt; &gt; &gt; experim=\r\nents \n&gt; &gt; &gt; &gt; and try ideas out.\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; To give some of my though=\r\nts, perhaps the most obvious thing \nis \n&gt; &gt; &gt; similar \n&gt; &gt; &gt; &gt; to what you =\r\nsuggest:  Have two separate input sets, one for \n&gt; &gt; &gt; the &quot;type A \n&gt; &gt; &gt; &gt;=\r\n info&quot; and one for &quot;type B info.&quot;  \n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; The way this would loo=\r\nk is e.g. if A was 2D and B was 2D then \n&gt; the \n&gt; &gt; &gt; CPPN \n&gt; &gt; &gt; &gt; would h=\r\nave 4 inputs, 2 represent A source nodes and 2 \n&gt; &gt; representing \n&gt; &gt; &gt; B \n=\r\n&gt; &gt; &gt; &gt; soource nodes.\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; Then, I would still have only one s=\r\net of outputs.  I am still \na \n&gt; &gt; &gt; little \n&gt; &gt; &gt; &gt; unclear on why you say=\r\n the outputs are from the second \n&gt; &gt; substrate?  \n&gt; &gt; &gt; In \n&gt; &gt; &gt; &gt; my und=\r\nerstanding, this is still all one substrate, it&#39;s just \n&gt; that \n&gt; &gt; &gt; it \n&gt;=\r\n &gt; &gt; &gt; collects inputs from two different sources.  But it can still \n&gt; &gt; e=\r\nxist \n&gt; &gt; &gt; &gt; with only one set of outputs.  So if the outputs are 2D, the =\r\n\n&gt; CPPN \n&gt; &gt; &gt; would \n&gt; &gt; &gt; &gt; have 2 more inputs representing the output ar=\r\nea for a total \nof \n&gt; 6 \n&gt; &gt; &gt; inputs \n&gt; &gt; &gt; &gt; (and one output representing=\r\n the weight of a connection in \nthe \n&gt; &gt; &gt; &gt; substrate).\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; I=\r\nf there was a hidden layer you could have it exist in either \n&gt; the \n&gt; &gt; &gt; =\r\nsame \n&gt; &gt; &gt; &gt; geometry as A, the same geometry as B, or the same geometry \n=\r\nas \n&gt; &gt; the \n&gt; &gt; &gt; &gt; outputs.  (For example by introducing a z dimension to=\r\n any of \n&gt; &gt; &gt; these)  \n&gt; &gt; &gt; &gt; Or you could have each kind of hidden layer=\r\n all at once.\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; Of course, as you note, there are a lot of w=\r\nays to do \nsomething \n&gt; &gt; &gt; like \n&gt; &gt; &gt; &gt; this.  I think it will be a great=\r\n issue to explore, but it \nmay \n&gt; &gt; &gt; require \n&gt; &gt; &gt; &gt; starting with a very=\r\n simple domain that isolates the issue \n&gt; (i.e. \n&gt; &gt; &gt; &gt; orthogonal input t=\r\nypes) so we can concentrate on that rather \n&gt; than \n&gt; &gt; &gt; on a \n&gt; &gt; &gt; &gt; spe=\r\ncific domain with its own complications.\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; One other issue t=\r\no note is that even if two sets of inputs \nare \n&gt; in \n&gt; &gt; &gt; &gt; effect unrela=\r\nted, it may not necessarily hurt antyhing to put \n&gt; &gt; them \n&gt; &gt; &gt; in \n&gt; &gt; &gt;=\r\n &gt; the same geometry.  For example, they could be two separate \n&gt; &gt; planes =\r\n\n&gt; &gt; &gt; in \n&gt; &gt; &gt; &gt; the same cube.  On the face of it, it seems like that co=\r\nuld \n&gt; cause \n&gt; &gt; &gt; &gt; problems, but we don&#39;t yet know whether the situation=\r\n is that \n&gt; &gt; &gt; difficult \n&gt; &gt; &gt; &gt; to deal with in practice.  There could a=\r\nlso be hidden \n&gt; &gt; relationships \n&gt; &gt; &gt; &gt; that are not obvious but still ex=\r\nploitable in a surprising \nway.\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; ken\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; \n&gt; &gt; =\r\n&gt; &gt; \n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; --- In neat@yahoogroups.com, &quot;Thomas Johnson&quot; \n&gt; &lt;tho=\r\nmas.j.johnson@&gt; \n&gt; &gt; &gt; &gt; wrote:\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; HyperNEAT folks,\n&gt; &gt; &gt; =\r\n&gt; &gt; \n&gt; &gt; &gt; &gt; &gt; What&#39;s the recommended strategy for handling inputs that \n&gt; =\r\n&gt; &gt; represent \n&gt; &gt; &gt; &gt; concepts\n&gt; &gt; &gt; &gt; &gt; that are not geometrically relate=\r\nd? For instance, consider \n&gt; &gt; &gt; something \n&gt; &gt; &gt; &gt; like a\n&gt; &gt; &gt; &gt; &gt; poker =\r\ngame where we want to input both players&#39; actions and \n&gt; the \n&gt; &gt; &gt; cards \n=\r\n&gt; &gt; &gt; &gt; we\n&gt; &gt; &gt; &gt; &gt; see. We might be able to come up with geometric encodi=\r\nngs \nfor \n&gt; &gt; &gt; either \n&gt; &gt; &gt; &gt; of\n&gt; &gt; &gt; &gt; &gt; these individually, but there&#39;=\r\ns no obvious spacial \n&gt; relationship \n&gt; &gt; &gt; &gt; between\n&gt; &gt; &gt; &gt; &gt; cards and a=\r\nctions.\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt;\n&gt;\n\n\n\n"}}