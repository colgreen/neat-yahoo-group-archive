{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":211599040,"authorName":"Jeff Clune","from":"Jeff Clune &lt;jclune@...&gt;","profile":"jeffreyclune","replyTo":"LIST","senderId":"7dIbzbTfNgwCFlkIPCo4Vi2zMDuGfY0_LRdfa23c68kW2iMDrlmD0-0avkjNm0qO_vsopuLpa9K4ZuNPny3Lz94B","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [neat] HybrID: A Hybridization of Indirect and Direct Encodings  for Evolutionary Computation","postDate":"1248667520","msgId":4786,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PEM2OTI5RkMwLjJCODg0JWpjbHVuZUBtc3UuZWR1Pg==","inReplyToHeader":"PDcyN2E0MDZjMDkwNzI2MDQ1MWczM2MyMWQ2NG85ZTg0ODk2NmRiM2Q4ODljQG1haWwuZ21haWwuY29tPg=="},"prevInTopic":4785,"nextInTopic":4790,"prevInTime":4785,"nextInTime":4787,"topicId":4772,"numMessagesInTopic":19,"msgSnippet":"... No problem. Thank you for your interest. ... While I agree that there could be a performance boost for very large and sparse networks by eliminating links,","rawEmail":"Return-Path: &lt;jclune@...&gt;\r\nX-Sender: jclune@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 63970 invoked from network); 27 Jul 2009 04:06:24 -0000\r\nX-Received: from unknown (69.147.108.200)\n  by m2.grp.re1.yahoo.com with QMQP; 27 Jul 2009 04:06:24 -0000\r\nX-Received: from unknown (HELO an-out-0708.google.com) (209.85.132.248)\n  by mta1.grp.re1.yahoo.com with SMTP; 27 Jul 2009 04:06:24 -0000\r\nX-Received: by an-out-0708.google.com with SMTP id c3so1469282ana.8\n        for &lt;neat@yahoogroups.com&gt;; Sun, 26 Jul 2009 21:05:24 -0700 (PDT)\r\nX-Received: by 10.100.34.10 with SMTP id h10mr7594910anh.182.1248667524115;\n        Sun, 26 Jul 2009 21:05:24 -0700 (PDT)\r\nReturn-Path: &lt;jclune@...&gt;\r\nX-Received: from ?10.0.1.2? (c-76-20-191-220.hsd1.mi.comcast.net [76.20.191.220])\n        by mx.google.com with ESMTPS id d38sm14407533and.1.2009.07.26.21.05.22\n        (version=TLSv1/SSLv3 cipher=RC4-MD5);\n        Sun, 26 Jul 2009 21:05:23 -0700 (PDT)\r\nUser-Agent: Microsoft-Entourage/12.13.0.080930\r\nDate: Mon, 27 Jul 2009 00:05:20 -0400\r\nTo: &quot;neat@yahoogroups.com&quot; &lt;neat@yahoogroups.com&gt;\r\nMessage-ID: &lt;C6929FC0.2B884%jclune@...&gt;\r\nThread-Topic: [neat] HybrID: A Hybridization of Indirect and Direct Encodings \n for Evolutionary Computation\r\nThread-Index: AcoOb3T3pn7vcx6MU0OM0kIbyjzr8g==\r\nIn-Reply-To: &lt;727a406c0907260451g33c21d64o9e848966db3d889c@...&gt;\r\nMime-version: 1.0\r\nContent-type: text/plain;\n\tcharset=&quot;US-ASCII&quot;\r\nContent-transfer-encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Jeff Clune &lt;jclune@...&gt;\r\nSubject: Re: [neat] HybrID: A Hybridization of Indirect and Direct Encodings \n for Evolutionary Computation\r\nX-Yahoo-Group-Post: member; u=211599040; y=b22v4tGy0sFz3KVnpNMuBrr8NGGLyk0Dr2ap0GWtBvNQ3C6VKSiL\r\nX-Yahoo-Profile: jeffreyclune\r\n\r\n&gt; Firstly thanks to Jeff for the paper, a very interesting idea and\n&gt; simple to implement to boot.\n\nNo problem. Thank you for your interest.\n\n&gt; Reading Jean-Baptiste&#39;s response got me thinking that there&#39;s a\n&gt; broader question here about the elimintation of connections with low\n&gt; weight (e.g.+-0.2). The &#39;pure&#39; approach is to leave them in, the\n&gt; motivation for taking them out is practicality - you may end up with\n&gt; millions of connections that have very little functional role, along\n&gt; with their resulting memory and CPU overhead.\n&gt; \n&gt; I think it&#39;s fair to say though that low weight connections aren&#39;t\n&gt; completely useless and therefore we have made a trade-off between\n&gt; practicality and the power of hyperNEAT&#39;s generative encoding.\n&gt;Overall\n&gt; it makes sense because the potential gain from weak connections is\n&gt; outweighed by the very large resource cost in simulating them.\n\nWhile I agree that there could be a performance boost for very large and\nsparse networks by eliminating links, I do not think the current code base\ngets any performance boost from &#39;missing&#39; links, because the links are still\nthere and are just set to 0. I believe the code still does the math on all\nlinks...for missing links it just multiplies by zero. (Please correct me if\nI am wrong Jason.) That of course could be changed.\n\n&gt;However\n&gt; there may be better alternatives:\n&gt; 1) Simply reduce the cut off range. (What is the optimal cut-off?)\n\nI have also often wondered what a good cutoff is. It would be interesting to\nsee some experimental data on this. Has anyone done any testing on this\nfront?\n\n&gt; 2) Devise CPPNs with an additional connection on/off output.\n&gt; 3) Use HybrID to refine *all* of the weights over their full range,\n&gt; regardles of whether they were eliminated during the HyperNEAT phase.\n\nIf I understand you correctly, this is what we did in the paper.\n\n&gt; This then raises the question of how much HybrID&#39;s performance is down\n&gt; to this effect.\n\nI do not understand what you mean by this comment. Can you expand it a bit?\n\n&gt; Even more broadly, given the potentially huge resource cost of ANNs\n&gt; generated by CPPNs perhaps we should consider factoring that cost into\n&gt; the fitness functions. That way we allow evolution to find the sweet\n&gt; spot between good performance on the objective function and lousy\n&gt; resource utilisation (which slows down search speed and thus the power\n&gt; of the overall search method)\n\nThat is a cool idea. It would be cool to see someone try it. I definitely\nhave run into the case where working with HyperNEAT on really large networks\nbecomes too slow to be tenable....so if you could show that evolution could\ncreate sparse networks as a solution (and that there were huge speedups to\nbe gained) that would be a nice result.\n\n&gt; BTW HybrID reminds me of Hinton&#39;s explanation of the relationship\n&gt; between RBMs and backprop - that RBMs are good at global search and\n&gt; backprop at refining that search once RBMs haved honed in on a good\n&gt; solution. These are promising results indeed.\n\nThanks! I agree with you that in HybrID the generative encoding can do the\nglobal search to get you in the right neighborhood and then the direct\nencoding can take over to do a more local search and\nfine-tuning/optimization.\n\n\nCheers,\nJeff Clune\n\nDigital Evolution Lab, Michigan State University\n\njclune@...\n\n\n\n"}}