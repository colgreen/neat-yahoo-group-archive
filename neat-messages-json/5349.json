{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":200957992,"authorName":"Jason Gauci","from":"Jason Gauci &lt;jgmath2000@...&gt;","profile":"jgmath2000","replyTo":"LIST","senderId":"E-kbfUvaJNx4Uxnf6sN7bw2_QR1nHnZHhe5Cq6sRN9asej_aBZigpnH-XgZxXPIoncqqGmbn3VMgW0whUez2Cf4JUvxDIygwbg","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: Serialization within HyperNEAT","postDate":"1288725441","msgId":5349,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PEFBTkxrVGltT1FHdGlDaWR4VHpMMkN3YjFTVU96UWRqWFlQZzIwVlRldEdxRkBtYWlsLmdtYWlsLmNvbT4="},"prevInTopic":5348,"nextInTopic":0,"prevInTime":5348,"nextInTime":5350,"topicId":5342,"numMessagesInTopic":8,"msgSnippet":"Although MPI is designed to work across multiple computers, there s no reason why you can t use MPI with a single computer. By default if you don t have an mpi","rawEmail":"Return-Path: &lt;jgmath2000@...&gt;\r\nX-Sender: jgmath2000@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 4492 invoked from network); 2 Nov 2010 19:17:23 -0000\r\nX-Received: from unknown (98.137.34.44)\n  by m7.grp.sp2.yahoo.com with QMQP; 2 Nov 2010 19:17:23 -0000\r\nX-Received: from unknown (HELO mail-wy0-f175.google.com) (74.125.82.175)\n  by mta1.grp.sp2.yahoo.com with SMTP; 2 Nov 2010 19:17:23 -0000\r\nX-Received: by wya21 with SMTP id 21so7151539wya.34\n        for &lt;neat@yahoogroups.com&gt;; Tue, 02 Nov 2010 12:17:22 -0700 (PDT)\r\nMIME-Version: 1.0\r\nX-Received: by 10.227.157.79 with SMTP id a15mr9083799wbx.208.1288725441773;\n Tue, 02 Nov 2010 12:17:21 -0700 (PDT)\r\nX-Received: by 10.217.6.4 with HTTP; Tue, 2 Nov 2010 12:17:21 -0700 (PDT)\r\nDate: Tue, 2 Nov 2010 15:17:21 -0400\r\nMessage-ID: &lt;AANLkTimOQGtiCidxTzL2Cwb1SUOzQdjXYPg20VTetGqF@...&gt;\r\nTo: neat@yahoogroups.com\r\nContent-Type: multipart/alternative; boundary=001636458f4e2b8969049416c6ff\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Jason Gauci &lt;jgmath2000@...&gt;\r\nSubject: Re: Serialization within HyperNEAT\r\nX-Yahoo-Group-Post: member; u=200957992; y=AEH9mDmKqy2aEm8demkzJWLmEDxvVrXdYvQSXFMyyL4PHqI3aQ\r\nX-Yahoo-Profile: jgmath2000\r\n\r\n\r\n--001636458f4e2b8969049416c6ff\r\nContent-Type: text/plain; charset=ISO-8859-1\r\n\r\nAlthough MPI is designed to work across multiple computers, there&#39;s no\nreason why you can&#39;t use MPI with a single computer. By default if you don&#39;t\nhave an mpi hosts file and specify the number of nodes (i.e. threads)\nmanually in mpirun.exe, it will treat your multi-core computer as several\nnodes.\n\r\n--001636458f4e2b8969049416c6ff\r\nContent-Type: text/html; charset=ISO-8859-1\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\nAlthough MPI is designed to work across multiple computers, there&#39;s no =\r\nreason why you can&#39;t use MPI with a single computer. By default if you =\r\ndon&#39;t have an mpi hosts file and specify the number of nodes (i.e. thre=\r\nads) manually in mpirun.exe, it will treat your multi-core computer as seve=\r\nral nodes.&lt;br&gt;\n\r\n--001636458f4e2b8969049416c6ff--\r\n\n"}}