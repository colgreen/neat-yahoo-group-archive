{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":487025037,"authorName":"afcarl2","from":"&quot;afcarl2&quot; &lt;afcarl2@...&gt;","profile":"afcarl2","replyTo":"LIST","senderId":"8VhmVbT-evF4b13bcMhk56E7E0UBbHQabAM6ouEqEmIRU-7qY6ObskExdJN1piOfGnjAvq9yYgAsTYU6__IYYvAcB-o","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: Python NEAT","postDate":"1310236586","msgId":5618,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGl2YTczYStxMDltQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGl2NjY1Zyt1MnZxQGVHcm91cHMuY29tPg=="},"prevInTopic":5615,"nextInTopic":5659,"prevInTime":5617,"nextInTime":5619,"topicId":535,"numMessagesInTopic":47,"msgSnippet":"Peter, I concur, as I went to a lot of trouble to generalize the C++ code for variable number of inputs/outputs, enabling embedding a network within a node,","rawEmail":"Return-Path: &lt;afcarl2@...&gt;\r\nX-Sender: afcarl2@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 89708 invoked from network); 9 Jul 2011 18:36:27 -0000\r\nX-Received: from unknown (98.137.34.44)\n  by m5.grp.sp2.yahoo.com with QMQP; 9 Jul 2011 18:36:27 -0000\r\nX-Received: from unknown (HELO n46b.bullet.mail.sp1.yahoo.com) (66.163.168.160)\n  by mta1.grp.sp2.yahoo.com with SMTP; 9 Jul 2011 18:36:27 -0000\r\nX-Received: from [69.147.65.149] by n46.bullet.mail.sp1.yahoo.com with NNFMP; 09 Jul 2011 18:36:27 -0000\r\nX-Received: from [98.137.34.119] by t9.bullet.mail.sp1.yahoo.com with NNFMP; 09 Jul 2011 18:36:26 -0000\r\nDate: Sat, 09 Jul 2011 18:36:26 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;iva73a+q09m@...&gt;\r\nIn-Reply-To: &lt;iv665g+u2vq@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nFrom: &quot;afcarl2&quot; &lt;afcarl2@...&gt;\r\nSubject: Re: Python NEAT\r\nX-Yahoo-Group-Post: member; u=487025037; y=NRleU4krCaXUQQP-iEbRA9bIHNv3uDHityywmvLUsr1fAw\r\nX-Yahoo-Profile: afcarl2\r\n\r\nPeter,\n\nI concur, as I went to a lot of trouble to generalize the C++ code =\r\nfor variable number of inputs/outputs, enabling embedding a network within =\r\na node, and modifying all of the class and namespace definitions to enable =\r\nrecursion within the same memory space without stepping on each other, with=\r\n an optional evolving supervisor network for parameter evolution and an xml=\r\n-based history retention to avoid duplication within the search space for h=\r\nigh-cost evaluation functions.\n\nAndy\n\n--- In neat@yahoogroups.com, &quot;petar_c=\r\nhervenski&quot; &lt;petar_chervenski@...&gt; wrote:\n&gt;\n&gt; Well, exactly. I didn&#39;t mentio=\r\nn anything about evaluation, though, this is simply a function that takes a=\r\nrbitrary graph (a genome) and returns a fitness value. A domain-dependent b=\r\nlack box. Graphs are perfect for complexification. Most (or all) implementa=\r\ntions of NEAT are designed for evolution of neural networks, and modifying =\r\nexisting code to evolve any graph becomes tricky. For example, recurrence i=\r\ns treated like a special case sometimes and no two nodes can have more than=\r\n one (or 2 with recurrence) connections. Inputs and outputs are fixed and i=\r\nnputs don&#39;t get connected with links pointing to them. Break one of these r=\r\nules and NEAT becomes less effective at evolution of neural networks - you =\r\nmay get what you want, but at the cost of damaging the existing code that p=\r\nreviously worked fine. It&#39;s better to start from scratch and make the code =\r\nas general as possible. NN evolution will be derived from it. \n&gt; \n&gt; Peter\n&gt;=\r\n \n&gt; --- In neat@yahoogroups.com, &quot;afcarl2&quot; &lt;afcarl2@&gt; wrote:\n&gt; &gt;\n&gt; &gt; \n&gt; &gt; \n=\r\n&gt; &gt; \n&gt; &gt; Peter,\n&gt; &gt; \n&gt; &gt; A couple of thoughts. First, IMO one of the primar=\r\ny values of NEAT are the various methods of managing complexification of st=\r\nructure, not how the infrastructure is constrained to match NN evaluation. =\r\nMany profoundly useful applications are entirely non-NN in nature, and the =\r\ngenome itself is the answer, not the NN evaluation or activation output. Th=\r\ne genome is passed thru a translator and the evaluation is handled by a sep=\r\narate analysis code. An example being the propulsion schematic of a rocket,=\r\n where the genome directly represents the schematic, and the objective func=\r\ntion is defined by how closely the resulting prediction matches desired per=\r\nformance on a weight and/or cost basis. The ability to seed the initial pop=\r\nulation with previous &quot;simular-to&quot; designs/genomes, allows the incorporatio=\r\nn human expert input directly, and an obvious interpretation of best result=\r\ning genomes output. In this instance, the determination of directed/non-dir=\r\nected/multigraph is moot within NEAT, the appropriate interpretation is mad=\r\ne within the translator prior to execution by the external analysis code, g=\r\niven proper and adequate meta-data.\n&gt; &gt; \n&gt; &gt; In the other instance of direc=\r\nt evaluation of the network, the generality on varying number of node input=\r\ns/output, evaluation functions of nodes and embedded networks within a node=\r\n can be addressed by interpretation of node type and genome definition nome=\r\nnclature. It would seem that the explosion in size of the search space woul=\r\nd tend to be addressed via the complexification process as additional dimen=\r\nsionality is justified as a consequence of incremental improved objective f=\r\nunction value.\n&gt; &gt; \n&gt; &gt; Andy\n&gt; &gt; \n&gt; &gt; --- In neat@yahoogroups.com, &quot;petar_c=\r\nhervenski&quot; &lt;petar_chervenski@&gt; wrote:\n&gt; &gt; &gt;\n&gt; &gt; &gt; Hi Andy, \n&gt; &gt; &gt; \n&gt; &gt; &gt; Th=\r\ne implementation is not yet complete, but things are really good so far. Ne=\r\ntworkX is a very good choice for the genomes, as it gives me a lot of freed=\r\nom, many bonus functions for the graphs, and saves me debugging time. Basic=\r\nally this implementation allows any kind of graph to be evolved. For exampl=\r\ne, nodes and edges can represent numeric data (integer & float) or objects =\r\nfrom a set (like characters from an alphabet, class instances or functions,=\r\n etc.). When you define the graph type, you have to write a distance functi=\r\non if you have objects from a set in the graph - like the distance between =\r\ncharacters in the alphabet), and also a few more functions like mutators, w=\r\nhich, of course, will change the objects randomly or the way you like. You =\r\ncan have any number of properties for nodes/edges. You can also have an und=\r\nirected graph, or a multigraph where many edges connect the same nodes. You=\r\n can even have nested genomes, where genomes are the objects from a set, wh=\r\nich are attached to nodes or edges. This makes things mind-blowing and lift=\r\ns NEAT to a much broader set of domains. I even think that the N in NEAT is=\r\n somehow unnecessary here, as the primary objective in this implementation =\r\nis not neural networks. Neural networks will be derived from a special func=\r\ntion that will translate the graph and then build a C++ object from it. The=\r\ny will be directed graphs with sigmoid or whatever functions attached to no=\r\ndes and floats attached to edges. (Node types also attached to nodes - to k=\r\nnow what is input and output). Perhaps I&#39;ll make a separate project designe=\r\nd for neural networks that will use the core module. \n&gt; &gt; &gt; I will release =\r\nthe first version of the code soon, which will probably not have rtNEAT and=\r\n novelty search built in. CPPNs and HyperNEAT are just special cases of gra=\r\nph evolution and interpretation, like neural networks. The special code abo=\r\nut them will be added later as the project evolves. Perhaps the community w=\r\nill like it and contribute some code. I can&#39;t promise a release date, but w=\r\nork is progressing. Any ideas to minimize the search space (which blows up =\r\nas you add more properties to nodes and edges) are appreciated. Also I coul=\r\nd use some help about innovation numbers and crossover between undirected a=\r\nnd multi graphs. I&#39;m so afraid of bugs in these cases that I haven&#39;t even s=\r\ntarted to think about it. :D\n&gt; &gt; &gt; \n&gt; &gt; &gt; Peter\n&gt; &gt; &gt; \n&gt; &gt; &gt; --- In neat@ya=\r\nhoogroups.com, &quot;afcarl2&quot; &lt;afcarl2@&gt; wrote:\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; Hi Peter,\n&gt; &gt; &gt; =\r\n&gt; \n&gt; &gt; &gt; &gt; How is your python implementation going? Took a look at the Netw=\r\norkX module. It looks very interesting! In my C++ version, I had added vari=\r\nable input/output connections, network within a node and seeding of the ini=\r\ntial population with the required infrastructure updates and a distributed =\r\nprocessing backend. But what you are doing goes so much farther, that I am =\r\neager to get a look at it!\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; Andy\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; --- In ne=\r\nat@yahoogroups.com, &quot;petar_chervenski&quot; &lt;petar_chervenski@&gt; wrote:\n&gt; &gt; &gt; &gt; &gt;=\r\n\n&gt; &gt; &gt; &gt; &gt; Hi all, \n&gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; I am almost done with the basic co=\r\nde and I&#39;ll mention some of its features now. I decided to use NetworkX for=\r\n the genomes, because this module has lots of useful algorithms and allows =\r\nany python object to be a node and edges can be associated with anything. T=\r\nhis makes the evolution of neural networks a tiny part of what&#39;s really pos=\r\nsible. Any graph can be evolved, including undirected graphs and nodes/edge=\r\ns containing discrete one-of-N values (integers, lists of python objects, e=\r\ntc). So given that an evaluation function exists for any kind of graph, you=\r\n can quickly setup evolution. Neural networks are a particular kind of grap=\r\nhs and the package will have built in code necessary to evolve neural netwo=\r\nrks - the initialization functions, mutators, and a C++ interface to a clas=\r\ns that represents the phenotypes. CPPNs support is trivial to make, and giv=\r\nen that python functions themselves can be attached to nodes, it&#39;s possible=\r\n to have algorithmic nodes working with more than one variable and .. well,=\r\n infinite stuff. OK, I gotta go. Wish me luck debugging. Talk to you soon. =\r\n:) \n&gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; Peter\n&gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; --- In neat@yahoogroups=\r\n.com, Jan van der Lugt &lt;janlugt@&gt; wrote:\n&gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; Hi Peter,\n=\r\n&gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; Sound like an ambitious and noble plan. Good luck =\r\ncoding, I&#39;m looking\n&gt; &gt; &gt; &gt; &gt; &gt; forward to seeing your results!\n&gt; &gt; &gt; &gt; &gt; &gt;=\r\n \n&gt; &gt; &gt; &gt; &gt; &gt; Regards,\n&gt; &gt; &gt; &gt; &gt; &gt; Jan\n&gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; On Mon, Mar=\r\n 14, 2011 at 12:43, petar_chervenski\n&gt; &gt; &gt; &gt; &gt; &gt; &lt;petar_chervenski@&gt;wrote:\n=\r\n&gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; Hi people,\n&gt; &gt; &gt; &gt; &gt;=\r\n &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; For about a month I&#39;ll be writing a Python implementatio=\r\nn of NEAT, which\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; includes all advances in the recent years, i=\r\nncluding rtNEAT, phased\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; searching, leaky integrators, HyperNE=\r\nAT, HyperNEAT with evolving substrates,\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; and novelty search. C=\r\noevolution code and visualizations will be included.\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; Optimize=\r\nd C++ code for running the NNs too. This code will be free and I\n&gt; &gt; &gt; &gt; &gt; =\r\n&gt; &gt; promise this will be the best NEAT code I can write. No bugs, no meanin=\r\ngless\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; NNs, etc.\n&gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; Peter\n&gt; &gt; &gt; &gt; &gt; &gt;=\r\n &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt;  \n&gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt;\n&gt;\n=\r\n\n\n\n"}}