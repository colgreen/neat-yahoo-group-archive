{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":82117382,"authorName":"Jim O&#39;Flaherty, Jr.","from":"&quot;Jim O&#39;Flaherty, Jr.&quot; &lt;jim_oflaherty_jr@...&gt;","profile":"jim_oflaherty_jr","replyTo":"LIST","senderId":"MelDPsH-JnMVQDAJXB5P1Oh-hMP7fYmfTrO_VNObTCVG_vOMkdUwQOxxHiGcObkn-ypc1PeID9HNziyKA7sHRgKtVWPdsUdlgNJJptgt3cx51MzteQL1H6k","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] Re: Bloat","postDate":"1086652147","msgId":1026,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDAwYzkwMWM0NGNlYSQwNjFmYzRhMCQzMjAxYThjMEBORVdBR0U+","referencesHeader":"PGNhMnQ4OStzMXRpQGVHcm91cHMuY29tPg=="},"prevInTopic":1025,"nextInTopic":1029,"prevInTime":1025,"nextInTime":1027,"topicId":904,"numMessagesInTopic":68,"msgSnippet":"Ken, Cool!  Thank you!  I am thoroughly enjoying the exploration. Jim ... From: Kenneth Stanley To: neat@yahoogroups.com Sent: Monday, June 07, 2004 6:20 PM ","rawEmail":"Return-Path: &lt;jim_oflaherty_jr@...&gt;\r\nX-Sender: jim_oflaherty_jr@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 25312 invoked from network); 7 Jun 2004 23:49:10 -0000\r\nReceived: from unknown (66.218.66.167)\n  by m5.grp.scd.yahoo.com with QMQP; 7 Jun 2004 23:49:10 -0000\r\nReceived: from unknown (HELO smtp014.mail.yahoo.com) (216.136.173.58)\n  by mta6.grp.scd.yahoo.com with SMTP; 7 Jun 2004 23:49:09 -0000\r\nReceived: from unknown (HELO NEWAGE) (jim?oflaherty?jr@24.1.159.151 with login)\n  by smtp014.mail.yahoo.com with SMTP; 7 Jun 2004 23:49:08 -0000\r\nMessage-ID: &lt;00c901c44cea$061fc4a0$3201a8c0@NEWAGE&gt;\r\nTo: &lt;neat@yahoogroups.com&gt;\r\nReferences: &lt;ca2t89+s1ti@...&gt;\r\nDate: Mon, 7 Jun 2004 18:49:07 -0500\r\nMIME-Version: 1.0\r\nContent-Type: multipart/alternative;\n\tboundary=&quot;----=_NextPart_000_00C6_01C44CC0.1CE0C1A0&quot;\r\nX-Priority: 3\r\nX-MSMail-Priority: Normal\r\nX-Mailer: Microsoft Outlook Express 6.00.2720.3000\r\nX-MimeOLE: Produced By Microsoft MimeOLE V6.00.2739.300\r\nX-eGroups-Remote-IP: 216.136.173.58\r\nFrom: &quot;Jim O&#39;Flaherty, Jr.&quot; &lt;jim_oflaherty_jr@...&gt;\r\nSubject: Re: [neat] Re: Bloat\r\nX-Yahoo-Group-Post: member; u=82117382\r\nX-Yahoo-Profile: jim_oflaherty_jr\r\n\r\n\r\n------=_NextPart_000_00C6_01C44CC0.1CE0C1A0\r\nContent-Type: text/plain;\n\tcharset=&quot;iso-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\nKen,\n\nCool!  Thank you!  I am thoroughly enjoying the exploration.\n\n\nJim\n\n\n=\r\n  ----- Original Message ----- \n  From: Kenneth Stanley \n  To: neat@yahoogr=\r\noups.com \n  Sent: Monday, June 07, 2004 6:20 PM\n  Subject: [neat] Re: Bloat=\r\n\n\n\n  Jim,\n\n  Please do not worry about any possible interpretations of disr=\r\nespect \n  on my part.  I definitely want people to feel free to challenge a=\r\nny \n  ideas whatsoever so we can have stimulating discussion.  \n\n  My respo=\r\nnse was just my attempt to articulate why I think topology \n  is such an im=\r\nportant part of search.  By all means feel free to \n  disagree anytime :)\n\n=\r\n  ken\n\n\n  --- In neat@yahoogroups.com, &quot;Jim O&#39;Flaherty, Jr.&quot; \n  &lt;jim_oflahe=\r\nrty_jr@y...&gt; wrote:\n  &gt; Ken,\n  &gt; \n  &gt; I meant no disrespect in my original =\r\npost.  I was honestly \n  confused why huge variations in topology were desi=\r\nrable when finding \n  the proper weight values is just as important.  That =\r\nwhole argument \n  is pretty much moot now.  ;^)\n  &gt; \n  &gt; Now that I have re=\r\n-read this and I understand the &quot;mutation rate&quot; \n  issues, a huge part of m=\r\ny concern is now alleviated.  And what \n  little I have remaining is insign=\r\nificant.  So until I have done some \n  more experiments with NEAT, I will w=\r\nait to address any other issues.\n  &gt; \n  &gt; \n  &gt; Jim\n  &gt; \n  &gt; \n  &gt;   ----- Or=\r\niginal Message ----- \n  &gt;   From: Kenneth Stanley \n  &gt;   To: neat@yahoogrou=\r\nps.com \n  &gt;   Sent: Saturday, June 05, 2004 7:06 PM\n  &gt;   Subject: [neat] R=\r\ne: Bloat\n  &gt; \n  &gt; \n  &gt;   Jim, I hope you will allow me a rather long and de=\r\ntailed \n  response to \n  &gt;   your point.  I feel this is the right time for=\r\n me to respond \n  &gt;   broadly, since you have touched on the central theme =\r\nbehind much \n  &gt;   discussion on this group, and ultimately behind my own \n=\r\n  motivations \n  &gt;   for introducing NEAT.  Therefore, forgive me for a lon=\r\ng-winded \n  &gt;   response, but one I would to get on the record.\n  &gt; \n  &gt;   =\r\nI doubt that the importance of topology can be overstated.  That \n  &gt;   sai=\r\nd, I want to concede up front that there is no question that \n  most \n  &gt;  =\r\n of the key steps in exploration are through weight mutation, and \n  &gt;   th=\r\nat weight mutation will get you far.  In fact, there are very \n  &gt;   sophis=\r\nticated methods for altering the weight mutation \n  distribution \n  &gt;   to =\r\npoint it in more promising directions, and these methods can \n  be \n  &gt;   q=\r\nuite powerful.\n  &gt; \n  &gt;   Nevertheless, weight mutation is no more than exp=\r\nloring a fixed \n  &gt;   space, and exploring a fixed space is well understood=\r\n and tried \n  and \n  &gt;   tested.  In fact, it is proven that there  is only=\r\n so much a \n  black \n  &gt;   box method can do to explore a space.  No method=\r\n can promise \n  always \n  &gt;   to escape local optima, and no method ever wi=\r\nll make such a \n  promise \n  &gt;   (so says the No Free Lunch Theorem).  \n  &gt;=\r\n \n  &gt;   There are fundamental questions at the core of AI that fixed-\n  spa=\r\nce \n  &gt;   exploration can never address.  Most perplexing and fundamental \n=\r\n  is \n  &gt;   the question of what space should we be exploring in the first =\r\n\n  &gt;   place?  Fogel tried 3 topologies (and probably more, off the \n  &gt;   =\r\necord).  But where did those topologies come from?  What was the \n  &gt;   bas=\r\nis of the decisions to use them?  Isn&#39;t our mission, as \n  &gt;   researchers =\r\nin AI, to make *that* decision automatic?  After \n  all, \n  &gt;   *that* deci=\r\nsion- the decision of what topology to search, i.e. \n  what \n  &gt;   space to=\r\n search in- is really the only hard decision, the one \n  that \n  &gt;   requir=\r\nes &quot;intelligence&quot;.  It is a relatively trivial matter, \n  once \n  &gt;   you k=\r\nnow what to search, just to go searching.  The fact that \n  weight \n  &gt;   m=\r\nutation alone (once the correct topology has been identified) \n  is \n  &gt;   =\r\nsufficient to solve checkers says more about checkers and human \n  &gt;   inte=\r\nlligence (intelligence for choosing the right space to \n  search) \n  &gt;   th=\r\nan about the prowess of weight mutation.\n  &gt; \n  &gt;   Yet this is not only a =\r\nphilosophical argument about what AI \n  should \n  &gt;   be able to do automat=\r\nically.  It is also a critical practical \n  &gt;   matter.  Contrary to your r=\r\neasoning, the real danger is not in \n  &gt;   adding a single dimension to a s=\r\nearch space, but in beginning \n  search \n  &gt;   in a bad space in the first =\r\nplace.  If you are concerned that \n  &gt;   addition of a single dimension has=\r\n some exponential expense \n  (which I \n  &gt;   believe is not correct anyway)=\r\n, what cost then must there be in \n  &gt;   searching in a topology with dozen=\r\ns or even hundreds of \n  unnecessary \n  &gt;   dimensions?  The effect on sear=\r\nch could be catastrophic.\n  &gt; \n  &gt;   And yet for most difficult problems we=\r\n have not the slightest \n  idea \n  &gt;   what the right space is to search, o=\r\nther than that it is large.  \n  How \n  &gt;   many dimensions are in the brain=\r\n of a robotic maid?  Surely at \n  least \n  &gt;   thousands; maybe millions.  =\r\nShould we begin search then in a \n  network \n  &gt;   of a million connections=\r\n?  Weight space exploration offers no \n  &gt;   comfort: The search is intract=\r\nable in million dimensional space, \n  &gt;   even if the solution is somewhere=\r\n within.  \n  &gt; \n  &gt;   Yet even as there is danger from above in the form of=\r\n too-high \n  &gt;   dimensional space, there is danger from below in spaces of=\r\n too-\n  few \n  &gt;   dimensions, where a solution may not even exist.  What i=\r\nf Fogel \n  had \n  &gt;   chosen to search in networks with 2 fewer neurons?  5=\r\n fewer? At \n  some \n  &gt;   point, the good player just doesn&#39;t exist in that=\r\n space \n  anymore.  \n  &gt;   But how could we know this in advance?  There is=\r\n no analysis \n  that \n  &gt;   can tell us a priori how many dimensions we nee=\r\nd.  And if we try \n  to \n  &gt;   go lean and get just the right amount, we mi=\r\nght miss the boat \n  &gt;   entirely, even by a single connection, and end up =\r\nsearching \n  forever \n  &gt;   in futility in a space without a solution.\n  &gt; =\r\n\n  &gt;   Worse, even if we knew *exactly* the minimal number of \n  connection=\r\ns \n  &gt;   necessary to solve a problem *and* the perfect topology, even \n  t=\r\nhen, \n  &gt;   if the space is too large, weight mutation alone is likely to \n=\r\n  fail.  \n  &gt;   The problem is, where in a large space do you *begin* to se=\r\narch? \n  And \n  &gt;   that problem is impossible to address since by definiti=\r\non you \n  don&#39;t \n  &gt;   know anything about the space before you begin searc=\r\nhing!  \n  &gt;   Therefore, in a high-dimensional space, you are highly likely=\r\n to \n  &gt;   begin in an unpromising part of the space; it&#39;s simply too large=\r\n.\n  &gt; \n  &gt;   Therefore, to begin minimally and complexify into the the prop=\r\ner \n  &gt;   space is addressing a fundamental issue and I believe is \n  ultim=\r\nately \n  &gt;   unavoidable as a critical component of any black box search fo=\r\nr \n  &gt;   complex behaviors.  Rather than adding expense as you imply, it \n =\r\n is \n  &gt;   reducing expense by spending most of search in lower-dimensional=\r\n \n  &gt;   space than the final solution.  A complexifying method only may \n  =\r\nbe \n  &gt;   searching in the space of the final solution for 10% of the \n  ru=\r\nn.  \n  &gt;   Fixed-topology search spends 100% of the run in the high \n  dime=\r\nnsional \n  &gt;   space of the final solution, which, according to your \n  for=\r\nmulation \n  &gt;   should incur an incomprehensibly vast exponential penalty. =\r\n \n  &gt; \n  &gt;   I think ultimately what you are misunderstanding is that NEAT =\r\nis \n  not\n  &gt;   an attempt to search in high-dimensional space.  It is a me=\r\nthod \n  for \n  &gt;   spending most of your search in *lower-dimensional space=\r\n* than \n  the \n  &gt;   final solution, and complexifying up to the complexity=\r\n of the \n  final \n  &gt;   solution.  The goal is to be able to find solutions=\r\n that *exist* \n  in \n  &gt;   high-dimensional space.  That&#39;s not the same as =\r\na goal of \n  searching \n  &gt;   directly in high-dimensional space no matter =\r\nthe problem.  The \n  &gt;   latter goal is the antithesis of what NEAT is abou=\r\nt.  NEAT is \n  &gt;   designed to avoid searching in unnecessarily high-dimens=\r\nional \n  space.\n  &gt; \n  &gt;   Thus, I feel strongly that the idea of searching=\r\n through \n  topologies \n  &gt;   must be taken seriously, and should not be vi=\r\newed as merely \n  &gt;   a &quot;fun&quot;, &quot;sexy,&quot; or &quot;somwhat spatially interesting&quot;  =\r\n\n  recreation.  It \n  &gt;   is not mere intellectual exercise.  Prior topolog=\r\ny-evolving \n  systems \n  &gt;   before NEAT were perhaps better targets for yo=\r\nur criticism, \n  since \n  &gt;   they were essentially aimed at flipping throu=\r\ngh random \n  topologies \n  &gt;   unsystematically for its own sake.  However,=\r\n NEAT is designed to \n  to \n  &gt;   use topology as a way of minimizing dimen=\r\nsionality in search, \n  and \n  &gt;   ultimately to automatically address that=\r\n fundamental question of \n  &gt;   what space to be searching in, a completely=\r\n different endeavor. \n  &gt; \n  &gt;   (\n  &gt; \n  &gt;   A couple side notes:\n  &gt; \n  &gt;=\r\n   I agree that structural mutation needs to be relatively rare.  \n  In \n  =\r\n&gt;   NEAT, it is generally 5% or lower.  Years of experimentation \n  with \n =\r\n &gt;   NEAT have gone into testing different rates of structure-adding.\n  &gt; \n=\r\n  &gt;   Finally, I believe your mathematical formulation is incorrect. \n  &gt;  =\r\n Adding a dimensions to an already-partially-optimized structure \n  &gt;   cer=\r\ntainly does not incur exponential expense in the search \n  process.  \n  &gt;  =\r\n In fact, the effect can be quite the opposite, adding new routes \n  off \n =\r\n &gt;   the top of a local optimum.\n  &gt; \n  &gt;   Not to be picky, and this isn&#39;t=\r\n really important, but here&#39;s \n  what \n  &gt;   doesn&#39;t make sense to me about=\r\n your formal argument:\n  &gt; \n  &gt;   -&quot;What is occurring to me is that just do=\r\ning weight mutation is \n  a \n  &gt;   search at a rate X in a huge space.&quot;  Ho=\r\nw do you define &quot;search \n  at \n  &gt;   rate X?&quot;  This does not seem to mean a=\r\nnything formally \n  speaking.  \n  &gt;   What are the units of search rate?  H=\r\now is it derived?\n  &gt; \n  &gt;   -&quot;And it seems to me adding topological variat=\r\nion is not just a \n  &gt;   multiplier, but an exponent increasing X.&quot;  If X i=\r\ns a rate (as \n  you \n  &gt;   defined it), then increasing X means the rate be=\r\ncomes faster.  \n  So I \n  &gt;   assume X is not a rate.  But then what is it?=\r\n\n  &gt; \n  &gt;   -&quot;topological complexity curve for having a more fit player is =\r\n\n  &gt;   exponetial&quot;  What is a topological complexity curve?  Is it \n  based=\r\n \n  &gt;   somehow on rate X?  Complexity is usually defined as the size of \n =\r\n the \n  &gt;   space or number of connections in a network.  Under that usual =\r\n\n  &gt;   definition, complexity goes up linearly with the addition of new\n  &gt;=\r\n     structure, not exponentially.\n  &gt; \n  &gt;   -&quot;X^T, where T =3D Y^Z and Z =\r\nis the complexity curve&quot;  I still am \n  not \n  &gt;   sure what X really means=\r\n formally, but you haven&#39;t given a \n  &gt;   definition for T or Y or Z either=\r\n.  What are these variables?\n  &gt; \n  &gt;   Ultimately I think you are arguing =\r\nfrom intuition rather than \n  &gt;   formally, and intuitions can be misleadin=\r\ng.\n  &gt;   )\n  &gt; \n  &gt; \n  &gt;   Sorry to all for the long-windedness of this res=\r\nponse!  I hope \n  it is \n  &gt;   still useful!\n  &gt; \n  &gt;   --- In neat@yahoogr=\r\noups.com, &quot;Jim O&#39;Flaherty, Jr.&quot; \n  &gt;   &lt;jim_oflaherty_jr@y...&gt; wrote:\n  &gt;  =\r\n &gt; John, Colin, Ken and Derek,\n  &gt;   &gt; \n  &gt;   &gt; I am wondering if there is =\r\nnot a wee bit too much focus on \n  &gt;   topological vairation and insignific=\r\nant focus on just weight \n  &gt;   mutation.  I get that NEAT is unique in the=\r\n fact that it has a \n  very \n  &gt;   effective search mechanism for topoligic=\r\nal variation, with the \n  &gt;   ability to stress both additive and subtracti=\r\nve aspects of \n  change.  \n  &gt;   And I get that it is fun to focus on the t=\r\nopological variation \n  as it \n  &gt;   is somewhat spatially interesting.\n  &gt;=\r\n   &gt; \n  &gt;   &gt; However, I am realizing that just doing effective weight \n  &gt;=\r\n   mutations, sans topological changes, can end up producing \n  solutions \n=\r\n  &gt;   that are very &quot;fit&quot;.  I have been focused on reproducing the \n  &gt;   e=\r\nxperiments Fogel and Kumar produced which are covered in their \n  book \n  &gt;=\r\n   Blondie24.  In that, they had only 3 topologies they \n  experimented \n  =\r\n&gt;   with.  All of the GA searching was just done with weight \n  mutation \n =\r\n &gt;   within a step size that was both a GA parameter and nudged \n  towards =\r\n\n  &gt;   smaller values.  In the Fogel experiments, they arrived at an \n  exp=\r\nert \n  &gt;   player (well, at least against human opponents) using just co-\n =\r\n &gt;   evolution and a static topology.  And in my own replication of \n  the =\r\n\n  &gt;   experiments, something as simple as turn on/off biases had a \n  &gt;   =\r\nsubstantial effect in how long it took to arrive at a specimen \n  of \n  &gt;  =\r\n similar fitness.\n  &gt;   &gt; \n  &gt;   &gt; Now, I realize that mutating weights onl=\r\ny is not near as sexy \n  &gt;   sounding as both weight mutation and topologic=\r\nal variation.  \n  &gt;   However, what I am wondering and hope to be able to e=\r\nvaluate \n  with \n  &gt;   experimentation is whether the topological mutation =\r\nrates ought \n  not \n  &gt;   be very small with the focus more on trying out m=\r\nany weight \n  &gt;   mutations within a given topology?  What is occurring to =\r\nme is \n  that \n  &gt;   just doing weight mutation is a search at a rate X in =\r\na huge \n  space.  \n  &gt;   And it seems to me adding topological variation is=\r\n not just a \n  &gt;   multiplier, but an exponent increasing X.  Perhaps the s=\r\npace is \n  &gt;   being made too large too quickly, before a search just in th=\r\ne \n  weight \n  &gt;   mutation space might demonstrate a uniquely fit individu=\r\nal.\n  &gt;   &gt; \n  &gt;   &gt; My understanding is that the &quot;search in higher dimensi=\r\nonal \n  space&quot; \n  &gt;   might produce more robust and fit players.  At what \n=\r\n  computational \n  &gt;   cost?  If the topological complexity curve for havin=\r\ng a &quot;more \n  fit \n  &gt;   player&quot; is exponetial, then doesn&#39;t that mean that =\r\nthere is a \n  &gt;   threshold of diminishing returns somewhere?  Granted, it =\r\nmay not \n  &gt;   be.  However, when it is exponential (and my intuition says =\r\nit \n  is \n  &gt;   more of the time), we now have an exponent on an exponent o=\r\nf \n  &gt;   complexification which massively enlarges the search space, X^T, \n=\r\n  &gt;   where T =3D Y^Z and Z is the complexity curve.  If this is true, \n  t=\r\nhen \n  &gt;   we are massive amounts of processing power away from achieving \n=\r\n  &gt;   result in anything but the simplest of domains, like XOR and Tic-\n  T=\r\nac-\n  &gt;   Toe.\n  &gt;   &gt; \n  &gt;   &gt; Am I missing something here?  Perhaps I nee=\r\nd to do more direct \n  &gt;   experimentation and examine the results before j=\r\numping to this \n  kind \n  &gt;   of conclusion.  I just get the sense that sim=\r\nple weight mutation \n  &gt;   achieved quite a bit in Checkers, a domain more =\r\ncomplex than Tic-\n  Tac-\n  &gt;   Toe.  It would be interesting to see how Che=\r\nckers might do with \n  NEAT \n  &gt;   and see what kinds of mutation rates mig=\r\nht be more/less \n  effective \n  &gt;   and why.\n  &gt;   &gt; \n  &gt;   &gt; \n  &gt;   &gt; Jim\n=\r\n  &gt;   &gt; \n  &gt;   &gt; \n  &gt; \n  &gt; \n  &gt; \n  &gt;         Yahoo! Groups Sponsor \n  &gt;    =\r\n           ADVERTISEMENT\n  &gt;              \n  &gt;        \n  &gt;        \n  &gt; \n  &gt;=\r\n \n  &gt; -------------------------------------------------------------------\n =\r\n -----------\n  &gt;   Yahoo! Groups Links\n  &gt; \n  &gt;     a.. To visit your group=\r\n on the web, go to:\n  &gt;     http://groups.yahoo.com/group/neat/\n  &gt;       \n=\r\n  &gt;     b.. To unsubscribe from this group, send an email to:\n  &gt;     neat-=\r\nunsubscribe@yahoogroups.com\n  &gt;       \n  &gt;     c.. Your use of Yahoo! Group=\r\ns is subject to the Yahoo! Terms \n  of Service.\n\n\n        Yahoo! Groups Spo=\r\nnsor \n              ADVERTISEMENT\n             \n       \n       \n\n\n---------=\r\n---------------------------------------------------------------------\n  Yah=\r\noo! Groups Links\n\n    a.. To visit your group on the web, go to:\n    http:/=\r\n/groups.yahoo.com/group/neat/\n      \n    b.. To unsubscribe from this group=\r\n, send an email to:\n    neat-unsubscribe@yahoogroups.com\n      \n    c.. You=\r\nr use of Yahoo! Groups is subject to the Yahoo! Terms of Service. \n\n\n\r\n------=_NextPart_000_00C6_01C44CC0.1CE0C1A0\r\nContent-Type: text/html;\n\tcharset=&quot;iso-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\n&lt;!DOCTYPE HTML PUBLIC &quot;-//W3C//DTD HTML 4.0 Transitional//EN&quot;&gt;\n&lt;HTML&gt;&lt;HEAD&gt;=\r\n\n&lt;META http-equiv=3DContent-Type content=3D&quot;text/html; charset=3Diso-8859-1=\r\n&quot;&gt;\n&lt;META content=3D&quot;MSHTML 6.00.2737.800&quot; name=3DGENERATOR&gt;\n&lt;STYLE&gt;&lt;/STYLE&gt;=\r\n\n&lt;/HEAD&gt;\n&lt;BODY bgColor=3D#ffffff&gt;\n&lt;DIV&gt;&lt;FONT face=3DArial size=3D2&gt;Ken,&lt;/FO=\r\nNT&gt;&lt;/DIV&gt;&lt;FONT face=3DArial size=3D2&gt;\n&lt;DIV&gt;&lt;BR&gt;Cool!&nbsp; Thank you!&nbsp;=\r\n I am thoroughly enjoying the \nexploration.&lt;/DIV&gt;\n&lt;DIV&gt;&nbsp;&lt;/DIV&gt;\n&lt;DIV&gt;&n=\r\nbsp;&lt;/DIV&gt;\n&lt;DIV&gt;Jim&lt;/DIV&gt;\n&lt;DIV&gt;&nbsp;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;/FONT&gt;&nbsp;&lt;/DIV&gt;\n&lt;BLOCK=\r\nQUOTE \nstyle=3D&quot;PADDING-RIGHT: 0px; PADDING-LEFT: 5px; MARGIN-LEFT: 5px; BO=\r\nRDER-LEFT: #000000 2px solid; MARGIN-RIGHT: 0px&quot;&gt;\n  &lt;DIV style=3D&quot;FONT: 10p=\r\nt arial&quot;&gt;----- Original Message ----- &lt;/DIV&gt;\n  &lt;DIV \n  style=3D&quot;BACKGROUND:=\r\n #e4e4e4; FONT: 10pt arial; font-color: black&quot;&gt;&lt;B&gt;From:&lt;/B&gt; \n  &lt;A title=3Dk=\r\nstanley@... href=3D&quot;mailto:kstanley@...&quot;&gt;Kenneth \n  Sta=\r\nnley&lt;/A&gt; &lt;/DIV&gt;\n  &lt;DIV style=3D&quot;FONT: 10pt arial&quot;&gt;&lt;B&gt;To:&lt;/B&gt; &lt;A title=3Dnea=\r\nt@yahoogroups.com \n  href=3D&quot;mailto:neat@yahoogroups.com&quot;&gt;neat@yahoogroups.=\r\ncom&lt;/A&gt; &lt;/DIV&gt;\n  &lt;DIV style=3D&quot;FONT: 10pt arial&quot;&gt;&lt;B&gt;Sent:&lt;/B&gt; Monday, June =\r\n07, 2004 6:20 PM&lt;/DIV&gt;\n  &lt;DIV style=3D&quot;FONT: 10pt arial&quot;&gt;&lt;B&gt;Subject:&lt;/B&gt; [n=\r\neat] Re: Bloat&lt;/DIV&gt;\n  &lt;DIV&gt;&lt;BR&gt;&lt;/DIV&gt;&lt;TT&gt;Jim,&lt;BR&gt;&lt;BR&gt;Please do not worry a=\r\nbout any possible \n  interpretations of disrespect &lt;BR&gt;on my part.&nbsp; I =\r\ndefinitely want people \n  to feel free to challenge any &lt;BR&gt;ideas whatsoeve=\r\nr so we can have stimulating \n  discussion.&nbsp; &lt;BR&gt;&lt;BR&gt;My response was j=\r\nust my attempt to articulate why I \n  think topology &lt;BR&gt;is such an importa=\r\nnt part of search.&nbsp; By all means \n  feel free to &lt;BR&gt;disagree anytime =\r\n:)&lt;BR&gt;&lt;BR&gt;ken&lt;BR&gt;&lt;BR&gt;&lt;BR&gt;--- In \n  neat@yahoogroups.com, &quot;Jim O&#39;Flaherty, J=\r\nr.&quot; &lt;BR&gt;&lt;jim_oflaherty_jr@y...&gt; \n  wrote:&lt;BR&gt;&gt; Ken,&lt;BR&gt;&gt; &lt;BR&gt;&g=\r\nt; I meant no disrespect in my original \n  post.&nbsp; I was honestly &lt;BR&gt;c=\r\nonfused why huge variations in topology were \n  desirable when finding &lt;BR&gt;=\r\nthe proper weight values is just as \n  important.&nbsp; That whole argument=\r\n &lt;BR&gt;is pretty much moot now.&nbsp; \n  ;^)&lt;BR&gt;&gt; &lt;BR&gt;&gt; Now that I have=\r\n re-read this and I understand the \n  &quot;mutation rate&quot; &lt;BR&gt;issues, a huge pa=\r\nrt of my concern is now alleviated.&nbsp; \n  And what &lt;BR&gt;little I have rem=\r\naining is insignificant.&nbsp; So until I have \n  done some &lt;BR&gt;more experi=\r\nments with NEAT, I will wait to address any other \n  issues.&lt;BR&gt;&gt; &lt;BR&gt;&g=\r\nt; &lt;BR&gt;&gt; Jim&lt;BR&gt;&gt; &lt;BR&gt;&gt; &lt;BR&gt;&gt;&nbsp;&nbsp; \n  ----- Original Mes=\r\nsage ----- &lt;BR&gt;&gt;&nbsp;&nbsp; From: Kenneth Stanley \n  &lt;BR&gt;&gt;&nbsp;&nbs=\r\np; To: neat@yahoogroups.com &lt;BR&gt;&gt;&nbsp;&nbsp; Sent: \n  Saturday, June 05=\r\n, 2004 7:06 PM&lt;BR&gt;&gt;&nbsp;&nbsp; Subject: [neat] Re: \n  Bloat&lt;BR&gt;&gt; &lt;BR=\r\n&gt;&gt; &lt;BR&gt;&gt;&nbsp;&nbsp; Jim, I hope you will allow me a \n  rather long a=\r\nnd detailed &lt;BR&gt;response to &lt;BR&gt;&gt;&nbsp;&nbsp; your \n  point.&nbsp; I fee=\r\nl this is the right time for me to respond \n  &lt;BR&gt;&gt;&nbsp;&nbsp; broadly,=\r\n since you have touched on the central theme \n  behind much &lt;BR&gt;&gt;&nbsp;&=\r\nnbsp; discussion on this group, and ultimately \n  behind my own &lt;BR&gt;motivat=\r\nions &lt;BR&gt;&gt;&nbsp;&nbsp; for introducing NEAT.&nbsp; \n  Therefore, forgive=\r\n me for a long-winded &lt;BR&gt;&gt;&nbsp;&nbsp; response, but one \n  I would to =\r\nget on the record.&lt;BR&gt;&gt; &lt;BR&gt;&gt;&nbsp;&nbsp; I doubt that the \n  importa=\r\nnce of topology can be overstated.&nbsp; That &lt;BR&gt;&gt;&nbsp;&nbsp; \n  said,=\r\n I want to concede up front that there is no question that &lt;BR&gt;most \n  &lt;BR&gt;=\r\n&gt;&nbsp;&nbsp; of the key steps in exploration are through weight \n  muta=\r\ntion, and &lt;BR&gt;&gt;&nbsp;&nbsp; that weight mutation will get you \n  far.&nb=\r\nsp; In fact, there are very &lt;BR&gt;&gt;&nbsp;&nbsp; sophisticated methods \n  f=\r\nor altering the weight mutation &lt;BR&gt;distribution &lt;BR&gt;&gt;&nbsp;&nbsp; to \n =\r\n point it in more promising directions, and these methods can &lt;BR&gt;be \n  &lt;BR=\r\n&gt;&gt;&nbsp;&nbsp; quite powerful.&lt;BR&gt;&gt; &lt;BR&gt;&gt;&nbsp;&nbsp; \n  Neverthel=\r\ness, weight mutation is no more than exploring a fixed \n  &lt;BR&gt;&gt;&nbsp;&nb=\r\nsp; space, and exploring a fixed space is well understood and \n  tried &lt;BR&gt;=\r\nand &lt;BR&gt;&gt;&nbsp;&nbsp; tested.&nbsp; In fact, it is proven that \n  there&=\r\nnbsp; is only so much a &lt;BR&gt;black &lt;BR&gt;&gt;&nbsp;&nbsp; box method can do \n =\r\n to explore a space.&nbsp; No method can promise &lt;BR&gt;always \n  &lt;BR&gt;&gt;&nbs=\r\np;&nbsp; to escape local optima, and no method ever will make such \n  a &lt;BR=\r\n&gt;promise &lt;BR&gt;&gt;&nbsp;&nbsp; (so says the No Free Lunch Theorem).&nbsp; \n =\r\n &lt;BR&gt;&gt; &lt;BR&gt;&gt;&nbsp;&nbsp; There are fundamental questions at the core =\r\nof \n  AI that fixed-&lt;BR&gt;space &lt;BR&gt;&gt;&nbsp;&nbsp; exploration can never \n =\r\n address.&nbsp; Most perplexing and fundamental &lt;BR&gt;is &lt;BR&gt;&gt;&nbsp;&nbsp;=\r\n the \n  question of what space should we be exploring in the first \n  &lt;BR&gt;&=\r\ngt;&nbsp;&nbsp; place?&nbsp; Fogel tried 3 topologies (and probably more, \n=\r\n  off the &lt;BR&gt;&gt;&nbsp;&nbsp; ecord).&nbsp; But where did those topologies=\r\n come \n  from?&nbsp; What was the &lt;BR&gt;&gt;&nbsp;&nbsp; basis of the decisio=\r\nns to use \n  them?&nbsp; Isn&#39;t our mission, as &lt;BR&gt;&gt;&nbsp;&nbsp; researc=\r\nhers in AI, to \n  make *that* decision automatic?&nbsp; After &lt;BR&gt;all, &lt;BR&gt;=\r\n&gt;&nbsp;&nbsp; \n  *that* decision- the decision of what topology to searc=\r\nh, i.e. &lt;BR&gt;what \n  &lt;BR&gt;&gt;&nbsp;&nbsp; space to search in- is really the =\r\nonly hard decision, the \n  one &lt;BR&gt;that &lt;BR&gt;&gt;&nbsp;&nbsp; requires &quot;inte=\r\nlligence&quot;.&nbsp; It is a \n  relatively trivial matter, &lt;BR&gt;once &lt;BR&gt;&gt;&nb=\r\nsp;&nbsp; you know what to \n  search, just to go searching.&nbsp; The fact =\r\nthat &lt;BR&gt;weight \n  &lt;BR&gt;&gt;&nbsp;&nbsp; mutation alone (once the correct to=\r\npology has been \n  identified) &lt;BR&gt;is &lt;BR&gt;&gt;&nbsp;&nbsp; sufficient to so=\r\nlve checkers says more \n  about checkers and human &lt;BR&gt;&gt;&nbsp;&nbsp; int=\r\nelligence (intelligence for \n  choosing the right space to &lt;BR&gt;search) &lt;BR&gt;=\r\n&gt;&nbsp;&nbsp; than about the \n  prowess of weight mutation.&lt;BR&gt;&gt; &lt;BR&gt;=\r\n&gt;&nbsp;&nbsp; Yet this is not only \n  a philosophical argument about wha=\r\nt AI &lt;BR&gt;should &lt;BR&gt;&gt;&nbsp;&nbsp; be able \n  to do automatically.&nbsp; =\r\nIt is also a critical practical \n  &lt;BR&gt;&gt;&nbsp;&nbsp; matter.&nbsp; Contr=\r\nary to your reasoning, the real danger \n  is not in &lt;BR&gt;&gt;&nbsp;&nbsp; ad=\r\nding a single dimension to a search space, \n  but in beginning &lt;BR&gt;search &lt;=\r\nBR&gt;&gt;&nbsp;&nbsp; in a bad space in the first \n  place.&nbsp; If you are =\r\nconcerned that &lt;BR&gt;&gt;&nbsp;&nbsp; addition of a \n  single dimension has s=\r\nome exponential expense &lt;BR&gt;(which I \n  &lt;BR&gt;&gt;&nbsp;&nbsp; believe is not=\r\n correct anyway), what cost then must there \n  be in &lt;BR&gt;&gt;&nbsp;&nbsp; s=\r\nearching in a topology with dozens or even \n  hundreds of &lt;BR&gt;unnecessary &lt;=\r\nBR&gt;&gt;&nbsp;&nbsp; dimensions?&nbsp; The effect \n  on search could be cata=\r\nstrophic.&lt;BR&gt;&gt; &lt;BR&gt;&gt;&nbsp;&nbsp; And yet for most \n  difficult proble=\r\nms we have not the slightest &lt;BR&gt;idea &lt;BR&gt;&gt;&nbsp;&nbsp; \n  what the righ=\r\nt space is to search, other than that it is large.&nbsp; &lt;BR&gt;How \n  &lt;BR&gt;&gt=\r\n;&nbsp;&nbsp; many dimensions are in the brain of a robotic maid?&nbsp; \n  =\r\nSurely at &lt;BR&gt;least &lt;BR&gt;&gt;&nbsp;&nbsp; thousands; maybe millions.&nbsp; \n=\r\n  Should we begin search then in a &lt;BR&gt;network &lt;BR&gt;&gt;&nbsp;&nbsp; of a mi=\r\nllion \n  connections?&nbsp; Weight space exploration offers no &lt;BR&gt;&gt;&nbs=\r\np;&nbsp; \n  comfort: The search is intractable in million dimensional space=\r\n, \n  &lt;BR&gt;&gt;&nbsp;&nbsp; even if the solution is somewhere within.&nbsp; &lt;=\r\nBR&gt;&gt; \n  &lt;BR&gt;&gt;&nbsp;&nbsp; Yet even as there is danger from above in t=\r\nhe form of \n  too-high &lt;BR&gt;&gt;&nbsp;&nbsp; dimensional space, there is dan=\r\nger from below in \n  spaces of too-&lt;BR&gt;few &lt;BR&gt;&gt;&nbsp;&nbsp; dimensions,=\r\n where a solution may \n  not even exist.&nbsp; What if Fogel &lt;BR&gt;had &lt;BR&gt;&g=\r\nt;&nbsp;&nbsp; chosen to \n  search in networks with 2 fewer neurons?&nbsp; =\r\n5 fewer? At &lt;BR&gt;some \n  &lt;BR&gt;&gt;&nbsp;&nbsp; point, the good player just do=\r\nesn&#39;t exist in that space \n  &lt;BR&gt;anymore.&nbsp; &lt;BR&gt;&gt;&nbsp;&nbsp; But ho=\r\nw could we know this in \n  advance?&nbsp; There is no analysis &lt;BR&gt;that &lt;BR=\r\n&gt;&gt;&nbsp;&nbsp; can tell us \n  a priori how many dimensions we need.&nbsp=\r\n; And if we try &lt;BR&gt;to \n  &lt;BR&gt;&gt;&nbsp;&nbsp; go lean and get just the rig=\r\nht amount, we might miss the \n  boat &lt;BR&gt;&gt;&nbsp;&nbsp; entirely, even by=\r\n a single connection, and end up \n  searching &lt;BR&gt;forever &lt;BR&gt;&gt;&nbsp;&nb=\r\nsp; in futility in a space without a \n  solution.&lt;BR&gt;&gt; &lt;BR&gt;&gt;&nbsp;&nb=\r\nsp; Worse, even if we knew *exactly* the \n  minimal number of &lt;BR&gt;connectio=\r\nns &lt;BR&gt;&gt;&nbsp;&nbsp; necessary to solve a \n  problem *and* the perfect t=\r\nopology, even &lt;BR&gt;then, &lt;BR&gt;&gt;&nbsp;&nbsp; if the \n  space is too large, =\r\nweight mutation alone is likely to &lt;BR&gt;fail.&nbsp; \n  &lt;BR&gt;&gt;&nbsp;&nbsp; =\r\nThe problem is, where in a large space do you *begin* to \n  search? &lt;BR&gt;And=\r\n &lt;BR&gt;&gt;&nbsp;&nbsp; that problem is impossible to address \n  since by def=\r\ninition you &lt;BR&gt;don&#39;t &lt;BR&gt;&gt;&nbsp;&nbsp; know anything about the \n  space=\r\n before you begin searching!&nbsp; &lt;BR&gt;&gt;&nbsp;&nbsp; Therefore, in a \n  =\r\nhigh-dimensional space, you are highly likely to &lt;BR&gt;&gt;&nbsp;&nbsp; begin=\r\n in \n  an unpromising part of the space; it&#39;s simply too large.&lt;BR&gt;&gt; \n  =\r\n&lt;BR&gt;&gt;&nbsp;&nbsp; Therefore, to begin minimally and complexify into the =\r\nthe \n  proper &lt;BR&gt;&gt;&nbsp;&nbsp; space is addressing a fundamental issue =\r\nand I \n  believe is &lt;BR&gt;ultimately &lt;BR&gt;&gt;&nbsp;&nbsp; unavoidable as a cr=\r\nitical \n  component of any black box search for &lt;BR&gt;&gt;&nbsp;&nbsp; comple=\r\nx \n  behaviors.&nbsp; Rather than adding expense as you imply, it &lt;BR&gt;is \n =\r\n &lt;BR&gt;&gt;&nbsp;&nbsp; reducing expense by spending most of search in \n  low=\r\ner-dimensional &lt;BR&gt;&gt;&nbsp;&nbsp; space than the final solution.&nbsp; A =\r\n\n  complexifying method only may &lt;BR&gt;be &lt;BR&gt;&gt;&nbsp;&nbsp; searching in t=\r\nhe \n  space of the final solution for 10% of the &lt;BR&gt;run.&nbsp; &lt;BR&gt;&gt;&nb=\r\nsp;&nbsp; \n  Fixed-topology search spends 100% of the run in the high &lt;BR&gt;d=\r\nimensional \n  &lt;BR&gt;&gt;&nbsp;&nbsp; space of the final solution, which, acco=\r\nrding to your \n  &lt;BR&gt;formulation &lt;BR&gt;&gt;&nbsp;&nbsp; should incur an incom=\r\nprehensibly vast \n  exponential penalty.&nbsp; &lt;BR&gt;&gt; &lt;BR&gt;&gt;&nbsp;&nbsp=\r\n; I think ultimately \n  what you are misunderstanding is that NEAT is &lt;BR&gt;n=\r\not&lt;BR&gt;&gt;&nbsp;&nbsp; an \n  attempt to search in high-dimensional space.&n=\r\nbsp; It is a method &lt;BR&gt;for \n  &lt;BR&gt;&gt;&nbsp;&nbsp; spending most of your s=\r\nearch in *lower-dimensional space* \n  than &lt;BR&gt;the &lt;BR&gt;&gt;&nbsp;&nbsp; fin=\r\nal solution, and complexifying up to the \n  complexity of the &lt;BR&gt;final &lt;BR=\r\n&gt;&gt;&nbsp;&nbsp; solution.&nbsp; The goal is \n  to be able to find solutio=\r\nns that *exist* &lt;BR&gt;in &lt;BR&gt;&gt;&nbsp;&nbsp; \n  high-dimensional space.&nbsp=\r\n; That&#39;s not the same as a goal of &lt;BR&gt;searching \n  &lt;BR&gt;&gt;&nbsp;&nbsp; di=\r\nrectly in high-dimensional space no matter the \n  problem.&nbsp; The &lt;BR&gt;&g=\r\nt;&nbsp;&nbsp; latter goal is the antithesis of what \n  NEAT is about.&nbsp=\r\n; NEAT is &lt;BR&gt;&gt;&nbsp;&nbsp; designed to avoid searching \n  in unnecessar=\r\nily high-dimensional &lt;BR&gt;space.&lt;BR&gt;&gt; &lt;BR&gt;&gt;&nbsp;&nbsp; \n  Thus, I fee=\r\nl strongly that the idea of searching through &lt;BR&gt;topologies \n  &lt;BR&gt;&gt;&nb=\r\nsp;&nbsp; must be taken seriously, and should not be viewed as \n  merely &lt;B=\r\nR&gt;&gt;&nbsp;&nbsp; a &quot;fun&quot;, &quot;sexy,&quot; or &quot;somwhat spatially \n  interesting&quot;&n=\r\nbsp; &lt;BR&gt;recreation.&nbsp; It &lt;BR&gt;&gt;&nbsp;&nbsp; is not mere \n  intellect=\r\nual exercise.&nbsp; Prior topology-evolving &lt;BR&gt;systems \n  &lt;BR&gt;&gt;&nbsp;&n=\r\nbsp; before NEAT were perhaps better targets for your \n  criticism, &lt;BR&gt;sin=\r\nce &lt;BR&gt;&gt;&nbsp;&nbsp; they were essentially aimed at \n  flipping through =\r\nrandom &lt;BR&gt;topologies &lt;BR&gt;&gt;&nbsp;&nbsp; unsystematically \n  for its own =\r\nsake.&nbsp; However, NEAT is designed to &lt;BR&gt;to \n  &lt;BR&gt;&gt;&nbsp;&nbsp; use=\r\n topology as a way of minimizing dimensionality in \n  search, &lt;BR&gt;and &lt;BR&gt;&=\r\ngt;&nbsp;&nbsp; ultimately to automatically address that \n  fundamental que=\r\nstion of &lt;BR&gt;&gt;&nbsp;&nbsp; what space to be searching in, a \n  completel=\r\ny different endeavor. &lt;BR&gt;&gt; &lt;BR&gt;&gt;&nbsp;&nbsp; (&lt;BR&gt;&gt; \n  &lt;BR&gt;&gt;&n=\r\nbsp;&nbsp; A couple side notes:&lt;BR&gt;&gt; &lt;BR&gt;&gt;&nbsp;&nbsp; I agree \n  tha=\r\nt structural mutation needs to be relatively rare.&nbsp; &lt;BR&gt;In \n  &lt;BR&gt;&gt;=\r\n&nbsp;&nbsp; NEAT, it is generally 5% or lower.&nbsp; Years of \n  experimen=\r\ntation &lt;BR&gt;with &lt;BR&gt;&gt;&nbsp;&nbsp; NEAT have gone into testing \n  differe=\r\nnt rates of structure-adding.&lt;BR&gt;&gt; &lt;BR&gt;&gt;&nbsp;&nbsp; Finally, I \n  be=\r\nlieve your mathematical formulation is incorrect. &lt;BR&gt;&gt;&nbsp;&nbsp; \n  A=\r\ndding a dimensions to an already-partially-optimized structure \n  &lt;BR&gt;&gt;&=\r\nnbsp;&nbsp; certainly does not incur exponential expense in the \n  search &lt;=\r\nBR&gt;process.&nbsp; &lt;BR&gt;&gt;&nbsp;&nbsp; In fact, the effect can be \n  quite =\r\nthe opposite, adding new routes &lt;BR&gt;off &lt;BR&gt;&gt;&nbsp;&nbsp; the top of \n  =\r\na local optimum.&lt;BR&gt;&gt; &lt;BR&gt;&gt;&nbsp;&nbsp; Not to be picky, and this isn=\r\n&#39;t \n  really important, but here&#39;s &lt;BR&gt;what &lt;BR&gt;&gt;&nbsp;&nbsp; doesn&#39;t ma=\r\nke sense \n  to me about your formal argument:&lt;BR&gt;&gt; &lt;BR&gt;&gt;&nbsp;&nbsp; =\r\n-&quot;What is \n  occurring to me is that just doing weight mutation is &lt;BR&gt;a \n =\r\n &lt;BR&gt;&gt;&nbsp;&nbsp; search at a rate X in a huge space.&quot;&nbsp; How do you=\r\n \n  define &quot;search &lt;BR&gt;at &lt;BR&gt;&gt;&nbsp;&nbsp; rate X?&quot;&nbsp; This does not=\r\n seem \n  to mean anything formally &lt;BR&gt;speaking.&nbsp; &lt;BR&gt;&gt;&nbsp;&nbsp;=\r\n What are \n  the units of search rate?&nbsp; How is it derived?&lt;BR&gt;&gt; \n  =\r\n&lt;BR&gt;&gt;&nbsp;&nbsp; -&quot;And it seems to me adding topological variation is n=\r\not \n  just a &lt;BR&gt;&gt;&nbsp;&nbsp; multiplier, but an exponent increasing X.=\r\n&quot;&nbsp; \n  If X is a rate (as &lt;BR&gt;you &lt;BR&gt;&gt;&nbsp;&nbsp; defined it), the=\r\nn increasing X \n  means the rate becomes faster.&nbsp; &lt;BR&gt;So I &lt;BR&gt;&gt;&nb=\r\nsp;&nbsp; assume X is \n  not a rate.&nbsp; But then what is it?&lt;BR&gt;&gt; &lt;BR=\r\n&gt;&gt;&nbsp;&nbsp; \n  -&quot;topological complexity curve for having a more fit p=\r\nlayer is \n  &lt;BR&gt;&gt;&nbsp;&nbsp; exponetial&quot;&nbsp; What is a topological co=\r\nmplexity \n  curve?&nbsp; Is it &lt;BR&gt;based &lt;BR&gt;&gt;&nbsp;&nbsp; somehow on ra=\r\nte X?&nbsp; \n  Complexity is usually defined as the size of &lt;BR&gt;the &lt;BR&gt;&gt=\r\n;&nbsp;&nbsp; \n  space or number of connections in a network.&nbsp; Under t=\r\nhat usual \n  &lt;BR&gt;&gt;&nbsp;&nbsp; definition, complexity goes up linearly w=\r\nith the addition \n  of new&lt;BR&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp; structure, not e=\r\nxponentially.&lt;BR&gt;&gt; \n  &lt;BR&gt;&gt;&nbsp;&nbsp; -&quot;X^T, where T =3D Y^Z and Z =\r\nis the complexity curve&quot;&nbsp; \n  I still am &lt;BR&gt;not &lt;BR&gt;&gt;&nbsp;&nbsp; s=\r\nure what X really means formally, but \n  you haven&#39;t given a &lt;BR&gt;&gt;&nbsp;=\r\n&nbsp; definition for T or Y or Z \n  either.&nbsp; What are these variables=\r\n?&lt;BR&gt;&gt; &lt;BR&gt;&gt;&nbsp;&nbsp; \n  Ultimately I think you are arguing from i=\r\nntuition rather than \n  &lt;BR&gt;&gt;&nbsp;&nbsp; formally, and intuitions can b=\r\ne \n  misleading.&lt;BR&gt;&gt;&nbsp;&nbsp; )&lt;BR&gt;&gt; &lt;BR&gt;&gt; &lt;BR&gt;&gt;&nbsp;&nbsp=\r\n; Sorry \n  to all for the long-windedness of this response!&nbsp; I hope &lt;B=\r\nR&gt;it is \n  &lt;BR&gt;&gt;&nbsp;&nbsp; still useful!&lt;BR&gt;&gt; &lt;BR&gt;&gt;&nbsp;&nbsp; =\r\n--- In \n  neat@yahoogroups.com, &quot;Jim O&#39;Flaherty, Jr.&quot; &lt;BR&gt;&gt;&nbsp;&nbsp; =\r\n\n  &lt;jim_oflaherty_jr@y...&gt; wrote:&lt;BR&gt;&gt;&nbsp;&nbsp; &gt; John, Coli=\r\nn, Ken \n  and Derek,&lt;BR&gt;&gt;&nbsp;&nbsp; &gt; &lt;BR&gt;&gt;&nbsp;&nbsp; &gt; I a=\r\nm wondering \n  if there is not a wee bit too much focus on &lt;BR&gt;&gt;&nbsp;&n=\r\nbsp; topological \n  vairation and insignificant focus on just weight &lt;BR&gt;&g=\r\nt;&nbsp;&nbsp; \n  mutation.&nbsp; I get that NEAT is unique in the fact tha=\r\nt it has a &lt;BR&gt;very \n  &lt;BR&gt;&gt;&nbsp;&nbsp; effective search mechanism for =\r\ntopoligical variation, \n  with the &lt;BR&gt;&gt;&nbsp;&nbsp; ability to stress b=\r\noth additive and subtractive \n  aspects of &lt;BR&gt;change.&nbsp; &lt;BR&gt;&gt;&nbsp;=\r\n&nbsp; And I get that it is fun to \n  focus on the topological variation &lt;B=\r\nR&gt;as it &lt;BR&gt;&gt;&nbsp;&nbsp; is somewhat \n  spatially interesting.&lt;BR&gt;&gt;&=\r\nnbsp;&nbsp; &gt; &lt;BR&gt;&gt;&nbsp;&nbsp; &gt; \n  However, I am realizing that =\r\njust doing effective weight &lt;BR&gt;&gt;&nbsp;&nbsp; \n  mutations, sans topolog=\r\nical changes, can end up producing &lt;BR&gt;solutions \n  &lt;BR&gt;&gt;&nbsp;&nbsp; th=\r\nat are very &quot;fit&quot;.&nbsp; I have been focused on \n  reproducing the &lt;BR&gt;&gt;=\r\n&nbsp;&nbsp; experiments Fogel and Kumar produced \n  which are covered in t=\r\nheir &lt;BR&gt;book &lt;BR&gt;&gt;&nbsp;&nbsp; Blondie24.&nbsp; In \n  that, they had on=\r\nly 3 topologies they &lt;BR&gt;experimented &lt;BR&gt;&gt;&nbsp;&nbsp; \n  with.&nbsp; A=\r\nll of the GA searching was just done with weight &lt;BR&gt;mutation \n  &lt;BR&gt;&gt;&n=\r\nbsp;&nbsp; within a step size that was both a GA parameter and \n  nudged &lt;B=\r\nR&gt;towards &lt;BR&gt;&gt;&nbsp;&nbsp; smaller values.&nbsp; In the Fogel \n  experi=\r\nments, they arrived at an &lt;BR&gt;expert &lt;BR&gt;&gt;&nbsp;&nbsp; player (well, \n  =\r\nat least against human opponents) using just co-&lt;BR&gt;&gt;&nbsp;&nbsp; evolut=\r\nion \n  and a static topology.&nbsp; And in my own replication of &lt;BR&gt;the \n =\r\n &lt;BR&gt;&gt;&nbsp;&nbsp; experiments, something as simple as turn on/off biase=\r\ns \n  had a &lt;BR&gt;&gt;&nbsp;&nbsp; substantial effect in how long it took to a=\r\nrrive at \n  a specimen &lt;BR&gt;of &lt;BR&gt;&gt;&nbsp;&nbsp; similar fitness.&lt;BR&gt;&gt;=\r\n&nbsp;&nbsp; \n  &gt; &lt;BR&gt;&gt;&nbsp;&nbsp; &gt; Now, I realize that mutating=\r\n weights only is \n  not near as sexy &lt;BR&gt;&gt;&nbsp;&nbsp; sounding as both =\r\nweight mutation and \n  topological variation.&nbsp; &lt;BR&gt;&gt;&nbsp;&nbsp; Ho=\r\nwever, what I am wondering \n  and hope to be able to evaluate &lt;BR&gt;with &lt;BR&gt;=\r\n&gt;&nbsp;&nbsp; experimentation \n  is whether the topological mutation rat=\r\nes ought &lt;BR&gt;not &lt;BR&gt;&gt;&nbsp;&nbsp; \n  be very small with the focus more =\r\non trying out many weight \n  &lt;BR&gt;&gt;&nbsp;&nbsp; mutations within a given =\r\ntopology?&nbsp; What is \n  occurring to me is &lt;BR&gt;that &lt;BR&gt;&gt;&nbsp;&nbsp;=\r\n just doing weight mutation is \n  a search at a rate X in a huge &lt;BR&gt;space.=\r\n&nbsp; &lt;BR&gt;&gt;&nbsp;&nbsp; And it \n  seems to me adding topological variat=\r\nion is not just a &lt;BR&gt;&gt;&nbsp;&nbsp; \n  multiplier, but an exponent incre=\r\nasing X.&nbsp; Perhaps the space is \n  &lt;BR&gt;&gt;&nbsp;&nbsp; being made too =\r\nlarge too quickly, before a search just in \n  the &lt;BR&gt;weight &lt;BR&gt;&gt;&nbsp;=\r\n&nbsp; mutation space might demonstrate a \n  uniquely fit individual.&lt;BR&gt;&g=\r\nt;&nbsp;&nbsp; &gt; &lt;BR&gt;&gt;&nbsp;&nbsp; &gt; My \n  understanding is that t=\r\nhe &quot;search in higher dimensional &lt;BR&gt;space&quot; \n  &lt;BR&gt;&gt;&nbsp;&nbsp; might p=\r\nroduce more robust and fit players.&nbsp; At what \n  &lt;BR&gt;computational &lt;BR&gt;=\r\n&gt;&nbsp;&nbsp; cost?&nbsp; If the topological \n  complexity curve for hav=\r\ning a &quot;more &lt;BR&gt;fit &lt;BR&gt;&gt;&nbsp;&nbsp; player&quot; is \n  exponetial, then doe=\r\nsn&#39;t that mean that there is a &lt;BR&gt;&gt;&nbsp;&nbsp; \n  threshold of diminis=\r\nhing returns somewhere?&nbsp; Granted, it may not \n  &lt;BR&gt;&gt;&nbsp;&nbsp; b=\r\ne.&nbsp; However, when it is exponential (and my \n  intuition says it &lt;BR&gt;i=\r\ns &lt;BR&gt;&gt;&nbsp;&nbsp; more of the time), we now have \n  an exponent on an =\r\nexponent of &lt;BR&gt;&gt;&nbsp;&nbsp; complexification which \n  massively enlarg=\r\nes the search space, X^T, &lt;BR&gt;&gt;&nbsp;&nbsp; where T =3D Y^Z \n  and Z is =\r\nthe complexity curve.&nbsp; If this is true, &lt;BR&gt;then \n  &lt;BR&gt;&gt;&nbsp;&nbs=\r\np; we are massive amounts of processing power away from \n  achieving &lt;BR&gt;&g=\r\nt;&nbsp;&nbsp; result in anything but the simplest of domains, \n  like XOR =\r\nand Tic-&lt;BR&gt;Tac-&lt;BR&gt;&gt;&nbsp;&nbsp; Toe.&lt;BR&gt;&gt;&nbsp;&nbsp; &gt; \n  &lt;BR&gt;&=\r\ngt;&nbsp;&nbsp; &gt; Am I missing something here?&nbsp; Perhaps I need to \n=\r\n  do more direct &lt;BR&gt;&gt;&nbsp;&nbsp; experimentation and examine the resul=\r\nts \n  before jumping to this &lt;BR&gt;kind &lt;BR&gt;&gt;&nbsp;&nbsp; of conclusion.&n=\r\nbsp; I \n  just get the sense that simple weight mutation &lt;BR&gt;&gt;&nbsp;&nbs=\r\np; achieved \n  quite a bit in Checkers, a domain more complex than \n  Tic-&lt;=\r\nBR&gt;Tac-&lt;BR&gt;&gt;&nbsp;&nbsp; Toe.&nbsp; It would be interesting to see how \n=\r\n  Checkers might do with &lt;BR&gt;NEAT &lt;BR&gt;&gt;&nbsp;&nbsp; and see what kinds o=\r\nf \n  mutation rates might be more/less &lt;BR&gt;effective &lt;BR&gt;&gt;&nbsp;&nbsp; a=\r\nnd \n  why.&lt;BR&gt;&gt;&nbsp;&nbsp; &gt; &lt;BR&gt;&gt;&nbsp;&nbsp; &gt; &lt;BR&gt;&gt;&nbsp=\r\n;&nbsp; \n  &gt; Jim&lt;BR&gt;&gt;&nbsp;&nbsp; &gt; &lt;BR&gt;&gt;&nbsp;&nbsp; &gt; &lt;BR&gt;=\r\n&gt; &lt;BR&gt;&gt; \n  &lt;BR&gt;&gt; &lt;BR&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp=\r\n;&nbsp; Yahoo! \n  Groups Sponsor \n  &lt;BR&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&=\r\nnbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \n  ADVERTISEMENT&lt;BR&gt;&=\r\ngt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;=\r\n&nbsp; \n  &lt;BR&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \n  &lt;BR&gt;&gt;&nb=\r\nsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;BR&gt;&gt; &lt;BR&gt;&gt; &lt;BR&gt;&gt; \n  -----=\r\n--------------------------------------------------------------&lt;BR&gt;---------=\r\n--&lt;BR&gt;&gt;&nbsp;&nbsp; \n  Yahoo! Groups Links&lt;BR&gt;&gt; &lt;BR&gt;&gt;&nbsp;&nbsp;&=\r\nnbsp;&nbsp; a.. To visit your \n  group on the web, go to:&lt;BR&gt;&gt;&nbsp;&nbs=\r\np;&nbsp;&nbsp; &lt;A \n  href=3D&quot;http://groups.yahoo.com/group/neat/&quot;&gt;http://gr=\r\noups.yahoo.com/group/neat/&lt;/A&gt;&lt;BR&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; =\r\n\n  &lt;BR&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp; b.. To unsubscribe from this group, sen=\r\nd an \n  email to:&lt;BR&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp; \n  neat-unsubscribe@yahoo=\r\ngroups.com&lt;BR&gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \n  &lt;BR&gt;&gt;&nbsp;&nb=\r\nsp;&nbsp;&nbsp; c.. Your use of Yahoo! Groups is subject to \n  the Yahoo! T=\r\nerms &lt;BR&gt;of Service.&lt;BR&gt;&lt;BR&gt;&lt;/TT&gt;&lt;BR&gt;&lt;/BODY&gt;&lt;/HTML&gt;\n\r\n------=_NextPart_000_00C6_01C44CC0.1CE0C1A0--\r\n\n"}}