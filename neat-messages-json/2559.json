{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"XnXqM_1DanEsrttq9LKPRX0TClX0U06XPsZq28YdfVbsiY92nwcAWcP0qJoCOQKwGt0sZHAxRN9SGBbuSp9StX-tmiC7fxQ7DckqpB4K01zB","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: Bloat defeated!  (Maybe...)","postDate":"1141720376","msgId":2559,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGR1amdmbythMjZiQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDQ0MDlERDdCLjUwMTAyMDRAZHNsLnBpcGV4LmNvbT4="},"prevInTopic":2558,"nextInTopic":2560,"prevInTime":2558,"nextInTime":2560,"topicId":2532,"numMessagesInTopic":24,"msgSnippet":"This is an interesting discussion and I wanted to think about it for a minute at a more general level. Basically I m wondering, how would you guys articulate","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 65747 invoked from network); 7 Mar 2006 08:33:05 -0000\r\nReceived: from unknown (66.218.67.36)\n  by m34.grp.scd.yahoo.com with QMQP; 7 Mar 2006 08:33:05 -0000\r\nReceived: from unknown (HELO n29a.bullet.scd.yahoo.com) (209.73.160.86)\n  by mta10.grp.scd.yahoo.com with SMTP; 7 Mar 2006 08:33:05 -0000\r\nComment: DomainKeys? See http://antispam.yahoo.com/domainkeys\r\nReceived: from [66.218.69.6] by n29.bullet.scd.yahoo.com with NNFMP; 07 Mar 2006 08:34:25 -0000\r\nReceived: from [66.218.66.75] by t6.bullet.scd.yahoo.com with NNFMP; 07 Mar 2006 08:32:59 -0000\r\nDate: Tue, 07 Mar 2006 08:32:56 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;dujgfo+a26b@...&gt;\r\nIn-Reply-To: &lt;4409DD7B.5010204@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: Re: Bloat defeated!  (Maybe...)\r\nX-Yahoo-Group-Post: member; u=54567749; y=pneotJUqhdAWxpKW0kmMRNmvDHkh79mmqE30m1aa08qXESgYc8lt\r\nX-Yahoo-Profile: kenstanley01\r\n\r\nThis is an interesting discussion and I wanted to think about it for \na min=\r\nute at a more general level.\n\nBasically I&#39;m wondering, how would you guys a=\r\nrticulate the ideal \nsituation with respect to the distribution and directi=\r\non of change \nin complexities in a population when it is far from a solutio=\r\nn?  \nThis seems to be harder to pin down than it initially seems.\n\nI think =\r\nwe&#39;d all agree at first that ideally, you want to find the \nminimum necessa=\r\nry complexity for a task.  But the truth is that the \ngeneral principle of =\r\nfinding the &quot;smallest possible&quot; doesn&#39;t \nnecessarily inform practice to a g=\r\nreat extent since we don&#39;t know \nwhat the smallest possible is anyway.  It&#39;=\r\ns rather just a \ntheoretical goal.\n\nA more practical perspective is to ask =\r\nwhat is the overall ideal \ncomplexity distribution in a situation where you=\r\n don&#39;t have a \nsolution and don&#39;t know the complexity of a solution?  And i=\r\nt seems \nto me the goal should be to increase the *range* of complexity in =\r\n\nthe population.  That is, I would not articulate the objective in \nterms o=\r\nf average complexity, or max complexity, but in terms of \nrange and distrib=\r\nution (hopefully nicely distributed across that \nrange).\n\nThe reason is tha=\r\nt barring any notion of the &quot;right size,&quot; the right \napproach is to not put=\r\n all your eggs in any one basket.  You have to \nassume that the longer you =\r\nsearch without success, the less you \nreally know about where you should be=\r\n.  You want to probe the waters \nof deeper complexity, but you don&#39;t want t=\r\no give up on your simple \nspaces yet either, since any time they could yiel=\r\nd gold, especially \nif they show improvement (although not a solution).\n\nOr=\r\n would you disagree with this perspective?  Would you perhaps say \nthat the=\r\n whole population should move together in a certain \ndirection?  It becomes=\r\n an interesting question with NEAT since NEAT \ncan support a robust range o=\r\nf complexities, which is different from \nGP where the population tends to b=\r\nump itself up together because of \nthe free-for-all with no speciation.   A=\r\nnd if the complexity range \nis really a key factor, then methods that attem=\r\npt to limit bloat may \nbe taking an overly simplified view because they loo=\r\nk at the goal \nwith respect to a single value (i.e. max or average complexi=\r\nty) as \nopposed to a vector of values (the whole set of complexities).   \nT=\r\nherefore they be may ignoring the more significant methodological \nquestion=\r\n of how to keep a healthy range, or how rapidly to increase \nthat range in =\r\nthe face of stagnation.\n\nSo what is the right idea?  If you aren&#39;t close to=\r\n a solution, where \ndo you want complexities to go?  \n\nken\n\n--- In neat@yah=\r\noogroups.com, Colin Green &lt;cgreen@...&gt; wrote:\n&gt;\n&gt; Some idle ramblings... :)=\r\n\n&gt; \n&gt; I see there is a short thread on the GP list discussing bloat. \nThere=\r\n \n&gt; they are discussing modifying the fitness function to punish \nbloat - \n=\r\n&gt; &#39;parsimony pressure&#39; as someone termed it. E.g. if using \ntournament \n&gt; s=\r\nelection then if two genomes have the same fitness on the task at \nhand \n&gt; =\r\nthen the shortest genome wins, a sort of secondary objective if \nyou \n&gt; lik=\r\ne. I think there is a risk with that approach of restricting a \nsearch \n&gt; f=\r\nrom exploring areas of the search space with additional \ncomplexity with \n&gt;=\r\n no immediate gain. We want /some/ exploration of complexity, just \nnot \n&gt; =\r\ntoo much. The use of a secondary objective like this makes for a \nvery \n&gt; d=\r\nefinite limit or barrier to the complexity of genomes being \nsearched in \n&gt;=\r\n times of fitness stagnation - only selection based on the primary \n&gt; objec=\r\ntive can choose more complex genomes.\n&gt; \n&gt; The techniques discussed on here=\r\n of using &#39;softer&#39; controls for \nbloat \n&gt; make more sense to me, e.g. Ian&#39;s=\r\n equlibrium idea or my technqiue \nof \n&gt; allowing complexity to rise up to a=\r\n point N points above the \nprevious \n&gt; low [following a pruning phase]. The=\r\nse techniques don&#39;t define a \nhard \n&gt; &#39;complexity barrier&#39; and therefore gi=\r\nve a possible path out of \nfitness \n&gt; stagnation where there may not be a p=\r\nath that consists of a single \nstep \n&gt; (mutation/crossover).\n&gt; \n&gt; Actually =\r\nmy technqiue does sort of define a hard barrier, but it \nis \n&gt; above the av=\r\nerge complexity of the population following a pruning, \nhence \n&gt; it does gi=\r\nve some leeway for exploration where a secondary fitness \n&gt; objective would=\r\nn&#39;t.\n&gt; \n&gt; Both techniques (of soft control) control population complexity b=\r\ny \n&gt; adjusting the balance of additive and destructive mutations. In my \nca=\r\nse \n&gt; I use extremes of no additive mutations + a high deletion rate \n(and =\r\nno \n&gt; crossover BTW) versus normal operation where the equlibrium point \nis=\r\n at \n&gt; infinity (actually Ian that does /sort of/ make sense). So the \nactu=\r\nal \n&gt; complexity of genomes will (should!) be distributed around the \n&gt; equ=\r\nilibrium point (normal distribution?), with the bulk of genomes \nnear \n&gt; th=\r\ne eq point and the odd few much further out - which seems ideal, \nright?\n&gt; =\r\n\n&gt; Ian, as you said the way we handle recombination - e.g. disjoint \nand \n&gt;=\r\n excess genes - is important because this is another means by which \n&gt; comp=\r\nlexity can be affected. One solution is to ensure that the all \nof \n&gt; the o=\r\nffspring from a recombination round have the same total \ncomplexity \n&gt; as a=\r\nll of the parents, a sort of zero-sum game scenario. That way \nonly \n&gt; the =\r\nmutation operators have an affect on complexity and we are \nhappy \n&gt; that t=\r\nhey are balanced. Alternatively we allow non-zero-sum \n&gt; recombnation, but =\r\nset up a balance similar to the one used with \n&gt; additive/deletive mutation=\r\ns, so that once again the overall system \nwill \n&gt; find an equilibrium point=\r\n.\n&gt; \n&gt; Having said all that it just occured to me that a \nsecondary &#39;parsim=\r\nony \n&gt; pressure&#39; could be applied in a similarly &#39;soft&#39; manner, e.g. \ndon&#39;t=\r\n \n&gt; apply the secondary function until complexity has risen beyond \nsome \n&gt;=\r\n level and then only gradually increase its weighting. With this \napproach =\r\n\n&gt; though we assume that the underlying pressure is upwards and that \nwe ar=\r\ne \n&gt; defining a soft upper limit to control it, therefore once again \n&gt; [po=\r\nssibly] setting up an equilibrium point where the genomes are \n&gt; distribute=\r\nd around that point, although possibly in an unevenly \nshaped \n&gt; distributi=\r\non because the upward and downward pressures are applied \nby \n&gt; different p=\r\nrocesses.\n&gt; \n&gt; In summation then I think both approaches equate (or could b=\r\ne made \nto \n&gt; equate) to the same thing, and it&#39;s just a matter of which we=\r\n we \nthink \n&gt; is simplest and easiest to implement.\n&gt; \n&gt; Colin.\n&gt;\n\n\n\n\n\n"}}