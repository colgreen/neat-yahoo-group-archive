{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":283334584,"authorName":"petar_chervenski","from":"&quot;petar_chervenski&quot; &lt;petar_chervenski@...&gt;","profile":"petar_chervenski","replyTo":"LIST","senderId":"U2He6QGGHq7WkDLBvNANIwdvsVGP-_xR28oPu7ShuROALIijWDfL126cyVgS2Anys_c0yqoXVRa8rBSp3OVG6Mot3sckStk6kdD8-_KYO-pIp7xUKh8","spamInfo":{"isSpam":false,"reason":"6"},"subject":"RE : [neat] Re: HyperNEAT: Creating Neural Networks with CPPNs","postDate":"1175029803","msgId":3051,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGV1YzE3YitydnRiQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGV1YnRjNys1MGhuQGVHcm91cHMuY29tPg=="},"prevInTopic":3050,"nextInTopic":3052,"prevInTime":3050,"nextInTime":3052,"topicId":3028,"numMessagesInTopic":34,"msgSnippet":"Hi, Thanks for providing me with more insight about what is really going on with the HyperNEAT evolution of large-scale NNs. A note.. The CPPN concept is","rawEmail":"Return-Path: &lt;petar_chervenski@...&gt;\r\nX-Sender: petar_chervenski@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 54777 invoked from network); 27 Mar 2007 21:10:04 -0000\r\nReceived: from unknown (66.218.66.68)\n  by m47.grp.scd.yahoo.com with QMQP; 27 Mar 2007 21:10:04 -0000\r\nReceived: from unknown (HELO n23d.bullet.scd.yahoo.com) (66.218.67.213)\n  by mta11.grp.scd.yahoo.com with SMTP; 27 Mar 2007 21:10:04 -0000\r\nReceived: from [66.218.69.5] by n23.bullet.scd.yahoo.com with NNFMP; 27 Mar 2007 21:12:10 -0000\r\nReceived: from [66.218.66.87] by t5.bullet.scd.yahoo.com with NNFMP; 27 Mar 2007 21:10:04 -0000\r\nDate: Tue, 27 Mar 2007 21:10:03 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;euc17b+rvtb@...&gt;\r\nIn-Reply-To: &lt;eubtc7+50hn@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0\r\nFrom: &quot;petar_chervenski&quot; &lt;petar_chervenski@...&gt;\r\nSubject: RE : [neat] Re: HyperNEAT: Creating Neural Networks with CPPNs\r\nX-Yahoo-Group-Post: member; u=283334584; y=Nk5HIrDY3An6ku2sDnIElEr6umg_lUefFRIzeQGqOK_XxmRr2C89Yy5xLw\r\nX-Yahoo-Profile: petar_chervenski\r\n\r\nHi,\nThanks for providing me with more insight about what is really going \no=\r\nn with the HyperNEAT evolution of large-scale NNs.\nA note.. The CPPN concep=\r\nt is indeed very powerful.. It is a universal \nmathematical structure capab=\r\nle of representing ANY phenotype... I \nalso agree that any kind of time-or-=\r\niterative-based developmental \nencoding is unnesecary since the other encod=\r\nings are simply another \nway to represent the same thing. \nYeah.. You know?=\r\n It is interesting how Ken realised this when playing \nwith the Genetic Art=\r\n programs evolving the spaceship :) Just like \nNewton got the idea about gr=\r\navity when an apple fell on his head :))) \nThe automatic scaling of the sub=\r\nstrate and altering the substrate \nconfiguration are still an option that n=\r\need to be tested. I&#39;ll try to \ndo some experiments about it. I&#39;ll keep you =\r\ninformed about the \nresults of this stuff. \n\nPeter\n\n--- In neat@yahoogroups=\r\n.com, &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt; wrote:\n&gt;\n&gt; --- In neat@yahoogroups.c=\r\nom, Stephen Waits &lt;steve@&gt; wrote:\n&gt; &gt;\n&gt; &gt; Kenneth Stanley wrote:\n&gt; &gt; &gt; \n&gt; &gt;=\r\n &gt; Petar, I agree that it would be interesting to introduce a kind \n&gt; of\n&gt; =\r\n&gt; &gt; mutation that automatically bumps up the substrate density \n(i.e. \n&gt; #\n=\r\n&gt; &gt; &gt; of nodes in the substrate). It would also be interesting to \nallow\n&gt; =\r\n&gt; &gt; HyperNEAT to decide the locations of the nodes in the substrate \n&gt; on\n&gt;=\r\n &gt; &gt; its own.\n&gt; &gt; \n&gt; &gt; I think this is important.  Part of the draw to NEAT=\r\n, for me, is  \n&gt; the \n&gt; &gt; low level of human configuration and design requi=\r\nred.\n&gt; &gt; \n&gt; \n&gt; Even without automatic node placement, I&#39;m anticipating that=\r\n in the \n&gt; long run it will turn out that there are a few standard substrat=\r\ne \n&gt; configurations for certain types of problems (e.g. robot control, \n&gt; v=\r\nisual recognition, board games) that you can just start with \n&gt; without thi=\r\nnking about it.  The actual connectivity in the end will \n&gt; still be determ=\r\nined by HyperNEAT.  \n&gt; \n&gt; However, I think the customaizability of the subs=\r\ntrate is a real \n&gt; opportunity.  With this capability we can just say, &quot;Loo=\r\nk, here&#39;s \n&gt; how you should conceive the world, now take advantage of it.&quot;\n=\r\n&gt; \n&gt; The density choice is also potentially powerful.  It means you can \n&gt; =\r\ne.g. change the number of sensors on an old neural network and \nstill \n&gt; ha=\r\nve the system work.  It means you can increase the density of a \n&gt; brain an=\r\nd still have it generally respect the same pattern!  That \nis \n&gt; an unprece=\r\ndented capability.  \n&gt; \n&gt; I believe the most significant advantage of incre=\r\nasing density will \n&gt; indeed be a more powerful kind of complexification.  =\r\nIt is unlikely \n&gt; that adding one connection at a time will ever scale to m=\r\nillions or \n&gt; billions of connections.  However, density increases can \n&gt; r=\r\nealistically scale by orders of magnitude.  The end goal is not to \n&gt; be ab=\r\nle to simply do the same thing at higher resolution, but to be \n&gt; able to f=\r\nurther evolve something that mostly still works, now with \n&gt; millions more =\r\nconnections available for improving functionality.\n&gt; \n&gt; &gt; Ken, I had anothe=\r\nr question related to varying node density on \nthe \n&gt; &gt; substrate.  The CPN=\r\nN may (likely) connect &#39;regions&#39; heavily, and \n&gt; so as \n&gt; &gt; node density in=\r\ncreases, so will these regional connections.  My \n&gt; question \n&gt; &gt; is, is th=\r\nere evidence of this in biology?  That is, of groups of \n&gt; neurons \n&gt; &gt; reg=\r\nionally strongly connected to other groups of neurons?  I \n&gt; realize \n&gt; &gt; i=\r\nt&#39;s probably mostly irrelevant, but I&#39;m curious if it&#39;s \nsomething \n&gt; &gt; you=\r\n&#39;ve researched.\n&gt; &gt; \n&gt; \n&gt; There is evidencve for high density regional conn=\r\nections.  \n&gt; (i.e. &quot;group-wise connectivity.&quot;)  For example, the optic nerv=\r\ne \nthat \n&gt; connects from the back of the retina into the lateral geniculate=\r\n \n&gt; nucleus has a number of neurons proportional to the density of \n&gt; visua=\r\nl resolution.  It has even been shown that some animals are \n&gt; born with mo=\r\nre or less resolution (of various types: rods, cones, \n&gt; etc..) than their =\r\nparents, and that the density of the optic nerve \n&gt; adjusts proprtionally d=\r\nuring development.  The evidence for this is \n&gt; cited in our Taxonomy paper=\r\n in the section on canalization; it \n&gt; involved experiments with wild cats.=\r\n\n&gt; \n&gt; Perhaps the greatest group-wise connector of all is the corpus \n&gt; cal=\r\nlosum, which connects one hemisphere of the brain to the other. \n&gt; \n&gt; We ha=\r\nve determined in unpublished work how group-wise connectivity \n&gt; is effecti=\r\nvely represented by connective CPPNs.  Hopefully we will \n&gt; be able to publ=\r\nish that soon.\n&gt; \n&gt; ken\n&gt;\n\n\n\n"}}