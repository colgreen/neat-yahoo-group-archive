{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"ypmDIUik_ok5xriiKc4sqNc54qeppJh5J6mniNgRe6RvB_ZK0adWGmOXLlXHruVgK18aVxq5zS14yLhWK_DPbSpGPVLF5yngeGkrhdZANalZ","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: Applying NEAT to Tic-Tac-Toe","postDate":"1075160350","msgId":337,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGJ2NDhldStsYjQ5QGVHcm91cHMuY29tPg==","inReplyToHeader":"PGJ2M3ZqbysxMGFlOUBlR3JvdXBzLmNvbT4="},"prevInTopic":336,"nextInTopic":338,"prevInTime":336,"nextInTime":338,"topicId":336,"numMessagesInTopic":27,"msgSnippet":"You could just evaluate it against all 19,683 possible board positions, either as fitness, or just for testing.  The score is how many of the 19,683 moves are","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 19720 invoked from network); 26 Jan 2004 23:40:33 -0000\r\nReceived: from unknown (66.218.66.217)\n  by m3.grp.scd.yahoo.com with QMQP; 26 Jan 2004 23:40:33 -0000\r\nReceived: from unknown (HELO n30.grp.scd.yahoo.com) (66.218.66.87)\n  by mta2.grp.scd.yahoo.com with SMTP; 26 Jan 2004 23:40:28 -0000\r\nReceived: from [66.218.66.140] by n30.grp.scd.yahoo.com with NNFMP; 26 Jan 2004 23:39:11 -0000\r\nDate: Mon, 26 Jan 2004 23:39:10 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;bv48eu+lb49@...&gt;\r\nIn-Reply-To: &lt;bv3vjo+10ae9@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Length: 7232\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-eGroups-Remote-IP: 66.218.66.87\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: Re: Applying NEAT to Tic-Tac-Toe\r\nX-Yahoo-Group-Post: member; u=54567749\r\nX-Yahoo-Profile: kenstanley01\r\n\r\nYou could just evaluate it against all 19,683 possible board \npositions, either as fitness, or just for testing.  The score\nis how many of the 19,683 moves are the same as that proposed by \nBEST. (note that many of these are actually irrelevant since\nthey already represent winning positions, so ignore those)\n\nken\n\n--- In neat@yahoogroups.com, &quot;Derek James&quot; &lt;blue5432@y...&gt; wrote:\n&gt; Hello all,\n&gt; \n&gt; Philip and I are still doing some testing and continued development \n&gt; on our Java NEAT implementation, and very soon we&#39;ll start doing our \n&gt; first experiments in the domain of Tic-Tac-Toe.\n&gt; \n&gt; I had a few questions for the group, though.  First of all, what do \n&gt; you think the best way to evaluate the strength of an evolved Tic-Tac-\n&gt; Toe-playing network is?\n&gt; \n&gt; I was curious about how this was handled in Joseph&#39;s modular-NEAT \n&gt; paper, in which he uses Tic-Tac-Toe as a domain.  First, here&#39;s \n&gt; Joseph&#39;s explanation of how games and players are represented and \n&gt; implemented:\n&gt; \n&gt; &quot;For this domain, the network architecture is always \n&gt; 9 input neurons and 9 output neurons, representing the vector of the \n&gt; board space. At each step of the game, the network is given the \n&gt; current layout of the board, and using this input, the highest valued \n&gt; output is chosen as the location where the network will play. In the \n&gt; case that that square is already occupied, the next highest value is \n&gt; chosen, and so on. Removing the requirement that the network must \n&gt; learn legal moves makes it signi\fcantly easier to analyze the results \n&gt; since Modular NEAT converges faster. Each square on the board can \n&gt; have three states: a value of -1 indicates the opponent occupies the \n&gt; particular square, while 1 denotes the computer&#39;s own piece and 0 \n&gt; denotes an empty space.&quot;\n&gt; \n&gt; We&#39;re using exactly the same approach.  But I&#39;m a bit confused about \n&gt; how the evaluations are carried out:\n&gt; \n&gt; &quot;Two modes of evaluation are used depending on the data being \n&gt; gathered: strict evaluation and loose evaluation. Networks scored \n&gt; using strict evaluation play a total of 10 games against a perfect \n&gt; tic-tac-toe playing opponent. The first nine games are started by the \n&gt; opponent, one game for each different starting location. The last \n&gt; game is started by the network. With this form of evaluation, the \n&gt; network&#39;s assigned fitness will be the same no matter how often it is \n&gt; scored. In contrast, under loose evaluation, the network \n&gt; plays a total of 40 games, one round of strict evaluation followed by \n&gt; 30 games against a randomly moving opponent. In this case, the \n&gt; network&#39;s ability to actually play the game is more accurately \n&gt; analyzed, at the cost of consistency (two separate loose evaluations \n&gt; of the same network can yield different final fitness scores). Often \n&gt; networks trained using strict evaluation could be defeated easily if \n&gt; the opponent played sub-optimally (as it was never trained on those \n&gt; cases), whereas networks trained using loose evaluation were more \n&gt; likely to be able to adapt since their ability to play random games \n&gt; is a factor in their fitness. \n&gt; Unless otherwise stated, all data gathered from the tic-tac-toe \n&gt; domain is scored using strict evaluation.&quot;\n&gt; \n&gt; You say that the evaluation methods differ, but note that &quot;loose&quot; \n&gt; evaluation more accurately analyzes the strength of a player.  You \n&gt; note that players evolved using &quot;strict&quot; evaluation are easily \n&gt; defeated by a sub-optimal player (such as a random player).  Why then \n&gt; did you use &quot;strict&quot; evaluation as your primary mode of evaluation?  \n&gt; Time?  Computing resources?\n&gt; \n&gt; This makes me question the language in the following paragraph:\n&gt; \n&gt; &quot;This example network solves tic-tac- toe, only missing one game out \n&gt; of 10 against the perfect opponent (the genetic algorithm converged \n&gt; on the perfect solution 200 generations later).&quot;\n&gt; \n&gt; You say that the network &quot;solves&quot; Tic-Tac-Toe, but it&#39;s only being \n&gt; evaluated against a perfect player.  And by &quot;missing one game out of \n&gt; 10&quot;, I&#39;m assuming that you mean the evolved player lost 1 and tied 9 \n&gt; against the perfect player.  Is that right?  \n&gt; \n&gt; My main question is, if a network can tie 90% of its games against an \n&gt; optimal player, would we say it has solved Tic-Tac-Toe?  What if the \n&gt; same network loses 85% of its games against a random player?\n&gt; \n&gt; I&#39;ve found this in informal experiments I&#39;ve run so far...networks \n&gt; that learn blocking behavior don&#39;t necessarily learn the behavior \n&gt; needed to complete winning moves, and a network that performs well \n&gt; against a perfect player often performs poorly against a random \n&gt; mover, and vice versa.\n&gt; \n&gt; I like the way Angeline and Pollack tested the strength of their Tic-\n&gt; Tac-Toe AIs (modular LISP programs) in this 1993 paper:\n&gt; \n&gt; http://citeseer.nj.nec.com/cache/papers/cs/1362/http:zSzzSzwww.cis.ohi\n&gt; o-state.eduzSzlairzSzTechReportszSzijcai93.pdf/angeline93learning.pdf\n&gt; \n&gt; They evolved their programs directly against three hard-coded players:\n&gt; \n&gt; RAND - a random mover\n&gt; BEST - a perfect player\n&gt; NEAR - a BEST player that is susceptible to &quot;forking&quot;, or allowing \n&gt; its opponent to make a move that produces two winning moves their \n&gt; next turn\n&gt; \n&gt; They compared these direct evolutionary runs with a form of single-\n&gt; population coevolution, using a simple Single-Elimination Tournament.\n&gt; \n&gt; They then played the evolved champions against each type of player \n&gt; (2000 games) to test their relative strengths, producing results that \n&gt; look like this:\n&gt; \n&gt; Champion evolved against RAND:\n&gt; \n&gt; Wins vs. RAND: 1125\n&gt; Draws vs. RAND: 0\n&gt; Wins vs. NEAR: 0\n&gt; Draws vs. NEAR: 0\n&gt; Draws vs. BEST: 0\n&gt; \n&gt; Champion evolved against NEAR:\n&gt; \n&gt; Wins vs. RAND: 802\n&gt; Draws vs. RAND: 104\n&gt; Wins vs. NEAR: 144\n&gt; Draws vs. NEAR: 123\n&gt; Draws vs. BEST: 0\n&gt; \n&gt; Champion evolved against BEST:\n&gt; \n&gt; Wins vs. RAND: 310\n&gt; Draws vs. RAND: 535\n&gt; Wins vs. NEAR: 0\n&gt; Draws vs. NEAR: 360\n&gt; Draws vs. BEST: 0\n&gt; \n&gt; Champion from Single-Elimination Tournament Coevolution:\n&gt; \n&gt; Wins vs. RAND: 781\n&gt; Draws vs. RAND: 471\n&gt; Wins vs. NEAR: 61\n&gt; Draws vs. NEAR: 588\n&gt; Draws vs. BEST: 481\n&gt; \n&gt; Thus they have a way of comparing the resulting networks against a \n&gt; cross-section of strategies/opponents, giving a reasonable estimate \n&gt; of the network&#39;s strength.\n&gt; \n&gt; For our experiments, I planned on using a similar technique to \n&gt; evaluate the performance of evolved networks.  I&#39;d planned on using \n&gt; three strategies, like Angeline and Pollack, a RAND, BEST, and CENTER \n&gt; (a player that always plays in the center if it is open, and \n&gt; otherwise plays randomly...this seemed a good strategy between RAND \n&gt; and BEST because it is one of the first strategic concepts a human \n&gt; child learns when playing the game.)\n&gt; \n&gt; Does anyone have a better suggestion on how to evaluate the strength \n&gt; of evolved Tic-Tac-Toe players?  Is there a definite method for \n&gt; determining that a network has solved the game?\n&gt; \n&gt; When I play large samples of RAND vs. BEST, BEST wins about 90% and \n&gt; ties RAND about 10%.  If a network ties BEST 100% of the time and \n&gt; beats RAND 90% of the time, would you say that is the standard for \n&gt; solving Tic-Tac-Toe?  If not, what would be a better measure?\n&gt; \n&gt; Derek\n\n\n"}}