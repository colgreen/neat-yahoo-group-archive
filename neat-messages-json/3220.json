{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"k8MS1GK02kDqUvWacG0PeSlX5G2MR09IDHBVnDN3Q2sJMMmSiWqGBHf4l2kqW5K8ZAW23yAPHoe62gT84ms3MDVIrbvHIlS0hbaBcwWCmDvo","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: HyperNEAT and No Free Lunch","postDate":"1177907883","msgId":3220,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGYxM3JyYit0OGc0QGVHcm91cHMuY29tPg==","inReplyToHeader":"PDU4QTgwRTYxLTEwMEQtNEEwQy1CMUNBLUY5NDMyRjZCMjVGMUBjcy51dGV4YXMuZWR1Pg=="},"prevInTopic":3219,"nextInTopic":3221,"prevInTime":3219,"nextInTime":3221,"topicId":3214,"numMessagesInTopic":27,"msgSnippet":"... However, I think we re kind of splitting our theoretical hairs here more than necessary.  Sure, if the issue is that we need to take all possible","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 56776 invoked from network); 30 Apr 2007 04:38:33 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m42.grp.scd.yahoo.com with QMQP; 30 Apr 2007 04:38:33 -0000\r\nReceived: from unknown (HELO n18.bullet.sp1.yahoo.com) (69.147.64.215)\n  by mta5.grp.scd.yahoo.com with SMTP; 30 Apr 2007 04:38:33 -0000\r\nReceived: from [216.252.122.217] by n18.bullet.sp1.yahoo.com with NNFMP; 30 Apr 2007 04:38:04 -0000\r\nReceived: from [66.218.69.2] by t2.bullet.sp1.yahoo.com with NNFMP; 30 Apr 2007 04:38:04 -0000\r\nReceived: from [66.218.66.84] by t2.bullet.scd.yahoo.com with NNFMP; 30 Apr 2007 04:38:04 -0000\r\nDate: Mon, 30 Apr 2007 04:38:03 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;f13rrb+t8g4@...&gt;\r\nIn-Reply-To: &lt;58A80E61-100D-4A0C-B1CA-F9432F6B25F1@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: Re: HyperNEAT and No Free Lunch\r\nX-Yahoo-Group-Post: member; u=54567749; y=-_D4gOa8OU7eO33HQtLsZfG_iCSzVKBK9wutCj15pzPYf1VXZStZ\r\nX-Yahoo-Profile: kenstanley01\r\n\r\nJoe, I am willing to go with this:\n\n&gt; At the very least, you should limit  =\r\n\n&gt; your statement just to problems where the geometry is known /to \n&gt; the e=\r\nxperimenter.  In the case where the experimenter knows the \n&gt; geometry a pr=\r\niori, and he is correct in that knowledge, then yes, \n&gt; HyperNEAT would out=\r\nperform other Non-Hyper algorithms in the NFL \n&gt; sense.\n\nHowever, I think w=\r\ne&#39;re kind of splitting our theoretical hairs here \nmore than necessary.  Su=\r\nre, if the issue is that we need to take all \npossible precaution to convey=\r\n that we understand NFL to the letter, \nthen we can go down this road and o=\r\nptimize our statement to avoid any \npossible misunderstanding.  And that ma=\r\ny be important in some \ncontexts.  Yet I feel it is a bit of a distraction =\r\nfrom what&#39;s really \nimportant about what I&#39;m saying.  So let me try it in m=\r\nore layman&#39;s \nterms, avoiding mention of NFL:\n\nThe fact that one pixel in a=\r\n 2D visual field is next to another is \nabsolutely integral to processing s=\r\nuch a field.  Yet amazingly, \ncurrent neuroevolution algorithms (or any oth=\r\ner ML algorithm that I \nam aware of for that matter, including RL) have no =\r\ncapacity \nwhatsoever to exploit this fundemanetal relationship within a 2D =\r\n\nfield!  Rather, to the network the input nodes might as well be \nscrambled=\r\n in a completely random order, which would be ridiculous to \nus if it wasn&#39;=\r\nt for the fact that it actually is what we have been \nfeeding our algorithm=\r\ns to use as input!\n\nThe same goes for board games, where one piece being ne=\r\nxt to another \nis again a fundamental relationship, as are their relative \n=\r\nlocations.  Yet, amazingly again, we input board positions as if they \nare =\r\nwhat might as well be a randomized scramble of cells.\n\nAlmost every human s=\r\nense has a fundamental and obvious geometric \nrelationship.  Touch sense is=\r\n mapped across the body, audio \nfrequencies map to different locations on t=\r\nhe main auditory nerve.  \nThe brain exploits this geometry in every case.  =\r\nHow could it not?\n\nSure, one could argue that we can&#39;t be *certain* (in an =\r\nNFL sense) \nthat such layouts are useful whatsoever to a learning algorithm=\r\n \nacross all possible cases.  And I agree.  But common sense tells us \nthat=\r\n for interesting problems for them to be omitted produces an \nenormous pena=\r\nlty.  Imagine if I scrambled your visual field \nprojections into a random c=\r\nonfiguration and asked you to identify \nobjects; yet that&#39;s exactly what we=\r\n ask our ANNs to do!  The brain \nrelies on such geometry to work.  And now,=\r\n finally, we can provide it \nto our ANNs.\n\nSo it&#39;s not just an abstract iss=\r\nue about providing extra parameters \nthat users may or may not set correctl=\r\ny as some kind of theoretical \noracle.  It&#39;s about one of the most integral=\r\n and important a priori \nproperties of sensory and output distribution that=\r\n we can possibly \nhave.\n\nThat&#39;s the important point. \n\nken\n\n\n\n--- In neat@y=\r\nahoogroups.com, Joseph Reisinger &lt;joeraii@...&gt; wrote:\n&gt;\n&gt; (I&#39;ve changed the=\r\n thread title so that this post will be easier to  \n&gt; ignore.)\n&gt; \n&gt; Sorry f=\r\nor dragging us deeper into a discussion of NFL, but I think  \n&gt; this point =\r\nis really important, especially if you are going to be  \n&gt; making these cla=\r\nims in front of a broader audience. Also I think \nit  \n&gt; is helpful for the=\r\n audience here to be crystal clear on the NFL  \n&gt; issue. Its a very importa=\r\nnt theorem and it has been quite often  \n&gt; misapplied (for instance, there =\r\nis some evidence that Wolpert  \n&gt; initially came up with NFL specifically t=\r\no critique GAs).\n&gt; \n&gt; &gt; That&#39;s why I said it &quot;may&quot; be better for evolving v=\r\nery large \nscale  \n&gt; &gt; brains.\n&gt; &gt; The &quot;may&quot; hinges on the issues you bring=\r\n up and others.  However,\n&gt; &gt; even the opportunity to be better is an advan=\r\ntage over having no \nsuch\n&gt; &gt; opportunity.  The opportunity of course can b=\r\ne squandered with the\n&gt; &gt; wrong a priori knowledge.   Yet part of my point =\r\nis that it will\n&gt; &gt; often be the case that the natural geometry of a task i=\r\ns all you \nneed\n&gt; &gt; to provide a powerful bias (or at least a bias that is =\r\nbetter than\n&gt; &gt; nothing), so it will often be possible to seize the opportu=\r\nnity\n&gt; &gt; without a great deal of effort.\n&gt; \n&gt; Ok, note your use of the word=\r\n &quot;often&quot; instead of the \nword &quot;always.&quot;  \n&gt; Just that word substitution mea=\r\nns that you aren&#39;t within the realm \nof  \n&gt; situations that NFL covers. I t=\r\notally agree with you that the  \n&gt; application of such geometry can be usef=\r\nul. I just don&#39;t think you  \n&gt; can leverage NFL to back up the argument you=\r\n are making.\n&gt; \n&gt; &gt; It is true too that NEAT and other neural network algor=\r\nithms do\n&gt; &gt; indeed allow for some inclusion of prior knowledge through the=\r\nir\n&gt; &gt; input/output encoding.  However, note how I phrased my\n&gt; &gt; claim: &quot;H=\r\nyperNEAT is not subject to the No Free Lunch theorem when\n&gt; &gt; comparing to =\r\nalgorithms that do not allow injecting such a priori\n&gt; &gt; knowledge.&quot;  That =\r\nis, among algorithms that allow you to decide \non an\n&gt; &gt; input encoding in =\r\nthe traditional way, the provision of such \nencoding\n&gt; &gt; does not give one =\r\nalgorithm a leg up over another since they all\n&gt; &gt; allow for such knowledge=\r\n to be included.  HyperNEAT, on the other\n&gt; &gt; hand, allows a new kind of kn=\r\nowledge (i.e. geometry) to be \nincluded\n&gt; &gt; and therefore does have a poten=\r\ntial leg up on that class of\n&gt; &gt; algorithms.\n&gt; \n&gt; But this leg up would nec=\r\nessarily be different for each problem,  \n&gt; hence my original critique. You=\r\n can&#39;t build the experimenter into \nthe  \n&gt; system and still talk about NFL=\r\n. Its just not applicable because \nyou  \n&gt; are no longer doing any generali=\r\nzation across classes of problems.  \n&gt; Therefore you can&#39;t use it to build =\r\nyour case.\n&gt; \n&gt; Your argument runs something like: &quot;HyperNEAT has more para=\r\nmeters  \n&gt; than the experimenter can set than other Non-Hyper NEATs that \na=\r\nllow  \n&gt; him/her to build in even better prior knowledge.&quot; But you are  \n&gt; =\r\nneglecting the fact that the experimenter now has more possible  \n&gt; paramet=\r\ners to set incorrectly! At the very least, you should limit  \n&gt; your statem=\r\nent just to problems where the geometry is known /to \nthe  \n&gt; experimenter/=\r\n.  In the case where the experimenter knows the \ngeometry  \n&gt; a priori, and=\r\n he is correct in that knowledge, then yes, HyperNEAT  \n&gt; would outperform =\r\nother Non-Hyper algorithms in the NFL sense.\n&gt; \n&gt; &gt; Of course it depends on=\r\n how well the user takes\n&gt; &gt; advantage of the opportunity, but the opportun=\r\nity is now there.  \nThis\n&gt; &gt; fact does indeed mean that statements about Hy=\r\nperNEAT vs. other\n&gt; &gt; neuroevolution (or even machine learning) algorithms =\r\ncan cite an\n&gt; &gt; opportunity to genuinely be better on average, which in eff=\r\nect \nbrings\n&gt; &gt; it outside NFL in one particular sense.\n&gt; \n&gt; It can be bett=\r\ner on average, if the experimenter always makes the  \n&gt; correct choices w.r=\r\n.t. the geometry. But I don&#39;t think you can use  \n&gt; such &quot;oracle&quot; experimen=\r\nters as a basis for comparison. And anyway,  \n&gt; NFL still holds, in a broad=\r\ner sense: For example, imagine the \nclass  \n&gt; of problems that /seem/ to ha=\r\nve an underlying geometry feature, \nlets  \n&gt; call it &quot;A.&quot; There could possi=\r\nbly be instances of this class of  \n&gt; problems where naively exploiting A w=\r\nould actually cause the \nlearning  \n&gt; algorithm to perform worse than not e=\r\nxploiting it. And in fact we  \n&gt; could probably construct such a class.\n&gt; \n=\r\n&gt; Again, to sum up, I think the way you are framing the benefits of  \n&gt; Hyp=\r\nerNEAT&#39;s geometry exploiting features rely solely on an  \n&gt; intelligent exp=\r\nerimenter, and in that sense do sort of bypass NFL.  \n&gt; But making such a s=\r\ntatement is meaningless, because it basically  \n&gt; assumes that the experime=\r\nnter always guesses right. And if I were  \n&gt; that experimenter, then I woul=\r\ndn&#39;t be here talking to you, I would  \n&gt; have already solved strong AI :)\n&gt;=\r\n \n&gt; Humbly,\n&gt; \n&gt; -- Joe\n&gt;\n\n\n\n"}}