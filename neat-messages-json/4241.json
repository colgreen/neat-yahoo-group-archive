{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":274910130,"authorName":"David D&#39;Ambrosio","from":"&quot;David D&#39;Ambrosio&quot; &lt;ddambro84@...&gt;","profile":"ddambroeplex","replyTo":"LIST","senderId":"iewi9DBrR6G7Lbfmoszs2zGIVn5MjYbyM68aGObvuu4VWKg8tgUnJCKiJV-Nz1v_J_ACoFF5MIeBG0ye9T-BI3668XFIs_b7vEwqPb3KDw2k","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: Questions regarding HyperSharpNeat and basic NEAT questions","postDate":"1217568453","msgId":4241,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGc2dTZzNStwazk5QGVHcm91cHMuY29tPg==","inReplyToHeader":"PGc2c3EwditiMHRpQGVHcm91cHMuY29tPg=="},"prevInTopic":4240,"nextInTopic":4242,"prevInTime":4240,"nextInTime":4242,"topicId":4238,"numMessagesInTopic":6,"msgSnippet":"Christian, If you would like to use HyperSharpNEAT to just evolve neural networks with different activation functions you should implement everything I said,","rawEmail":"Return-Path: &lt;ddambro84@...&gt;\r\nX-Sender: ddambro84@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 85453 invoked from network); 1 Aug 2008 05:27:37 -0000\r\nX-Received: from unknown (66.218.67.95)\n  by m53.grp.scd.yahoo.com with QMQP; 1 Aug 2008 05:27:37 -0000\r\nX-Received: from unknown (HELO n29a.bullet.scd.yahoo.com) (66.94.237.31)\n  by mta16.grp.scd.yahoo.com with SMTP; 1 Aug 2008 05:27:37 -0000\r\nX-Received: from [209.73.164.86] by n29.bullet.scd.yahoo.com with NNFMP; 01 Aug 2008 05:27:34 -0000\r\nX-Received: from [66.218.66.75] by t8.bullet.scd.yahoo.com with NNFMP; 01 Aug 2008 05:27:34 -0000\r\nDate: Fri, 01 Aug 2008 05:27:33 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;g6u6s5+pk99@...&gt;\r\nIn-Reply-To: &lt;g6sq0v+b0ti@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nFrom: &quot;David D&#39;Ambrosio&quot; &lt;ddambro84@...&gt;\r\nSubject: Re: Questions regarding HyperSharpNeat and basic NEAT questions\r\nX-Yahoo-Group-Post: member; u=274910130; y=jsY_6V_RQEWmlLxrbXACqUGUb1Pzwu9iHt-NuxFEDokhy5qD\r\nX-Yahoo-Profile: ddambroeplex\r\n\r\nChristian,\n\nIf you would like to use HyperSharpNEAT to just evolve neural n=\r\networks\nwith different activation functions you should implement everything=\r\n I\nsaid, but do not run your genomes through the substrate class.  In\nfact =\r\nyou should not use the substrate class at all.  So in the network\nevaluator=\r\n where I use the substrate, that&#39;s where you should be\nevaluating the perfo=\r\nrmance of your networks.\n\nGood luck with your work.  Let us know how it tur=\r\nns out,\nDavid\n\n\n--- In neat@yahoogroups.com, &quot;Christian&quot; &lt;Christian.Hofmann=\r\n@...&gt; wrote:\n&gt;\n&gt; Hello David,\n&gt; \n&gt; thank you for your very detailed and hel=\r\npful answers!\n&gt; \n&gt; Using NN I have every time problems like choosing the ri=\r\nght activation\n&gt; function(s) or the correct hidden layer design. That&#39;s som=\r\nething I\n&gt; wanted to avoid using Neat/HyperNeat. It is not possible for me =\r\nto\n&gt; define a substrate for my domain, but I want to profit from the\n&gt; mult=\r\ni-threading approach and the CPPN feature. \n&gt; \n&gt; So using HyperSharpNeat wi=\r\nth the standard substrate method, it would\n&gt; be possible to get networks wi=\r\nth different activation functions in one\n&gt; NN and with the ability of using=\r\n multiple CPUs?\n&gt; \n&gt; My thoughts are that if I am choosing the wrong activa=\r\ntion functions\n&gt; or wrong input/output values, Neat will compensate this by=\r\n using\n&gt; different activation functions or an additional hidden layer to\n&gt; =\r\ntransform the input/output value to the correct / better values. \n&gt; \n&gt; Even=\r\n every geometric layout should get be designed by a normal CPPN\n&gt; network. =\r\nIt will use be more neurons, but at the end of the day every\n&gt; substrate / =\r\ngeometric layout should be expressed by additional neurons.\n&gt; \n&gt; Kind regar=\r\nds,\n&gt; \n&gt; Christian\n&gt; \n&gt; \n&gt; --- In neat@yahoogroups.com, &quot;David D&#39;Ambrosio&quot; =\r\n&lt;ddambro84@&gt; wrote:\n&gt; &gt;\n&gt; &gt; Hi Christian,\n&gt; &gt; \n&gt; &gt; I&#39;m the one who converte=\r\nd SharpNEAT to HyperSharpNEAT so hopefully\n&gt; &gt; I&#39;ll be able to shed some li=\r\nght on the issues you&#39;re facing.  I would\n&gt; &gt; like to point out that your f=\r\nirst question makes it seem like you may\n&gt; &gt; not be interested in HyperNEAT=\r\n, but instead CPPNs.  CPPNs are\n&gt; &gt; essentially neural networks with differ=\r\nent activation functions in the\n&gt; &gt; nodes.  HyperNEAT uses CPPNs to create =\r\npatterns of connections in\n&gt; &gt; traditional neural networks (i.e. only one a=\r\nctivation function) that\n&gt; &gt; are laid out on a substrate to create geometri=\r\nc relationships between\n&gt; &gt; the nodes.  The answers I give are still releva=\r\nnt either way, but\n&gt; &gt; that&#39;s something to think about.\n&gt; &gt; \n&gt; &gt; 1.  The ex=\r\nperiment stuff is mostly derived from Colin&#39;s SharpNEAT\n&gt; &gt; (http://sharpne=\r\nat.sourceforge.net/), and if you&#39;d like information\n&gt; &gt; about that, it migh=\r\nt be better to look at his documentation as it is\n&gt; &gt; MUCH better than mine=\r\n, of course I&#39;ll try and answer any questions you\n&gt; &gt; have.  With that said=\r\n, you do have the basic idea: each experiment\n&gt; &gt; needs an experiment, a ne=\r\ntwork evaluator and a population evaluator. \n&gt; &gt; The basic substrate should=\r\n work, but only for the simplest of\n&gt; &gt; situations, you should inherit it a=\r\nnd override the generateGenome\n&gt; &gt; function to represent your substrate.  O=\r\nf course, if you are only\n&gt; &gt; interested in evolving CPPNs, you can complet=\r\nely ignore the substrate\n&gt; &gt; class as well as the the related parameters in=\r\n the params.txt file,\n&gt; &gt; and the algorithm will produce just CPPNs.\n&gt; &gt; \n&gt;=\r\n &gt; 2.  The reason that MultipleSteps takes a variable number is because\n&gt; &gt;=\r\n NEAT evolves networks of varying topologies which can include\n&gt; &gt; recurren=\r\nt connections, making it impossible to determine the &quot;correct&quot;\n&gt; &gt; number o=\r\nf times to activate the network.  Colin did include\n&gt; &gt; RelaxNetwork functi=\r\non which will run a network for some number of\n&gt; &gt; steps or until the outpu=\r\nts stop changing.\n&gt; &gt; \n&gt; &gt; 3.  I don&#39;t use Hyperbolic Tanget for the simple=\r\n reason that there\n&gt; &gt; were several sigmoids included with SharpNEAT and th=\r\ney have about the\n&gt; &gt; same functionality for what I use them for.  Colin di=\r\nd some cool stuff\n&gt; &gt; to make it really easy to add your own activation fun=\r\nctions though. \n&gt; &gt; Simply make a class that implements IActivationFuction =\r\nand put it in\n&gt; &gt; the ActivationFunctions folder and in the NeuralNetwork n=\r\namespace.  It\n&gt; &gt; can then be accessed through the factory.  To use it with=\r\n my code,\n&gt; &gt; just add the name and probability of that function occurring =\r\nto the\n&gt; &gt; params.txt file.\n&gt; &gt; \n&gt; &gt; 4. I don&#39;t have much experience with t=\r\nhis issue, but my advice is that\n&gt; &gt; it depends on which functions you have=\r\n in your network.  If the\n&gt; &gt; functions in the network are bipolar (-1,1) t=\r\nhen your binary\n&gt; &gt; representation should be as well.  Ken&#39;s original XOR e=\r\nxperiment used\n&gt; &gt; &gt;.5 as 1 and &lt;.5 as 0, but that&#39;s because he used sigmoi=\r\nds that ranged\n&gt; &gt; from 0 to 1.\n&gt; &gt; \n&gt; &gt; Hopefully that&#39;s answered everythi=\r\nng, if you have any further\n&gt; &gt; questions you can email me at ddambro@ and =\r\nI&#39;m sure the\n&gt; &gt; group would be happy to address any general NEAT and Hyper=\r\nNEAT ideas\n&gt; &gt; or questions you might have.\n&gt; &gt; \n&gt; &gt; David D&#39;Ambrosio\n&gt;\n\n\n\n"}}