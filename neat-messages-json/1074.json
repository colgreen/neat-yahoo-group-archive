{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":127853030,"authorName":"Colin Green","from":"Colin Green &lt;cgreen@...&gt;","profile":"alienseedpod","replyTo":"LIST","senderId":"_IvsZb66CU-dp888JMzVB7CL69dX8A-aXI28DQxSFcTF4kLjz160jllyw-jt7hz-Wr6Gmpu1x3Gh_sznfxOfwJlWTPvi4htN6Q","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] Re: Computation Time","postDate":"1087339926","msgId":1074,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQwQ0Y3RDk2LjYwNDA4MDFAZHNsLnBpcGV4LmNvbT4=","inReplyToHeader":"PDYuMS4wLjYuMC4yMDA0MDYxNTEwMTA1MS4wMjUwMDlkMEBwb3AubWFpbC55YWhvby5jby51az4=","referencesHeader":"PGNhMzl2ays0djFhQGVHcm91cHMuY29tPiA8NDBDNjM4RDAuMTA2MDQwMkBkc2wucGlwZXguY29tPiA8Ni4xLjAuNi4wLjIwMDQwNjE1MTAxMDUxLjAyNTAwOWQwQHBvcC5tYWlsLnlhaG9vLmNvLnVrPg=="},"prevInTopic":1067,"nextInTopic":1076,"prevInTime":1073,"nextInTime":1075,"topicId":845,"numMessagesInTopic":99,"msgSnippet":"... Yep, via the various  instruction pipelines. Assuming the a 2.17Ghz clock I translate the above figures as follows: sigmoid: 78.66 cycles evsail:     51.00","rawEmail":"Return-Path: &lt;cgreen@...&gt;\r\nX-Sender: cgreen@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 38880 invoked from network); 15 Jun 2004 22:52:09 -0000\r\nReceived: from unknown (66.218.66.172)\n  by m23.grp.scd.yahoo.com with QMQP; 15 Jun 2004 22:52:09 -0000\r\nReceived: from unknown (HELO colossus.systems.pipex.net) (62.241.160.73)\n  by mta4.grp.scd.yahoo.com with SMTP; 15 Jun 2004 22:52:08 -0000\r\nReceived: from dsl.pipex.com (81-86-175-101.dsl.pipex.com [81.86.175.101])\n\tby colossus.systems.pipex.net (Postfix) with ESMTP id 124131C000DD\n\tfor &lt;neat@yahoogroups.com&gt;; Tue, 15 Jun 2004 23:52:06 +0100 (BST)\r\nMessage-ID: &lt;40CF7D96.6040801@...&gt;\r\nDate: Tue, 15 Jun 2004 23:52:06 +0100\r\nUser-Agent: Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.5) Gecko/20031007\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: neat@yahoogroups.com\r\nReferences: &lt;ca39vk+4v1a@...&gt; &lt;40C638D0.1060402@...&gt; &lt;6.1.0.6.0.20040615101051.025009d0@...&gt;\r\nIn-Reply-To: &lt;6.1.0.6.0.20040615101051.025009d0@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Remote-IP: 62.241.160.73\r\nFrom: Colin Green &lt;cgreen@...&gt;\r\nSubject: Re: [neat] Re: Computation Time\r\nX-Yahoo-Group-Post: member; u=127853030\r\nX-Yahoo-Profile: alienseedpod\r\n\r\nIan Badcoe wrote:\n\n&gt;&gt;Hi Philip,\n&gt;&gt;\n&gt;&gt;My curiosity got the better of me :) I tried the above functions using\n&gt;&gt;optimized C# on an AMD Athlon 2400+ (actually 2.17Ghz). The results are\n&gt;&gt;slightly bizarre,\n&gt;&gt; oh BTW I think you quoted the tanh function wrong, so I used y =\n&gt;&gt;tanh(0.9*x) which gives a nice sigmoid. Firstly I had to use 100 million\n&gt;&gt;(10^8) loops to get readable results, the approx. 50x difference is\n&gt;&gt;partly due to the CPU (obviously!) but maybe the rest is due to my\n&gt;&gt;oversimplistic implementation whereby I used the same value for x every\n&gt;&gt;time - did you generate random numbers perhaps? Also I know that Java\n&gt;&gt;has JIT compilers but sometime only optimize in code hot-spots during\n&gt;&gt;code execution, they can also run in interpreter mode - my run was with\n&gt;&gt;JITed code.\n&gt;&gt;\n&gt;&gt;Here are the figures:\n&gt;&gt;\n&gt;&gt;sigmoid:  3625ms\n&gt;&gt;evsail:    2359ms\n&gt;&gt;inv-abs:  188ms\n&gt;&gt;    \n&gt;&gt;\n&gt;\n&gt;By my calculations, this makes just over 3 cycles per complete \n&gt;calculation.  That&#39;s not impossible.  e.g. ISRT on the K7 (Athlon \n&gt;predecessor) a floating-divide took 3 cycles but that the chip was able to \n&gt;have 2 fdivs and 2fadds and some integer instructions running simulatneously.\n&gt;  \n&gt;\nYep, via the various  instruction pipelines. Assuming the a 2.17Ghz \nclock I translate the above figures as follows:\n\nsigmoid: 78.66 cycles\nevsail:     51.00\ninv-abs:    4.08\ntanh:      269.00\n\ncertainly interesting.\n\n&gt;It does sound suspiciously good, however.  I don&#39;t know much about C# but \n&gt;presumably it&#39;s inlining the function, and maybe unrolling the loop a \n&gt;little.  OTOH, if it did all that, then it should be able to see that you \n&gt;are making the same call every time and that the function has no side \n&gt;effects, so did it need to run the function at all?\n&gt;  \n&gt;\nThat particular test was a loop, no methods calls involved. But yes the \n.Net compiler does do inlining, although there is no inline hint keyword \nas in some C++ compilers - as I understood it the keyword was largely \nignored in later compilers anyway - based on the idea that the compiler \nknows best.\n\n&gt;Compilers can be blind to that sort of thing, however, like I mentioned before.\n&gt;\n&gt;  \n&gt;\n&gt;&gt;tanh:     12,400ms\n&gt;&gt;\n&gt;&gt;weird huh.  The tanh loop took 66x longer then the ins-abs one!\n&gt;&gt;    \n&gt;&gt;\n&gt;\n&gt;All trig, exp and log are very expensive.\n&gt;Sqrt is expensive but maybe not so bad.\n&gt;Divide is releatively cheap nowadays.\n&gt;\n&gt;The thing about the more exotic instructions, like tan, is that not only do \n&gt;they take a lot of cycles, but the chip only has one processor for \n&gt;them.  Also slow instructions have a disproportionate effect on throughput \n&gt;because all the shorter instructions, which could run in parallel, can only \n&gt;go so far before they hit a dependency on the result of the long \n&gt;instruction and have to stop.  Thus effectively the whole chip hangs on the \n&gt;result of the tan.\n&gt;  \n&gt;\nI think modern cpu&#39;s have more than one fpu pipeline - but yes, the \nprinciple still holds.\n\n&gt;&gt;  I wonder though if the technqiue of trying to do\n&gt;&gt;many sequentail ops in order wll only become beneficial when the\n&gt;&gt;networks get *really* big, simply because the memory caches in modern\n&gt;&gt;CPU&#39;s are so large. So there may be some network size at which we would\n&gt;&gt;see a dramatic slow down of our code if it&#39;s not optimized in such a way.\n&gt;&gt;    \n&gt;&gt;\n&gt;\n&gt;That&#39;s what I would expect, not just the network size, however, also the \n&gt;total size of the data you want to pass through.  If you run many copies of \n&gt;the same small network on different data then memory-access may be your \n&gt;bottleneck.\n&gt;  \n&gt;\n\nOk but the input data is copied into the input nodes and then the bulk \nof the network CPU time is in activating the whole network several times \nover. So this really depends on how many network epochs you run on each \nset of input data.\n\n&gt;  \n&gt;\n&gt;&gt;Another way of estimating how efficient my code is is to caclulate the\n&gt;&gt;average number of clock cycles that it requires per neuron and\n&gt;&gt;connection. So e.g. My 53 neuron / 413 connection network performs 413\n&gt;&gt;additions and 53 activations per epoch. So that&#39;s 466 necessary\n&gt;&gt;operations in all, this is an absolute minimum. ok, plus a couple\n&gt;&gt;because the activation fn is several operations (but this is just a\n&gt;&gt;rough bit of maths). Using a simple bit of maths I can then determine:\n&gt;&gt;\n&gt;&gt;ops per epoch = 466\n&gt;&gt;ops per test run = 466 * 100,000 (loops) = 46,600,000\n&gt;&gt;ops per second = 46,600,000 / 5000ms(approx) = 9,320,000\n&gt;&gt;CPU clock cycles per op = 2.17Ghz / 9,320,000 = 232.\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;Now 232 isn&#39;t all that bad when you consider this doesn&#39;t take into\n&gt;&gt;account the extra code that is required to do the looping/indexing\n&gt;&gt;through all of the neurons and connections. So perhaps hand optimized\n&gt;&gt;assembler could get this down to 100 cylcles or maybe 50, but this is in\n&gt;&gt;the same ball park as optimum - and therefore I wouldn&#39;t expect any\n&gt;&gt;massive improvements. Well, not unless you start using SIMD\n&gt;&gt;instructions, which I&#39;m definitely NOT! :)\n&gt;&gt;    \n&gt;&gt;\n&gt;\n&gt;You easily can do a better analysis than that.  Run the timing a few times \n&gt;with different sizes of network (number of Ops) then plot the line of \n&gt;number of ops (x) vs time (y).  You should get an +ve intercept on the \n&gt;y-axis which is the constant cost of your program and a +ve sloping line, \n&gt;which is the cost per op...\n&gt;  \n&gt;\nOK I&#39;ve made 3 measurements, the same network as before but with 104, \n207, and 413 connections. The times are:\n\n104: 2481ms\n207: 3343ms\n413:  4678ms\n\nIf you plot these on a graph it is slightly non-linear, the line is \ncurving upwards - which is what you might expect if, say, the cache is \nbecoming less efficient with the accessing of more data. Assuming a \nstraight line between the first and last reading, this then gives:\n\nsecs per connection: 7.11 * 10^-8\nconnections/sec : 14,064,633\nclock cycles/connection: 154\n\n\nThese times don&#39;t include calculating the activation fn, this is all \ntime spent executing loops to fetch a neuron output value, multiply the \nvalue by a weight and then add that to a total  ready to be put through \nthe activation fn. So perhaps there is room for improvment there, a \nmultiply an add and a couple of memory accesses taking 154 cycles is a \nlittle bit sloppy, but then this is .NET remember - and as such there is \nalso a single type cast in there because .NET does not yet support \ntemplates (to be called Generics I believe), this could well be the bulk \nof the 154 cycles!\n\nColin\n\n\n\n"}}