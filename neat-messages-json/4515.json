{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"JiawBa9d4tbVgwEg9k31X3KRqnD3lW_WsCewuOnVCDvOIvGyS5CoFJ3GZAPZu_WEnY0Ie7JmnovwyFIYqNgIbCt20KcxdqetHT8_FZOjs5mn","spamInfo":{"isSpam":false,"reason":"2"},"subject":"Re: Parameter settings for comparing HyperNEAT to P-NEAT","postDate":"1229471572","msgId":4515,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGdpOWYwaytnMDA0QGVHcm91cHMuY29tPg==","inReplyToHeader":"PEM1NkQ2N0VGLjI3MEFFJWpjbHVuZUBtc3UuZWR1Pg=="},"prevInTopic":4514,"nextInTopic":4516,"prevInTime":4514,"nextInTime":4516,"topicId":4496,"numMessagesInTopic":6,"msgSnippet":"Jeff, you make a legitimate critique: The study could be improved with an extensive parameter sweep for P-NEAT.  As a routine matter, we do generally try some","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 79794 invoked from network); 16 Dec 2008 23:52:55 -0000\r\nX-Received: from unknown (66.218.67.97)\n  by m55.grp.scd.yahoo.com with QMQP; 16 Dec 2008 23:52:55 -0000\r\nX-Received: from unknown (HELO n18c.bullet.sp1.yahoo.com) (69.147.64.129)\n  by mta18.grp.scd.yahoo.com with SMTP; 16 Dec 2008 23:52:55 -0000\r\nX-Received: from [69.147.65.171] by n18.bullet.sp1.yahoo.com with NNFMP; 16 Dec 2008 23:52:55 -0000\r\nX-Received: from [66.218.66.88] by t13.bullet.mail.sp1.yahoo.com with NNFMP; 16 Dec 2008 23:52:55 -0000\r\nDate: Tue, 16 Dec 2008 23:52:52 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;gi9f0k+g004@...&gt;\r\nIn-Reply-To: &lt;C56D67EF.270AE%jclune@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 2:2:2:0:0\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: Re: Parameter settings for comparing HyperNEAT to P-NEAT\r\nX-Yahoo-Group-Post: member; u=54567749; y=BLe8iyZAsZxZnNax_gUmHeksaf8yDHWBAn4rhu_xesv4Cw6VI7ih\r\nX-Yahoo-Profile: kenstanley01\r\n\r\nJeff, you make a legitimate critique: The study could be improved with\nan e=\r\nxtensive parameter sweep for P-NEAT.  As a routine matter, we do\ngenerally =\r\ntry some variation in parameters, but we did not\nsystematically sweep param=\r\neters as you did.  Of course, parameter\ntuning is always a thorny issue, an=\r\nd a common complaint for any method\nthat performs worse in a comparison is =\r\nthat it was simply run with the\nwrong parameters, which is very difficult t=\r\no refute.  Still, as a\nmatter of rigor, it would be most convincing to try =\r\nto do something\nsystematic, as it looks like you&#39;ve tried.\n\nHowever, someth=\r\ning else should be pointed out about the 2007 results.\n I think the main re=\r\nsult is *not* how fast one method or another finds\nthe solution to the boxe=\r\ns problem.   It&#39;s nice that HyperNEAT does it\nfast, but the real result is =\r\nthat it *generalizes* almost perfectly. \nThat result is much less likely to=\r\n be related to parameters. \nGeneralization is the gap between performance o=\r\nn the training set and\ntest set for the *same* method.  Then we can compare=\r\n that gap from one\nmethod to the next.  Even if P-NEAT found a solution qui=\r\nckly, the gap\nin generalization would still be big.  The other important re=\r\nsult from\nthat paper is scaling, for which P-NEAT has no mechanism whatsoev=\r\ner. \nSo I think while the criticism is legitimate, it is not really related=\r\n\nto the most conceptually important results of the paper.\n\nken\n\n--- In neat=\r\n@yahoogroups.com, Jeff Clune &lt;jclune@...&gt; wrote:\n&gt;\n&gt; Jason and Ken-\n&gt; \n&gt; Th=\r\nanks for your responses.\n&gt; \n&gt; I think comparing HyperNEAT and P-NEAT is ver=\r\ny instructive, because it\n&gt; allows us to test the benefits of generative vs=\r\n. direct encodings. I\nforgot\n&gt; that it was not used in David=EF=BF=BDs expe=\r\nriments. However, I did use it in my\n&gt; 2008 PPSN paper (=EF=BF=BDHow a gene=\r\nrative encoding fares as problem-regularity\n&gt; decreases=EF=BF=BD), and I am=\r\n also using it in current research (not yet\npublished).\n&gt; I mention all of =\r\nthis in order to let people know that I am not\ntrying to be\n&gt; pedantic with=\r\n regard to past work, but just trying to ensure that\n&gt; comparisons that we =\r\nare making are fair.\n&gt; \n&gt; Jason, I was just advocating doing a sweep on the=\r\n mutation rate for\nP-NEAT\n&gt; up to N generations (e.g. however many generati=\r\nons you used in your\noriginal\n&gt; paper), and using that same N for HyperNEAT=\r\n. That is not that large\na drain\n&gt; on resources. \n&gt; \n&gt; In fact, I have just=\r\n completed such a sweep and found that, on the\nproblem I\n&gt; am testing on, t=\r\nhe optimal P-NEAT mutation rate was about an order of\n&gt; magnitude lower tha=\r\nn the one I had been using. However, the boost in\n&gt; P-NEAT=EF=BF=BDs perfor=\r\nmance did not bring it anywhere near HyperNEAT. Of\ncourse,\n&gt; the optimal ra=\r\nte will depend on the problem, but my point here is\nthat it\n&gt; will also dep=\r\nend on the encoding. While I have not swept HyperNEAT=EF=BF=BDs\n&gt; mutation =\r\nrate, I=EF=BF=BDll wager that HyperNEAT=EF=BF=BDs optimal rate is very\ndiff=\r\nerent\n&gt; from P-NEAT.  \n&gt; \n&gt; Ken- I agree with your assessment that sometime=\r\ns small changes in\nP-NEAT can\n&gt; lead to large changes in behavior. However,=\r\n as you note in your updated\n&gt; reply, in the boxes domain this is probably =\r\nnot the case.\n&gt; \n&gt; It seems that it is difficult to determine a priori whet=\r\nher P-NEAT (or\n&gt; HyperNEAT, or any encoding) will benefit from a higher, or=\r\n lower,\nmutation\n&gt; rate. But we know that the mutation rate greatly affects=\r\n the\nperformance of\n&gt; an evolutionary algorithm. So, to get at the high-lev=\r\nel issue, it\nstill does\n&gt; strike me as somewhat unfair to compare two diffe=\r\nrent encodings, where\n&gt; mutations have such different types of effects, wit=\r\nh the same\nmutation rate.\n&gt; \n&gt; Our reasoning about the encodings gives us e=\r\nxpectations that\nHyperNEAT will\n&gt; do much better. I would like to state aga=\r\nin that I fully agree with\nthose\n&gt; reasons. However, the point of actually =\r\nrunning the comparison is to see\n&gt; whether the data support the intuitions.=\r\n It is somewhat circular to\nuse our\n&gt; intuitions as a reason for not settin=\r\ng up the proper controls, given\nthat\n&gt; the controls are meant to test the a=\r\nccuracy of our intuitions.\n&gt; \n&gt; That said, one can lose months doing parame=\r\nter sweeps. My argument with\n&gt; regard to mutation rate could also be applie=\r\nd to many other NEAT\nparameters,\n&gt; as well as their interactions. Clearly i=\r\nt would be a waste of energy\nto try\n&gt; to sweep through that multidimensiona=\r\nl space. So, maybe it is better\nto say\n&gt; that since the tests are in accord=\r\nance with what our reasoning\nsuggests, we\n&gt; do not need to spend the time t=\r\no make sure they were exactly the right\n&gt; control.  Or, perhaps a middle gr=\r\nound is appropriate: doing a\ncursory sweep\n&gt; of a few parameters (e.g. muta=\r\ntion rate) that we know to have large\neffects.\n&gt; That is why I was wonderin=\r\ng what you feel are the few key parameters\nthat\n&gt; should be swept. At prese=\r\nnt, my thinking is that it is probably fine\nto just\n&gt; sweep MutateLinkProba=\r\nbility. Does that sound right?\n&gt;  \n&gt; In conclusion, I do not think it is pa=\r\nrticularly fair to use the same\n&gt; settings for both encodings. But I am not=\r\n sure whether it is worth\nthe time\n&gt; to try to make the controls more fair,=\r\n given that our intuitions largely\n&gt; predict the outcome of the test anyway=\r\n, and given that initial\nsweeps (like\n&gt; the one I just did) corroborate the=\r\n predictions of those intuitions.\n&gt; \n&gt; \n&gt; \n&gt; Cheers, \n&gt; Jeff\n&gt; \n&gt; On 12/7/0=\r\n8 3:53 AM, &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt; wrote:\n&gt; \n&gt; &gt;  \n&gt; &gt;  \n&gt; &gt; \n&gt; &gt; =\r\nJeff, a few other thoughts on the issue...\n&gt; &gt; \n&gt; &gt; Note that I believe the=\r\n only time P-NEAT comes up is in the boxes\n&gt; &gt; domain.  None of David&#39;s exp=\r\neriments involve P-NEAT.  I think Jason\n&gt; &gt; did some cursory checking of ot=\r\nher settings for P-NEAT, but nothing\n&gt; &gt; systematic.  Jason can correct me =\r\nif I am wrong.\n&gt; &gt; \n&gt; &gt; I understand that like Jason and myself, you don&#39;t =\r\nthink it would\n&gt; &gt; really make a big difference no matter what we do with P=\r\n-NEAT, but\n&gt; &gt; just for the record, I think the reasoning you give for the\n=\r\n&gt; &gt; differences with P-NEAT is not entirely accurate.  In particular, you\n&gt;=\r\n &gt; suggest that small mutations in HyperNEAT may lead to big changes on\n&gt; &gt;=\r\n the substrate, while the same is not true for P-NEAT.  I disagree with\n&gt; &gt;=\r\n that perspective because ultimately what is important is not the\n&gt; &gt; subst=\r\nrate, but the behavior produced by the substrate.  The substrate\n&gt; &gt; is jus=\r\nt a level of indirection in the mapping between genotype and\n&gt; &gt; behavior. =\r\n In that view, a small mutation in P-NEAT is just as likely\n&gt; &gt; to produce =\r\na large change in *behavior* as it is in HyperNEAT.\n&gt; &gt; Consider that behav=\r\nior is an indirect holistic product of genotype as\n&gt; &gt; well.  That is, a si=\r\nngle gene mutating in P-NEAT can change how an\n&gt; &gt; individual behaves in ev=\r\nery possible situation it may encounter.\n&gt; &gt; Hence it is equally holistic a=\r\ns HyperNEAT.\n&gt; &gt; \n&gt; &gt; In fact, one could argue that the situation is actual=\r\nly opposite of\n&gt; &gt; what you say:  Because a single mutation in HyperNEAT pr=\r\noduces a\n&gt; &gt; systematic concerted change in the substrate, it is less likel=\r\ny to\n&gt; &gt; produce a haphazard change in behavior than a single mutation in\n&gt;=\r\n &gt; P-NEAT.  That is a difficult argument to make concrete, but it&#39;s not\n&gt; &gt;=\r\n unreasonable.  Just because a lot of connection weights change does\n&gt; &gt; no=\r\nt mean that the change is &quot;big.&quot;  If they all change in a concerted\n&gt; &gt; man=\r\nner, it can be quite subtle, or no change at all.  And in fact,\n&gt; &gt; concert=\r\ned change is exactly what indirect encoding is about.\n&gt; &gt; \n&gt; &gt; Note that I =\r\nam referring to P-NEAT in general.  In the boxes domain,\n&gt; &gt; it is perhaps =\r\nmore as you say since individual connections in that\n&gt; &gt; domain do indeed h=\r\nave small effects.\n&gt; &gt; \n&gt; &gt; In any case, the larger concern is still valid.=\r\n  There may indeed be\n&gt; &gt; different optimal parameter settings for P-NEAT a=\r\nnd HyperNEAT, and\n&gt; &gt; that is something people can look at.  But if there a=\r\nre, in my view it\n&gt; &gt; is probably for different reasons than the ones you c=\r\nite (as I explain\n&gt; &gt; above).  Still, barring finding the optimal P-NEAT pa=\r\nrameters (which I\n&gt; &gt; don&#39;t know), I think the most fair thing is indeed to=\r\n give them the\n&gt; &gt; same parameters because they are both ultimately variant=\r\ns of NEAT\n&gt; &gt; evolving a solution to the same problem.  Still, I do see tha=\r\nt one\n&gt; &gt; might be interested in optimizing P-NEAT further to see how good =\r\nit\n&gt; &gt; can really be.  However, as you say and Jason supports, it won&#39;t be\n=\r\n&gt; &gt; easy to get P-NEAT to optimize a 14,000-dimensional space, whatever\n&gt; &gt;=\r\n parameters you give it.\n&gt; &gt; \n&gt; &gt; ken\n&gt; &gt; \n&gt; &gt; --- In neat@yahoogroups.com =\r\n&lt;mailto:neat%40yahoogroups.com&gt; ,\n&quot;Jason G&quot;\n&gt; &gt; &lt;jgmath2000@&gt; wrote:\n&gt; &gt;&gt; &gt;=\r\n\n&gt; &gt;&gt; &gt; Hey Jeff,\n&gt; &gt;&gt; &gt; \n&gt; &gt;&gt; &gt; I understand the concern with the mutation=\r\n probability.  I\nbelieve the\n&gt; &gt;&gt; &gt; reason that NEAT cannot solve the boxes=\r\n problem in training is\na credit\n&gt; &gt;&gt; &gt; assignment problem. Because the num=\r\nber of connections is so\nhigh, it is\n&gt; &gt;&gt; &gt; difficult for a direct encoding=\r\n to learn which connections were\n&gt; &gt; responsible\n&gt; &gt;&gt; &gt; for an increase/dec=\r\nrease in fitness.\n&gt; &gt;&gt; &gt; \n&gt; &gt;&gt; &gt; It might be possible to improve the credit=\r\n assignment by\nlowering the\n&gt; &gt;&gt; &gt; mutation rate.  The problem is that if t=\r\nhe mutation rate was\nlowered\n&gt; &gt; to the\n&gt; &gt;&gt; &gt; point where credit assignmen=\r\nt could be manageable by NEAT, I\nbelieve\n&gt; &gt; that\n&gt; &gt;&gt; &gt; this would require=\r\n orders of magnitude more generations to\nconverge.\n&gt; &gt; Even if\n&gt; &gt;&gt; &gt; NEAT =\r\nconverged to a solution, this does not negate the fact that a\n&gt; &gt; lot of\n&gt; =\r\n&gt;&gt; &gt; the connections are never used in training, and these connections\n&gt; &gt; =\r\nwill have\n&gt; &gt;&gt; &gt; effectively random values, resulting in poor validation\npe=\r\nrformance.\n&gt; &gt;&gt; &gt; \n&gt; &gt;&gt; &gt; Consider this: if NEAT was able to learn the corr=\r\nect value for a\n&gt; &gt; single link\n&gt; &gt;&gt; &gt; every generation, and this correct v=\r\nalue was preserved through the\n&gt; &gt; whole run\n&gt; &gt;&gt; &gt; (i.e. it was not accide=\r\nntally mutated), it would still take\nabout 5000\n&gt; &gt;&gt; &gt; generations to compl=\r\netely solve the training phase of the boxes\n&gt; &gt; problem.  At\n&gt; &gt;&gt; &gt; roughly=\r\n an hour per generation, that&#39;s about 7 months per run.\n&gt; &gt;&gt; &gt; \n&gt; &gt;&gt; &gt; Any =\r\nsweep of the mutation rate parameter would require many\n&gt; &gt; generations in\n=\r\n&gt; &gt;&gt; &gt; each run. I believe that, given a very low mutation rate, NEAT\nmight=\r\n\n&gt; &gt; be able\n&gt; &gt;&gt; &gt; to solve the boxes problem; however, it would take tens=\r\n of\nthousands of\n&gt; &gt;&gt; &gt; generations at least, and there&#39;s still the general=\r\nization issue.\n&gt; &gt; Given the\n&gt; &gt;&gt; &gt; amount of resources necessary to find t=\r\nhe magic numbers, I didn&#39;t\n&gt; &gt; see the\n&gt; &gt;&gt; &gt; utility in trying to pursue i=\r\nt.\n&gt; &gt;&gt; &gt; \n&gt; &gt;&gt; &gt; I think what might be more interesting would be to try di=\r\nfferent\n&gt; &gt; parameters\n&gt; &gt;&gt; &gt; for HyperNEAT.  As we fix bugs and develop ma=\r\nturity in the\ncodebase, it\n&gt; &gt;&gt; &gt; would be important to note how the parame=\r\nters should be changed\nfor the\n&gt; &gt;&gt; &gt; older experiments (e.g. the mutation =\r\nrate is lower now because we\n&gt; &gt; fixed a\n&gt; &gt;&gt; &gt; bug in mutation).\n&gt; &gt;&gt; &gt; \n&gt;=\r\n &gt;&gt; &gt; --- In neat@yahoogroups.com &lt;mailto:neat%40yahoogroups.com&gt; ,\nJeff Cl=\r\nune\n&gt; &gt;&gt; &lt;jclune@&gt; wrote:\n&gt; &gt;&gt;&gt; &gt; &gt;\n&gt; &gt;&gt;&gt; &gt; &gt; Hi all-\n&gt; &gt;&gt;&gt; &gt; &gt;\n&gt; &gt;&gt;&gt; &gt; &gt; A=\r\n quick question for Ken, Jason and Dave.\n&gt; &gt;&gt;&gt; &gt; &gt;\n&gt; &gt;&gt;&gt; &gt; &gt; I notice that =\r\nmost of the parameter settings (e.g.\n&gt; &gt;&gt;&gt; &gt; &gt; MutateLinkWeightsProbability=\r\n) were the same when you guys\ncompared\n&gt; &gt;&gt; &gt; HyperNEAT\n&gt; &gt;&gt;&gt; &gt; &gt; to P-NEAT=\r\n.\n&gt; &gt;&gt;&gt; &gt; &gt;\n&gt; &gt;&gt;&gt; &gt; &gt; Someone in my lab raised the issue yesterday that eac=\r\nh\n&gt; &gt; configuration may\n&gt; &gt;&gt;&gt; &gt; &gt; have entirely different optimal settings,=\r\n making a\ncomparison with the\n&gt; &gt;&gt; &gt; same\n&gt; &gt;&gt;&gt; &gt; &gt; settings potentially un=\r\nfair. Out of curiosity, did you do any\n&gt; &gt; parameter\n&gt; &gt;&gt;&gt; &gt; &gt; sweeps to se=\r\ne if P-NEAT&#39;s performance did much better, and\nbetter\n&gt; &gt;&gt; &gt; approached\n&gt; &gt;=\r\n&gt;&gt; &gt; &gt; HyperNEAT&#39;s, with different parameter settings?\n&gt; &gt;&gt;&gt; &gt; &gt;\n&gt; &gt;&gt;&gt; &gt; &gt; =\r\nFor example, since mutations to HyperNEAT have such larger\neffects\n&gt; &gt; on t=\r\nhe\n&gt; &gt;&gt;&gt; &gt; &gt; final substrate, it could be argued that P-NEAT needs a much\nh=\r\nigher\n&gt; &gt;&gt; &gt; mutation\n&gt; &gt;&gt;&gt; &gt; &gt; rate to compete.\n&gt; &gt;&gt;&gt; &gt; &gt;\n&gt; &gt;&gt;&gt; &gt; &gt; Note: =\r\nI don&#39;t think P-NEAT will beat HyperNEAT no matter\nwhat the\n&gt; &gt;&gt; &gt; settings=\r\n,\n&gt; &gt;&gt;&gt; &gt; &gt; but I do think the question is fair and I would not be surprise=\r\nd\n&gt; &gt; if the\n&gt; &gt;&gt;&gt; &gt; &gt; settings that worked great for HyperNEAT are not the=\r\n ones that\n&gt; &gt; work great\n&gt; &gt;&gt;&gt; &gt; &gt; for P-NEAT (and vice versa). In short, =\r\nI am confident that the\n&gt; &gt; qualitative\n&gt; &gt;&gt;&gt; &gt; &gt; results of your paper are=\r\n still all valid (and represent\n&gt; &gt; breakthroughs for\n&gt; &gt;&gt;&gt; &gt; &gt; the field),=\r\n but am interested to know whether the quantitative\n&gt; &gt; difference\n&gt; &gt;&gt;&gt; &gt; =\r\n&gt; between the encodings might be much less with different\nsettings.\n&gt; &gt;&gt;&gt; &gt;=\r\n &gt;\n&gt; &gt;&gt;&gt; &gt; &gt; PS. As a side note, there are a ton of settings and sweeping\nt=\r\nhem\n&gt; &gt; all is\n&gt; &gt;&gt;&gt; &gt; &gt; nearly impossible, especially when considering int=\r\neractions.\nWhat does\n&gt; &gt;&gt; &gt; &g\n&gt; \n&gt; \n&gt; \n&gt; \n&gt; \n&gt; \n&gt; \n&gt; \n&gt; \n&gt; Cheers,\n&gt; Jeff =\r\nClune\n&gt; \n&gt; Digital Evolution Lab, Michigan State University\n&gt; \n&gt; jclune@...=\r\n\n&gt; \n&gt; \n&gt; \n&gt; \n&gt; &gt; From: Kenneth Stanley &lt;kstanley@...&gt;\n&gt; &gt; Reply-To: &quot;neat@y=\r\nahoogroups.com&quot; &lt;neat@yahoogroups.com&gt;\n&gt; &gt; Date: Sun, 07 Dec 2008 08:53:00 =\r\n-0000\n&gt; &gt; To: &quot;neat@yahoogroups.com&quot; &lt;neat@yahoogroups.com&gt;\n&gt; &gt; Subject: [n=\r\neat] Re: Parameter settings for comparing HyperNEAT to\nP-NEAT\n&gt; &gt; \n&gt; &gt; Jeff=\r\n, a few other thoughts on the issue...\n&gt; &gt; \n&gt; &gt; Note that I believe the onl=\r\ny time P-NEAT comes up is in the boxes\n&gt; &gt; domain.  None of David&#39;s experim=\r\nents involve P-NEAT.  I think Jason\n&gt; &gt; did some cursory checking of other =\r\nsettings for P-NEAT, but nothing\n&gt; &gt; systematic.  Jason can correct me if I=\r\n am wrong.\n&gt; &gt; \n&gt; &gt; I understand that like Jason and myself, you don&#39;t thin=\r\nk it would\n&gt; &gt; really make a big difference no matter what we do with P-NEA=\r\nT, but\n&gt; &gt; just for the record, I think the reasoning you give for the\n&gt; &gt; =\r\ndifferences with P-NEAT is not entirely accurate.  In particular, you\n&gt; &gt; s=\r\nuggest that small mutations in HyperNEAT may lead to big changes on\n&gt; &gt; the=\r\n substrate, while the same is not true for P-NEAT.  I disagree with\n&gt; &gt; tha=\r\nt perspective because ultimately what is important is not the\n&gt; &gt; substrate=\r\n, but the behavior produced by the substrate.  The substrate\n&gt; &gt; is just a =\r\nlevel of indirection in the mapping between genotype and\n&gt; &gt; behavior.  In =\r\nthat view, a small mutation in P-NEAT is just as likely\n&gt; &gt; to produce a la=\r\nrge change in *behavior* as it is in HyperNEAT.\n&gt; &gt; Consider that behavior =\r\nis an indirect holistic product of genotype as\n&gt; &gt; well.  That is, a single=\r\n gene mutating in P-NEAT can change how an\n&gt; &gt; individual behaves in every =\r\npossible situation it may encounter.\n&gt; &gt; Hence it is equally holistic as Hy=\r\nperNEAT.\n&gt; &gt; \n&gt; &gt; In fact, one could argue that the situation is actually o=\r\npposite of\n&gt; &gt; what you say:  Because a single mutation in HyperNEAT produc=\r\nes a\n&gt; &gt; systematic concerted change in the substrate, it is less likely to=\r\n\n&gt; &gt; produce a haphazard change in behavior than a single mutation in\n&gt; &gt; P=\r\n-NEAT.  That is a difficult argument to make concrete, but it&#39;s not\n&gt; &gt; unr=\r\neasonable.  Just because a lot of connection weights change does\n&gt; &gt; not me=\r\nan that the change is &quot;big.&quot;  If they all change in a concerted\n&gt; &gt; manner,=\r\n it can be quite subtle, or no change at all.  And in fact,\n&gt; &gt; concerted c=\r\nhange is exactly what indirect encoding is about.\n&gt; &gt; \n&gt; &gt; Note that I am r=\r\neferring to P-NEAT in general.  In the boxes domain,\n&gt; &gt; it is perhaps more=\r\n as you say since individual connections in that\n&gt; &gt; domain do indeed have =\r\nsmall effects.\n&gt; &gt; \n&gt; &gt; In any case, the larger concern is still valid.  Th=\r\nere may indeed be\n&gt; &gt; different optimal parameter settings for P-NEAT and H=\r\nyperNEAT, and\n&gt; &gt; that is something people can look at.  But if there are, =\r\nin my view it\n&gt; &gt; is probably for different reasons than the ones you cite =\r\n(as I explain\n&gt; &gt; above).  Still, barring finding the optimal P-NEAT parame=\r\nters (which I\n&gt; &gt; don&#39;t know), I think the most fair thing is indeed to giv=\r\ne them the\n&gt; &gt; same parameters because they are both ultimately variants of=\r\n NEAT\n&gt; &gt; evolving a solution to the same problem.  Still, I do see that on=\r\ne\n&gt; &gt; might be interested in optimizing P-NEAT further to see how good it\n&gt;=\r\n &gt; can really be.  However, as you say and Jason supports, it won&#39;t be\n&gt; &gt; =\r\neasy to get P-NEAT to optimize a 14,000-dimensional space, whatever\n&gt; &gt; par=\r\nameters you give it.\n&gt; &gt; \n&gt; &gt; ken\n&gt; &gt; \n&gt; &gt; --- In neat@yahoogroups.com, &quot;Ja=\r\nson G&quot; &lt;jgmath2000@&gt; wrote:\n&gt; &gt;&gt; \n&gt; &gt;&gt; Hey Jeff,\n&gt; &gt;&gt; \n&gt; &gt;&gt; I understand th=\r\ne concern with the mutation probability.  I\nbelieve the\n&gt; &gt;&gt; reason that NE=\r\nAT cannot solve the boxes problem in training is a\ncredit\n&gt; &gt;&gt; assignment p=\r\nroblem. Because the number of connections is so high,\nit is\n&gt; &gt;&gt; difficult =\r\nfor a direct encoding to learn which connections were\n&gt; &gt; responsible\n&gt; &gt;&gt; =\r\nfor an increase/decrease in fitness.\n&gt; &gt;&gt; \n&gt; &gt;&gt; It might be possible to imp=\r\nrove the credit assignment by lowering the\n&gt; &gt;&gt; mutation rate.  The problem=\r\n is that if the mutation rate was lowered\n&gt; &gt; to the\n&gt; &gt;&gt; point where credi=\r\nt assignment could be manageable by NEAT, I believe\n&gt; &gt; that\n&gt; &gt;&gt; this woul=\r\nd require orders of magnitude more generations to converge.\n&gt; &gt; Even if\n&gt; &gt;=\r\n&gt; NEAT converged to a solution, this does not negate the fact that a\n&gt; &gt; lo=\r\nt of\n&gt; &gt;&gt; the connections are never used in training, and these connections=\r\n\n&gt; &gt; will have\n&gt; &gt;&gt; effectively random values, resulting in poor validation=\r\n performance.\n&gt; &gt;&gt; \n&gt; &gt;&gt; Consider this: if NEAT was able to learn the corre=\r\nct value for a\n&gt; &gt; single link\n&gt; &gt;&gt; every generation, and this correct valu=\r\ne was preserved through the\n&gt; &gt; whole run\n&gt; &gt;&gt; (i.e. it was not accidentall=\r\ny mutated), it would still take about\n5000\n&gt; &gt;&gt; generations to completely s=\r\nolve the training phase of the boxes\n&gt; &gt; problem.  At\n&gt; &gt;&gt; roughly an hour =\r\nper generation, that&#39;s about 7 months per run.\n&gt; &gt;&gt; \n&gt; &gt;&gt; Any sweep of the =\r\nmutation rate parameter would require many\n&gt; &gt; generations in\n&gt; &gt;&gt; each run=\r\n. I believe that, given a very low mutation rate, NEAT might\n&gt; &gt; be able\n&gt; =\r\n&gt;&gt; to solve the boxes problem; however, it would take tens of\nthousands of\n=\r\n&gt; &gt;&gt; generations at least, and there&#39;s still the generalization issue.\n&gt; &gt; =\r\nGiven the\n&gt; &gt;&gt; amount of resources necessary to find the magic numbers, I d=\r\nidn&#39;t\n&gt; &gt; see the\n&gt; &gt;&gt; utility in trying to pursue it.\n&gt; &gt;&gt; \n&gt; &gt;&gt; I think w=\r\nhat might be more interesting would be to try different\n&gt; &gt; parameters\n&gt; &gt;&gt;=\r\n for HyperNEAT.  As we fix bugs and develop maturity in the\ncodebase, it\n&gt; =\r\n&gt;&gt; would be important to note how the parameters should be changed\nfor the\n=\r\n&gt; &gt;&gt; older experiments (e.g. the mutation rate is lower now because we\n&gt; &gt; =\r\nfixed a\n&gt; &gt;&gt; bug in mutation).\n&gt; &gt;&gt; \n&gt; &gt;&gt; --- In neat@yahoogroups.com, Jeff=\r\n Clune &lt;jclune@&gt; wrote:\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt; Hi all-\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt; A quick question =\r\nfor Ken, Jason and Dave.\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt; I notice that most of the parameter s=\r\nettings (e.g.\n&gt; &gt;&gt;&gt; MutateLinkWeightsProbability) were the same when you gu=\r\nys compared\n&gt; &gt;&gt; HyperNEAT\n&gt; &gt;&gt;&gt; to P-NEAT.\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt; Someone in my lab =\r\nraised the issue yesterday that each\n&gt; &gt; configuration may\n&gt; &gt;&gt;&gt; have entir=\r\nely different optimal settings, making a comparison\nwith the\n&gt; &gt;&gt; same\n&gt; &gt;&gt;=\r\n&gt; settings potentially unfair. Out of curiosity, did you do any\n&gt; &gt; paramet=\r\ner\n&gt; &gt;&gt;&gt; sweeps to see if P-NEAT&#39;s performance did much better, and better\n=\r\n&gt; &gt;&gt; approached\n&gt; &gt;&gt;&gt; HyperNEAT&#39;s, with different parameter settings?\n&gt; &gt;&gt;&gt;=\r\n \n&gt; &gt;&gt;&gt; For example, since mutations to HyperNEAT have such larger effects\n=\r\n&gt; &gt; on the\n&gt; &gt;&gt;&gt; final substrate, it could be argued that P-NEAT needs a mu=\r\nch higher\n&gt; &gt;&gt; mutation\n&gt; &gt;&gt;&gt; rate to compete.\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt; Note: I don&#39;t t=\r\nhink P-NEAT will beat HyperNEAT no matter what the\n&gt; &gt;&gt; settings,\n&gt; &gt;&gt;&gt; but=\r\n I do think the question is fair and I would not be surprised\n&gt; &gt; if the\n&gt; =\r\n&gt;&gt;&gt; settings that worked great for HyperNEAT are not the ones that\n&gt; &gt; work=\r\n great\n&gt; &gt;&gt;&gt; for P-NEAT (and vice versa). In short, I am confident that the=\r\n\n&gt; &gt; qualitative\n&gt; &gt;&gt;&gt; results of your paper are still all valid (and repre=\r\nsent\n&gt; &gt; breakthroughs for\n&gt; &gt;&gt;&gt; the field), but am interested to know whet=\r\nher the quantitative\n&gt; &gt; difference\n&gt; &gt;&gt;&gt; between the encodings might be mu=\r\nch less with different settings.\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt; PS. As a side note, there are=\r\n a ton of settings and sweeping them\n&gt; &gt; all is\n&gt; &gt;&gt;&gt; nearly impossible, es=\r\npecially when considering interactions.\nWhat does\n&gt; &gt;&gt;&gt; everyone in the gro=\r\nup consider to be the important ones to sweep?\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt; \n=\r\n&gt; &gt;&gt;&gt; Cheers,\n&gt; &gt;&gt;&gt; Jeff Clune\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt; Digital Evolution Lab, Michigan=\r\n State University\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt; jclune@\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt; \n&gt; &gt;&gt; \n&gt; &gt;&gt; On Thu, Dec =\r\n4, 2008 at 11:49 AM, Jeff Clune &lt;jclune@&gt; wrote:\n&gt; &gt;&gt; \n&gt; &gt;&gt;&gt;   Hi all-\n&gt; &gt;&gt;=\r\n&gt; \n&gt; &gt;&gt;&gt; A quick question for Ken, Jason and Dave.\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt; I notice th=\r\nat most of the parameter settings (e.g.\n&gt; &gt;&gt;&gt; MutateLinkWeightsProbability)=\r\n were the same when you guys compared\n&gt; &gt;&gt;&gt; HyperNEAT\n&gt; &gt;&gt;&gt; to P-NEAT.\n&gt; &gt;&gt;=\r\n&gt; \n&gt; &gt;&gt;&gt; Someone in my lab raised the issue yesterday that each\n&gt; &gt; configu=\r\nration may\n&gt; &gt;&gt;&gt; have entirely different optimal settings, making a compari=\r\nson with\n&gt; &gt; the same\n&gt; &gt;&gt;&gt; settings potentially unfair. Out of curiosity, =\r\ndid you do any\n&gt; &gt; parameter\n&gt; &gt;&gt;&gt; sweeps to see if P-NEAT&#39;s performance di=\r\nd much better, and better\n&gt; &gt;&gt;&gt; approached\n&gt; &gt;&gt;&gt; HyperNEAT&#39;s, with differen=\r\nt parameter settings?\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt; For example, since mutations to HyperNEA=\r\nT have such larger effects\n&gt; &gt; on the\n&gt; &gt;&gt;&gt; final substrate, it could be ar=\r\ngued that P-NEAT needs a much higher\n&gt; &gt;&gt;&gt; mutation\n&gt; &gt;&gt;&gt; rate to compete.\n=\r\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt; Note: I don&#39;t think P-NEAT will beat HyperNEAT no matter what =\r\nthe\n&gt; &gt; settings,\n&gt; &gt;&gt;&gt; but I do think the question is fair and I would not=\r\n be surprised\n&gt; &gt; if the\n&gt; &gt;&gt;&gt; settings that worked great for HyperNEAT are=\r\n not the ones that\n&gt; &gt; work great\n&gt; &gt;&gt;&gt; for P-NEAT (and vice versa). In sho=\r\nrt, I am confident that the\n&gt; &gt; qualitative\n&gt; &gt;&gt;&gt; results of your paper are=\r\n still all valid (and represent\n&gt; &gt; breakthroughs for\n&gt; &gt;&gt;&gt; the field), but=\r\n am interested to know whether the quantitative\n&gt; &gt; difference\n&gt; &gt;&gt;&gt; betwee=\r\nn the encodings might be much less with different settings.\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt; PS=\r\n. As a side note, there are a ton of settings and sweeping them\n&gt; &gt; all is\n=\r\n&gt; &gt;&gt;&gt; nearly impossible, especially when considering interactions.\nWhat doe=\r\ns\n&gt; &gt;&gt;&gt; everyone in the group consider to be the important ones to sweep?\n&gt;=\r\n &gt;&gt;&gt; \n&gt; &gt;&gt;&gt; Cheers,\n&gt; &gt;&gt;&gt; Jeff Clune\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt; Digital Evolution Lab, Mi=\r\nchigan State University\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt; jclune@ &lt;jclune%40msu.edu&gt;\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;=\r\n&gt;  \n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt; \n&gt; &gt; \n&gt; &gt;\n&gt;\n\n\n\n"}}