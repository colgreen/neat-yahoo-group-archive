{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"QQUdZA1neg6TXk1ortCxBX09YTDJlnhcQQkguasI_hcU4zS5PMLxtE2lVApJ-CPcQz0qEOo0_ttwzBt6rWHR2mr79zCl1-D2nRQpJe6jv38q","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: How to choose parameter values when comparing 2 different evolutionary algorithm","postDate":"1200956905","msgId":3792,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGZuMzhsOStsaDFlQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGZtdm9nZCs4Z205QGVHcm91cHMuY29tPg=="},"prevInTopic":3791,"nextInTopic":3793,"prevInTime":3791,"nextInTime":3793,"topicId":3791,"numMessagesInTopic":3,"msgSnippet":"Parameter setting is a nasty issue in machine learning in general for which there is no perfect approach.  The reason most papers say they found their","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 59383 invoked from network); 21 Jan 2008 23:08:27 -0000\r\nX-Received: from unknown (66.218.67.94)\n  by m56.grp.scd.yahoo.com with QMQP; 21 Jan 2008 23:08:27 -0000\r\nX-Received: from unknown (HELO n35a.bullet.mail.sp1.yahoo.com) (66.163.168.129)\n  by mta15.grp.scd.yahoo.com with SMTP; 21 Jan 2008 23:08:27 -0000\r\nX-Received: from [216.252.122.218] by n35.bullet.mail.sp1.yahoo.com with NNFMP; 21 Jan 2008 23:08:27 -0000\r\nX-Received: from [209.73.164.83] by t3.bullet.sp1.yahoo.com with NNFMP; 21 Jan 2008 23:08:27 -0000\r\nX-Received: from [66.218.66.90] by t7.bullet.scd.yahoo.com with NNFMP; 21 Jan 2008 23:08:27 -0000\r\nDate: Mon, 21 Jan 2008 23:08:25 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;fn38l9+lh1e@...&gt;\r\nIn-Reply-To: &lt;fmvogd+8gm9@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: Re: How to choose parameter values when comparing 2 different evolutionary algorithm\r\nX-Yahoo-Group-Post: member; u=54567749; y=CNwT0uuz7gd1VwG2aBEN6WkcWBsGb8ie2O3GHi-KeBJK7bg4EUo6\r\nX-Yahoo-Profile: kenstanley01\r\n\r\nParameter setting is a nasty issue in machine learning in general \nfor whic=\r\nh there is no perfect approach.  The reason most papers say \nthey found the=\r\nir parameters through &quot;preliminary experimentation&quot; is \nbecause there is ge=\r\nnerally no unversally-accepted approach to \nfinding the best parameters.  Y=\r\nou just have to trust the authors.\n\nHowever, when you are comparing two var=\r\niants of the same method \n(such as NEAT), you do have an advantage because =\r\nthey can both \nutilize the same parameter settings to make the comparison f=\r\nair.  So \nin that case, that is what is important:  Find settings that are =\r\nthe \nsame for both and work well for both.  That&#39;s what I would suggest. \n\n=\r\nIt is not important whether the settings are &quot;perfect.&quot;   The truth \nis tha=\r\nt what&#39;s perfect in one domain is not in another; and besides, \nno one can =\r\nbe sure what the optimal values are without an exhaustive \nmeta-search, whi=\r\nch isn&#39;t generally worth the effort.  What is \nimportant is just that they =\r\nare reasonable.  If there is precedent, \ni.e. if people have found good par=\r\nameter settings in the past, \nloosely following that precedent also lends s=\r\nome credibility, so you \ncan say that your settings are based on some prior=\r\n published work.\n\nI suggest not getting hung up on parameter settings.  The=\r\ny are the \nkind of thing you can fiddle with forever without really learnin=\r\ng \nanything deep.  What you want to do is isolate the parts of your \nmethod=\r\ns are are interesting and keep the parameters from being the \nmain issue.\n\n=\r\nken \n\n--- In neat@yahoogroups.com, &quot;zymeth02&quot; &lt;zymeth02@...&gt; wrote:\n&gt;\n&gt; Hi,=\r\n I&#39;m having some doubts in doing my research and was hoping \nthat I\n&gt; could=\r\n seek some help here.\n&gt; \n&gt; Basically, I have two algorithms that I want to =\r\ncompare (both use \nNEAT\n&gt; btw), but have to first choose the parameter valu=\r\nes for each\n&gt; algorithm. I suppose that to have a fair comparison, each alg=\r\norithm\n&gt; should be tested with different parameter settings to determine it=\r\ns\n&gt; ideal setting. My question is, how comprehensive should it be? (eg.\n&gt; f=\r\nor 1 variable, should we test 10 different values? should we have \n10\n&gt; tri=\r\nals for each run?) This has become my concern because I don&#39;t \nwant\n&gt; to wa=\r\nste too much time in finding the right parameters.\n&gt; \n&gt; I am mostly concern=\r\ned with finding the appropriate mutation\n&gt; probabilities (mut add link, mut=\r\n add node, mut link weight, mut \npower).\n&gt; \n&gt; I looked through some papers =\r\non how they chose their parameters \nwhen\n&gt; comparing, and noticed that rese=\r\narchers usually say that the \nsettings\n&gt; are found experimentally, without =\r\nmention of how comprehensive it \nis.\n&gt; Also, most don&#39;t show experimental s=\r\ntatistics of these parameter\n&gt; testing. Is it because such thing is trivial=\r\n? \n&gt; \n&gt; I appreciate your answers. Thanks!\n&gt;\n\n\n\n"}}