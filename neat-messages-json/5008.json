{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":360532607,"authorName":"Sebastian Risi","from":"Sebastian Risi &lt;sebastian.risi@...&gt;","profile":"sebastian.risi","replyTo":"LIST","senderId":"oFGsrBus-HmftwlQum3ryW6zpfXbXSUMwBUMG5vmfPrKietrq-v2uo3UM1yq7Udkh8m-_oshozNHz3pRlvbtI3-h6Gf5Jh8l2N21VhLwGdU","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [neat] Trying to reproduce novelty results.","postDate":"1260645659","msgId":5008,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGU4ZjJiYWM4MDkxMjEyMTEyMGw0ODhkYzQyMXMyMzQwMTY3MTExMWFjNzM4QG1haWwuZ21haWwuY29tPg==","inReplyToHeader":"PDJhMGQyYTRjMDkxMjA5MTAwMm81NDZkZDI3YnhjYjBiYjAyNzJiZDIxMDM2QG1haWwuZ21haWwuY29tPg==","referencesHeader":"PDJhMGQyYTRjMDkxMjA0MDk1MG43ODhjNWMzZnBhMjk3MjAwOTFkZjM3NTJlQG1haWwuZ21haWwuY29tPgkgPGU4ZjJiYWM4MDkxMjA2MjE0NGczZGUzNWU0YWlmZGE3ZjVjNjlmMDIxZTU4QG1haWwuZ21haWwuY29tPgkgPDJhMGQyYTRjMDkxMjA5MTAwMm81NDZkZDI3YnhjYjBiYjAyNzJiZDIxMDM2QG1haWwuZ21haWwuY29tPg=="},"prevInTopic":4996,"nextInTopic":0,"prevInTime":5007,"nextInTime":5009,"topicId":4973,"numMessagesInTopic":4,"msgSnippet":"Hi Paul, right now the source code is not in a publishable state but hopefully I have some time to clean it up soon. The domain I used is the discrete T-Maze","rawEmail":"Return-Path: &lt;sebastian.risi@...&gt;\r\nX-Sender: sebastian.risi@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 52242 invoked from network); 12 Dec 2009 19:21:00 -0000\r\nX-Received: from unknown (98.137.34.45)\n  by m3.grp.sp2.yahoo.com with QMQP; 12 Dec 2009 19:21:00 -0000\r\nX-Received: from unknown (HELO mail-pz0-f191.google.com) (209.85.222.191)\n  by mta2.grp.sp2.yahoo.com with SMTP; 12 Dec 2009 19:21:00 -0000\r\nX-Received: by pzk29 with SMTP id 29so1456146pzk.17\n        for &lt;neat@yahoogroups.com&gt;; Sat, 12 Dec 2009 11:21:00 -0800 (PST)\r\nMIME-Version: 1.0\r\nX-Received: by 10.142.61.25 with SMTP id j25mr1784651wfa.320.1260645660023; Sat, \n\t12 Dec 2009 11:21:00 -0800 (PST)\r\nIn-Reply-To: &lt;2a0d2a4c0912091002o546dd27bxcb0bb0272bd21036@...&gt;\r\nReferences: &lt;2a0d2a4c0912040950n788c5c3fpa29720091df3752e@...&gt;\n\t &lt;e8f2bac80912062144g3de35e4aifda7f5c69f021e58@...&gt;\n\t &lt;2a0d2a4c0912091002o546dd27bxcb0bb0272bd21036@...&gt;\r\nDate: Sat, 12 Dec 2009 14:20:59 -0500\r\nMessage-ID: &lt;e8f2bac80912121120l488dc421s23401671111ac738@...&gt;\r\nTo: neat@yahoogroups.com\r\nContent-Type: multipart/alternative; boundary=001636e0b604c0df7f047a8cf08f\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Sebastian Risi &lt;sebastian.risi@...&gt;\r\nSubject: Re: [neat] Trying to reproduce novelty results.\r\nX-Yahoo-Group-Post: member; u=360532607; y=5bEAvBJpHWGdfnz5xijKBprkM4e5LOGfBAicqMlb-DXvv3QtDBwcJO4\r\nX-Yahoo-Profile: sebastian.risi\r\n\r\n\r\n--001636e0b604c0df7f047a8cf08f\r\nContent-Type: text/plain; charset=UTF-8\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\nHi Paul,\n\nright now the source code is not in a publishable state but hopef=\r\nully I have\nsome time to clean it up soon.\n\nThe domain I used is the discre=\r\nte T-Maze described by Soltoggio and your\nplan to try this version is proba=\r\nbly a good idea. If you can&#39;t get it\nworking with either fitness or novelty=\r\n search my feeling is that the problem\nlies more in the adaptive model or t=\r\nhe way the networks are evolved.\n\nWhat algorithm are you using to evolve th=\r\ne networks and is it working for\nother domains?\n\nCheers,\nSebastian\n\n\n\n\n\nOn =\r\nWed, Dec 9, 2009 at 1:02 PM, Nemo 136 &lt;nemo136@...&gt; wrote:\n\n&gt;\n&gt;\n&gt; Hi =\r\nSebastian,\n&gt;\n&gt; I wasn&#39;t able to evolve the learning behavior with a regular=\r\n fitness, even\n&gt; after quite long simulation times. Though, the modulatory =\r\nNeurons are used,\n&gt; as I have behaviors which change along the time and dep=\r\nending on the absence\n&gt; or presence of reward.\n&gt;\n&gt; My adaptation is the sam=\r\ne as the one described in\n&gt; http://alifexi.alife.org/papers/ALIFExi_pp569-5=\r\n76.pdf\n&gt;\n&gt; When I mutate a link into a node, the node can either be modulat=\r\nory or\n&gt; normal.\n&gt;\n&gt; As the first paper cited in the article for the tmaze =\r\n(\n&gt; http://www.springerlink.com/index/46ATV1CN3FEM1E67.pdf) was continuous,=\r\n I\n&gt; assumed your experiment was also in a continuous environment, and in t=\r\nhese\n&gt; environment, the robot manages to find all the possible behaviors bu=\r\nt not\n&gt; the learning ones.\n&gt;\n&gt;  I am now trying in a discrete T maze as in =\r\nSoltoggio&#39;s experiment.\n&gt;\n&gt; I understand that using NEAT could make a diffe=\r\nrence and that&#39;s\n&gt; exactly one of the things we are trying to understand by=\r\n re-implementing your\n&gt;\n&gt; experiment.\n&gt;\n&gt; Could you publish the source code=\r\n (as for most NEAT experiment from\n&gt; the EPLEX group) so as we can compare =\r\nthe evolutionary path between\n&gt; our implementations ?\n&gt;\n&gt; Best regards,\n&gt;\n&gt;=\r\n Paul\n&gt;\n&gt;\n&gt;\n&gt; 2009/12/7 Sebastian Risi &lt;sebastian.risi@...&gt;\n&gt;\n&gt;\n&gt;&gt;\n&gt;&gt;=\r\n Hi Paul,\n&gt;&gt;\n&gt;&gt; As you&#39;ve already pointed out it could make a difference th=\r\nat you&#39;re not\n&gt;&gt; using NEAT. We believe that NEAT is beneficial to Novelty =\r\nSearch because it\n&gt;&gt; gradually complexifies behaviors which reduces the nov=\r\nelty search space.\n&gt;&gt; Your novelty metric seems similar to the one I used s=\r\no I think that\n&gt;&gt; shouldn&#39;t make a big difference.\n&gt;&gt;\n&gt;&gt; Were you able to e=\r\nvolve learning behavior with a regular fitness function?\n&gt;&gt; What kind of ad=\r\naptation mechanism are you using?\n&gt;&gt;\n&gt;&gt; Cheers,\n&gt;&gt; Sebastian\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; On=\r\n Fri, Dec 4, 2009 at 12:50 PM, Nemo 136 &lt;nemo136@...&gt; wrote:\n&gt;&gt;\n&gt;&gt;&gt;\n&gt;=\r\n&gt;&gt;\n&gt;&gt;&gt; Hello,\n&gt;&gt;&gt;\n&gt;&gt;&gt; I&#39;m currently trying to reproduce the results in\n&gt;&gt;&gt; =\r\nhttp://eplex.cs.ucf.edu/publications/2009/risi.gecco09.html and finding\n&gt;&gt;&gt;=\r\n out if a NEAT algorithm is necessary to obtain theses results. Right now, =\r\nI\n&gt;&gt;&gt; manage without too many problems to get behaviors changing depending =\r\non the\n&gt;&gt;&gt; trial / deploy.\n&gt;&gt;&gt;\n&gt;&gt;&gt;  Though, the novelty keeps trying new be=\r\nhaviors based upon crashing / not\n&gt;&gt;&gt; crashing or getting one reward or not=\r\n getting it, and returning or not\n&gt;&gt;&gt; returning to the origin (no reward). =\r\nThis fills up the archive quite fast\n&gt;&gt;&gt; without any actual &quot;learning&quot; appe=\r\naring (the best scores show 60% to 70% of\n&gt;&gt;&gt; all high reward reached witho=\r\nut any clear &quot;learning&quot; pattern).\n&gt;&gt;&gt;\n&gt;&gt;&gt; I would like to know if some code=\r\n is provided online for this experiment,\n&gt;&gt;&gt; or if you could send me the co=\r\nde used in this experiment, if only to try to\n&gt;&gt;&gt; see which parameters are =\r\ndifferent in the two of them, so that I can try to\n&gt;&gt;&gt; pinpoint the differe=\r\nnces.\n&gt;&gt;&gt;\n&gt;&gt;&gt; Right now, the differences I can see are :\n&gt;&gt;&gt; - simple netwo=\r\nrk evolution algorithm vs NEAT mechanisms\n&gt;&gt;&gt; - possible difference in the =\r\nway of counting for novelty score : I am\n&gt;&gt;&gt; using a bitset of size 4*nb_de=\r\nploy*nb_trials with each bit standing for : a\n&gt;&gt;&gt; crash, a high reward, a l=\r\now reward, returning to the starting point wit the\n&gt;&gt;&gt; reward. I do a hammi=\r\nng distance between behaviors to get my novelty score.\n&gt;&gt;&gt; But, it seems to=\r\n be the same mechanisms as in the paper\n&gt;&gt;&gt; - generation based algorithm wh=\r\nile the paper uses a a steady state\n&gt;&gt;&gt; approach in which only one individu=\r\nal is generated at each generation\n&gt;&gt;&gt;\n&gt;&gt;&gt; Cheers.\n&gt;&gt;&gt;\n&gt;&gt;&gt; Paul Tonelli\n&gt;&gt;&gt;=\r\n\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;\n&gt;  \n&gt;\n\r\n--001636e0b604c0df7f047a8cf08f\r\nContent-Type: text/html; charset=UTF-8\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\nHi Paul,&lt;br&gt;&lt;br&gt;right now the source code is not in a publishable state but=\r\n hopefully I have some time to clean it up soon.&lt;br&gt;&lt;br&gt;The domain I used i=\r\ns the discrete T-Maze described by Soltoggio and your plan to try this vers=\r\nion is probably a good idea. If you can&#39;t get it working with either fi=\r\ntness or novelty search my feeling is that the problem lies more in the ada=\r\nptive model or the way the networks are evolved.&lt;br&gt;\n&lt;br&gt;What algorithm are=\r\n you using to evolve the networks and is it working for other domains?&lt;br&gt;&lt;=\r\nbr&gt;Cheers,&lt;br&gt;Sebastian&lt;br&gt;&lt;br&gt;=C2=A0&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;div class=3D&quot;gmail_qu=\r\note&quot;&gt;On Wed, Dec 9, 2009 at 1:02 PM, Nemo 136 &lt;span dir=3D&quot;ltr&quot;&gt;&lt;&lt;a href=\r\n=3D&quot;mailto:nemo136@...&quot;&gt;nemo136@...&lt;/a&gt;&gt;&lt;/span&gt; wrote:&lt;br&gt;\n&lt;=\r\nblockquote class=3D&quot;gmail_quote&quot; style=3D&quot;border-left: 1px solid rgb(204, 2=\r\n04, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;&quot;&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;div=\r\n style=3D&quot;background-color: rgb(255, 255, 255);&quot;&gt;\n&lt;span&gt;=C2=A0&lt;/span&gt;\n\n\n&lt;di=\r\nv&gt;\n  &lt;div&gt;\n\n\n    &lt;div&gt;\n      \n      \n      &lt;p&gt;Hi Sebastian,&lt;br&gt;&lt;br&gt;I wasn&#=\r\n39;t able to evolve the learning behavior with a regular fitness, even afte=\r\nr quite long simulation times. Though, the modulatory Neurons are used, as =\r\nI have behaviors which change along the time and depending on the absence o=\r\nr presence of reward.&lt;br&gt;\n\n\n&lt;br&gt;My adaptation is the same as the one descri=\r\nbed in &lt;a href=3D&quot;http://alifexi.alife.org/papers/ALIFExi_pp569-576.pdf&quot; ta=\r\nrget=3D&quot;_blank&quot;&gt;http://alifexi.alife.org/papers/ALIFExi_pp569-576.pdf&lt;/a&gt;&lt;b=\r\nr&gt;&lt;br&gt;When I mutate a link into a node, the node can either be modulatory o=\r\nr normal. &lt;br&gt;\n\n&lt;br&gt;As the first paper cited in the article for the tmaze (=\r\n&lt;a href=3D&quot;http://www.springerlink.com/index/46ATV1CN3FEM1E67.pdf&quot; target=\r\n=3D&quot;_blank&quot;&gt;http://www.springerlink.com/index/46ATV1CN3FEM1E67.pdf&lt;/a&gt;) was=\r\n continuous, I assumed your experiment was also in a continuous environment=\r\n, and in these environment, the robot manages to find all the possible beha=\r\nviors but not the learning ones.&lt;br&gt;\n\n&lt;br&gt;=C2=A0I am now trying in a discre=\r\nte T maze as in Soltoggio&#39;s experiment.&lt;br&gt;&lt;br&gt;\n&lt;/p&gt;&lt;pre style=3D&quot;font-=\r\nfamily: arial,helvetica,sans-serif;&quot;&gt;I understand that using NEAT could mak=\r\ne a difference and that&#39;s&lt;br&gt;exactly one of the things we are trying to=\r\n understand by re-implementing your&lt;br&gt;\nexperiment. &lt;br&gt;&lt;br&gt;Could you publi=\r\nsh the source code (as for most NEAT experiment from&lt;br&gt;the EPLEX group) so=\r\n as we can compare the evolutionary path between&lt;br&gt;our implementations ?&lt;b=\r\nr&gt;&lt;br&gt;Best regards,&lt;br&gt;&lt;br&gt;Paul&lt;br&gt;\n\n&lt;/pre&gt;\n&lt;br&gt;&lt;br&gt;&lt;div class=3D&quot;gmail_quo=\r\nte&quot;&gt;2009/12/7 Sebastian Risi &lt;span dir=3D&quot;ltr&quot;&gt;&lt;&lt;a href=3D&quot;mailto:sebast=\r\nian.risi@...&quot; target=3D&quot;_blank&quot;&gt;sebastian.risi@...&lt;/a&gt;&gt;&lt;/spa=\r\nn&gt;&lt;div class=3D&quot;im&quot;&gt;&lt;br&gt;&lt;blockquote class=3D&quot;gmail_quote&quot; style=3D&quot;border-l=\r\neft: 1px solid rgb(204, 204, 204);&quot;&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;div style=3D&quot;backgrou=\r\nnd-color: rgb(255, 255, 255);&quot;&gt;\n&lt;span&gt;=C2=A0&lt;/span&gt;\n\n\n&lt;div&gt;\n  &lt;div&gt;\n\n\n    &lt;=\r\ndiv&gt;\n      \n      \n      &lt;p&gt;Hi Paul,&lt;br&gt;&lt;br&gt;As you&#39;ve already pointed o=\r\nut it could make a difference that you&#39;re not using NEAT. We believe th=\r\nat NEAT is beneficial to Novelty Search because it gradually complexifies b=\r\nehaviors which reduces the novelty search space. Your novelty metric seems =\r\nsimilar to the one I used so I think that shouldn&#39;t make a big differen=\r\nce.&lt;br&gt;\n\n\n\n&lt;br&gt;Were you able to evolve learning behavior with a regular fit=\r\nness function? What kind of adaptation mechanism are you using? &lt;br&gt;&lt;br&gt;Che=\r\ners,&lt;br&gt;Sebastian&lt;/p&gt;&lt;div&gt;&lt;div&gt;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;div class=3D&quot;gmail_q=\r\nuote&quot;&gt;\nOn Fri, Dec 4, 2009 at 12:50 PM, Nemo 136 &lt;span dir=3D&quot;ltr&quot;&gt;&lt;&lt;a h=\r\nref=3D&quot;mailto:nemo136@...&quot; target=3D&quot;_blank&quot;&gt;nemo136@...&lt;/a&gt;&gt=\r\n;&lt;/span&gt; wrote:&lt;br&gt;\n&lt;blockquote class=3D&quot;gmail_quote&quot; style=3D&quot;border-left:=\r\n 1px solid rgb(204, 204, 204);&quot;&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;div style=3D&quot;background-colo=\r\nr: rgb(255, 255, 255);&quot;&gt;\n&lt;span&gt;=C2=A0&lt;/span&gt;\n\n\n&lt;div&gt;\n  &lt;div&gt;\n\n\n    &lt;div&gt;\n  =\r\n    \n      \n      &lt;p&gt;&lt;/p&gt;&lt;div class=3D&quot;gmail_quote&quot;&gt;\nHello, &lt;br&gt;&lt;br&gt;I&#39;m=\r\n currently trying to reproduce the results in &lt;a href=3D&quot;http://eplex.cs.uc=\r\nf.edu/publications/2009/risi.gecco09.html&quot; target=3D&quot;_blank&quot;&gt;http://eplex.c=\r\ns.ucf.edu/publications/2009/risi.gecco09.html&lt;/a&gt; and finding out if a NEAT=\r\n algorithm is necessary to obtain theses results. Right now, I manage witho=\r\nut too many problems to get behaviors changing depending on the trial / dep=\r\nloy.&lt;br&gt;\n\n\n\n\n\n&lt;br&gt;=C2=A0Though, the novelty keeps trying new behaviors base=\r\nd upon crashing / not crashing or getting one reward or not getting it, and=\r\n returning or not returning to the origin (no reward). This fills up the ar=\r\nchive quite fast without any actual &quot;learning&quot; appearing (the bes=\r\nt scores show 60% to 70% of all high reward reached without any clear &quot=\r\n;learning&quot; pattern).&lt;br&gt;\n\n\n\n\n\n&lt;br&gt;I would like to know if some code is=\r\n provided online for this experiment, or if you could send me the code used=\r\n in this experiment, if only to try to see which parameters are different i=\r\nn the two of them, so that I can try to pinpoint the differences. &lt;br&gt;\n\n\n\n\n=\r\n\n&lt;br&gt;Right now, the differences I can see are :&lt;br&gt;- simple network evoluti=\r\non algorithm vs NEAT mechanisms&lt;br&gt;- possible difference in the way of coun=\r\nting for novelty score : I am using a bitset of size 4*nb_deploy*nb_trials =\r\nwith each bit standing for : a crash, a high reward, a low reward, returnin=\r\ng to the starting point wit the reward. I do a hamming distance between beh=\r\naviors to get my novelty score. But, it seems to be the same mechanisms as =\r\nin the paper&lt;br&gt;\n\n\n\n\n\n- generation based algorithm while the paper uses a a=\r\n steady state approach in\nwhich only one individual is generated at each ge=\r\nneration&lt;br&gt;&lt;br&gt;Cheers.&lt;br&gt;&lt;br&gt;Paul Tonelli&lt;br&gt;&lt;br&gt;\n&lt;/div&gt;&lt;br&gt;\n\n\n    &lt;/div&gt;=\r\n\n     \n\n    \n    &lt;div style=3D&quot;color: rgb(255, 255, 255);&quot;&gt;&lt;/div&gt;\n\n\n&lt;/div&gt;\n=\r\n\n\n\n  \n\n\n\n\n\n\n&lt;/div&gt;&lt;/div&gt;&lt;/blockquote&gt;&lt;/div&gt;&lt;br&gt;\n&lt;/div&gt;&lt;/div&gt;\n\n    &lt;/div&gt;\n  =\r\n   \n\n    \n    &lt;div style=3D&quot;color: rgb(255, 255, 255);&quot;&gt;&lt;/div&gt;\n\n\n&lt;/div&gt;\n\n\n\n=\r\n  \n\n\n\n\n\n\n&lt;/div&gt;&lt;/div&gt;&lt;/blockquote&gt;&lt;/div&gt;&lt;/div&gt;&lt;br&gt;\n\n\n    &lt;/div&gt;\n     \n\n    =\r\n\n    &lt;div style=3D&quot;color: rgb(255, 255, 255); min-height: 0pt;&quot;&gt;&lt;/div&gt;\n\n\n&lt;/=\r\ndiv&gt;\n\n\n\n  \n\n\n\n\n\n\n&lt;/blockquote&gt;&lt;/div&gt;&lt;br&gt;\n\r\n--001636e0b604c0df7f047a8cf08f--\r\n\n"}}