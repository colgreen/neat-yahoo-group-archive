{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"9sifrPvF61LrhGju0bMuuCI0fJfYOdrg4mw7G0dO8sp1zfLREPpViCkxtZ8wdYmyq9-TklulZfZATIIxOLW8sbQV4LM-A2Sck9XJm5NuASe1","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: New paper investigating HyperNEAT&#39;s sensitivity to different geometric representations of a problem","postDate":"1240190492","msgId":4635,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGdzZ2ltcyszaWVjQGVHcm91cHMuY29tPg==","inReplyToHeader":"PEM2MTEzRUIxLjJBMzdEJWpjbHVuZUBtc3UuZWR1Pg=="},"prevInTopic":4633,"nextInTopic":4636,"prevInTime":4634,"nextInTime":4636,"topicId":4627,"numMessagesInTopic":14,"msgSnippet":"Another interesting implication of the ability to exploit seemingly scrambled geometry is that it may be a good omen for the future goal of evolving the","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 30665 invoked from network); 20 Apr 2009 01:22:34 -0000\r\nX-Received: from unknown (69.147.108.202)\n  by m8.grp.re1.yahoo.com with QMQP; 20 Apr 2009 01:22:34 -0000\r\nX-Received: from unknown (HELO n16c.bullet.sp1.yahoo.com) (69.147.64.123)\n  by mta3.grp.re1.yahoo.com with SMTP; 20 Apr 2009 01:22:34 -0000\r\nX-Received: from [69.147.65.151] by n16.bullet.sp1.yahoo.com with NNFMP; 20 Apr 2009 01:21:34 -0000\r\nX-Received: from [98.137.34.36] by t5.bullet.mail.sp1.yahoo.com with NNFMP; 20 Apr 2009 01:21:34 -0000\r\nDate: Mon, 20 Apr 2009 01:21:32 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;gsgims+3iec@...&gt;\r\nIn-Reply-To: &lt;C6113EB1.2A37D%jclune@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: Re: New paper investigating HyperNEAT&#39;s sensitivity to different geometric representations of a problem\r\nX-Yahoo-Group-Post: member; u=54567749; y=JRp78k55WxByWyn-fyB7VRVkvpPBYN_Ez9rEoQaaKqRhxAsbC-72\r\nX-Yahoo-Profile: kenstanley01\r\n\r\nAnother interesting implication of the ability to exploit seemingly scrambl=\r\ned geometry is that it may be a good omen for the future goal of evolving t=\r\nhe configuration and density of the substrate itself.  It&#39;s pretty clear th=\r\nat sooner or later we will want to evolve the substrate in addition to its =\r\nconnectivity, and the idea that you can&#39;t go too terribly wrong means that =\r\ngetting such an evolutionary process started might be easier than one would=\r\n expect.  \n\nStill, the right way to evolve the substrate remains a very int=\r\neresting open question.\n\nken\n\n--- In neat@yahoogroups.com, Jeff Clune &lt;jclu=\r\nne@...&gt; wrote:\n&gt;\n&gt; Hello Ken and Peter-\n&gt; \n&gt; Please see my responses inters=\r\npersed below.\n&gt; \n&gt; &gt; Peter, Jeff and I have had a lot of in-depth conversat=\r\nions about whether or\n&gt; &gt; not the good performance of randomized geometry h=\r\nas anything to do with its\n&gt; &gt; geometry at all or whether it&#39;s just the ind=\r\nirect encoding.\n&gt; &gt; \n&gt; &gt; As I&#39;ve said to Jeff, I believe that anything Hype=\r\nrNEAT does has something to\n&gt; &gt; do with geometry so the connection is inesc=\r\napable.  While it is an indirect\n&gt; &gt; encoding, the encoding itself is a fun=\r\nction of geometry, so it cannot be\n&gt; &gt; considered independently of that geo=\r\nmetry.\n&gt; &gt; \n&gt; &gt; My hypothesis, which I&#39;ve discussed with Jeff, is that in s=\r\nome domains almost\n&gt; &gt; *any* random layout has some kind of exploitable geo=\r\nmetric relationship.  It\n&gt; &gt; may not be the most convenient one, but it is =\r\nstill better than nothing.\n&gt; \n&gt; The position I took when discussing this wi=\r\nth Ken was that &#39;the geometry of\n&gt; a problem&#39; refers to meaningful geometri=\r\nc relationships *in the problem\n&gt; itself* (e.g,. the concept of a straight =\r\nline in Tic Tac Toe, whether on the\n&gt; diagonals, columns, or rows). If that=\r\n geometry is entirely scrambled when\n&gt; creating the geometric representatio=\r\nn that HyperNEAT uses, then it cannot\n&gt; meaningfully be said that HyperNEAT=\r\n exploits the geometry of the problem.\n&gt; \n&gt; Ken&#39;s response, which he descri=\r\nbed well, is that HyperNEAT is always\n&gt; exploiting geometry. I can see his =\r\npoint of view. Maybe one way to think\n&gt; about it is that HyperNEAT is alway=\r\ns exploiting the geometry of *the problem\n&gt; it is presented with,* which ma=\r\ny or may not relate to the geometry of the\n&gt; actual problem. \n&gt; \n&gt; My point=\r\n was that if HyperNEAT does better than other neuroevolutionary\n&gt; algorithm=\r\ns on a problem, and the true geometry of the problem is scrambled,\n&gt; then w=\r\nhy is the problem easier for HyperNEAT? There are two things which\n&gt; make H=\r\nyperNEAT different: its generative nature and its ability to exploit\n&gt; geom=\r\netry. If the geometry is truly gone because of the scrambling, then it\n&gt; se=\r\nems that what is left to explain HyperNEAT&#39;s success is its generative\n&gt; pr=\r\noperties. However, Ken has convinced me that it is not that simple to\n&gt; dis=\r\nentangle these two issues. HyperNEAT may be exploiting new geometric\n&gt; corr=\r\nelations that occur by chance (but is that exploiting the geometry of\n&gt; the=\r\n real problem?). Alternately, HyperNEAT may be transforming the geometry\n&gt; =\r\nof the scrambled problem back into the geometry of the real problem (but\n&gt; =\r\ncan&#39;t other neural net algorithms perform similar transformations?). I don&#39;=\r\nt\n&gt; raise these issues because I have clear answers to them. I raise them t=\r\no\n&gt; show that it&#39;s a complicated subject.  In the end, Ken and I decided th=\r\nat it\n&gt; is difficult to experimentally isolate the contributions from the t=\r\nwo major\n&gt; HyperNEAT features (exploiting geometry vs. its generative encod=\r\ning\n&gt; properties).\n&gt; \n&gt; &gt; The question for me is whether HyperNEAT can succ=\r\need with random geometries in\n&gt; &gt; general, or just in particular domains.  =\r\nIt clearly can do it in the quadruped\n&gt; &gt; domain, but I wonder about others=\r\n.  Perhaps the quadruped has properties that\n&gt; &gt; make the geometry more for=\r\ngiving than in most domains.\n&gt; &gt; \n&gt; &gt; However, you (Peter) note that you al=\r\nso saw the same thing in your own\n&gt; &gt; experiments.  So maybe it&#39;s a somewha=\r\nt widespread phenomenon.\n&gt; \n&gt; I also think this is interesting, and look fo=\r\nrward to seeing how results\n&gt; from other domains.\n&gt;  \n&gt; &gt; I think that what=\r\n is important is not so much that you can randomize geometry,\n&gt; &gt; because t=\r\nhere isn&#39;t really much reason you&#39;d want it to be completely random,\n&gt; &gt; bu=\r\nt that it shows that you don&#39;t need to get it perfect to still do well.\n&gt; &gt;=\r\n That is definitely good news.\n&gt; \n&gt; I agree. That HyperNEAT can perform wel=\r\nl even with a really poor geometric\n&gt; representation also speaks to the iss=\r\nues that others have raised on this\n&gt; forum, namely, that some problems don=\r\n&#39;t seem to have any obvious geometry to\n&gt; them. If that&#39;s true, than it sho=\r\nuld be similar to asking HyperNEAT to solve\n&gt; a problem with geometry but s=\r\ncrambling the geometry. Since HyperNEAT seems\n&gt; (so far) to deal with that =\r\njust fine, it leads me to believe that it will\n&gt; also do well on problems t=\r\nhat do not have any obviously meaningful geometry.\n&gt; \n&gt; Jeff\n&gt; \n&gt; \n&gt; \n&gt; &gt; -=\r\n-- In neat@yahoogroups.com, &quot;petar_chervenski&quot; &lt;petar_chervenski@&gt; wrote:\n&gt;=\r\n &gt;&gt; \n&gt; &gt;&gt; Hi!\n&gt; &gt;&gt; \n&gt; &gt;&gt; Thanks for posting this paper, great work!\n&gt; &gt;&gt; By=\r\n the way I also noticed that HyperNEAT performs well even in cases of a\n&gt; &gt;=\r\n&gt; substrate that is messed up. My first release of the NEVH experiment had =\r\na\n&gt; &gt;&gt; bug, it was not symmetrical and basically it could be considered ran=\r\ndomized.\n&gt; &gt;&gt; Yet it was still able to evolve good behaviors, though. I won=\r\nder, is this\n&gt; &gt;&gt; linked to the geometry at all? Perhaps only the fact that=\r\n this indirect\n&gt; &gt;&gt; encoding searches trough less dimensions, makes it more=\r\n effective?\n&gt; &gt;&gt; \n&gt; &gt;&gt; Peter\n&gt; &gt;&gt; \n&gt; &gt;&gt; --- In neat@yahoogroups.com, &quot;Kenne=\r\nth Stanley&quot; &lt;kstanley@&gt; wrote:\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt; Just for the record, I highly r=\r\necommend this paper.  It&#39;s the most extensive\n&gt; &gt;&gt;&gt; study published on the =\r\neffect of varying geometry on HyperNEAT performance.\n&gt; &gt;&gt;&gt; Some of its resu=\r\nlts are quite interesting.\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt; For example, it suggests that Hyper=\r\nNEAT is still significantly better than a\n&gt; &gt;&gt;&gt; direct encoding encoding ev=\r\nen if its geometry is randomized! (Although it is\n&gt; &gt;&gt;&gt; still worse than an=\r\n engineered geometry.)  Jeff and I have discussed how to\n&gt; &gt;&gt;&gt; interpret th=\r\nis result and my own opinion is that most randomized geometries\n&gt; &gt;&gt;&gt; never=\r\ntheless still have some useful geometry, and something is better than\n&gt; &gt;&gt;&gt;=\r\n nothing.  So it suggests that you don&#39;t have to necessarily hit on the\n&gt; &gt;=\r\n&gt;&gt; &quot;perfect&quot; substrate geometry to get traction out of HyperNEAT.\n&gt; &gt;&gt;&gt; \n&gt; =\r\n&gt;&gt;&gt; Yet I think a big question that emerges from this result is whether it =\r\nis\n&gt; &gt;&gt;&gt; somehow tied specifically to the quadruped domain.  It is possible=\r\n that it\n&gt; &gt;&gt;&gt; has something to with the fact that a quadruped can get alon=\r\ng pretty well if\n&gt; &gt;&gt;&gt; all its legs move in tandem, so you have the opportu=\r\nnity to represent that\n&gt; &gt;&gt;&gt; repetition regardless of how the legs are orga=\r\nnized geometrically.  It is\n&gt; &gt;&gt;&gt; possible that other types of systems are =\r\nless geometrically forgiving,\n&gt; &gt;&gt;&gt; though that remains to be seen.\n&gt; &gt;&gt;&gt; \n=\r\n&gt; &gt;&gt;&gt; Of course, in practice, if the right geometry is even partially clear=\r\n, the\n&gt; &gt;&gt;&gt; resultant informed substrate configuration is probably going to=\r\n be better\n&gt; &gt;&gt;&gt; than a random geometry.  The &quot;engineered&quot; geometries (mean=\r\ning they are\n&gt; &gt;&gt;&gt; designed by a human to make sense) still outperform the =\r\nrandom ones in the\n&gt; &gt;&gt;&gt; paper, even as the random ones outperform the dire=\r\nct encoding.\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt; Anyway, in my view, the most important implicatio=\r\nn is that geometry matters.\n&gt; &gt;&gt;&gt; If you can construct a sensible geometry,=\r\n HyperNEAT will indeed take\n&gt; &gt;&gt;&gt; advantage of it to learn the task.  And e=\r\nven if it&#39;s not perfect, it&#39;s\n&gt; &gt;&gt;&gt; better than having no geometric knowled=\r\nge at all.\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt; ken\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt; --- In neat@yahoogroups.com, Jeff =\r\nClune &lt;jclune@&gt; wrote:\n&gt; &gt;&gt;&gt;&gt; \n&gt; &gt;&gt;&gt;&gt; Hello all-\n&gt; &gt;&gt;&gt;&gt; \n&gt; &gt;&gt;&gt;&gt; Below is a =\r\nlink to a new HyperNEAT paper that I will present at GECCO 2009\n&gt; &gt;&gt;&gt;&gt; this=\r\n summer. It studies how sensitive HyperNEAT is to different geometric\n&gt; &gt;&gt;&gt;=\r\n&gt; representations of the same problem.\n&gt; &gt;&gt;&gt;&gt; \n&gt; &gt;&gt;&gt;&gt; I am especially excit=\r\ned about this paper because it was nominated for the\n&gt; &gt;&gt;&gt;&gt; Best Paper Awar=\r\nd in the Generative and Developmental Systems track. If you\n&gt; &gt;&gt;&gt;&gt; are goin=\r\ng to GECCO this year, and enjoyed the paper, please keep it in mind\n&gt; &gt;&gt;&gt;&gt; =\r\nwhen casting your ballot.\n&gt; &gt;&gt;&gt;&gt; \n&gt; &gt;&gt;&gt;&gt; The paper can be viewed here:\n&gt; &gt;&gt;=\r\n&gt;&gt; \n&gt; &gt;&gt;&gt;&gt; \n&gt; https://www.msu.edu/~jclune/webfiles/publications/Clune-Hyper=\r\nNEATSensitivit&gt;&gt;&gt;&gt;\n&gt; y\n&gt; &gt;&gt;&gt;&gt; ToGeometry.pdf\n&gt; &gt;&gt;&gt;&gt; \n&gt; &gt;&gt;&gt;&gt; Abstract:\n&gt; &gt;&gt;&gt;=\r\n&gt; \n&gt; &gt;&gt;&gt;&gt; HyperNEAT, a generative encoding for evolving artificial neural n=\r\networks\n&gt; &gt;&gt;&gt;&gt; (ANNs), has the unique and powerful ability to exploit the g=\r\neometry of a\n&gt; &gt;&gt;&gt;&gt; problem (e.g., symmetries) by encoding ANNs as a functi=\r\non of a problem&#39;s\n&gt; &gt;&gt;&gt;&gt; geometry. This paper provides the first extensive =\r\nanalysis of the\n&gt; &gt;&gt;&gt;&gt; sensitivity of HyperNEAT to different geometric repr=\r\nesentations of a\n&gt; &gt;&gt;&gt;&gt; problem. Understanding how geometric representation=\r\ns affect the quality of\n&gt; &gt;&gt;&gt;&gt; evolved solutions should improve future desi=\r\ngns of such representations.\n&gt; &gt;&gt;&gt;&gt; HyperNEAT has been shown to produce coo=\r\nrdinated gaits for a simulated\n&gt; &gt;&gt;&gt;&gt; quadruped robot with a specific two-d=\r\nimensional geometric representation.\n&gt; &gt;&gt;&gt;&gt; Here, the same problem domain i=\r\ns tested, but with different geometric\n&gt; &gt;&gt;&gt;&gt; representations of the proble=\r\nm. Overall, experiments show that the quality\n&gt; &gt;&gt;&gt;&gt; and kind of solutions =\r\nproduced by HyperNEAT can be substantially affected\n&gt; &gt;&gt;&gt;&gt; by\n&gt; &gt;&gt;&gt;&gt; the ge=\r\nometric representation. HyperNEAT outperforms a direct encoding\n&gt; &gt;&gt;&gt;&gt; cont=\r\nrol even with randomized geometric representations, but performs even\n&gt; &gt;&gt;&gt;=\r\n&gt; better when a human engineer designs a representation that reflects the\n&gt;=\r\n &gt;&gt;&gt;&gt; actual geometry of the robot. Unfortunately, even choices in geometri=\r\nc\n&gt; &gt;&gt;&gt;&gt; layout that seem to be inconsequential a priori can significantly =\r\naffect\n&gt; &gt;&gt;&gt;&gt; fitness. Additionally, a geometric representation can bias th=\r\ne type of\n&gt; &gt;&gt;&gt;&gt; solutions generated (e.g., make left-right symmetry more c=\r\nommon than\n&gt; &gt;&gt;&gt;&gt; front-back symmetry). The results suggest that HyperNEAT =\r\npractitioners can\n&gt; &gt;&gt;&gt;&gt; obtain good results even if they do not know how t=\r\no geometrically represent\n&gt; &gt;&gt;&gt;&gt; a problem, and that further improvements a=\r\nre possible with a well-chosen\n&gt; &gt;&gt;&gt;&gt; geometric representation.\n&gt; &gt;&gt;&gt;&gt; \n&gt; &gt;=\r\n&gt;&gt;&gt; \n&gt; &gt;&gt;&gt;&gt; \n&gt; &gt;&gt;&gt;&gt; As always, I look forward to any questions or comments =\r\nyou may have.\n&gt; &gt;&gt;&gt;&gt; \n&gt; &gt;&gt;&gt;&gt; Cheers,\n&gt; &gt;&gt;&gt;&gt; Jeff Clune\n&gt; &gt;&gt;&gt;&gt; \n&gt; &gt;&gt;&gt;&gt; Digit=\r\nal Evolution Lab, Michigan State University\n&gt; &gt;&gt;&gt;&gt; \n&gt; &gt;&gt;&gt;&gt; jclune@\n&gt; &gt;&gt;&gt;&gt; \n=\r\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt; \n&gt; &gt; \n&gt; &gt;\n&gt;\n\n\n\n"}}