{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Ken","from":"&quot;Ken&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"vqjtFQS9xsi4STtOU6B54VnJD7HDHvSJh5oW1pg-2TbS_3ZeCQmyROD2qYTRLHByQfJK8WfqwxtBUkvtr-WC9H4zyKrt","spamInfo":{"isSpam":false,"reason":"3"},"subject":"Re: New paper on why modules evolve, and how to evolve modular artificial neural networks","postDate":"1360530473","msgId":5985,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGtmOTI3OSs4Y3IxQGVHcm91cHMuY29tPg=="},"prevInTopic":5984,"nextInTopic":5986,"prevInTime":5984,"nextInTime":5986,"topicId":5976,"numMessagesInTopic":30,"msgSnippet":"I realized that my thoughts on biasing search were a little misleading in a couple ways so I wanted to clarify a couple points.  First, I didn t do a good job","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 91772 invoked from network); 10 Feb 2013 21:07:55 -0000\r\nX-Received: from unknown (10.193.84.131)\n  by m9.grp.bf1.yahoo.com with QMQP; 10 Feb 2013 21:07:55 -0000\r\nX-Received: from unknown (HELO ng10-vm5.bullet.mail.gq1.yahoo.com) (98.136.219.122)\n  by mta4.grp.bf1.yahoo.com with SMTP; 10 Feb 2013 21:07:54 -0000\r\nX-Received: from [98.137.0.87] by ng10.bullet.mail.gq1.yahoo.com with NNFMP; 10 Feb 2013 21:07:53 -0000\r\nX-Received: from [10.193.94.41] by tg7.bullet.mail.gq1.yahoo.com with NNFMP; 10 Feb 2013 21:07:53 -0000\r\nDate: Sun, 10 Feb 2013 21:07:53 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;kf9279+8cr1@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 2:3:4:0:0\r\nFrom: &quot;Ken&quot; &lt;kstanley@...&gt;\r\nSubject: Re: New paper on why modules evolve, and how to evolve modular artificial neural networks\r\nX-Yahoo-Group-Post: member; u=54567749; y=SWWtPFLTvYtHeAs9eSJM5q2BItlHvCij2zsCllSji9Z36PB2GlW3\r\nX-Yahoo-Profile: kenstanley01\r\n\r\n\n\nI realized that my thoughts on biasing search were a little misleading in=\r\n a couple ways so I wanted to clarify a couple points.  First, I didn&#39;t do =\r\na good job acknowledging that the multiobjective framework does help to mit=\r\nigate the danger of manipulating fitness.  I shouldn&#39;t have used the term &quot;=\r\nfitness penalty&quot; because that sounds like it&#39;s not a separate objective.  \n=\r\n\nThe other thing is that I should have elaborated when I said that using th=\r\ne encoding to bias the search is a good idea &quot;if you can figure out a way t=\r\no do it.&quot;  The problem is that often it will not be entirely unclear how th=\r\ne encoding can be manipulated to encourage a certain type of behavior or pr=\r\noperty, so it&#39;s not always an option.  Sometimes fitness may be the only op=\r\ntion, so it&#39;s not something that can simply be dismissed, and I think my pr=\r\nior post made it sound like I was being dismissive.\n\nBut hopefully my gener=\r\nal point is still useful, because at least when it comes to modularity, it =\r\nis possible to encourage it through the encoding (and arguably that&#39;s how i=\r\nt happened in nature).  And also, while multiobjective optimization does re=\r\nduce deception to some extent, the problem is still there.  For example, if=\r\n you have a performance objective in a task and a low connectivity objectiv=\r\ne, then if you have someone with (high performance, low connectivity) and s=\r\nomeone with (equally high performance, higher connectivity), the one with h=\r\nigher connectivity will be dominated, even in the multiobjective case, whic=\r\nh means that if it is a deceptive stepping stone, we will not be less likel=\r\ny to explore it.  You also still maintain this &quot;dead-weight&quot; subpopulation =\r\nin the multiobjective case, because a subpopulation can survive by having e=\r\nxtremely low connectivity (yet performing terribly) and thereby dominating =\r\non the low connectivity objective.  That subpopulation will never go away. =\r\n \n\nSo the issues are still there, but I didn&#39;t do the best job acknowledgin=\r\ng the mitigating factors.  At the least, while you could reasonably argue f=\r\nor using fitness to provide a bias in some cases, it&#39;s helpful to think abo=\r\nut whether encoding bias is a viable alternative in any particular case.\n\nB=\r\nest,\n\nken\n\n--- In neat@yahoogroups.com, &quot;Ken&quot;  wrote:\n&gt;\n&gt; \n&gt; \n&gt; I wanted to=\r\n follow up on Alex&#39;s second question, on whether any approach biased to low=\r\n connectivity could work. I think that&#39;s a really interesting question with=\r\n a lot of deep implications and deserves some discussion.\n&gt; \n&gt; First let me=\r\n echo others&#39; congratulations to Jeff, Jean-Baptiste, and Hod for this impo=\r\nrtant publication, which dispels a lot of myths about the origins of modula=\r\nrity.  Jeff has heard some of what I&#39;m posting here before in our own priva=\r\nte discussions, but it&#39;s something I think worth sharing on this group as w=\r\nell given the importance of the issue.\n&gt; \n&gt; In particular, the question of =\r\nhow to bias a search is significant not only for biasing towards low connec=\r\ntivity (for the purpose of producing modularity), but potentially for all k=\r\ninds of different properties we might realize are important in the future. =\r\n And I believe there are two primary options:  (1) Use the fitness function=\r\n to bias the search, as Jeff et al. do; or (2) use the encoding to bias the=\r\n search.  That is, in the second option, you would not manipulate the fitne=\r\nss function in any way, but instead do something to the genetic encoding to=\r\n make it more likely to produce the phenotypic properties you want to see. =\r\n In the case of low connectivity, one example of such a genetic bias is thr=\r\nough an additional link expression output (LEO) on CPPNs in HyperNEAT, whic=\r\nh was introduced by Philip Verbancsics and myself:\n&gt; \n&gt; http://eplex.cs.ucf=\r\n.edu/publications/2011/verbancsics-gecco11\n&gt; \n&gt; To bias towards low connect=\r\nivity with the LEO, you start with a CPPN that tends to express short conne=\r\nctions (instead of long ones), which keeps connectivity down overall, and a=\r\nlso thereby seemed to help lead to modular structures.  \n&gt; \n&gt; But like I sa=\r\nid there is a larger issue here than just modularity or low connectivity.  =\r\nThe issue is how you should implement a search bias of any type.  And I bel=\r\nieve that doing it by manipulating the fitness function is generally danger=\r\nous in the long run.  Like with many ideas in EC, it is likely to work well=\r\n for relatively simple problems, but the more we aim for high complexity, t=\r\nhe more I think it will burden the search with unintended consequences that=\r\n increasingly warp the search off the best path.  \n&gt; \n&gt; As a simple example=\r\n, if you directly manipulate fitness to be lower when connectivity is high,=\r\n then you create a &quot;niche&quot; for networks of low connectivity that otherwise =\r\ndo nothing of any substance because they will be able to survive by having =\r\na higher fitness than networks of high connectivity that also do nothing of=\r\n any substance.  This niche of non-functional low-connectivity networks is =\r\nessentially a permanent dead-weight in the population that will last foreve=\r\nr.    Maybe it&#39;s not enough to kill you in simple problems, but in complex =\r\nproblems it&#39;s something you probably can&#39;t afford.\n&gt; \n&gt; More generally the =\r\nissue is the usual problem of deception, which is compounded by anything yo=\r\nu do with fitness.  For example, in a complex search space, there is a reas=\r\nonable chance that the stepping stone to a good low-connectivity solution i=\r\ns something with higher connectivity.  By manipulating fitness, you are cut=\r\nting out all chances of encountering such a deceptive stepping stone.  But =\r\neven if you don&#39;t believe that could be true, the single-mindedness of alwa=\r\nys favoring low-connectivity could deceive you from many parts of the searc=\r\nh space that might be stepping stones to something worthwhile, relating to =\r\nconnection density or not.\n&gt; \n&gt; On the other hand, manipulating the encodin=\r\ng is different because in effect it actually reorganizes the structure of t=\r\nhe search space itself, which seems to me a more principled thing to do (if=\r\n you can figure out a way to do it).  Because the thing is, in that case, y=\r\nou do not need to worry about a permanent dead weight taking up some propor=\r\ntion of your population forever.  Instead, while the encoding may *tend* to=\r\n produce e.g. low-connectivity solutions, it can still escape that tendency=\r\n without any penalty to fitness.  Furthermore, in reality the best situatio=\r\nn regarding modularity and connectivity is probably rather subtle, with mos=\r\nt of the brain respecting the principle of low connectivity, but with a num=\r\nber of critical exceptions in key areas, such as major inter-module hubs.  =\r\nA sophisticated encoding can allow its bias to bend to make such nuanced ex=\r\nceptions (e.g. based on locations within a geometry), whereas a fitness pen=\r\nalty is a heavy hand and blunt instrument that cannot but help always to de=\r\nmand global and holistic subservience to dogmatic universals (unless you ar=\r\ne willing to take a hit in fitness).\n&gt; \n&gt; An interesting question in nature=\r\n (where our brains evolved modular structure) is whether its tendency towar=\r\nds low connectivity is a result of an aspect of fitness in the wild, or an =\r\naspect of encoding bias.  I think there is a lot of room in this question f=\r\nor arguing either way, but my hunch is that the bias is mostly in the encod=\r\ning.  My logic is that I think the reason that the connectivity of the brai=\r\nn is so much lower than what it could be (e.g. it is a tiny fraction of eve=\r\nrything-to-everything connectivity) is an artifact of physics rather than a=\r\nn artifact of fitness.  It is simply physically impossible for a giant 100-=\r\nbillion-to-100-billion connectivity to fit in a head anything close to our =\r\nsize.  And physical impossibility is in some sense a property of encoding. =\r\n That is, mutations that could step from a low-connectivity brain to a high=\r\n one are few and far between simply because of physical constraint.  So hig=\r\nh-connectivity structures are simply a very small part of the search space =\r\nof brains in the physical universe.  However, at the same time, you can sti=\r\nll get long-range connections from time to time because there is no univers=\r\nal penalty for doing so, just a lower a priori probability of such mutation=\r\ns occurring.\n&gt; \n&gt; In summary, the key difference between the alternatives i=\r\ns that with fitness you are saying &quot;stay out of this part of the search spa=\r\nce&quot; whereas with encoding you are saying &quot;this part of the search space is =\r\nmuch smaller and hence less likely to encounter.&quot;\n&gt; \n&gt; So, my speculation i=\r\ns that if you want to bias the search in highly complex domains, the best w=\r\nay is through the encoding.  Fitness is a nasty quagmire that is deceptivel=\r\ny tempting to manipulate, but never plays by the rules you wish it would.  =\r\nOf course, these are merely my own unproven intuitions and their veracity r=\r\nemains to be demonstrated.  But at least it&#39;s something to think about.\n&gt; \n=\r\n&gt; Best,\n&gt; \n&gt; ken\n&gt; \n&gt; --- In neat@yahoogroups.com, Alexandre Devert  wrote:=\r\n\n&gt; &gt;\n&gt; &gt; Hi,\n&gt; &gt; \n&gt; &gt; =C2=A0 Simple, clean experiment, with sharp results, =\r\ncongrats on that, definitely\n&gt; &gt; a step forward ! Of course, it begs for mo=\r\nre questions. I would love to hear\n&gt; &gt; you on such (fairly open) questions\n=\r\n&gt; &gt; \n&gt; &gt; =C2=A0 =C2=A01) Do you think that selection pressure for low conne=\r\nctivity is sufficient in\n&gt; &gt; itself to evolve large coherent networks, or i=\r\ns it just a piece of the puzzle ?\n&gt; &gt; =C2=A0 =C2=A02) Do you see your work =\r\nas an indication that any approach biased to low\n&gt; &gt; connectivity would rep=\r\nroduce the result ? Or does the way you guys enforced\n&gt; &gt; this bias matters=\r\n ?\n&gt; &gt; \n&gt; &gt; To me=C2=A0\n&gt; &gt; 1) =3D&gt; Part of the puzzle. Should see how well=\r\n it scales for increasingly\n&gt; &gt; complex task, when the connection graph get=\r\ns bigger. A randomized=C2=A0\n&gt; &gt; search process=C2=A0on large graph sounds =\r\nnot so efficient, need something to guide it.\n&gt; &gt; I advocate construction p=\r\nrocess that have a feedback from what the neuron=C2=A0\n&gt; &gt; network is compu=\r\nting. Don&#39;t know how to do it without creepling computational\n&gt; &gt; cost tho.=\r\n..\n&gt; &gt; 2) =3D&gt; I guess that the bias alone is enough, the way to introduce =\r\nit might\n&gt; &gt; not be such a big deal.=C2=A0\n&gt; &gt; \n&gt; &gt; Again, great work, very=\r\n helpful contribution :)\n&gt; &gt; \n&gt; &gt; Alex\n&gt; &gt; =C2=A0\n&gt; &gt; Dr. Devert Alexandre\n=\r\n&gt; &gt; Researcher at the Nature Inspired Computation and Applications Laborato=\r\nry (NICAL)\n&gt; &gt; Lecturer at School Of Software Engineering of USTC\n&gt; &gt; -----=\r\n-----------------------------------------------\n&gt; &gt; Homepage :=C2=A0http://=\r\nwww.marmakoide.org\n&gt; &gt; ----------------------------------------------------=\r\n\n&gt; &gt; 166 Renai Road, Dushu Lake Higher Education Town\n&gt; &gt; Suzhou Industrial=\r\n Park,\n&gt; &gt; Suzhou, Jiangsu, People&#39;s Republic of China\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; ______=\r\n__________________________\n&gt; &gt;  From: Jeff Clune \n&gt; &gt; To: neat users group =\r\ngroup  \n&gt; &gt; Cc: Jean-Baptiste Mouret ; Hod Lipson  \n&gt; &gt; Sent: Thursday, Feb=\r\nruary 7, 2013 1:57 AM\n&gt; &gt; Subject: [neat] New paper on why modules evolve, =\r\nand how to evolve modular artificial neural networks\n&gt; &gt;  \n&gt; &gt; \n&gt; &gt; =C2=A0 =\r\n\n&gt; &gt; Hello all,\n&gt; &gt; \n&gt; &gt; I&#39;m extremely pleased to announce a new paper on a=\r\n subject that many--including myself--think is critical to making significa=\r\nnt progress in our field: the evolution of modularity.=C2=A0\n&gt; &gt; \n&gt; &gt; Jean-=\r\nBaptiste Mouret, Hod Lipson and I have a new paper that=C2=A0\n&gt; &gt; \n&gt; &gt; 1) s=\r\nheds light on why modularity may evolve in biological networks (e.g. neural=\r\n, genetic, metabolic, protein-protein, etc.)\n&gt; &gt; \n&gt; &gt; 2) provides a simple =\r\ntechnique for evolving neural networks that are modular and have increased =\r\nevolvability, in that they adapt faster to new environments. The modules th=\r\nat formed solved subproblems in the domain.=C2=A0\n&gt; &gt; Cite:=C2=A0Clune J, M=\r\nouret J-B, Lipson H (2013) The evolutionary origins of modularity. Proceedi=\r\nngs of the Royal Society B. 280: 20122863.=C2=A0http://dx.doi.org/10.1098/r=\r\nspb.2012.2863=C2=A0(pdf)\n&gt; &gt; \n&gt; &gt; Abstract: A central biological question i=\r\ns how natural organisms are so evolvable (capable of quickly adapting to ne=\r\nw environments). A key driver of evolvability is the widespread modularity =\r\nof biological networks=E2=80&quot;their organization as functional, sparsely con=\r\nnected subunits=E2=80&quot;but there is no consensus regarding why modularity it=\r\nself evolved. Although most hypotheses assume indirect selection for evolva=\r\nbility, here we demonstrate that the ubiquitous, direct selection pressure =\r\nto reduce the cost of connections between network nodes causes the emergenc=\r\ne of modular networks. Computational evolution experiments with selection p=\r\nressures to maximize network performance and minimize connection costs yiel=\r\nd networks that are significantly more modular and more evolvable than cont=\r\nrol experiments that only select for performance. These results will cataly=\r\nse research in numerous disciplines, such as neuroscience and genetics, and=\r\n enhance our ability to harness\n&gt; &gt;  evolution for engineering purposes.\n&gt; =\r\n&gt; \n&gt; &gt; Video:=C2=A0http://www.youtube.com/watch?feature=3Dplayer_embedded&v=\r\n=3DSG4_aW8LMng\n&gt; &gt; \n&gt; &gt; There has been some nice coverage of this work in t=\r\nhe popular press, in case you are interested:\n&gt; &gt; National Geographic:=C2=\r\n=A0http://phenomena.nationalgeographic.com/2013/01/30/the-parts-of-life/MIT=\r\n&#39;s Technology Review:=C2=A0http://www.technologyreview.com/view/428504/comp=\r\nuter-scientists-reproduce-the-evolution-of-evolvability/=C2=A0Fast Company:=\r\n=C2=A0http://www.fastcompany.com/3005313/evolved-brains-robots-creep-closer=\r\n-animal-learningCornell Chronicle:=C2=A0http://www.news.cornell.edu/stories=\r\n/Jan13/modNetwork.htmlScienceDaily:=C2=A0http://www.sciencedaily.com/releas=\r\nes/2013/01/130130082300.htm\n&gt; &gt; \n&gt; &gt; Please let me know what you think and =\r\nif you have any questions. I hope this work will help our field move forwar=\r\nd!\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; Best regards,\n&gt; &gt; Jeff Clune\n&gt; &gt; \n&gt; &gt; Assistant =\r\nProfessor\n&gt; &gt; Computer Science\n&gt; &gt; University of Wyoming\n&gt; &gt; jclune@\n&gt; &gt; je=\r\nffclune.com\n&gt; &gt;\n&gt;\n\n\n\n"}}