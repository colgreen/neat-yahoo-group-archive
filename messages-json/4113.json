{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":345796568,"authorName":"peterberrington","from":"&quot;peterberrington&quot; &lt;peterberrington@...&gt;","profile":"peterberrington","replyTo":"LIST","senderId":"mSHKs7hodNvKQ_Tmv4ROR-T4jhEG9sK504rS_4nvBRUCWiq06eSMcLylYSLJFGtSpJLd1sBZXUtwWHcD3NrosL0CRrp-FsTgHHsnLESmIVU4Jko","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Can novelty search be treated as an optimization technique?","postDate":"1211722576","msgId":4113,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGcxYnEwZytubTlsQGVHcm91cHMuY29tPg=="},"prevInTopic":0,"nextInTopic":4115,"prevInTime":4112,"nextInTime":4114,"topicId":4113,"numMessagesInTopic":6,"msgSnippet":"I ve been tinkering with many optimization techniques and considering the possibility of integrating some with the mechanics of neat, in the vein of EANT,","rawEmail":"Return-Path: &lt;peterberrington@...&gt;\r\nX-Sender: peterberrington@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 12924 invoked from network); 25 May 2008 13:36:18 -0000\r\nX-Received: from unknown (66.218.67.94)\n  by m55.grp.scd.yahoo.com with QMQP; 25 May 2008 13:36:18 -0000\r\nX-Received: from unknown (HELO n5.bullet.mail.re1.yahoo.com) (69.147.103.132)\n  by mta15.grp.scd.yahoo.com with SMTP; 25 May 2008 13:36:18 -0000\r\nX-Received: from [68.142.237.89] by n5.bullet.mail.re1.yahoo.com with NNFMP; 25 May 2008 13:36:18 -0000\r\nX-Received: from [66.218.69.3] by t5.bullet.re3.yahoo.com with NNFMP; 25 May 2008 13:36:18 -0000\r\nX-Received: from [66.218.66.88] by t3.bullet.scd.yahoo.com with NNFMP; 25 May 2008 13:36:17 -0000\r\nDate: Sun, 25 May 2008 13:36:16 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;g1bq0g+nm9l@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: &quot;peterberrington&quot; &lt;peterberrington@...&gt;\r\nSubject: Can novelty search be treated as an optimization technique?\r\nX-Yahoo-Group-Post: member; u=345796568; y=UNlA2g9GvTdG7Lgq4UXlYrg-rvYsFt6zq7N4G0AzeLaoOHlvQFhcpnSJ\r\nX-Yahoo-Profile: peterberrington\r\n\r\nI&#39;ve been tinkering with many optimization techniques and considering\nthe p=\r\nossibility of integrating some with the mechanics of neat, in the\nvein of E=\r\nANT, which sadly has no open source implementation. \n\nWhile I&#39;m trying to a=\r\ndd that functionality to the neat-python\nimplementation I use, I&#39;m really w=\r\nondering if its at all applicable to\nnovelty search. In novelty search, the=\r\nre is a specific pressure to do\nsomething new and indeed we do assign a num=\r\neric novelty value to each\nindividual behaviour at the time of its evaluati=\r\non for addition to the\narchive. However, the score an individual receives d=\r\nepends on who its\nup against, so its not objective. Without a function to m=\r\ninimize, the\ndynamics which allow optimization techniques to work may be da=\r\nmaged to\nthe point of rendering it no better than random search. \n\nI&#39;m stil=\r\nl puzzled on the discussion Peter C raised about reevaluating\nthe novelty o=\r\nf behaviours in the archive at future intervals.\nShouldn&#39;t the novelty scor=\r\ne assigned to each point be useless after\nwe&#39;ve decided whether or not to a=\r\nrchive the behaviour? I&#39;m not sure\nthat joint angles over every timestep is=\r\n really the best way the\ncharacterize behaviour for a 3d ragdoll rig, as yo=\r\nu are essentially\nproviding no clear way to distinguish a good behaviour fr=\r\nom a bad\nbehaviour. Although I haven&#39;t been able to use optimized libraries=\r\n\nwith my current simulation configuration (this is hopefully changing\nvery =\r\nsoon), I have noticed a significant boost in speed with my\nragdoll evolutio=\r\nn by defining behaviour simply as the final x y z\ntriplet for the center of=\r\n mass. In this way the search quickly\nexhausts all the easy ways of falling=\r\n close to its origin and pressure\nmounts for it explore end locations succe=\r\nssively farther and farther\naway from its origin; in essence the objective =\r\nfunction is realized in\nthat characterization of behavioural distance. \n\nIn=\r\n any case, we are explicitly trying to reward novelty by selecting\nfor furt=\r\nher evaluation any individuals which satisfy a minimum\nthreshold of how dif=\r\nferent their behaviour is. Something about that\ngives me a feeling that may=\r\nbe optimization techniques are applicable,\nbut only if the dynamics of opti=\r\nmization can be adapted to the\nunwieldy workings of novelty search.\n\nRather=\r\n than maximizing fitness, optimization techniques usually\nminimize an objec=\r\ntive function so the fitness scale is simply reversed\nwith a perfect score =\r\nbeing 0.0\nSince in this domain theres no such thing as perfect novelty, can=\r\n\nanyone provide any insights as to a framework where optimization\ntechnique=\r\ns can be harnessed to maximize novelty? I feel this is a\nreally important a=\r\nrea to focus attention on, as it could potentially\nlead to a dramatic incre=\r\nase in the fitness per number of evaluations.\n\nAs some background: I&#39;ve lon=\r\ng been tempted after reading papers on\nEANT to try dropping in a more advan=\r\nced optimization function into\nNEAT, like some of the new variations on the=\r\n covariance matrix\nadaption evolution strategy. The way this was implemente=\r\nd in EANT was\nby splitting NEATs main loop into a &quot;structural exploration&quot; =\r\nloop for\nbuilding networks, and optimizing the weight connection values\nsep=\r\narately within a nested loop, using CMA-ES. This division of work\npermits t=\r\nhe net to be treated as an n-dimensional equation where n is\nthe number of =\r\nconnection or node weights (or other properties) we wish\nto optimize (i.e. =\r\neverything that isn&#39;t defining the topology of the\nnet). I think it can be =\r\nargued that standard fitness based neat is a\ngreedier approach because it c=\r\nartwheels through topological space and\nweight parameter space at the same =\r\ntime. I want to plug in an\noptimization function, but I can&#39;t think of how =\r\nanymore, or even if\nthe two search techniques are reconcilable. \n\nTheres no=\r\n point optimizing within a specific net topology in novelty\nsearch because =\r\nyou want to kind of push out and fill behavioural space\nas evenly as possib=\r\nle, so theres no nook or crack which escapes the\npoking and prodding of you=\r\nr search; without the\ncompetition/coevolutionary aspect of defining fitness=\r\n against the\nbackdrop of current rivals and ancestors, I doubt you&#39;d see an=\r\nything\nmore effective than random selection. If you can&#39;t optimize each net=\r\n\ntopology in a vacuum, is there another way fitness-based searches and\nnove=\r\nlty-based searches can reach a compromise where each complements\nthe other?=\r\n\n\nI&#39;ve given some thought to the idea of phased searching, but you would\nha=\r\nve to find some way of defining stopping critera for when you&#39;d want\nto opt=\r\nimize an objective function and when you&#39;d want to diversify\nfrom bottom up=\r\n complexity wise. \n\n\nAs an interesting side note, to anyone whos had a snea=\r\nk peak at Peter\nC&#39;s maze navigation novelty search program, isn&#39;t it uncann=\r\ny, the\nresemblance between the past behaviours that have accumulated, and t=\r\nhe\ngrowth of a plant?  As more organisms fill up the space of possible\nbeha=\r\nviours its tempting to imagine it as molasses which slowly oozes\nits way in=\r\nto every crack and crevice it can reach. Perhaps there is a\nuseful insight =\r\nto be drawn from that, I have no idea, I just thought\nit looked very &quot;organ=\r\nic&quot;. \n\n\n"}}