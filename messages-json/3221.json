{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":151231063,"authorName":"Joseph Reisinger","from":"Joseph Reisinger &lt;joeraii@...&gt;","profile":"joeraii","replyTo":"LIST","senderId":"nHLXphW3BKu2u68RrkP_L39TTpETAOyUqhxEyFy9lCxZqzHsIXFj7A-fIugnojqxzj3YvR2r5MHUY17WfkOxSfwlTe0PIeymkc5SlZrnJQ","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] Re: HyperNEAT and No Free Lunch","postDate":"1177916664","msgId":3221,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDExOEIyRDVFLTFENkEtNDgwQy05MkI0LTlEMUQwNzAyNDdERUBjcy51dGV4YXMuZWR1Pg==","inReplyToHeader":"PGYxM3JyYit0OGc0QGVHcm91cHMuY29tPg==","referencesHeader":"PGYxM3JyYit0OGc0QGVHcm91cHMuY29tPg=="},"prevInTopic":3220,"nextInTopic":3222,"prevInTime":3220,"nextInTime":3222,"topicId":3214,"numMessagesInTopic":27,"msgSnippet":"... Sure, but I m still not sure if I ve convinced you that your original application of NFL was incorrect. I only bring the issue up because I think that we","rawEmail":"Return-Path: &lt;joeraii@...&gt;\r\nX-Sender: joeraii@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 56641 invoked from network); 30 Apr 2007 07:04:53 -0000\r\nReceived: from unknown (66.218.66.68)\n  by m51.grp.scd.yahoo.com with QMQP; 30 Apr 2007 07:04:53 -0000\r\nReceived: from unknown (HELO spunkymail-a9.g.dreamhost.com) (208.97.132.83)\n  by mta11.grp.scd.yahoo.com with SMTP; 30 Apr 2007 07:04:53 -0000\r\nReceived: from [10.0.0.115] (72-48-75-214.dyn.grandenetworks.net [72.48.75.214])\n\tby spunkymail-a9.g.dreamhost.com (Postfix) with ESMTP id 6066820E8E\n\tfor &lt;neat@yahoogroups.com&gt;; Mon, 30 Apr 2007 00:04:27 -0700 (PDT)\r\nMime-Version: 1.0 (Apple Message framework v752.2)\r\nIn-Reply-To: &lt;f13rrb+t8g4@...&gt;\r\nReferences: &lt;f13rrb+t8g4@...&gt;\r\nContent-Type: text/plain; charset=US-ASCII; delsp=yes; format=flowed\r\nMessage-Id: &lt;118B2D5E-1D6A-480C-92B4-9D1D070247DE@...&gt;\r\nContent-Transfer-Encoding: 7bit\r\nDate: Mon, 30 Apr 2007 02:04:24 -0500\r\nTo: neat@yahoogroups.com\r\nX-Mailer: Apple Mail (2.752.2)\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Joseph Reisinger &lt;joeraii@...&gt;\r\nSubject: Re: [neat] Re: HyperNEAT and No Free Lunch\r\nX-Yahoo-Group-Post: member; u=151231063; y=WNPnJctBIL_A_A0jbrxrrmNGx7uhZeeDInLwW_OX32Shwg\r\nX-Yahoo-Profile: joeraii\r\n\r\n\n&gt; Joe, I am willing to go with this:\n&gt;\n&gt;&gt; At the very least, you should limit\n&gt;&gt; your statement just to problems where the geometry is known /to\n&gt;&gt; the experimenter.  In the case where the experimenter knows the\n&gt;&gt; geometry a priori, and he is correct in that knowledge, then yes,\n&gt;&gt; HyperNEAT would outperform other Non-Hyper algorithms in the NFL\n&gt;&gt; sense.\n&gt;\n&gt; However, I think we&#39;re kind of splitting our theoretical hairs here\n&gt; more than necessary.  Sure, if the issue is that we need to take all\n&gt; possible precaution to convey that we understand NFL to the letter,\n&gt; then we can go down this road and optimize our statement to avoid any\n&gt; possible misunderstanding.  And that may be important in some\n&gt; contexts.  Yet I feel it is a bit of a distraction from what&#39;s really\n&gt; important about what I&#39;m saying.  So let me try it in more layman&#39;s\n&gt; terms, avoiding mention of NFL:\n\nSure, but I&#39;m still not sure if I&#39;ve convinced you that your original  \napplication of NFL was incorrect. I only bring the issue up because I  \nthink that we /do/ need to formalize some of what we&#39;re talking  \nabout, at least so we can get a better handle on it conceptually. You  \ncan see why just from this conversation we&#39;ve had: You have some  \nintuition about why HyperNEAT is useful, but when you try to pin it  \ndown more theoretically, things get really complicated.  I feel that  \nif we do spend some time &quot;splitting hairs&quot; theoretically, we might  \nactually get to something really interesting. Like a more flexible  \nframework than NFL that describes why HyperNEAT could possibly be  \nbetter than NEAT. The way you were trying to describe the difference  \nis an excellent point! Its quite novel, but too subtle to fit in the  \nold NFL framework. Thats what I&#39;m trying to convince you of. Instead  \nof trying to shoehorn the subject into old formalisms, it might be  \nbetter if we just came up with a new formalism ourselves.\n\n&gt; That&#39;s the important point.\n\nDefinitely! And I agree that it is under appreciated in ML at large.  \nAlthough it may not be as under-appreciated as you think. There are  \nfew examples I can think of from RL, mainly Sherstov and Stone&#39;s  \nknowledge transfer work, and also Shimon&#39;s adaptive tile coding work.\n\n-- Joe\n\n&gt;\n&gt; ken\n&gt;\n&gt;\n&gt;\n&gt; --- In neat@yahoogroups.com, Joseph Reisinger &lt;joeraii@...&gt; wrote:\n&gt;&gt;\n&gt;&gt; (I&#39;ve changed the thread title so that this post will be easier to\n&gt;&gt; ignore.)\n&gt;&gt;\n&gt;&gt; Sorry for dragging us deeper into a discussion of NFL, but I think\n&gt;&gt; this point is really important, especially if you are going to be\n&gt;&gt; making these claims in front of a broader audience. Also I think\n&gt; it\n&gt;&gt; is helpful for the audience here to be crystal clear on the NFL\n&gt;&gt; issue. Its a very important theorem and it has been quite often\n&gt;&gt; misapplied (for instance, there is some evidence that Wolpert\n&gt;&gt; initially came up with NFL specifically to critique GAs).\n&gt;&gt;\n&gt;&gt;&gt; That&#39;s why I said it &quot;may&quot; be better for evolving very large\n&gt; scale\n&gt;&gt;&gt; brains.\n&gt;&gt;&gt; The &quot;may&quot; hinges on the issues you bring up and others.  However,\n&gt;&gt;&gt; even the opportunity to be better is an advantage over having no\n&gt; such\n&gt;&gt;&gt; opportunity.  The opportunity of course can be squandered with the\n&gt;&gt;&gt; wrong a priori knowledge.   Yet part of my point is that it will\n&gt;&gt;&gt; often be the case that the natural geometry of a task is all you\n&gt; need\n&gt;&gt;&gt; to provide a powerful bias (or at least a bias that is better than\n&gt;&gt;&gt; nothing), so it will often be possible to seize the opportunity\n&gt;&gt;&gt; without a great deal of effort.\n&gt;&gt;\n&gt;&gt; Ok, note your use of the word &quot;often&quot; instead of the\n&gt; word &quot;always.&quot;\n&gt;&gt; Just that word substitution means that you aren&#39;t within the realm\n&gt; of\n&gt;&gt; situations that NFL covers. I totally agree with you that the\n&gt;&gt; application of such geometry can be useful. I just don&#39;t think you\n&gt;&gt; can leverage NFL to back up the argument you are making.\n&gt;&gt;\n&gt;&gt;&gt; It is true too that NEAT and other neural network algorithms do\n&gt;&gt;&gt; indeed allow for some inclusion of prior knowledge through their\n&gt;&gt;&gt; input/output encoding.  However, note how I phrased my\n&gt;&gt;&gt; claim: &quot;HyperNEAT is not subject to the No Free Lunch theorem when\n&gt;&gt;&gt; comparing to algorithms that do not allow injecting such a priori\n&gt;&gt;&gt; knowledge.&quot;  That is, among algorithms that allow you to decide\n&gt; on an\n&gt;&gt;&gt; input encoding in the traditional way, the provision of such\n&gt; encoding\n&gt;&gt;&gt; does not give one algorithm a leg up over another since they all\n&gt;&gt;&gt; allow for such knowledge to be included.  HyperNEAT, on the other\n&gt;&gt;&gt; hand, allows a new kind of knowledge (i.e. geometry) to be\n&gt; included\n&gt;&gt;&gt; and therefore does have a potential leg up on that class of\n&gt;&gt;&gt; algorithms.\n&gt;&gt;\n&gt;&gt; But this leg up would necessarily be different for each problem,\n&gt;&gt; hence my original critique. You can&#39;t build the experimenter into\n&gt; the\n&gt;&gt; system and still talk about NFL. Its just not applicable because\n&gt; you\n&gt;&gt; are no longer doing any generalization across classes of problems.\n&gt;&gt; Therefore you can&#39;t use it to build your case.\n&gt;&gt;\n&gt;&gt; Your argument runs something like: &quot;HyperNEAT has more parameters\n&gt;&gt; than the experimenter can set than other Non-Hyper NEATs that\n&gt; allow\n&gt;&gt; him/her to build in even better prior knowledge.&quot; But you are\n&gt;&gt; neglecting the fact that the experimenter now has more possible\n&gt;&gt; parameters to set incorrectly! At the very least, you should limit\n&gt;&gt; your statement just to problems where the geometry is known /to\n&gt; the\n&gt;&gt; experimenter/.  In the case where the experimenter knows the\n&gt; geometry\n&gt;&gt; a priori, and he is correct in that knowledge, then yes, HyperNEAT\n&gt;&gt; would outperform other Non-Hyper algorithms in the NFL sense.\n&gt;&gt;\n&gt;&gt;&gt; Of course it depends on how well the user takes\n&gt;&gt;&gt; advantage of the opportunity, but the opportunity is now there.\n&gt; This\n&gt;&gt;&gt; fact does indeed mean that statements about HyperNEAT vs. other\n&gt;&gt;&gt; neuroevolution (or even machine learning) algorithms can cite an\n&gt;&gt;&gt; opportunity to genuinely be better on average, which in effect\n&gt; brings\n&gt;&gt;&gt; it outside NFL in one particular sense.\n&gt;&gt;\n&gt;&gt; It can be better on average, if the experimenter always makes the\n&gt;&gt; correct choices w.r.t. the geometry. But I don&#39;t think you can use\n&gt;&gt; such &quot;oracle&quot; experimenters as a basis for comparison. And anyway,\n&gt;&gt; NFL still holds, in a broader sense: For example, imagine the\n&gt; class\n&gt;&gt; of problems that /seem/ to have an underlying geometry feature,\n&gt; lets\n&gt;&gt; call it &quot;A.&quot; There could possibly be instances of this class of\n&gt;&gt; problems where naively exploiting A would actually cause the\n&gt; learning\n&gt;&gt; algorithm to perform worse than not exploiting it. And in fact we\n&gt;&gt; could probably construct such a class.\n&gt;&gt;\n&gt;&gt; Again, to sum up, I think the way you are framing the benefits of\n&gt;&gt; HyperNEAT&#39;s geometry exploiting features rely solely on an\n&gt;&gt; intelligent experimenter, and in that sense do sort of bypass NFL.\n&gt;&gt; But making such a statement is meaningless, because it basically\n&gt;&gt; assumes that the experimenter always guesses right. And if I were\n&gt;&gt; that experimenter, then I wouldn&#39;t be here talking to you, I would\n&gt;&gt; have already solved strong AI :)\n&gt;&gt;\n&gt;&gt; Humbly,\n&gt;&gt;\n&gt;&gt; -- Joe\n&gt;&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt; Yahoo! Groups Links\n&gt;\n&gt;\n&gt;\n\n\n"}}