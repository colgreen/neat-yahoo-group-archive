{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":211599040,"authorName":"Jeff Clune","from":"Jeff Clune &lt;jclune@...&gt;","profile":"jeffreyclune","replyTo":"LIST","senderId":"vHErAWwFpGUbFiCSbuc3JBHVLE5Q77xMedRR83XXECEGDfGsHES6w2LFbN5oMRQOoe7FkpWIyVcJborKeRm-X8g9","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [neat] Re: HybrID: A Hybridization of Indirect and Direct Encodings for Evolutionary Computation","postDate":"1248716212","msgId":4790,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PEM2OTM1REY0LjJCOEM1JWpjbHVuZUBtc3UuZWR1Pg==","inReplyToHeader":"PGg0aW9zYitobXJ0QGVHcm91cHMuY29tPg=="},"prevInTopic":4786,"nextInTopic":4793,"prevInTime":4789,"nextInTime":4791,"topicId":4772,"numMessagesInTopic":19,"msgSnippet":"Hello Ken- Thanks for sharing your thoughts on this issue. I agree with you that one of the interesting things the HybrID paper does is get us thinking about","rawEmail":"Return-Path: &lt;jclune@...&gt;\r\nX-Sender: jclune@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 29304 invoked from network); 27 Jul 2009 17:38:01 -0000\r\nX-Received: from unknown (69.147.108.200)\n  by m2.grp.re1.yahoo.com with QMQP; 27 Jul 2009 17:38:01 -0000\r\nX-Received: from unknown (HELO mail-ew0-f217.google.com) (209.85.219.217)\n  by mta1.grp.re1.yahoo.com with SMTP; 27 Jul 2009 17:38:01 -0000\r\nX-Received: by ewy17 with SMTP id 17so1247308ewy.26\n        for &lt;neat@yahoogroups.com&gt;; Mon, 27 Jul 2009 10:37:00 -0700 (PDT)\r\nX-Received: by 10.216.19.141 with SMTP id n13mr1809680wen.47.1248716219560;\n        Mon, 27 Jul 2009 10:36:59 -0700 (PDT)\r\nReturn-Path: &lt;jclune@...&gt;\r\nX-Received: from ?10.0.1.2? (c-76-20-191-220.hsd1.mi.comcast.net [76.20.191.220])\n        by mx.google.com with ESMTPS id t2sm21139627gve.15.2009.07.27.10.36.57\n        (version=TLSv1/SSLv3 cipher=RC4-MD5);\n        Mon, 27 Jul 2009 10:36:58 -0700 (PDT)\r\nUser-Agent: Microsoft-Entourage/12.13.0.080930\r\nDate: Mon, 27 Jul 2009 13:36:52 -0400\r\nTo: &quot;neat@yahoogroups.com&quot; &lt;neat@yahoogroups.com&gt;\r\nMessage-ID: &lt;C6935DF4.2B8C5%jclune@...&gt;\r\nThread-Topic: [neat] Re: HybrID: A Hybridization of Indirect and Direct\n Encodings for Evolutionary Computation\r\nThread-Index: AcoO4NOo8R/It5oXskSmqNjiEG0+oA==\r\nIn-Reply-To: &lt;h4iosb+hmrt@...&gt;\r\nMime-version: 1.0\r\nContent-type: text/plain;\n\tcharset=&quot;US-ASCII&quot;\r\nContent-transfer-encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Jeff Clune &lt;jclune@...&gt;\r\nSubject: Re: [neat] Re: HybrID: A Hybridization of Indirect and Direct\n Encodings for Evolutionary Computation\r\nX-Yahoo-Group-Post: member; u=211599040; y=axy_1H1jlpdM5-2w07hnlCZIrzsg9BjnlOiO3CmtDpdveWsSlHOw\r\nX-Yahoo-Profile: jeffreyclune\r\n\r\nHello Ken-\n\nThanks for sharing your thoughts on this issue. I agree with you that one of\nthe interesting things the HybrID paper does is get us thinking about what\nwe want out of encodings.\n\nHere are a few reactions to your comments:\n\nInitially, I fully acknowledge (and even trumpet the fact) that HyperNEAT\ncan produce many types of variations on themes and irregularities. If I have\ngiven a different impression somewhere, that was unintentional. You are\nright that HyperNEAT just seems to have difficulties making *certain types*\nof exceptions, however I still think the exceptions it has trouble making\nare ones that we want it to be able to make.\n\nYou have convinced me (in previous conversations) that we do not need (nor\nwant) our encodings to have the ability to match an arbitrary target.\nHowever, the experimental results from the quadruped domain in the HybrID\npaper show that the ability to make exceptions boosts fitness, even in a\ncomplicated, real-world problem. That is the type of problems that nature\n(with DNA) did solve, so my guess is that we still have some work to do when\nit comes to creating encodings that simultaneously generate regularities and\nexceptions. I am guessing that the solutions HybrID produced that HyperNEAT\nalone did not produce were not the only possible solution (such as might be\ndescribed as a pre-set target), but instead were instances of a class of\nsolutions that were less-regular than the HyperNEAT-only gaits. To me that\nindicates that HyperNEAT is not only constrained from discovering arbitrary,\nad-hoc, academic irregularities, but instead has difficulty producing some\nirregularities which help on normal, everyday, complex problems.  My\ninstincts tell me that to get to true AI we are going to need to make these\ntypes of exceptions.\n\nYou raise the issue that getting better at making exceptions also makes us\nworse at creating regularities.  But is it necessarily the case that there\nis a non-zero-sum tradeoff in encoding regularities and exceptions? I can\nconceive of a different activation function set for HyperNEAT in which we\nadd functions that help it make exceptions in small areas of the geometric\nspace. This enlarged function set would still have all of the same power to\ngenerate regularities as the standard set, but it would also have the\nadditional ability to create exceptions. Especially if this activation\nfunction was rarely used (either because it mutated in at a lower rate, or\nbecause evolution chose to use it sparingly), the overall type of solutions\nmight not be biased back into the chaos of direct encodings. Clearly there\nis some bias back in that direction, but I am wondering (out loud) if it\nreally has to be non-zero-some in the sense that whatever you gain in\nexception-making you lose in regularity-making. Might an elegant solution\nget us a lot of one without too much loss of the other?\n\nIn fact, isn&#39;t HybrID an example of this? Mutation-selection balance aside,\nFT-NEAT should only make a change to a pattern HyperNEAT produced if the\nchange helps fitness. I realize HybrID is not the long-term solution (partly\nbecause of the scaling issues you mentioned), but isn&#39;t it at least an\nexample of how an algorithm can make advances on the exception-making front\nwithout losing much, if anything, on the producing-regularity front?\n\nThanks again for your interesting comments.\n\n\nCheers,\nJeff Clune\n\nDigital Evolution Lab, Michigan State University\n\njclune@...\n\n\n\n\n&gt; From: Kenneth Stanley &lt;kstanley@...&gt;\n&gt; Reply-To: &quot;neat@yahoogroups.com&quot; &lt;neat@yahoogroups.com&gt;\n&gt; Date: Sun, 26 Jul 2009 23:29:15 -0000\n&gt; To: &quot;neat@yahoogroups.com&quot; &lt;neat@yahoogroups.com&gt;\n&gt; Subject: [neat] Re: HybrID: A Hybridization of Indirect and Direct Encodings\n&gt; for Evolutionary Computation\n&gt; \n&gt; Jeff knows some of my thoughts on the issue of regularity vs. irregularity in\n&gt; neuroevolution and also that I recognize the nice results with HybrID, so this\n&gt; post is just my attempt to put out some thoughts on this important issue,\n&gt; rather than a challenge to the HybrID concept.  Basically, HybrID creates a\n&gt; good opportunity to discuss these issues.\n&gt; \n&gt; Taking a long term view, what I would like to question is the idea that there\n&gt; is a problem with a method that has &quot;trouble&quot; discovering arbitrary\n&gt; irregularities.  It important to note that HyperNEAT has no trouble\n&gt; representing &quot;irregularity&quot; in the general sense.  In my 2007 paper on CPPNs\n&gt; (http://eplex.cs.ucf.edu/hyperNEATpage/HyperNEAT.html), I included explicit\n&gt; examples of irregularities and how easy they are to represent.  For example,\n&gt; see the &quot;Warped Symmetry&quot; panel in figure 9 on page 24.  So I think the issue\n&gt; is definitively *not* that HyperNEAT (or more specifically, CPPNs with a\n&gt; certain set of activation functions) has trouble representing or discovering\n&gt; irregularity, but that it has trouble discovering *particular* irregularities.\n&gt; \n&gt; Yet such trouble may actually be a good thing.  If it were too easy to\n&gt; represent any arbitrary irregularity, then you would have an encoding that is\n&gt; poor at discovering regularities.  So this argument could go in circles.  But\n&gt; it&#39;s important to recognize that we are looking at a trade-off.  Nature faces\n&gt; the same trade-off.  It is difficult to say for sure whether DNA is biased\n&gt; towards regularity or irregularity, but I would guess looking at biological\n&gt; organisms that regularity reigns, and that irregularities are of certain\n&gt; *types*, that is, you would be hard pressed to &quot;breed&quot; a human with a hand\n&gt; protruding from his or her right knee, no matter how many generations were\n&gt; available.  Yet we see things like the heart on one side, right-handedness,\n&gt; etc.  But those do not mean that the encoding can simply bend to the arbitrary\n&gt; whims of a random target.\n&gt; \n&gt; So I think what you need if you want to evolve a really interesting artifact\n&gt; is not an encoding that is entirely flexible with respect to irregularity.  If\n&gt; a particular encoding would often fail to meet ad hoc irregular targets, that\n&gt; is probably a sign of its long term potential, rather something we&#39;d want to\n&gt; fix.  If we did &quot;fix&quot; it, we&#39;d be heading back towards the chaos and entropy\n&gt; of direct encoding.\n&gt; \n&gt; Therefore, we should be careful about whether we consider difficulty achieving\n&gt; specific irregularities a problem to solve or an asset in the long run.\n&gt; Certainly there should an ability to create &quot;repetition with variation,&quot; but\n&gt; CPPNs (as well as nature) exhibit that consistently.  So it&#39;s important to be\n&gt; careful what we consider to be a pathology versus an asset.\n&gt; \n&gt; That said, HybrID is a good practical idea.  It is clear that in some problems\n&gt; it is just what is needed to perfect solutions with particular irregular\n&gt; needs.  Of course, when the networks become really big, e.g. with millions of\n&gt; connections, HybrID might begin to lose some of its ability to cope with the\n&gt; very high dimensionality, even if it is just tweaking on top of preexisting\n&gt; regularities encoded by the CPPN.  Nevertheless, at lower dimensionalities, as\n&gt; Jeff&#39;s paper shows, it can really help.\n&gt; \n&gt; In any case, my main point is that the issue of what we actually *want* in an\n&gt; encoding with respect to regularity and irregularity is quite subtle and\n&gt; deserves considered contemplation.  The CPPN with the usual set of activation\n&gt; functions may not ultimately be the very best representation to bias towards\n&gt; what we want, but if there is something better, it would not be better by\n&gt; virtue of simply being able to capture irregularities more easily.  Rather,\n&gt; any advantage would be gained through a much more subtle argument, which\n&gt; likely refers to both regularities and irregularities of certain types, and\n&gt; why they are appropriate for the kinds of neural structures we hope to evolve.\n&gt; \n&gt; ken\n&gt; \n&gt; --- In neat@yahoogroups.com, Jeff Clune &lt;jclune@...&gt; wrote:\n&gt;&gt; \n&gt;&gt; Hello all-\n&gt;&gt; \n&gt;&gt; Recently I have shown that HyperNEAT has trouble making exceptions to the\n&gt;&gt; rules it discovers (Clune et al. PPSN 2008). I would like to introduce a new\n&gt;&gt; paper that will be at ECAL 2009 which shows one way to remedy this problem:\n&gt;&gt; combining a generative encoding (HyperNEAT in this case) with a direct\n&gt;&gt; encoding (FT-NEAT in this case).\n&gt;&gt; \n&gt;&gt; The resulting algorithm, which we call HybrID (Hybridization of Indirect and\n&gt;&gt; Direct Encodings), combines the best of both encodings, and outperformed\n&gt;&gt; HyperNEAT on all of the problems we tested it on, sometimes by as much as\n&gt;&gt; 40%. \n&gt;&gt; \n&gt;&gt; I&#39;d be really interested to hear what the people on this list think of the\n&gt;&gt; work. For example, I&#39;ll bet there are interesting ways to remedy the problem\n&gt;&gt; of making exceptions within HyperNEAT, and it would be interesting for us as\n&gt;&gt; a community to explore them. But in the interim, if any of you are deploying\n&gt;&gt; HyperNEAT on some task and want a possible performance boost, HybrID is easy\n&gt;&gt; to implement and may lead to a significant improvement.\n&gt;&gt; \n&gt;&gt; Here is a link to the paper:\n&gt;&gt; https://www.msu.edu/~jclune/webfiles/publications/Clune-HybrID-ECAL-2009.pdf\n&gt;&gt; \n&gt;&gt; Here is the abstract from the paper:\n&gt;&gt; \n&gt;&gt; Evolutionary algorithms typically use direct encodings, where each element\n&gt;&gt; of the phenotype is specified independently in the genotype. Because direct\n&gt;&gt; encodings have difficulty evolving modular and symmetric phenotypes, some\n&gt;&gt; researchers use indirect encodings, wherein one genomic element can\n&gt;&gt; influence multiple parts of a phenotype. We have previously shown that\n&gt;&gt; HyperNEAT, an indirect encoding, outperforms FT-NEAT, a direct-encoding\n&gt;&gt; control, on many problems, especially as the regularity of the problem\n&gt;&gt; increases. However, HyperNEAT is no panacea; it had difficulty accounting\n&gt;&gt; for irregularities in problems. In this paper, we propose a new algorithm, a\n&gt;&gt; Hybridized Indirect and Direct encoding (HybrID), which discovers the\n&gt;&gt; regularity of a problem with an indirect encoding and accounts for\n&gt;&gt; irregularities via a direct encoding. In three different problem domains,\n&gt;&gt; HybrID outperforms HyperNEAT in most situations, with performance\n&gt;&gt; improvements as large as 40%. Our work suggests that hybridizing indirect\n&gt;&gt; and direct encodings can be an effective way to improve the performance of\n&gt;&gt; evolutionary algorithms.\n&gt;&gt; \n&gt;&gt; \n&gt;&gt; \n&gt;&gt; \n&gt;&gt; Cheers,\n&gt;&gt; Jeff Clune\n&gt;&gt; \n&gt;&gt; Digital Evolution Lab, Michigan State University\n&gt;&gt; \n&gt;&gt; jclune@...\n&gt;&gt; \n&gt; \n&gt; \n\n\n\n"}}