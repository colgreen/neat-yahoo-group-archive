{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":200957992,"authorName":"Jason Gauci","from":"&quot;Jason Gauci&quot; &lt;jgmath2000@...&gt;","profile":"jgmath2000","replyTo":"LIST","senderId":"u8YH3RVTQxrwLuIb3G70Y28ikKXEV4auj5iXgGpBLLFlbaBPUxz1y_Sf7hAGONfffJuOfx92Nzn5592XOzKlhm62DlO2jk5AiHWW","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: HyperNEAT release site change and more info","postDate":"1176478476","msgId":3141,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGV2bzd1YytscWFAZUdyb3Vwcy5jb20+","inReplyToHeader":"PDE5QUYyMDRELTI3ODUtNDQwNC04MkRELTc2RDQ4QUM4NkEyMUB3YWl0cy5uZXQ+"},"prevInTopic":3140,"nextInTopic":3143,"prevInTime":3140,"nextInTime":3142,"topicId":3121,"numMessagesInTopic":16,"msgSnippet":"I appriciate your effort in explaining this method to me.  It looks like, given the right circumstances, it could speed up the activation of neural networks. I","rawEmail":"Return-Path: &lt;jgmath2000@...&gt;\r\nX-Sender: jgmath2000@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 50375 invoked from network); 13 Apr 2007 15:35:40 -0000\r\nReceived: from unknown (66.218.66.71)\n  by m49.grp.scd.yahoo.com with QMQP; 13 Apr 2007 15:35:40 -0000\r\nReceived: from unknown (HELO n28.bullet.scd.yahoo.com) (66.94.237.21)\n  by mta13.grp.scd.yahoo.com with SMTP; 13 Apr 2007 15:35:40 -0000\r\nReceived: from [209.73.164.83] by n28.bullet.scd.yahoo.com with NNFMP; 13 Apr 2007 15:34:38 -0000\r\nReceived: from [66.218.66.86] by t7.bullet.scd.yahoo.com with NNFMP; 13 Apr 2007 15:34:38 -0000\r\nDate: Fri, 13 Apr 2007 15:34:36 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;evo7uc+lqa@...&gt;\r\nIn-Reply-To: &lt;19AF204D-2785-4404-82DD-76D48AC86A21@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0\r\nFrom: &quot;Jason Gauci&quot; &lt;jgmath2000@...&gt;\r\nSubject: Re: HyperNEAT release site change and more info\r\nX-Yahoo-Group-Post: member; u=200957992; y=VWQcU5RS5UMyDy651_Vnt0HRV1ql-kWTlIxwNKXjbjcqp2ip5Q\r\nX-Yahoo-Profile: jgmath2000\r\n\r\nI appriciate your effort in explaining this method to me.  It looks \nlike, =\r\ngiven the right circumstances, it could speed up the activation \nof neural =\r\nnetworks.\n\nI am really interested in this area of neural network control be=\r\ncause \nI think that there are properties of neural networks and CPPNs which=\r\n \ncan exploit recurrent connections to do interesting things.\n\nThe main pro=\r\nblem I&#39;m battling is decided how many iterations I should \nrun of the netwo=\r\nrk.  It sounds like you are using your neural \nnetworks for control systems=\r\n, in which case they are being updated \ncontinuously at some rate (say, 50h=\r\nz).\n\nMy work has been more focused on receiving a single input and \nproduci=\r\nng a single result, so the number of times that you update has \nan impact. =\r\n As you pointed out, this second example will converge to \n1.0 because of t=\r\nhe recurrent link.  There are, however, examples \nwhere the output flutuate=\r\ns between -1 and 1 as you continue to update \nthe network, even as the inpu=\r\nts remain constant.  In this case, there \nis no convergence so it is hard t=\r\no say what output value should be \nused.\n\nI am going to try to implement th=\r\ne method you outlined and see how it \nchanges the results.  Thanks for your=\r\n input =3D)\n\n--- In neat@yahoogroups.com, Stephen Waits &lt;steve@...&gt; wrote:\n=\r\n&gt;\n&gt; \n&gt; On Apr 13, 2007, at 5:15 AM, Jason Gauci wrote:\n&gt; \n&gt; &gt; OK, actually =\r\nI wrote a couple of lua programs to figure this out.\n&gt; \n&gt; I did too.  Unfor=\r\ntunately I made a typo, which you caught, that  \n&gt; should have said (1+1) =\r\n=3D 2.  That&#39;s what I get for doing something  \n&gt; manually rather than lett=\r\ning a machine do it for me.\n&gt; \n&gt; You made a mistake in your program by sayi=\r\nng H1 =3D I + H0; whereas, \nif  \n&gt; you look at my original post, H1 =3D I. =\r\n For clarification, please \nsee  \n&gt; my original post, where I present the n=\r\network, the sorted list of  \n&gt; nodes, and the proper update algorithm.\n&gt; \n&gt;=\r\n Here&#39;s my program:\n&gt; \n&gt; #!/usr/bin/env ruby\n&gt; \n&gt; $i =3D 0\n&gt; $h0 =3D 0\n&gt; $h=\r\n1 =3D 0\n&gt; $o =3D 0\n&gt; \n&gt; 40.times do |iter|\n&gt;          $i =3D 1\n&gt;          $=\r\nh0 =3D $i + $h1\n&gt;          $h1 =3D $i\n&gt;          $o =3D $h0 + $h1\n&gt;        =\r\n  puts &quot;#{$i} #{$h0} #{$h1} #{$o}&quot;\n&gt; end\n&gt; \n&gt; \n&gt; Here&#39;s its output:\n&gt; \n&gt; 1 =\r\n1 1 2\n&gt; 1 2 1 3\n&gt; 1 2 1 3\n&gt; 1 2 1 3\n&gt; 1 2 1 3\n&gt; ...\n&gt; \n&gt; It&#39;s painfully cle=\r\nar to me  that with this network (and many \nothers),  \n&gt; my activation algo=\r\nrithm is producing the proper/expected result,  \n&gt; recurrent node included!=\r\n\n&gt; \n&gt; I wanted to look at the network you actually used, where H1 is \ninput=\r\n  \n&gt; by both I and H0.  Then I get matching results with you:\n&gt; \n&gt; 1 1 2 3\n=\r\n&gt; 1 3 4 7\n&gt; 1 5 6 11\n&gt; 1 7 8 15\n&gt; 1 9 10 19\n&gt; ...\n&gt; \n&gt; And, after looking a=\r\nt it for a second, they make perfect sense.  \nThe  \n&gt; link you added was a =\r\nforward link, from H0 to H1.  Everything in \nthis  \n&gt; network too works pro=\r\nperly.  H1 is indeed an accumulator (part of \nthe  \n&gt; beauty of recurrent c=\r\nonnections), and of course with sigmoidal  \n&gt; activation, would simply max =\r\nout at 1.0 (as long as the 1.0 inputs  \n&gt; continued!).  So, again, everythi=\r\nng works as expected.  (It&#39;s \nclear  \n&gt; that H0 is updated BEFORE H1, there=\r\nfore the recurrent link from H1  \n&gt; back to H0 will indeed use its &quot;prior&quot; =\r\nactivation).\n&gt; \n&gt; One last thing.  To sort a network, I consider it a cycli=\r\nc digraph  \n&gt; where the output nodes are all children of the root.  Then do=\r\n at  \n&gt; depth-first search, and as you encounter leaf nodes, add them to \nt=\r\nhe  \n&gt; end of your sorted node list.  Pretty simple.\n&gt; \n&gt; Now, I don&#39;t want=\r\n to belabor this any more, I believe my point is  \n&gt; clear - I understand i=\r\nt anyway.  I only kept going this long \nbecause  \n&gt; I was just trying to he=\r\nlp you or others understand; therefore, \nI&#39;ll  \n&gt; let this thread die from =\r\nmy end of the world.\n&gt; \n&gt; --Steve\n&gt;\n\n\n\n"}}