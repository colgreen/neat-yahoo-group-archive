{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":183620858,"authorName":"Derek James","from":"Derek James &lt;djames@...&gt;","profile":"blue5432","replyTo":"LIST","senderId":"B1k3zDQ87N01csGqYeGDUyEq594nusuRVglWf0BcvqWphePM-iHxe6VckgnJLUCaUGUML5vUw4zEgac1EIz3WT8_uOom","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] High rez input (i.e. Video) generalization","postDate":"1101841464","msgId":1744,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDE5YjEwZDUxMDQxMTMwMTEwNDc2ZjZhZDY5QG1haWwuZ21haWwuY29tPg==","inReplyToHeader":"PGNvaTh2Yis2OGc5QGVHcm91cHMuY29tPg==","referencesHeader":"PGNvaTh2Yis2OGc5QGVHcm91cHMuY29tPg=="},"prevInTopic":1743,"nextInTopic":1745,"prevInTime":1743,"nextInTime":1745,"topicId":1743,"numMessagesInTopic":9,"msgSnippet":"... I guess my first question is: Why are you taking this particular approach?  Are you wanting to apply this approach to another real-world domain, and this","rawEmail":"Return-Path: &lt;djames@...&gt;\r\nX-Sender: djames@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 85394 invoked from network); 30 Nov 2004 19:04:25 -0000\r\nReceived: from unknown (66.218.66.167)\n  by m2.grp.scd.yahoo.com with QMQP; 30 Nov 2004 19:04:25 -0000\r\nReceived: from unknown (HELO rproxy.gmail.com) (64.233.170.203)\n  by mta6.grp.scd.yahoo.com with SMTP; 30 Nov 2004 19:04:25 -0000\r\nReceived: by rproxy.gmail.com with SMTP id c16so103689rne\n        for &lt;neat@yahoogroups.com&gt;; Tue, 30 Nov 2004 11:04:25 -0800 (PST)\r\nDomainKey-Signature: a=rsa-sha1; q=dns; c=nofws;\n        s=beta; d=gmail.com;\n        h=received:message-id:date:from:reply-to:to:subject:in-reply-to:mime-version:content-type:content-transfer-encoding:references;\n        b=RHzezOFCxtDzb9oH+s0y5FQt0/ikRmxu1lbVuL1dyTbluKTBxJ/B/aiN+AU6tu3MEiUbSxaZe3y5h6T3n5nsxjgJqUHb3A3WnJ6v/o8tO/ol0N2wEygVXM7OGGl0oD6ZycB6qwQ0A4M0Oh4DGRFyb7ch0OMxVOigG+sj7tgFxd0=\r\nReceived: by 10.38.162.18 with SMTP id k18mr622321rne;\n        Tue, 30 Nov 2004 11:04:24 -0800 (PST)\r\nReceived: by 10.38.76.21 with HTTP; Tue, 30 Nov 2004 11:04:24 -0800 (PST)\r\nMessage-ID: &lt;19b10d51041130110476f6ad69@...&gt;\r\nDate: Tue, 30 Nov 2004 13:04:24 -0600\r\nTo: neat@yahoogroups.com\r\nIn-Reply-To: &lt;coi8vb+68g9@...&gt;\r\nMime-Version: 1.0\r\nContent-Type: text/plain; charset=US-ASCII\r\nContent-Transfer-Encoding: 7bit\r\nReferences: &lt;coi8vb+68g9@...&gt;\r\nX-eGroups-Remote-IP: 64.233.170.203\r\nFrom: Derek James &lt;djames@...&gt;\r\nReply-To: Derek James &lt;djames@...&gt;\r\nSubject: Re: [neat] High rez input (i.e. Video) generalization\r\nX-Yahoo-Group-Post: member; u=183620858\r\nX-Yahoo-Profile: blue5432\r\n\r\nOn Tue, 30 Nov 2004 17:00:27 -0000, Reuben &lt;reuben.grinberg@...&gt; wrote:\n&gt;\n&gt; I just recently discovered NEAT and am about to try to use it to evolve a vision and \n&gt; control system for pole-balancing. That is, instead of feeding in the angles and cart \n&gt; position, I&#39;m\n&gt; going to use a &quot;video feed&quot; of the system. I say &quot;video feed&quot; in quotes because I&#39;m going &gt; to\n&gt; use an inverted pendulum simulator to keep track of the physics and feed corresponding\n&gt; still frames as input.\n&gt; \n&gt; Over email, Ken told me that some work they&#39;ve down with low-rez video. It seems that\n&gt; the results don&#39;t generalize well.\n\nI guess my first question is: Why are you taking this particular\napproach?  Are you wanting to apply this approach to another\nreal-world domain, and this is a first cut?\n\nTo what extent are you wanting it to generalize?  \n\nWe&#39;re currently experimenting with fingerprint classification, so\nthere might be some overlap in the sorts of issues we&#39;re interested\nin.  For our domain, we&#39;re applying an active vision approach, in\norder to drastically reduce the visual input for a given time step,\nand to more closely simulate biological vision (there&#39;s plenty in the\nmessage archives not only on this, but on plenty of interesting\ndomains).\n\nYou say you&#39;re going to feed in still frames as input.  Could you be a\nlittle more specific on how you intend to do this?  With pole\nbalancing (by the way, are you going to try single and double?), you\nwouldn&#39;t need to input the entire scene.  Especially since, if you&#39;re\ntalking about &quot;hi-resolution&quot;, a given scene could have thousands of\npixels, or more.\n\nThe only things you care about in the scene are the angles of the\npoles and the position and velocity of the cart, right?  So if you set\nup your virtual camera so that it is viewing the profile of the cart,\nyou could just feed in pixel values from small windows on either side\nof the starting pole position, and from a thin strip along the path of\nthe cart.  But then, this is hand-picking what the system sees, and\nwouldn&#39;t really be much different from just directly inputting the\nangles and other information.\n\nI would imagine that this would be the way a human would solve the\nproblem, by moving the cart with their hand, while positioning their\neyes to be in profile with the cart to watch the angle of the pole. \nYou might want such a system to be robust to slight changes in the\nvisual input, but you probably wouldn&#39;t need a system that could, for\nexample, balance the pole(s) by only looking at a top view of the\nscene.  Is this what you&#39;re going for?\n\nBy the way, these double pole-balancing movies using the ESP technique\nare pretty cool:\n\nhttp://nn.cs.utexas.edu/pages/research/espdemo/\n\nDerek\n\n&gt; Ken wrote: &quot;However, we noticed an interesting problem.  It is learning which pixel means\n&gt; what.  In other words, it is not learning an abstraction at all.  It&#39;s just learning off the\n&gt; specific pixels.  That means you get really bad generalization performance if you test it on\n&gt; stuff it hasn&#39;t seen before. &quot;\n&gt; \n&gt; Ken - when you were running NEAT in the car domain, did you try to perturb the way the\n&gt; video was presented? While it would certainly increase the number of generations needed\n&gt; to get something acceptable, it might also make a solution that is able to generalize. That\n&gt; is, it might make a solution where there isn&#39;t a pixel-feature correspondance.\n&gt; \n&gt; Any thoughts?\n&gt; \n&gt; -Reuben\n&gt; --------------\n&gt; Reuben Grinberg\n&gt; reuben.grinberg@...\n&gt; Trumbull College, Yale University\n&gt; Computer Science, Class of &#39;05\n&gt; \n&gt; Yale Social Robotics Lab - http://gundam.cs.yale.edu\n&gt; \n&gt; \n&gt; \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; \n&gt; \n&gt;\n\n"}}