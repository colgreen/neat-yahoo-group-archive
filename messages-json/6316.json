{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":464818732,"authorName":"Jeff Clune","from":"Jeff Clune &lt;jclune@...&gt;","profile":"jeffreyclune","replyTo":"LIST","senderId":"szfIQAf1Bu2Q7oe6-4R1PM0r9SRSH2xMcK6f7X8PNwHJeEARGQB9u0MkRV7AmunNA_lPjRcdRp0xPkSe6NesaDyX7io","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [neat] Self-adaptive Mutation Rates, Novelty Search and CMA-ES","postDate":"1399410704","msgId":6316,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDA0NkZFQkY4LTE0NkEtNDk3QS04Q0U3LTdCRDRCNjhCRDUzNEBnbWFpbC5jb20+","inReplyToHeader":"PENBTnRYaG10Nitmd25IcFVTQzYxVHljcVF4QjNVNTVhY011dVBXbVdqWW42QUp2ay1FQUBtYWlsLmdtYWlsLmNvbT4=","referencesHeader":"PENBTnRYaG1zMXdqek9tN2hXYUNaRG5ZSFQzNE9nUEhuNVJKVXIzOEVkc19fcWlBV2trZ0BtYWlsLmdtYWlsLmNvbT4gPDUxMDBCOUQyLUFDQ0ItNDVCRi04RTc0LTJFMzY3NUE1NTQzN0BnbWFpbC5jb20+IDwwMDJmMDFjZjY2ZWQkMjI2NmFiYjAkNjczNDAzMTAkQHdhdHRzeXMuY29tPiA8Q0ErZHVpbVBHcnc1QTJqUHVoRnhEWmlPMEMrTTJpK0pTOEcwYyt3eUs9VmJvbXE9V2pBQG1haWwuZ21haWwuY29tPiA8bGs0NzhuKzE4MHZuanNAWWFob29Hcm91cHMuY29tPiA8Q0FOdFhobXV6MEVUTDZkYXVPR3NYT3lUQnJIdUYxTnRKcWpYM05oTnp0VjItWnQ2c0NRQG1haWwuZ21haWwuY29tPiA8QzAxOTk3ODItMzBBMy00NzFGLUJFQzQtMDYxRkNGMzZENkFCQGdtYWlsLmNvbT4gPENBTnRYaG10Nitmd25IcFVTQzYxVHljcVF4QjNVNTVhY011dVBXbVdqWW42QUp2ay1FQUBtYWlsLmdtYWlsLmNvbT4="},"prevInTopic":6312,"nextInTopic":6318,"prevInTime":6315,"nextInTime":6317,"topicId":6292,"numMessagesInTopic":19,"msgSnippet":"Hello Vassilis, Please see below. ... They are two sides of the same coin. You can lower your mutation rate by lowering the frequency of mutations, but also by","rawEmail":"Return-Path: &lt;jclune@...&gt;\r\nX-Sender: jclune@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 3367 invoked by uid 102); 6 May 2014 21:11:49 -0000\r\nX-Received: from unknown (HELO mtaq5.grp.bf1.yahoo.com) (10.193.84.36)\n  by m9.grp.bf1.yahoo.com with SMTP; 6 May 2014 21:11:49 -0000\r\nX-Received: (qmail 10967 invoked from network); 6 May 2014 21:11:49 -0000\r\nX-Received: from unknown (HELO mail-ig0-f179.google.com) (209.85.213.179)\n  by mtaq5.grp.bf1.yahoo.com with SMTP; 6 May 2014 21:11:49 -0000\r\nX-Received: by mail-ig0-f179.google.com with SMTP id hn18so231566igb.12\n        for &lt;neat@yahoogroups.com&gt;; Tue, 06 May 2014 14:11:49 -0700 (PDT)\r\nX-Received: by 10.50.66.143 with SMTP id f15mr36671109igt.18.1399410708909;\n        Tue, 06 May 2014 14:11:48 -0700 (PDT)\r\nReturn-Path: &lt;jclune@...&gt;\r\nX-Received: from temp07.cs.uwyo.edu (uwyo-129-72-147-63.uwyo.edu. [129.72.147.63])\n        by mx.google.com with ESMTPSA id sc2sm42355929igb.5.2014.05.06.14.11.45\n        for &lt;neat@yahoogroups.com&gt;\n        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);\n        Tue, 06 May 2014 14:11:45 -0700 (PDT)\r\nContent-Type: multipart/alternative; boundary=&quot;Apple-Mail=_B6E3766E-9212-462C-8081-973E16653259&quot;\r\nMessage-Id: &lt;046FEBF8-146A-497A-8CE7-7BD4B68BD534@...&gt;\r\nMime-Version: 1.0 (Mac OS X Mail 7.2 &#92;(1874&#92;))\r\nDate: Tue, 6 May 2014 15:11:44 -0600\r\nReferences: &lt;CANtXhms1wjzOm7hWaCZDnYHT34OgPHn5RJUr38Eds__qiAWkkg@...&gt; &lt;5100B9D2-ACCB-45BF-8E74-2E3675A55437@...&gt; &lt;002f01cf66ed$2266abb0$67340310$@...&gt; &lt;CA+duimPGrw5A2jPuhFxDZiO0C+M2i+JS8G0c+wyK=Vbomq=WjA@...&gt; &lt;lk478n+180vnjs@...&gt; &lt;CANtXhmuz0ETL6dauOGsXOyTBrHuF1NtJqjX3NhNztV2-Zt6sCQ@...&gt; &lt;C0199782-30A3-471F-BEC4-061FCF36D6AB@...&gt; &lt;CANtXhmt6+fwnHpUSC61TycqQxB3U55acMuuPWmWjYn6AJvk-EA@...&gt;\r\nTo: neat users group group &lt;neat@yahoogroups.com&gt;\r\nIn-Reply-To: &lt;CANtXhmt6+fwnHpUSC61TycqQxB3U55acMuuPWmWjYn6AJvk-EA@...&gt;\r\nX-Mailer: Apple Mail (2.1874)\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Jeff Clune &lt;jclune@...&gt;\r\nSubject: Re: [neat] Self-adaptive Mutation Rates, Novelty Search and CMA-ES\r\nX-Yahoo-Group-Post: member; u=464818732; y=6osDxF-sCi4shDeSsA_Z-Aaz9RGfLILN0HdlGbKPqMLJ0DEo-e7T\r\nX-Yahoo-Profile: jeffreyclune\r\n\r\n\r\n--Apple-Mail=_B6E3766E-9212-462C-8081-973E16653259\r\nContent-Transfer-Encoding: quoted-printable\r\nContent-Type: text/plain;\n\tcharset=windows-1252\r\n\r\nHello Vassilis,\n\nPlease see below. \n\n&gt; I was thinking about the mutation ra=\r\nte and mutation strength/step as being the same thing which is not. I apolo=\r\ngize for the confusion. I realized that after reading your email. I also ap=\r\nologize for the notation; it would be nice if we could write some parts of =\r\nthe email in LaTeX (perhaps we can and I don&#39;t know about it) :-)\n&gt; \n\nThey =\r\nare two sides of the same coin. You can lower your mutation rate by lowerin=\r\ng the frequency of mutations, but also by reducing the size of mutations. I=\r\n just think the former is a bit simpler to think about, so I asked after it=\r\n. IF CMA-ES has the option to lower the size of mutations (across the board=\r\n, not just to reallocate a fixed magnitude to different areas of the genome=\r\n) and does not do that, it would falsify my hypothesis. \n\n&gt; As far as I und=\r\nerstand, in real-valued optimization with evolution strategies (ES), the co=\r\nncept of mutation rate and mutation strength/step is different than in gene=\r\ntic algorithms. In genetic algorithms, a mutation rate is a probability tha=\r\nt a value will be mutated by an amount equal to the mutation strength/step.=\r\n In ES these two concepts are intertwined. That is, each individual paramet=\r\ner is *always* perturbed by sampling the perturbation from a Gaussian distr=\r\nibution with mean 0. The standard deviation of this Gaussian distribution i=\r\ns different for each parameter, and this value is what is self-adapted and =\r\n&quot;hitchhikes along with the other parameters on the genome&quot;. Since the pertu=\r\nrbation is probabilistic and the distribution (of this perturbation) is cen=\r\ntered around 0, there is a high probability that it will be very small. Als=\r\no, note that self-adaptation might make the standard deviation of some of t=\r\nhe parameters become very small, which effectively means that their mutatio=\r\nn rate approaches zero.\n&gt; \n\nThat makes sense. So now my questions are this:=\r\n\n1) Do these vectors of standard deviations evolve in the usual sense? I.e.=\r\n does selection act on the performance on the problem, and the vector of SD=\r\ns hitchhike along for the ride? \n2) Has anyone shown that these self-adapte=\r\nd SDs are better than leaving them at a fixed size (on rugged fitness lands=\r\ncapes: note that the sphere function is not rugged)? I=92d like to see a sw=\r\neep across different fixed-sizes to find the long-term optimum, and then se=\r\ne if self-adapted SDs outperform the best of those. My hypothesis is that t=\r\nhe self-adaptation is extremely maladaptive (again, provided that the probl=\r\nem is non-trivial). I=92ve asked people in the ES community for similar pap=\r\ners, but have never seen one produced. \n3) If CMA-ES is self-adapting its m=\r\nutation rate in a way that helps: why? My paper lays out intuitive reasons =\r\nfor why we should expect mutation rates to evolve to be low and hurt long-t=\r\nerm performance (because evolution is focusing on the short-term average fi=\r\ntness, not the long-term average fitness). So why doesn=92t this reasoning =\r\napply to CMA-ES?\n\n&gt; Summarizing, in ES there are no self-adaptive mutation =\r\nrates, but there are self-adaptive standard deviations of Gaussian distribu=\r\ntions (which indirectly affect both the mutation rates and mutation strengt=\r\nhs). In CMA-ES, a covariance matrix (which captures correlations between di=\r\nmensions) is self-adapted as well and this makes convergence to the optimum=\r\n faster.\n&gt; \n&gt; Note that discrete spaces are differently handled. You need t=\r\no have a mutation rate which is usually sampled from a uniform distribution=\r\n. But the original ES were designed for real-valued spaces.\n&gt; \n&gt; Regarding =\r\nthe issue of the population, my view is the following. In ES, there is some=\r\nthing called &quot;plus&quot; selection (&quot;+&quot;) and &quot;comma&quot; selection (&quot;,&quot;).\n&gt; \n&gt; (mu +=\r\n lambda) means that there are mu parents that create lambda offspring, with=\r\n mu &gt; 0, lambda &gt; 0. What survives in the next generation is the best indiv=\r\niduals from both parents and offspring. So, the effective population size i=\r\ns the maximum of the two.\n&gt; \n&gt; (mu, lambda) means that there are mu parents=\r\n that create lambda offspring, with lambda &gt;=3D mu, mu &gt; 0. What survives i=\r\nn the next generation is the best individuals, but only from the offspring.=\r\n Since lambda is at least equal to mu, the effective population size is lam=\r\nbda.\n&gt; \n\nThat is also my understanding of how it works in tradition ES. But=\r\n I thought it worked differently in CMA-ES. For example, I don=92t think in=\r\ndividuals in CMA-ES are mutated versions of their parents. I think they are=\r\n freshly drawn from the learned distributions. And I don=92t think offsprin=\r\ng are kept, but are only used to update the learned distributions. But as I=\r\n said, my knowledge of CMA-ES sis fuzzy. Partly because I don=92t understan=\r\nd it perfectly, it=92s hard for me to see if there is some mechanism that a=\r\nllows self-adaptation to work well.  \n\n&gt; \n&gt; &quot;You also mention the 1/5th rul=\r\ne, which is not self-adaptive&quot;. \n&gt; I agree. I only mentioned it for histori=\r\ncal reasons, in order to make the transition to self-adaptation.\n&gt; \n&gt; &quot;It s=\r\neems like you said that the fitness of organisms does not affect the mutati=\r\non rate (the issue of =93unbiasedness=94)&quot;. \n&gt; What I meant is that when de=\r\nsigning the mutation operator one should think about exploring the whole se=\r\narch space, i.e., to be unbiased as to which space to explore. Selection is=\r\n responsible for guiding search using the fitness information. Therefore, b=\r\ny including the standard deviations in the genome, the process becomes self=\r\n-adaptive.\n&gt; \n\nSo fitness does impact which SDs survive? That does sound li=\r\nke self-adaptation, making me skeptical about whether it really works. \n&gt; \n=\r\n&gt; Let me know if anything is unclear.\n\nThanks for the clarification. It=92s=\r\n an interesting, important question and I=92m glad we=92re trying to get to=\r\n the bottom of it. \n\nBest,\nJeff\n\n&gt; \n&gt; \n&gt; Best,\n&gt; Vassilis\n&gt; \n&gt; \n&gt; \n&gt; \n&gt; On =\r\nMon, May 5, 2014 at 8:11 AM, Jeff Clune &lt;jclune@...&gt; wrote:\n&gt;  \n&gt; Hel=\r\nlo Vassilis,\n&gt; \n&gt; \n&gt; Thanks for the interest in, and excitement about, the =\r\nfirst batch of Evolving AI Lab publications. :-)\n&gt; \n&gt; As for CMA-ES, I did =\r\nnot understand from your email if it is indeed self-adaptive. It seems like=\r\n you said that the fitness of organisms does not affect the mutation rate (=\r\nthe issue of =93unbiasedness=94). That would certainly make the problem of =\r\nself-adaptive mutation rates go away! \n&gt; \n&gt; You also mention the 1/5th rule=\r\n, which is not self-adaptive: it does not have a mutation rate is on each g=\r\nenome that hitchhikes along with the other parameters on the genome. \n&gt; \n&gt; =\r\nDoes CMA-ES do something different? I couldn=92t discern the answer from yo=\r\nur notion. For example, in CMA-ES, is there a parameter that evolves that c=\r\nontrols the number of mutations per genome per generation? I thought CMA-ES=\r\n didn=92t even have a population of individuals, so I=92m not sure how it c=\r\nould even have self-adaptive mutation rates.\n&gt; \n&gt; \n&gt; Best regards,\n&gt; Jeff C=\r\nlune\n&gt; \n&gt; Assistant Professor\n&gt; Computer Science\n&gt; University of Wyoming\n&gt; =\r\njeffclune@...\n&gt; jeffclune.com\n&gt; \n&gt; On May 4, 2014, at 8:00 AM, Vassili=\r\ns Vassiliades &lt;vassilisvas@...&gt; wrote:\n&gt; \n&gt;&gt;  \n&gt;&gt; \n&gt;&gt; Hello all,\n&gt;&gt; \n=\r\n&gt;&gt; Joel and Ken, I think I understand what you are saying about &quot;projecting=\r\n intuitions about the objective world into novelty search&quot; and the dynamic =\r\nand divergent nature of Novelty Search (NS)... and also why naively combini=\r\nng Covariance Matrix Adaptation - Evolution Strategies (CMA-ES) might not w=\r\nork well with NS. A &quot;fitness&quot; peak in generation (g) might diminish in gene=\r\nration (g+1), and generally the &quot;fitness&quot; peaks are constantly moving.\n&gt;&gt; \n=\r\n&gt;&gt; This reminds me a bit of multiagent learning problems, where one could s=\r\nay that the &quot;target&quot; is constantly and adaptively being moved. However, I a=\r\nm not sure whether/how we could frame NS as a multiagent learning scenario.=\r\n I am also imagining the landscape of NS as a... &quot;boiling soup&quot; where bubbl=\r\nes (fitness peaks) keep appearing and disappearing all the time. :) These b=\r\nubbles could be guided by the archive, so that they would never appear at t=\r\nhe same place, or if we were to bound the archive (or use the probabilistic=\r\n approach), they could appear again after some time (or probabilistically).=\r\n \n&gt;&gt; \n&gt;&gt; Questions:\n&gt;&gt; \n&gt;&gt; 1) Does anyone think that there is any relations=\r\nhip of NS with thermodynamics?\n&gt;&gt; \n&gt;&gt; 2) Do you think there are any relatio=\r\nnships between NS and dynamic optimization?\n&gt;&gt; \n&gt;&gt; 3) Joel, you often menti=\r\non competitive coevolution when talking about NS. You also said that &quot;there=\r\n exist similar fixed-point concepts for competitive co-evolution (like medi=\r\nocre stable states and disengagement)&quot;. To be honest, I haven&#39;t read the li=\r\nterature on these concepts. I was thinking, however, that if we were to thi=\r\nnk about coevolution in game theoretic terms, then competitive coevolution =\r\nis like a zero-sum game: the gains of one individual are balanced by the lo=\r\nsses of the others. Is this correct? Does NS behave in this manner? It seem=\r\ns to me that it is not a zero-sum game.\n&gt;&gt; \n&gt;&gt; \n&gt;&gt; \n&gt;&gt; Jeff, I agree with y=\r\nou that CMA-ES learns correlations. However, allow me to express here my un=\r\nderstanding as to how it works and its relationship to self-adaptive mutati=\r\non rates (SAMR). * I included this part at the end of the email because it =\r\ngot very big :) Anyone who read this please do correct me if something is w=\r\nrong.\n&gt;&gt; \n&gt;&gt; Jeff you wrote: &quot;my guess is that if you tried CMA-ES with and=\r\n without a mutation rate on the genome, you=92d get the same result I repor=\r\nt in my paper&quot;. CMA-ES by construction uses the mutation rates. It is an im=\r\nportant part of the algorithm. I would be very curious to see how CMA-ES be=\r\nhaves in the same setup used in your paper.\n&gt;&gt; \n&gt;&gt; I noticed something in y=\r\nour paper on SAMR. I also noticed that in Joel&#39;s and Ken&#39;s paper on SAMR wi=\r\nth NS (which I still haven&#39;t read very carefully, only skimmed though): in =\r\nboth papers the SAMR are updated differently than the SAMR of ES (see at th=\r\ne end of the email). Do you think that using an ES approach for SAMR in you=\r\nr setups would lead to any different results/conclusions?\n&gt;&gt; \n&gt;&gt; &quot;&#39;novelty =\r\nplateaus&#39;, a concept my students and I introduce in our last, not-yet-annou=\r\nnced GECCO paper&quot;\n&gt;&gt; Sounds interesting! Congrats by the way on all new pap=\r\ners!\n&gt;&gt; \n&gt;&gt; \n&gt;&gt; \n&gt;&gt; Ken Lloyd, thanks for the link on Adaptive Stochastic R=\r\nesonance. I haven&#39;t read it yet, but I also noticed that there are several =\r\nworks that have similarities with NS (e.g., intrinsic motivations, maximiza=\r\ntion of predictive information, causal entropic forces, empowerment etc.). =\r\nDoes anyone think it would it be interesting to crowdsource a list of simil=\r\nar works on a different thread? I think Joel and Ken (Stanley) probably kno=\r\nw a lot of them already.\n&gt;&gt; \n&gt;&gt; \n&gt;&gt; \n&gt;&gt; Oliver, you mentioned &quot;some work ha=\r\ns probabilistically applied the objective function&quot;. I would be interested =\r\nto see this work if you find the link. If I remember correctly Jeff, JBM an=\r\nd Hod Lipson in &quot;The evolutionary origins of modularity&quot; used something sim=\r\nilar, but on the secondary objective (connection cost) objective.\n&gt;&gt; \n&gt;&gt; \n&gt;=\r\n&gt; \n&gt;&gt; Best,\n&gt;&gt; Vassilis\n&gt;&gt; \n&gt;&gt; \n&gt;&gt; * Notes on CMA-ES:\n&gt;&gt; \n&gt;&gt; In evolution s=\r\ntrategies (ES) the mutation operator is usually the primary source of varia=\r\ntion. Thus, we could say that while selection exploits the fitness informat=\r\nion in order to guide search into promising regions, mutation (or variation=\r\n) explores the search space and should not use any fitness information to d=\r\no that, i.e., it should be unbiased. This is a theoretical consideration/re=\r\nquirement (called &quot;unbiasedness&quot;), which naturally leads to the maximum ent=\r\nropy principle. In the case of real-valued search spaces this leads to gaus=\r\nsian distributions, while it has been also shown in the literature how to p=\r\notentially handle integer and discrete parameters.\n&gt;&gt; \n&gt;&gt; Let&#39;s stay on rea=\r\nl-valued spaces and let&#39;s say that an individual &quot;a&quot; comprises an object pa=\r\nrameter vector &quot;x&quot;, and its fitness function value &quot;F(x)&quot;: a =3D (x, F(x)).=\r\n Mutations on x work like this:\n&gt;&gt; \n&gt;&gt; x&#39; =3D x + z\n&gt;&gt; \n&gt;&gt; with z being rel=\r\nated to gaussian distributions. \n&gt;&gt; \n&gt;&gt; The simplest case is z =3D sigma * =\r\n( N_1(0,1), N_2(0,1), ... , N_d(0,1) ), where sigma is the standard deviati=\r\non of the normal distribution, d is the dimensionality, and N_i(0,1) are in=\r\ndependent random samples from the normal distribution.\n&gt;&gt; \n&gt;&gt; People have n=\r\noticed that when sigma is constant and the very simple (1+1)-ES is used (me=\r\naning 1 parent creates 1 offspring and the strongest of them survives), the=\r\nn in very simplified unimodal fitness functions (such as the sphere functio=\r\nn), the ES initially displays a period of improvements; however, after a wh=\r\nile it becomes very very slow and loses its evolvability because when appro=\r\naching the minimum sigma is too big and tends to overshoot. By analyzing ho=\r\nw sigma influences the success probability by which an offspring replaces a=\r\n parent, as well as the progress rate, people have come up with something c=\r\nalled the &quot;evolution window&quot; as well as the 1/5th control rule that appropr=\r\niately scales sigma periodically.\n&gt;&gt; \n&gt;&gt; Now, the 1/5th rule is a heuristic=\r\n and very specific to (1+1)-ES and the fitness landscape, so people needed =\r\nsomething better. Hence the following idea: \n&gt;&gt; \n&gt;&gt; Let&#39;s add an endogenous=\r\n/evolvable strategy parameter vector &quot;s&quot; to the individual, that contains a=\r\nny other parameters we want (such as the standard deviation). So now an ind=\r\nividual is: a =3D (x,s,F(x)). But how do we update sigma / the standard dev=\r\niation? How do we mutate this mutation strength/rate? The maximum entropy p=\r\nrinciple specifies that we should use gaussian distributions, but using the=\r\nse on the standard deviation could lead to negative values. A neat solution=\r\n is to do it in log scale:\n&gt;&gt; \n&gt;&gt; ln( sigma&#39; ) =3D ln( sigma ) + zeta\n&gt;&gt; \n&gt;=\r\n&gt; which leads to the multiplicative update:\n&gt;&gt; \n&gt;&gt; sigma&#39; =3D sigma * exp( =\r\nzeta )\n&gt;&gt; \n&gt;&gt; where zeta =3D tau * N(0,1), and tau is an exogenous learning=\r\n parameter which determines the rate and precision of self-adaptation. It i=\r\ns usually proportional to 1/sqrt(d).\n&gt;&gt; \n&gt;&gt; Until now we were talking about=\r\n a single strategy parameter (mutation rate) for the whole genotype, i.e., =\r\ns =3D sigma, also known as isotropic mutations. We can extend this to the c=\r\nase where we have a vector of strategy parameters (mutation rates), i.e., s=\r\n =3D (sigma_1, sigma_2, ..., sigma_d), also known as non-isotropic mutation=\r\ns. This technique is more flexible especially in high dimensional problems.=\r\n However, it is still not very effective in some non-separable problems, e.=\r\ng., consider a fitness landscape that is not aligned with the coordinate sy=\r\nstem. How do we deal with arbitrary rotations of the fitness landscape, whi=\r\nch is the most general situation?\n&gt;&gt; \n&gt;&gt; Now the idea of Covariance Matrix =\r\nAdaptation is introduced: let&#39;s estimate the shape of the fitness landscape=\r\n and adapt a rotation matrix in order to be able to align our coordinate ax=\r\nes with the principal axes of the fitness landscape. This covariance matrix=\r\n introduces correlations between the components of z.\n&gt;&gt; \n&gt;&gt; So, in:\n&gt;&gt; \n&gt;&gt;=\r\n 1) isotropic mutations: z =3D sigma * ( N_1(0,1), N_2(0,1), ... , N_d(0,1)=\r\n ) =3D sigma * N(0, I), where I is the identity matrix\n&gt;&gt; \n&gt;&gt; 2) non-isotro=\r\npic mutations: z =3D ( sigma_1 * N_1(0,1), sigma_2 * N_2(0,1), ..., sigma_d=\r\n * N_d(0,1) ) =3D D * N(0, I), where D is a diagonal matrix containing all =\r\nsigma values\n&gt;&gt; \n&gt;&gt; 3) correlated mutations: z =3D M * ( sigma_1 * N_1(0,1)=\r\n, sigma_2 * N_2(0,1), ..., sigma_d * N_d(0,1) ) =3D M * D * N(0, I), where =\r\nM is the rotation matrix that introduces correlations between the component=\r\ns of z, and C =3D M^T * M is the covariance matrix (M^T is the transpose of=\r\n M).\n&gt;&gt; \n&gt;&gt; \n&gt;&gt; I won&#39;t get into more detail (e.g., how to estimate the cov=\r\nariance matrix etc.), but note that CMA-ES has been shown to work very well=\r\n in various benchmarks and with small population sizes.\n&gt;&gt; \n&gt;&gt; \n&gt; \n&gt; \n&gt; \n&gt; =\r\n\n&gt; \n\n\r\n--Apple-Mail=_B6E3766E-9212-462C-8081-973E16653259\r\nContent-Transfer-Encoding: quoted-printable\r\nContent-Type: text/html;\n\tcharset=windows-1252\r\n\r\n&lt;html&gt;&lt;head&gt;&lt;meta http-equiv=3D&quot;Content-Type&quot; content=3D&quot;text/html charset=\r\n=3Dwindows-1252&quot;&gt;&lt;/head&gt;&lt;body style=3D&quot;word-wrap: break-word; -webkit-nbsp-=\r\nmode: space; -webkit-line-break: after-white-space;&quot;&gt;Hello&nbsp;Vassilis,&lt;d=\r\niv&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Please see below.&nbsp;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;div&gt;&lt;blockquote ty=\r\npe=3D&quot;cite&quot;&gt;&lt;div style=3D&quot;background-color: rgb(255, 255, 255); position: s=\r\ntatic; z-index: auto;&quot;&gt;&lt;div id=3D&quot;ygrp-mlmsg&quot; style=3D&quot;position:relative;&quot;&gt;=\r\n&lt;div id=3D&quot;ygrp-msg&quot; style=3D&quot;z-index: 1;&quot;&gt;&lt;div id=3D&quot;ygrp-text&quot;&gt;&lt;div dir=\r\n=3D&quot;ltr&quot;&gt;&lt;div&gt;I was thinking about the mutation rate and mutation strength/=\r\nstep as being the same thing which is not. I apologize for the confusion. I=\r\n realized that after reading your email. I also apologize for the notation;=\r\n it would be nice if we could write some parts of the email in LaTeX (perha=\r\nps we can and I don&#39;t know about it) :-)&lt;/div&gt;\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;=\r\n/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/blockquote&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;They are two sides of the=\r\n same coin. You can lower your mutation rate by lowering the frequency of m=\r\nutations, but also by reducing the size of mutations. I just think the form=\r\ner is a bit simpler to think about, so I asked after it. IF CMA-ES has the =\r\noption to lower the size of mutations (across the board, not just to reallo=\r\ncate a fixed magnitude to different areas of the genome) and does not do th=\r\nat, it would falsify my hypothesis.&nbsp;&lt;/div&gt;&lt;br&gt;&lt;blockquote type=3D&quot;cite=\r\n&quot;&gt;&lt;div style=3D&quot;background-color: rgb(255, 255, 255); position: static; z-i=\r\nndex: auto;&quot;&gt;&lt;div id=3D&quot;ygrp-mlmsg&quot; style=3D&quot;position:relative;&quot;&gt;&lt;div id=3D=\r\n&quot;ygrp-msg&quot; style=3D&quot;z-index: 1;&quot;&gt;&lt;div id=3D&quot;ygrp-text&quot;&gt;&lt;div dir=3D&quot;ltr&quot;&gt;&lt;di=\r\nv&gt;As far as I understand, in real-valued optimization with evolution strate=\r\ngies (ES), the concept of mutation rate and mutation strength/step is diffe=\r\nrent than in genetic algorithms. In genetic algorithms, a mutation rate is =\r\na probability that a value will be mutated by an amount equal to the mutati=\r\non strength/step. In ES these two concepts are intertwined. That is, each i=\r\nndividual parameter is *always* perturbed by sampling the perturbation from=\r\n a Gaussian distribution with mean 0. The standard deviation of this Gaussi=\r\nan distribution is different for each parameter, and this value is what is =\r\nself-adapted and &quot;hitchhikes along with the other parameters on the genome&quot;=\r\n. Since the perturbation is probabilistic and the distribution (of this per=\r\nturbation) is centered around 0, there is a high probability that it will b=\r\ne very small. Also, note that self-adaptation might make the standard devia=\r\ntion of some of the parameters become very small, which effectively means t=\r\nhat their mutation rate approaches zero.&lt;/div&gt;\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;=\r\n/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/blockquote&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;That makes sense. So now =\r\nmy questions are this:&lt;/div&gt;&lt;div&gt;1) Do these vectors of standard deviations=\r\n evolve in the usual sense? I.e. does selection act on the performance on t=\r\nhe problem, and the vector of SDs hitchhike along for the ride?&nbsp;&lt;/div&gt;=\r\n&lt;div&gt;2) Has anyone shown that these self-adapted SDs are better than leavin=\r\ng them at a fixed size (on rugged fitness landscapes: note that the sphere =\r\nfunction is not rugged)? I=92d like to see a sweep across different fixed-s=\r\nizes to find the long-term optimum, and then see if self-adapted SDs outper=\r\nform the best of those. My hypothesis is that the self-adaptation is extrem=\r\nely maladaptive (again, provided that the problem is non-trivial). I=92ve a=\r\nsked people in the ES community for similar papers, but have never seen one=\r\n produced.&nbsp;&lt;/div&gt;&lt;div&gt;3) If CMA-ES is self-adapting its mutation rate =\r\nin a way that helps: why? My paper lays out intuitive reasons for why we sh=\r\nould expect mutation rates to evolve to be low and hurt long-term performan=\r\nce (because evolution is focusing on the short-term average fitness, not th=\r\ne long-term average fitness). So why doesn=92t this reasoning apply to CMA-=\r\nES?&lt;/div&gt;&lt;br&gt;&lt;blockquote type=3D&quot;cite&quot;&gt;&lt;div style=3D&quot;background-color: rgb(=\r\n255, 255, 255); position: static; z-index: auto;&quot;&gt;&lt;div id=3D&quot;ygrp-mlmsg&quot; st=\r\nyle=3D&quot;position:relative;&quot;&gt;&lt;div id=3D&quot;ygrp-msg&quot; style=3D&quot;z-index: 1;&quot;&gt;&lt;div =\r\nid=3D&quot;ygrp-text&quot;&gt;&lt;div dir=3D&quot;ltr&quot;&gt;&lt;div&gt;Summarizing, in ES there are no self=\r\n-adaptive mutation rates, but there are self-adaptive standard deviations o=\r\nf Gaussian distributions (which indirectly affect both the mutation rates a=\r\nnd mutation strengths). In CMA-ES, a covariance matrix (which captures corr=\r\nelations between dimensions) is self-adapted as well and this makes converg=\r\nence to the optimum faster.&lt;/div&gt;\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Note that discrete sp=\r\naces are differently handled. You need to have a mutation rate which is usu=\r\nally sampled from a uniform distribution. But the original ES were designed=\r\n for real-valued spaces.&lt;br&gt;&lt;/div&gt;\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Regarding the issue =\r\nof the population, my view is the following. In ES, there is something call=\r\ned &quot;plus&quot; selection (&quot;+&quot;) and &quot;comma&quot; selection (&quot;,&quot;).&lt;/div&gt;&lt;div&gt;&lt;br&gt;\n&lt;/div=\r\n&gt;&lt;div&gt;(mu + lambda) means that there are mu parents that create lambda offs=\r\npring, with mu &gt; 0, lambda &gt; 0. What survives in the next generation =\r\nis the best individuals from both parents and offspring. So, the effective =\r\npopulation size is the maximum of the two.&lt;/div&gt;\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;(mu, l=\r\nambda) means that there are mu parents that create lambda offspring, with l=\r\nambda &gt;=3D mu, mu &gt; 0. What survives in the next generation is the be=\r\nst individuals, but only from the offspring. Since lambda is at least equal=\r\n to mu, the effective population size is lambda.&lt;/div&gt;\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;/div=\r\n&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/blockquote&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;That is also my u=\r\nnderstanding of how it works in tradition ES. But I thought it worked diffe=\r\nrently in CMA-ES. For example, I don=92t think individuals in CMA-ES are mu=\r\ntated versions of their parents. I think they are freshly drawn from the le=\r\narned distributions. And I don=92t think offspring are kept, but are only u=\r\nsed to update the learned distributions. But as I said, my knowledge of CMA=\r\n-ES sis fuzzy. Partly because I don=92t understand it perfectly, it=92s har=\r\nd for me to see if there is some mechanism that allows self-adaptation to w=\r\nork well. &nbsp;&lt;/div&gt;&lt;br&gt;&lt;blockquote type=3D&quot;cite&quot;&gt;&lt;div style=3D&quot;backgroun=\r\nd-color: rgb(255, 255, 255); position: static; z-index: auto;&quot;&gt;&lt;div id=3D&quot;y=\r\ngrp-mlmsg&quot; style=3D&quot;position:relative;&quot;&gt;&lt;div id=3D&quot;ygrp-msg&quot; style=3D&quot;z-ind=\r\nex: 1;&quot;&gt;&lt;div id=3D&quot;ygrp-text&quot;&gt;&lt;div dir=3D&quot;ltr&quot;&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;&quot;You als=\r\no mention the 1/5th rule, which is not self-adaptive&quot;.&nbsp;&lt;/div&gt;&lt;div&gt;I ag=\r\nree. I only mentioned it for historical reasons, in order to make the trans=\r\nition to self-adaptation.&lt;/div&gt;\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;&quot;It seems like you said=\r\n that the fitness of organisms does not affect the mutation rate (the issue=\r\n of =93unbiasedness=94)&quot;.&nbsp;&lt;/div&gt;&lt;div&gt;What I meant is that when designi=\r\nng the mutation operator one should think about exploring the whole search =\r\nspace, i.e., to be unbiased as to which space to explore. Selection is resp=\r\nonsible for guiding search using the fitness information. Therefore, by inc=\r\nluding the standard deviations in the genome, the process becomes self-adap=\r\ntive.&lt;/div&gt;\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/blockquote&gt;&lt;div&gt;=\r\n&lt;br&gt;&lt;/div&gt;So fitness does impact which SDs survive? That does sound like se=\r\nlf-adaptation, making me skeptical about whether it really works.&nbsp;&lt;br&gt;=\r\n&lt;blockquote type=3D&quot;cite&quot;&gt;&lt;div style=3D&quot;background-color: rgb(255, 255, 255=\r\n); position: static; z-index: auto;&quot;&gt;&lt;div id=3D&quot;ygrp-mlmsg&quot; style=3D&quot;positi=\r\non:relative;&quot;&gt;&lt;div id=3D&quot;ygrp-msg&quot; style=3D&quot;z-index: 1;&quot;&gt;&lt;div id=3D&quot;ygrp-te=\r\nxt&quot;&gt;&lt;div dir=3D&quot;ltr&quot;&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Let me know if anything is unclear=\r\n.&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/blockquote&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Thank=\r\ns for the clarification. It=92s an interesting, important question and I=92=\r\nm glad we=92re trying to get to the bottom of it.&nbsp;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div=\r\n&gt;&lt;div&gt;Best,&lt;/div&gt;&lt;div&gt;Jeff&lt;/div&gt;&lt;br&gt;&lt;blockquote type=3D&quot;cite&quot;&gt;&lt;div style=3D=\r\n&quot;background-color: rgb(255, 255, 255); position: static; z-index: auto;&quot;&gt;&lt;d=\r\niv id=3D&quot;ygrp-mlmsg&quot; style=3D&quot;position:relative;&quot;&gt;&lt;div id=3D&quot;ygrp-msg&quot; styl=\r\ne=3D&quot;z-index: 1;&quot;&gt;&lt;div id=3D&quot;ygrp-text&quot;&gt;&lt;div dir=3D&quot;ltr&quot;&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;di=\r\nv&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Best,&lt;/div&gt;&lt;div&gt;Vassilis&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/di=\r\nv&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;div class=3D&quot;gmail_quote&quot;&gt;=\r\n\nOn Mon, May 5, 2014 at 8:11 AM, Jeff Clune &lt;span dir=3D&quot;ltr&quot;&gt;&lt;&lt;a href=\r\n=3D&quot;mailto:jclune@...&quot; target=3D&quot;_blank&quot;&gt;jclune@...&lt;/a&gt;&gt;&lt;/sp=\r\nan&gt; wrote:&lt;br&gt;&lt;blockquote class=3D&quot;gmail_quote&quot; style=3D&quot;margin:0px 0px 0px=\r\n 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left=\r\n-style:solid;&quot;&gt;\n\n\n\n&lt;u&gt;&lt;/u&gt;\n\n\n\n\n\n\n\n\n\n \n&lt;div style=3D&quot;background-color:rgb(25=\r\n5,255,255);&quot;&gt;\n&lt;span&gt;&nbsp;&lt;/span&gt;\n\n\n&lt;div&gt;\n  &lt;div&gt;\n\n\n    &lt;div&gt;&lt;p&gt;Hello&nbsp;=\r\nVassilis,&lt;/p&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Thanks for the interest in, and excitement=\r\n about, the first batch of Evolving AI Lab publications. :-)&lt;/div&gt;&lt;div&gt;&lt;br&gt;=\r\n&lt;/div&gt;&lt;div&gt;As for CMA-ES, I did not understand from your email if it is ind=\r\need self-adaptive. It seems like you said that the fitness of organisms doe=\r\ns not affect the mutation rate (the issue of =93unbiasedness=94). That woul=\r\nd certainly make the problem of self-adaptive mutation rates go away!&nbsp;=\r\n&lt;/div&gt;\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;You also mention the 1/5th rule, which is not se=\r\nlf-adaptive: it does not have a mutation rate is on each genome that hitchh=\r\nikes along with the other parameters on the genome.&nbsp;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/d=\r\niv&gt;&lt;div&gt;\nDoes CMA-ES do something different? I couldn=92t discern the answe=\r\nr from your notion. For example, in CMA-ES, is there a parameter that evolv=\r\nes that controls the number of mutations per genome per generation? I thoug=\r\nht CMA-ES didn=92t even have a population of individuals, so I=92m not sure=\r\n how it could even have self-adaptive mutation rates.&lt;br&gt;\n&lt;div&gt;\n&lt;span style=\r\n=3D&quot;font-family: Times; font-size: 14px;&quot;&gt;&lt;div style=3D&quot;font-family: Times;=\r\n font-style: normal; font-variant: normal; font-weight: normal; letter-spac=\r\ning: normal; text-indent: 0px; text-transform: none; white-space: normal; w=\r\nord-spacing: 0px;&quot;&gt;\n&lt;span style=3D&quot;font-family: Times;&quot;&gt;&lt;div style=3D&quot;font-=\r\nfamily: Times; font-style: normal; font-variant: normal; font-weight: norma=\r\nl; letter-spacing: normal; text-indent: 0px; text-transform: none; white-sp=\r\nace: normal; word-spacing: 0px;&quot;&gt;\n&lt;span style=3D&quot;font-family: Times;&quot;&gt;&lt;div =\r\nstyle=3D&quot;font-family: Times; font-style: normal; font-variant: normal; font=\r\n-weight: normal; letter-spacing: normal; text-indent: 0px; text-transform: =\r\nnone; white-space: normal; word-spacing: 0px;&quot;&gt;\n&lt;div style=3D&quot;font-family: =\r\nTimes; font-style: normal; font-variant: normal; font-weight: normal; lette=\r\nr-spacing: normal; text-indent: 0px; text-transform: none; white-space: nor=\r\nmal; word-spacing: 0px;&quot;&gt;&lt;span style=3D&quot;font-family: Times;&quot;&gt;&lt;div style=3D&quot;=\r\nfont-family: Times; font-style: normal; font-variant: normal; font-weight: =\r\nnormal; letter-spacing: normal; text-indent: 0px; text-transform: none; whi=\r\nte-space: normal; word-spacing: 0px;&quot;&gt;\n&lt;span style=3D&quot;font-family: Times;&quot;&gt;=\r\n&lt;div style=3D&quot;font-family: Times; font-style: normal; font-variant: normal;=\r\n font-weight: normal; letter-spacing: normal; text-indent: 0px; text-transf=\r\norm: none; white-space: normal; word-spacing: 0px;&quot;&gt;\n&lt;span style=3D&quot;font-fa=\r\nmily: Times;&quot;&gt;&lt;div style=3D&quot;font-family: Times; font-style: normal; font-va=\r\nriant: normal; font-weight: normal; letter-spacing: normal; text-indent: 0p=\r\nx; text-transform: none; white-space: normal; word-spacing: 0px;&quot;&gt;\n&lt;span st=\r\nyle=3D&quot;font-family: Times;&quot;&gt;&lt;div style=3D&quot;font-family: Times; font-style: n=\r\normal; font-variant: normal; font-weight: normal; letter-spacing: normal; t=\r\next-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0=\r\npx;&quot;&gt;\n&lt;span style=3D&quot;font-family: Times;&quot;&gt;&lt;div style=3D&quot;font-family: Times;=\r\n font-style: normal; font-variant: normal; font-weight: normal; letter-spac=\r\ning: normal; text-indent: 0px; text-transform: none; white-space: normal; w=\r\nord-spacing: 0px;&quot;&gt;\n&lt;div style=3D&quot;font-family: Times; font-style: normal; f=\r\nont-variant: normal; font-weight: normal; letter-spacing: normal; text-inde=\r\nnt: 0px; text-transform: none; white-space: normal; word-spacing: 0px;&quot;&gt;&lt;di=\r\nv style=3D&quot;font-family: Times; font-style: normal; font-variant: normal; fo=\r\nnt-weight: normal; letter-spacing: normal; text-indent: 0px; text-transform=\r\n: none; white-space: normal; word-spacing: 0px;&quot;&gt;\n&lt;span style=3D&quot;border-col=\r\nlapse: separate; font-family: Times; font-style: normal; font-variant: norm=\r\nal; font-weight: normal; letter-spacing: normal; text-indent: 0px; text-tra=\r\nnsform: none; white-space: normal;&quot;&gt;&lt;div&gt;&lt;span style=3D&quot;border-collapse: se=\r\nparate; font-family: Times; font-style: normal; font-variant: normal; font-=\r\nweight: normal; letter-spacing: normal; text-indent: 0px; text-transform: n=\r\none; white-space: normal;&quot;&gt;&lt;div&gt;\n&lt;span style=3D&quot;border-collapse: separate; =\r\nfont-variant: normal; letter-spacing: normal; text-indent: 0px; text-transf=\r\norm: none; white-space: normal; word-spacing: 0px;&quot;&gt;&lt;div&gt;&lt;span style=3D&quot;bor=\r\nder-collapse: separate; font-variant: normal; letter-spacing: normal; text-=\r\nindent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;&quot;=\r\n&gt;&lt;div&gt;\n&lt;br&gt;&lt;br&gt;&lt;/div&gt;&lt;div style=3D&quot;font-weight:normal;font-style:normal;&quot;&gt;B=\r\nest regards,&lt;br&gt;&lt;font color=3D&quot;#0a5d19&quot;&gt;&lt;b&gt;Jeff Clune&lt;/b&gt;&lt;/font&gt;&lt;br&gt;&lt;br&gt;Ass=\r\nistant Professor&lt;br&gt;Computer Science&lt;/div&gt;&lt;div style=3D&quot;font-weight:normal;=\r\nfont-style:normal;&quot;&gt;\nUniversity of Wyoming&lt;br&gt;&lt;a href=3D&quot;mailto:jeffclune@u=\r\nwyo.edu&quot; target=3D&quot;_blank&quot;&gt;jeffclune@...&lt;/a&gt;&lt;br&gt;&lt;a href=3D&quot;http://jeff=\r\nclune.com/&quot; target=3D&quot;_blank&quot;&gt;jeffclune.com&lt;/a&gt;&lt;/div&gt;&lt;/span&gt;&lt;/div&gt;&lt;/span&gt;&lt;/=\r\ndiv&gt;&lt;/span&gt;&lt;/div&gt;&lt;/span&gt;&lt;/div&gt;\n&lt;/div&gt;&lt;/div&gt;&lt;/span&gt;&lt;/div&gt;&lt;/span&gt;&lt;/div&gt;&lt;/span=\r\n&gt;&lt;/div&gt;&lt;/span&gt;&lt;/div&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/span&gt;&lt;/div&gt;&lt;/span&gt;&lt;/div&gt;&lt;/span&gt;\n&lt;/=\r\ndiv&gt;&lt;div&gt;&lt;div class=3D&quot;h5&quot;&gt;\n\n&lt;br&gt;&lt;div&gt;&lt;div&gt;On May 4, 2014, at 8:00 AM, Vass=\r\nilis Vassiliades &lt;&lt;a href=3D&quot;mailto:vassilisvas@...&quot; target=3D&quot;_bl=\r\nank&quot;&gt;vassilisvas@...&lt;/a&gt;&gt; wrote:&lt;/div&gt;&lt;br&gt;&lt;blockquote type=3D&quot;cite=\r\n&quot;&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n \n&lt;div style=3D&quot;background-color:rgb(255,255,255);&quot;&gt;\n&lt;span&gt;&=\r\nnbsp;&lt;/span&gt;\n\n\n\n    &lt;div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div dir=3D&quot;ltr&quot;&gt;&lt;div class=3D&quot;gmai=\r\nl_extra&quot;&gt;Hello all,&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D=\r\n&quot;gmail_extra&quot;&gt;Joel and Ken, I think I understand what you are saying about =\r\n&quot;projecting intuitions about the objective world into novelty search&quot; and t=\r\nhe dynamic and divergent nature of Novelty Search (NS)... and also why naiv=\r\nely combining Covariance Matrix Adaptation - Evolution Strategies (CMA-ES) =\r\nmight not work well with NS. A &quot;fitness&quot; peak in generation (g) might dimin=\r\nish in generation (g+1), and generally the &quot;fitness&quot; peaks are constantly m=\r\noving.&lt;/div&gt;\n\n&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_extr=\r\na&quot;&gt;This reminds me a bit of multiagent learning problems, where one could s=\r\nay that the &quot;target&quot; is constantly and adaptively being moved. However, I a=\r\nm not sure whether/how we could frame NS as a multiagent learning scenario.=\r\n I am also imagining the landscape of NS as a... &quot;boiling soup&quot; where bubbl=\r\nes (fitness peaks) keep appearing and disappearing all the time. :) These b=\r\nubbles could be guided by the archive, so that they would never appear at t=\r\nhe same place, or if we were to bound the archive (or use the probabilistic=\r\n approach), they could appear again after some time (or probabilistically).=\r\n&nbsp;&lt;/div&gt;\n\n&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_extr=\r\na&quot;&gt;Questions:&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail=\r\n_extra&quot;&gt;1) Does anyone think that there is any relationship of NS with ther=\r\nmodynamics?&lt;/div&gt;\n&lt;div class=3D&quot;gmail_extra&quot;&gt;\n&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail=\r\n_extra&quot;&gt;2) Do you think there are any relationships between NS and dynamic =\r\noptimization?&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail=\r\n_extra&quot;&gt;3) Joel, you often mention competitive coevolution when talking abo=\r\nut NS. You also said that &quot;&lt;span style=3D&quot;font-family:arial,sans-serif;font=\r\n-size:13px;&quot;&gt;there exist similar fixed-point concepts for competitive co-ev=\r\nolution (like mediocre stable states and disengagement)&quot;. To be honest, I h=\r\naven&#39;t read the literature on these concepts. I was thinking, however, that=\r\n if we were to think about coevolution in game theoretic terms, then&nbsp;&lt;=\r\n/span&gt;&lt;span style=3D&quot;font-family:arial,sans-serif;font-size:13px;&quot;&gt;competit=\r\nive coevolution is like a zero-sum game: the gains of one individual are ba=\r\nlanced by the losses of the others. Is this correct? Does NS behave in this=\r\n manner? It seems to me that it is not a zero-sum game.&lt;/span&gt;&lt;/div&gt;\n\n&lt;div =\r\nclass=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div c=\r\nlass=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;Jeff, I agree wit=\r\nh you that CMA-ES learns correlations. However, allow me to express here my=\r\n understanding as to how it works and its relationship to self-adaptive mut=\r\nation rates (SAMR). * I included this part at the end of the email because =\r\nit got very big :) Anyone who read this please do correct me if something i=\r\ns wrong.&lt;/div&gt;\n\n&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_ex=\r\ntra&quot;&gt;Jeff you wrote: &quot;my guess is that if you tried CMA-ES with and without=\r\n a mutation rate on the genome, you=92d get the same result I report in my =\r\npaper&quot;. CMA-ES by construction uses the mutation rates. It is an important =\r\npart of the algorithm. I would be very curious to see how CMA-ES behaves in=\r\n the same setup used in your paper.&lt;/div&gt;\n\n&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;=\r\n/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;I noticed something in your paper on SAMR. =\r\nI also noticed that in Joel&#39;s and Ken&#39;s paper on SAMR with NS (which I stil=\r\nl haven&#39;t read very carefully, only skimmed though): in both papers the SAM=\r\nR are updated differently than the SAMR of ES (see at the end of the email)=\r\n. Do you think that using an ES approach for SAMR in your setups would lead=\r\n to any different results/conclusions?&lt;br&gt;\n\n&lt;/div&gt;&lt;div class=3D&quot;gmail_extra=\r\n&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;&quot;&#39;novelty plateaus&#39;, a concept my st=\r\nudents and I introduce in our last, not-yet-announced GECCO paper&quot;&lt;/div&gt;&lt;di=\r\nv class=3D&quot;gmail_extra&quot;&gt;Sounds interesting! Congrats by the way on all new =\r\npapers!&lt;/div&gt;\n\n&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_ext=\r\nra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_extr=\r\na&quot;&gt;Ken Lloyd, thanks for the link on Adaptive Stochastic Resonance. I haven=\r\n&#39;t read it yet, but I also noticed that there are several works that have s=\r\nimilarities with NS (e.g., intrinsic motivations, maximization of predictiv=\r\ne information, causal entropic forces, empowerment etc.). Does anyone think=\r\n it would it be interesting to crowdsource a list of similar works on a dif=\r\nferent thread? I think Joel and Ken (Stanley) probably know a lot of them a=\r\nlready.&lt;/div&gt;\n\n&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_ext=\r\nra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_extr=\r\na&quot;&gt;Oliver, you mentioned &quot;&lt;span style=3D&quot;font-family:arial,sans-serif;font-=\r\nsize:13px;&quot;&gt;some work has probabilistically applied the objective function&quot;=\r\n. I would be interested to see this work if you find the link. If I remembe=\r\nr correctly Jeff, JBM and Hod Lipson in &quot;The evolutionary origins of modula=\r\nrity&quot; used something similar, but on the secondary objective (connection co=\r\nst) objective.&lt;/span&gt;&lt;/div&gt;\n\n&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div clas=\r\ns=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=\r\n=3D&quot;gmail_extra&quot;&gt;Best,&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;Vassilis&lt;/div&gt;&lt;div c=\r\nlass=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;\n\n&lt;br&gt;&lt;/div&gt;&lt;div =\r\nclass=3D&quot;gmail_extra&quot;&gt;* Notes on CMA-ES:&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;b=\r\nr&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;In evolution =\r\nstrategies (ES) the mutation operator is usually the primary source of vari=\r\nation. Thus, we could say that while selection exploits the fitness informa=\r\ntion in order to guide search into promising regions, mutation (or variatio=\r\nn) explores the search space and should not use any fitness information to =\r\ndo that, i.e., it should be unbiased. This is a theoretical consideration/r=\r\nequirement (called &quot;unbiasedness&quot;), which naturally leads to the maximum en=\r\ntropy principle. In the case of real-valued search spaces this leads to gau=\r\nssian distributions, while it has been also shown in the literature how to =\r\npotentially handle integer and discrete parameters.&lt;/div&gt;\n\n&lt;div class=3D&quot;gm=\r\nail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;Let&#39;s stay on real-valued s=\r\npaces and let&#39;s say that an individual &quot;a&quot; comprises an object parameter ve=\r\nctor &quot;x&quot;, and its fitness function value &quot;F(x)&quot;: a =3D (x, F(x)). Mutations=\r\n on x work like this:&lt;/div&gt;\n\n&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div clas=\r\ns=3D&quot;gmail_extra&quot;&gt;x&#39; =3D x + z&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;d=\r\niv class=3D&quot;gmail_extra&quot;&gt;with z being related to gaussian distributions.&nb=\r\nsp;&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;\n\n&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;=\r\nThe simplest case is z =3D sigma * ( N_1(0,1), N_2(0,1), ... , N_d(0,1) ), =\r\nwhere sigma is the standard deviation of the normal distribution, d is the =\r\ndimensionality, and N_i(0,1) are independent random samples from the normal=\r\n distribution.&lt;/div&gt;\n\n&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gm=\r\nail_extra&quot;&gt;People have noticed that when sigma is constant and the very sim=\r\nple&nbsp;(1+1)-ES&nbsp;is used (meaning 1 parent creates 1 offspring and th=\r\ne strongest of them survives), then in very simplified unimodal fitness fun=\r\nctions (such as the sphere function), the&nbsp;ES initially displays a peri=\r\nod of improvements; however, after a while it becomes very very slow and lo=\r\nses its evolvability because when approaching the minimum sigma is too big =\r\nand tends to overshoot. By analyzing how sigma influences the success proba=\r\nbility by which an offspring replaces a parent, as well as the progress rat=\r\ne, people have come up with something called the &quot;evolution window&quot; as well=\r\n as the 1/5th control rule that appropriately scales sigma periodically.&lt;/d=\r\niv&gt;\n\n&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;Now, t=\r\nhe 1/5th rule is a heuristic and very specific to (1+1)-ES and the fitness =\r\nlandscape, so people needed something better. Hence the following idea:&nbs=\r\np;&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;\n\n&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;L=\r\net&#39;s add an endogenous/evolvable strategy parameter vector &quot;s&quot; to the indiv=\r\nidual, that contains any other parameters we want (such as the standard dev=\r\niation). So now an individual is: a =3D (x,s,F(x)). But how do we update si=\r\ngma / the standard deviation? How do we mutate this mutation strength/rate?=\r\n The maximum entropy principle specifies that we should use gaussian distri=\r\nbutions, but using these on the standard deviation could lead to negative v=\r\nalues. A neat solution is to do it in log scale:&lt;/div&gt;\n\n&lt;div class=3D&quot;gmail=\r\n_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;ln( sigma&#39; ) =3D ln( sigma ) +=\r\n zeta&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;=\r\nwhich leads to the multiplicative update:&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;\n=\r\n\n&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;sigma&#39; =3D sigma * exp( zeta )&lt;/div&gt;&lt;=\r\ndiv class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;where zeta =\r\n=3D tau * N(0,1), and tau is an exogenous learning parameter which determin=\r\nes the rate and precision of self-adaptation. It is usually proportional to=\r\n 1/sqrt(d).&lt;/div&gt;\n\n&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail=\r\n_extra&quot;&gt;Until now we were talking about a single strategy parameter (mutati=\r\non rate) for the whole genotype, i.e., s =3D sigma, also known as isotropic=\r\n mutations. We can extend this to the case where we have a vector of strate=\r\ngy parameters (mutation rates), i.e., s =3D (sigma_1, sigma_2, ..., sigma_d=\r\n), also known as non-isotropic mutations. This technique is more flexible e=\r\nspecially in high dimensional problems. However, it is still not very effec=\r\ntive in some non-separable problems, e.g., consider a fitness landscape tha=\r\nt is not aligned with the coordinate system. How do we deal with arbitrary =\r\nrotations of the fitness landscape, which is the most general situation?&lt;/d=\r\niv&gt;\n\n&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;Now th=\r\ne idea of Covariance Matrix Adaptation is introduced: let&#39;s estimate the sh=\r\nape of the fitness landscape and adapt a rotation matrix in order to be abl=\r\ne to align our coordinate axes with the principal axes of the fitness lands=\r\ncape. This covariance matrix introduces correlations between the components=\r\n of z.&lt;/div&gt;\n\n&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_extr=\r\na&quot;&gt;So, in:&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_ex=\r\ntra&quot;&gt;1) isotropic mutations: z =3D sigma * ( N_1(0,1), N_2(0,1), ... , N_d(=\r\n0,1) ) =3D sigma * N(0, I), where I is the identity matrix&lt;/div&gt;\n\n&lt;div clas=\r\ns=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;2) non-isotropic mut=\r\nations: z =3D ( sigma_1 * N_1(0,1), sigma_2 * N_2(0,1), ..., sigma_d * N_d(=\r\n0,1) ) =3D D * N(0, I), where D is a diagonal matrix containing all sigma v=\r\nalues&lt;/div&gt;\n\n&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_extra=\r\n&quot;&gt;3) correlated mutations: z =3D M * ( sigma_1 * N_1(0,1), sigma_2 * N_2(0,=\r\n1), ..., sigma_d * N_d(0,1) ) =3D M * D * N(0, I), where M is the rotation =\r\nmatrix that introduces correlations between the components of z, and C =3D =\r\nM^T * M is the covariance matrix (M^T is the transpose of M).&lt;/div&gt;\n\n&lt;div c=\r\nlass=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div cl=\r\nass=3D&quot;gmail_extra&quot;&gt;I won&#39;t get into more detail (e.g., how to estimate the=\r\n covariance matrix etc.), but note that CMA-ES has been shown to work very =\r\nwell in various benchmarks and with small population sizes.&lt;/div&gt;\n\n&lt;/div&gt;&lt;d=\r\niv class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;\n\n    &lt;/div&gt;\n     \n=\r\n\n    \n\n&lt;/div&gt;\n\n\n\n\n\n&lt;/blockquote&gt;&lt;/div&gt;&lt;br&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;br class=\r\n=3D&quot;webkit-block-placeholder&quot;&gt;&lt;/div&gt;\n\n    &lt;/div&gt;\n     \n\n    \n    &lt;div style=\r\n=3D&quot;color:rgb(255,255,255);min-height:0px;&quot;&gt;&lt;/div&gt;\n\n\n&lt;/div&gt;\n\n\n\n  \n\n\n\n\n\n\n&lt;/d=\r\niv&gt;&lt;/div&gt;&lt;/blockquote&gt;&lt;/div&gt;&lt;br&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;br class=3D&quot;webkit-block-=\r\nplaceholder&quot;&gt;&lt;/div&gt;\n\n    &lt;/div&gt;\n     \n\n    \n\n&lt;/div&gt;\n\n\n\n&lt;!-- end group email=\r\n --&gt;\n\n&lt;/blockquote&gt;&lt;/div&gt;&lt;br&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;\r\n--Apple-Mail=_B6E3766E-9212-462C-8081-973E16653259--\r\n\n"}}