{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":115403844,"authorName":"John Arrowwood","from":"&quot;John Arrowwood&quot; &lt;jarrowwx@...&gt;","profile":"jarrowwx","replyTo":"LIST","senderId":"izJJvA_D5Ng3kBxjo4Gc5Sy8I5KwRpqxg3aFAzoEogJVhSkIyPhKwp8Nd7FLYMyG3NDUXwP4ec0FZ_Jwcgm0TRSAVRpiMzwJW9NYSN8M","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] Computation Time","postDate":"1087922090","msgId":1121,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PEJBWTItRjE0MFVTZ2ZjUExLekUwMDA0NTg4N0Bob3RtYWlsLmNvbT4="},"prevInTopic":1118,"nextInTopic":1127,"prevInTime":1120,"nextInTime":1122,"topicId":845,"numMessagesInTopic":99,"msgSnippet":"... So, if I set the pointer to the value of the first function, and then call that function a million times in a loop, then for those million calls, it is as","rawEmail":"Return-Path: &lt;jarrowwx@...&gt;\r\nX-Sender: jarrowwx@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 61881 invoked from network); 22 Jun 2004 16:34:52 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m19.grp.scd.yahoo.com with QMQP; 22 Jun 2004 16:34:52 -0000\r\nReceived: from unknown (HELO hotmail.com) (65.54.247.140)\n  by mta5.grp.scd.yahoo.com with SMTP; 22 Jun 2004 16:34:52 -0000\r\nReceived: from mail pickup service by hotmail.com with Microsoft SMTPSVC;\n\t Tue, 22 Jun 2004 09:34:51 -0700\r\nReceived: from 64.122.44.102 by by2fd.bay2.hotmail.msn.com with HTTP;\n\tTue, 22 Jun 2004 16:34:50 GMT\r\nX-Originating-Email: [jarrowwx@...]\r\nX-Sender: jarrowwx@...\r\nTo: neat@yahoogroups.com\r\nBcc: \r\nDate: Tue, 22 Jun 2004 09:34:50 -0700\r\nMime-Version: 1.0\r\nContent-Type: text/plain; format=flowed\r\nMessage-ID: &lt;BAY2-F140USgfcPLKzE00045887@...&gt;\r\nX-OriginalArrivalTime: 22 Jun 2004 16:34:51.0165 (UTC) FILETIME=[D71DB4D0:01C45876]\r\nX-eGroups-Remote-IP: 65.54.247.140\r\nFrom: &quot;John Arrowwood&quot; &lt;jarrowwx@...&gt;\r\nReply-To: john@...\r\nSubject: Re: [neat] Computation Time\r\nX-Yahoo-Group-Post: member; u=115403844\r\nX-Yahoo-Profile: jarrowwx\r\n\r\n&gt;From: Ian Badcoe &lt;ian_badcoe@...&gt;\n&gt; &gt;Which brings up an important question:  Suppose I have something like \n&gt;this:\n&gt; &gt;\n&gt; &gt;an array of function pointers to custom activation functions\n&gt; &gt;an array of inputs and expected outputs\n&gt;\n&gt;&lt;aside&gt;\n&gt;\n&gt;Before we start, you have an array of function pointers...\n&gt;\n&gt;This is potentially inefficient (at the instruction level) because calls\n&gt;through pointers are a type of conditional branch with the same costs as\n&gt;any conditional.\n\nSo, if I set the pointer to the value of the first function, and then call \nthat function a million times in a loop, then for those million calls, it is \nas efficient as a direct call, yes?  But then when I exit the inner loop and \nthen move the pointer to the next function in the list, then the first time \nI call it I will likely get a pipeline stall.\n\nSo, I have a choice.  I can either generate code that has one loop per \nnetwork with a hard-coded function call, or I can have a double loop and \ncall through the function pointer, where the pointer is set by the outer \nloop.  The double loop has smaller code, and thus fits entirely in the \ncache.  But once per network it will encounter a pipeline stall.  The many \nloops will avoid the stall, but will not all fit in cache memory.  It will \nbe a linear memory access pattern, but it is hard to be certain if the CPU \nwill pre-fetch the instructions in time to prevent a stall.  And a memory \nstall is more expensive than a pipeline stall, is it not?\n\nIn this case, I don&#39;t think unrolling the outer loop is worthwhile.  For one \nthing, I will have to transmit the generated code over the internet.  And \nI&#39;ll bet that if I unroll that loop, the added cost of transmitting the \nsource will exceed the calculation speedup.  And there is a risk that the \nunrolled loop would be slower, anyway.\n\n&gt; &gt;If the size of the activation function is such that it can fit in the \n&gt;code\n&gt; &gt;cache, then it is better to have that be iterated in the outer loop, and \n&gt;the\n&gt; &gt;inputs/outputs iterated in the inner loop.\n&gt; &gt;\n&gt; &gt;If the function is too big to stay in the code cache anyway, then it is\n&gt; &gt;faster to reverse that, right?  Let the data stay in the data cache, and\n&gt; &gt;call all the different functions with that data.\n&gt;\n&gt;Don&#39;t quite get you here.  Could you sketch out which variants on the loops\n&gt;you mean?\n\nfor each function\n  for each input/output combination\n    evaluate( function, inputs, expected_output )\n\nvs.\n\nfor each input/output\n  for each function\n    evaluate( function, inputs, expected_output )\n\nIf the function is HUGE then it won&#39;t all fit in the cache.  In that case, \nwhen you go to call it the second time, the first few instructions will have \nbeen bumped out of the cache by the last instructions of the last call.  \nThus the whole cache is effectively useless.  You will be reading from RAM \nas fast as you can to keep up, and you won&#39;t be able to keep up.  In that \n(unlikely) event, you are better off using the second loop, because then at \nleast the data you are operating on is being referenced often, so it stays \nin the cache.  But it is so unlikely as to be not worth considering.\n\n&gt; &gt;   How big of a\n&gt; &gt;network would I need in order for the generated activation function (no\n&gt; &gt;loops) to exceed the size of the cache?  At what point would the cost of \n&gt;a\n&gt; &gt;loop counter and weight lookups be less than the unrolled loop?  Is it as \n&gt;I\n&gt; &gt;suspect, only for networks so big that worrying about it isn&#39;t worth the\n&gt; &gt;effort?\n&gt;\n[snip]\n&gt;For the network size, you must consider all reads and writes going on in\n&gt;the loop, so the input data and whatever output process you use must also\n&gt;be considered.  ISRT your input is 9x9 and as double?  So that&#39;s just over\n&gt;1/2 K (B+W).  Then there&#39;s the weights (8 bytes per connection), and the\n&gt;current node values (8 bytes per node).  So yes, on this basis I don&#39;t see\n&gt;one network growing so far as to stress the cache.\n\nThat&#39;s what I figured.\n\n&gt;One thing, when you do output, absolutely avoid writing files (including\n&gt;stdio), or consoles in every loop (sorry if that&#39;s insultingly obvious but\n&gt;it&#39;s worth stating the obvious every now and then :) )\n\nI&#39;m not insulted...I had to learn for myself the cost of printf().  :)\n\n&gt; &gt;A better example is extracting a rectangle from a larger rectangle.  You\n&gt; &gt;want to have the Y loop on the outside, and the X loop on the inside, for\n&gt; &gt;precisely that reason.  But if your memory is stored in a one-dimensional\n&gt; &gt;array (which technically it always is), you have to calculate the offset\n&gt; &gt;based on x,y every time.  And even if you store it in a 2-dimensional \n&gt;array,\n&gt; &gt;you just push that job off to the compiler to do transparently for you.\n&gt; &gt;Now, if you instead calculate the offset of the first point in the row, \n&gt;you\n&gt; &gt;can access the next data item by just incrementing your pointer!   The\n&gt; &gt;pointer increment is much faster than the calculated memory offset.\n&gt;\n&gt;It was once, but modern CPUs care a lot less (if at all) and modern\n&gt;compilers will often do the optimization for you.\n\nLet&#39;s just say that making that change made a noticable difference for me \nwhen I was doing it.  But that could be because of the expression that I had \nused before couldn&#39;t be optimized enough to eliminate in-loop calculations.  \nSo, when I deliberately eliminated in-loop calculations, it made a big \ndifference.\n\n\n\n"}}