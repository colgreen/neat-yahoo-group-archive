{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":115403844,"authorName":"John Arrowwood","from":"&quot;John Arrowwood&quot; &lt;jarrowwx@...&gt;","profile":"jarrowwx","replyTo":"LIST","senderId":"9gKcqWMYCViyKETSau-1Mwf1aKSANIrlWdIdAOs3fkKp_Hc3QnpSnBA7zLkO42gOCn9JaJpEeIMt7u2MCEvOGA6JfhYauOO4Tgo-tqQ7","spamInfo":{"isSpam":false,"reason":"0"},"subject":"IEX musings","postDate":"1093486725","msgId":1468,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PEJBWTItRjI4bkUwOFgyYklRamIwMDBjOWYwNEBob3RtYWlsLmNvbT4="},"prevInTopic":0,"nextInTopic":1481,"prevInTime":1467,"nextInTime":1469,"topicId":1468,"numMessagesInTopic":15,"msgSnippet":"First, a quick update.  I decided to try starting with more complicated networks, to see how well NEAT could optimize it.  Problem was, my decision to generate","rawEmail":"Return-Path: &lt;jarrowwx@...&gt;\r\nX-Sender: jarrowwx@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 40055 invoked from network); 26 Aug 2004 02:26:28 -0000\r\nReceived: from unknown (66.218.66.217)\n  by m24.grp.scd.yahoo.com with QMQP; 26 Aug 2004 02:26:28 -0000\r\nReceived: from unknown (HELO hotmail.com) (65.54.247.28)\n  by mta2.grp.scd.yahoo.com with SMTP; 26 Aug 2004 02:26:28 -0000\r\nReceived: from mail pickup service by hotmail.com with Microsoft SMTPSVC;\n\t Wed, 25 Aug 2004 19:18:45 -0700\r\nReceived: from 67.170.177.71 by by2fd.bay2.hotmail.msn.com with HTTP;\n\tThu, 26 Aug 2004 02:18:45 GMT\r\nX-Originating-Email: [jarrowwx@...]\r\nX-Sender: jarrowwx@...\r\nTo: neat@yahoogroups.com\r\nBcc: \r\nDate: Wed, 25 Aug 2004 19:18:45 -0700\r\nMime-Version: 1.0\r\nContent-Type: text/plain; format=flowed\r\nMessage-ID: &lt;BAY2-F28nE08X2bIQjb000c9f04@...&gt;\r\nX-OriginalArrivalTime: 26 Aug 2004 02:18:45.0505 (UTC) FILETIME=[03A5BF10:01C48B13]\r\nX-eGroups-Remote-IP: 65.54.247.28\r\nFrom: &quot;John Arrowwood&quot; &lt;jarrowwx@...&gt;\r\nReply-To: john@...\r\nSubject: IEX musings\r\nX-Yahoo-Group-Post: member; u=115403844\r\nX-Yahoo-Profile: jarrowwx\r\n\r\nFirst, a quick update.  I decided to try starting with more complicated \nnetworks, to see how well NEAT could optimize it.  Problem was, my decision \nto generate optimized code doesn&#39;t scale well.  The generation and \ncompilation of the code can take almost 30 minutes, while the actual \nactivations required for a single generation can take a couple of minutes.  \nThat just won&#39;t do...  So now I need to re-think things, and go back to the \nmore general approach to network activations.  I&#39;ll probably still do an \n&#39;activation order determination&#39; pass on the network before I begin, though, \njust to keep it simple and fast.\n\nBut for scalability reasons, I&#39;m beginning to lean towards the idea of \nhaving NEAT add topology, and then having the client machine optimize the \nweights by whatever means necessary.  Those optimized weights are passed \nback to NEAT, which are then used as the basis for any mutations that take \nplace based on that topology.  Which started me researching weight \noptimization techniques.  Which leads me here...  This is basically me just \ncapturing some thoughts on the subject and sharing them with the group in \nthe hopes that some of you will have some constructive insights into it.\n\nFirst off, what I&#39;m working on is not a controller.  It&#39;s more of a function \napproximation, which means I may be able to take some shortcuts I wouldn&#39;t \nbe able to, otherwise.  Second, understanding of how a network solves a \nproblem might be insightful in finding a way to speed up weight \noptimization.  So I&#39;ll give myself a quick review of how a neural network \nworks, first...\n\nA single neuron is a linear classifier.  Either the inputs meet some \ncriteria, or they don&#39;t.  Generally speaking, in order for a single neuron \nto do its job, some of its inputs should indicate that the neuron should \nhave an output in one direction (e.g. 1), and another set of inputs indicate \nthat the output should go the other way (e.g. 0 or -1).  The weights simply \nwork to segregate the inputs, which must be linearly separable.  If they \naren&#39;t, the network can&#39;t find a solution.\n\nHowever, those &#39;inputs&#39; can be anything.  They can be raw inputs, or they \ncan be the outputs of other classifiers.  Each neuron in the first hidden \n&#39;layer&#39; is essentially a &#39;feature extractor.&#39;  The weights of each of those \nneurons is used to divide the raw inputs into a particular (linear \nseparable) class.  So a value of 1 on the hidden node means that the input \nHAS the characteristic that it is looking for.\n\nThe combination of 1st-layer hidden nodes is, in essence, a feature map.  If \nthese two hidden nodes are active, but the rest are not, that means the \ninputs contain these two features.  But what features are they?  Which \nfeatures of the inputs should that first layer be characterizing?  That \nreally depends on the problem, of course.\n\nBut if that first layer is basically a feature map, why couldn&#39;t the weights \nbe initially set the same way a self-organizing map is?  Then, optimization \nof the weights of the first layer is a fairly straight-forward problem with \na provable (?) solution guaranteed (?) to converge fairly quickly (?).  You \ncan also apply domain-specific knowledge to the initial configuration of \nthose weights.  For example, for the image enlargement domain, you could \ndefine the initial weights in such a way that each node tends to look for a \nparticular shape in the inputs, like a gradient at a particular angle.  \nThose initial weights are randomly perturbed slightly, to avoid symetry \ncausing multiple nodes to be &#39;best&#39; during training of the SOM.  Then just \nlearn those weights as a SOM.  Next, train the second-level weights via \nback-propagation with the first layer weights clamped.  Once optimal as they \ncan be, unclamp the first layer weights, and use back-propagation to zero in \non an optimal solution from a starting point that is more likely than a \nrandom configuration of being able to find the global minimum.  Or at least, \nit seems intuitive that it would be more likely, simply because you know \nthat the first layer is actually dividing up the inputs in a reasonable way.\n\nThe above technique should apply to networks with multiple hidden layers, as \nwell.  For each subsequent &#39;layer&#39; that is not the output layer, first set \nthe weights as an SOM.  Then move on to the next hidden layer.  Once you \nreach the output layer, set the initial weights via back-propagation (or \nsome other gradient-descent technique).  Then start unclamping weights one \nlayer at a time, starting at the lowest level and working your way back up, \noptimizing each layer at a time until the whole network is optimized, but \nfrom a starting position that may well already be near the optimum.\n\nA key to success of that approach is being able to treat the layer as a \nself-organizing map.  But I suspect that a generalized approach to doing so \nis feasible.  Anyone have any thoughts on this idea?\n\nBut there is another aspect of the problem of image enlargement that is not \neven addressed by this discussion.  That is, the &#39;input window&#39; parameters.\n\nIn a desire to create a network that can do arbitrary scale image \nenlargement, I designed a network where one input represents the left-most \nedge of the area of interest, one represents the right-most, one represents \nthe top, and one the bottom.  The output of the network is supposed to \nrepresent the average height of the curve within that region.  That \nrequirement may well be the primary reason I&#39;ve been unable to evolve a \nsolution so far...\n\nSince there is only one output node, and since a neuron/node is a linear \nclassifier, the network must have a structure that presents inputs to that \noutput node that are in essense a simple linearly separable problem.  Can \nsuch a thing even exist for what I&#39;m trying to do?  And would it require me \nto first have a network that enlarges to some high scale, and then uses \nthose inputs to &#39;look up&#39; the ouput value, thereby making me better off just \ndoing a fixed scale enlargement?  I&#39;m going to have to figure that out \nbefore I will know if what I have set out to do is even feasible.\n\nLet&#39;s analyze the problem a bit.  Suppose the inputs are this:\n\n1 2 3\n2 2 3\n3 3 3\n\nSuppose the &#39;position&#39; inputs were 0,0, 1/3, 1/3.  The expected output is 1. \n  At 1/3,0,2/3,1/3, the  expected output is 2.  But at 0,0,2/3,2/3, the \nexpected output is 7/4.  And at 0,0,1,1, the expected output is 22/9.\n\nPerhaps what I should really do at this point is see if I can evolve a \nnetwork that does this.  One set of inputs, 14 expected outputs, and see \nwhat, if anything, NEAT can evolve that would solve the problem.  If NEAT \ncan&#39;t evolve a solution to that, then I know that NEAT can&#39;t evolve an image \nenlargement network that utilizes this technique.  I&#39;ll let you know what I \nfind out...\n\n-- John\n\n\n\n"}}