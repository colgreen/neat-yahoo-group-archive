{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":283334584,"authorName":"petar_chervenski","from":"&quot;petar_chervenski&quot; &lt;petar_chervenski@...&gt;","profile":"petar_chervenski","replyTo":"LIST","senderId":"44mK_z8J4dDe3KLmGCQTtdm4A6MJ7XIk-DtZ052c752iNugpAlD-BzvFmp48h7uGu58pxLGuPvKsi8b14SjHIrRVPfehKQSQQyoaZ9oZadU8IaK5P5Y","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Backpropagation and NEAT","postDate":"1205023846","msgId":3853,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGZxdmM5NitzcW81QGVHcm91cHMuY29tPg==","inReplyToHeader":"PGZxdjIzayszNnNpQGVHcm91cHMuY29tPg=="},"prevInTopic":3852,"nextInTopic":3858,"prevInTime":3852,"nextInTime":3854,"topicId":3846,"numMessagesInTopic":41,"msgSnippet":"Hi Ken, I am evolving time series predictors, in fact even a simplified version of time series predictors, where the network has to answer is the future value","rawEmail":"Return-Path: &lt;petar_chervenski@...&gt;\r\nX-Sender: petar_chervenski@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 31096 invoked from network); 9 Mar 2008 00:50:47 -0000\r\nX-Received: from unknown (66.218.67.97)\n  by m44.grp.scd.yahoo.com with QMQP; 9 Mar 2008 00:50:47 -0000\r\nX-Received: from unknown (HELO n29a.bullet.scd.yahoo.com) (66.94.237.31)\n  by mta18.grp.scd.yahoo.com with SMTP; 9 Mar 2008 00:50:47 -0000\r\nX-Received: from [66.218.69.1] by n29.bullet.scd.yahoo.com with NNFMP; 09 Mar 2008 00:50:47 -0000\r\nX-Received: from [66.218.66.81] by t1.bullet.scd.yahoo.com with NNFMP; 09 Mar 2008 00:50:47 -0000\r\nDate: Sun, 09 Mar 2008 00:50:46 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;fqvc96+sqo5@...&gt;\r\nIn-Reply-To: &lt;fqv23k+36si@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;petar_chervenski&quot; &lt;petar_chervenski@...&gt;\r\nSubject: Re: Backpropagation and NEAT\r\nX-Yahoo-Group-Post: member; u=283334584; y=1G7Tj7mfNbpKbLKejhYwzcpa9dcyHq8x8BUZLpVKuI3hgix2obkc2KipAQ\r\nX-Yahoo-Profile: petar_chervenski\r\n\r\nHi Ken, \n\nI am evolving time series predictors, in fact even a simplified \n=\r\nversion of time series predictors, where the network has to answer is \nthe =\r\nfuture value going up or down. The actual output neuron is a \nsimple step f=\r\nunction, but back-prop can be applied if it is turned \nout to a sigmoid wit=\r\nh a very steep slope. \nThe networks are allowed to have any topology and th=\r\ney are evaluated \non the run, meaning that on each timestep, an error is be=\r\ning \ncalculated (being 0 or 1, depending on the prediction made). \nFirst of=\r\n all, do you think that applying back-prop to these networks \nmay bring any=\r\n accuracy improvement? I know that it is going to eat \nthe CPU resourses, s=\r\no it can be applied at regular intervals, say \neach 50 generations, to push=\r\n the networks&#39;s weights in the right \ndirection, a kind of a hint to the se=\r\narch. I am still thinking of &quot;is \nit worth it?&quot;.. \n\nPeter\n\n\n\n--- In neat@ya=\r\nhoogroups.com, &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt; wrote:\n&gt;\n&gt; A number of peop=\r\nle have programmed backprop into NEAT.  Chris \n&gt; Christenson did a Masters =\r\nthesis on combining NEAT and backprop; a \n&gt; paper based on this work is act=\r\nually in the files section of this \ngroup:\n&gt; \n&gt; \nhttp://f1.grp.yahoofs.com/=\r\nv1/UP7SR8rDovimxlLlvcmGOziLUBIVncb2Tfr7sruoB\n8b\n&gt; taAfELU62JLyQ9XCxXF_Akhcm=\r\ni-\n&gt; \nTH4gpVHIikwnzB59ArOMQfPOAzyw25/Evolving_Trainable_Neural_Networks_6_p=\r\na\nge\n&gt; s.doc\n&gt; \n&gt; Shimon Whiteson implemented it as part of his NEAT+Q rein=\r\nforcement \n&gt; learning method:\n&gt; \n&gt; http://staff.science.uva.nl/~whiteson/pu=\r\nbs/whitesonaaai06.pdf\n&gt; \n&gt; There has been a lot written on backprop in NEAT=\r\n in the archives of \n&gt; this group: just search for &quot;backprop&quot; from the yaho=\r\no page for this \n&gt; group and many messages will pop up.\n&gt; \n&gt; In general, if=\r\n you do not allow recurrence then I believe there is \nno \n&gt; special change =\r\nneeded in the traditional backprop algorithm.  With \n&gt; recurrence you would=\r\n need something like recurrent backprop like \nDerek \n&gt; suggested.  But let&#39;=\r\ns just say you are evolving nonrecurrent \nnetworks- \n&gt; is there a particula=\r\nr problem you have in mind that comes up with \n&gt; applying backprop to such =\r\nnetworks?\n&gt; \n&gt; ken\n&gt; \n&gt; --- In neat@yahoogroups.com, &quot;petar_chervenski&quot; &lt;pe=\r\ntar_chervenski@&gt; \n&gt; wrote:\n&gt; &gt;\n&gt; &gt; Hello there. \n&gt; &gt; \n&gt; &gt; I am looking for =\r\nany back-propagation algorithm that can work on \n&gt; &gt; networks with arbitrar=\r\ny topology such as these that NEAT evolves. \nAll \n&gt; &gt; libraries I found so =\r\nfar either assume layered networks or only \nfeed-\n&gt; &gt; forward ones.. I am c=\r\nonfused. Is there any source code that might \n&gt; help \n&gt; &gt; me? Any back-prop=\r\n implementation that can work on NEAT networks \nsuch \n&gt; &gt; that it can easil=\r\ny be integrated. Or maybe some papers on the \ntopic? \n&gt; &gt; I appreciate any =\r\nhelp from the community. \n&gt; &gt; \n&gt; &gt; Peter\n&gt; &gt;\n&gt;\n\n\n\n"}}