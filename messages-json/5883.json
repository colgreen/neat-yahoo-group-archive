{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Ken","from":"&quot;Ken&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"m0-zKq2iWp6VMeV0yquKtQQGRgytcuwdjNz1gjokLhi5VUxxRUz80o72Do4CBjRFeiUXIcmAO0zOIzG9ZcAAa-H1C3IJ","spamInfo":{"isSpam":false,"reason":"3"},"subject":"Re: Sequence Learning","postDate":"1348934515","msgId":5883,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGs0NzYxaitpZGRpQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGszb3ZlYyszam11QGVHcm91cHMuY29tPg=="},"prevInTopic":5881,"nextInTopic":5889,"prevInTime":5882,"nextInTime":5884,"topicId":5877,"numMessagesInTopic":5,"msgSnippet":"Hi Martin, I think a helpful reference in this area is JÃ¼rgen Schmidhuber s work on memory in recurrent neural networks, which includes writings on sequence","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 44080 invoked from network); 29 Sep 2012 16:01:57 -0000\r\nX-Received: from unknown (98.137.35.161)\n  by m2.grp.sp2.yahoo.com with QMQP; 29 Sep 2012 16:01:57 -0000\r\nX-Received: from unknown (HELO ng8-vm5.bullet.mail.gq1.yahoo.com) (98.136.219.96)\n  by mta5.grp.sp2.yahoo.com with SMTP; 29 Sep 2012 16:01:57 -0000\r\nX-Received: from [98.137.0.85] by ng8.bullet.mail.gq1.yahoo.com with NNFMP; 29 Sep 2012 16:01:57 -0000\r\nX-Received: from [98.137.34.34] by tg5.bullet.mail.gq1.yahoo.com with NNFMP; 29 Sep 2012 16:01:57 -0000\r\nDate: Sat, 29 Sep 2012 16:01:55 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;k4761j+iddi@...&gt;\r\nIn-Reply-To: &lt;k3ovec+3jmu@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 2:3:4:0:0\r\nFrom: &quot;Ken&quot; &lt;kstanley@...&gt;\r\nSubject: Re: Sequence Learning\r\nX-Yahoo-Group-Post: member; u=54567749; y=OEo4HLMJlP8eMNLSHCDZAlrHcTkFDtQ5wTiFADxCe-3K-gH1jRy8\r\nX-Yahoo-Profile: kenstanley01\r\n\r\n\n\nHi Martin, I think a helpful reference in this area is J=FCrgen Schmidhub=\r\ner&#39;s work on memory in recurrent neural networks, which includes writings o=\r\nn sequence learning:\n\nhttp://www.idsia.ch/~juergen/rnn.html\n\nHis &quot;deep memo=\r\nry&quot; papers (also shown on that page) also might be relevant and give you id=\r\neas.\n\nWhile most of the work there does not involve evolution (though some =\r\ndoes), it does show you the kinds of tasks that have been attempted in sequ=\r\nence learning and memory in general.\n\nken\n\n--- In neat@yahoogroups.com, &quot;ma=\r\nrtin_pyka&quot; &lt;martin.pyka@...&gt; wrote:\n&gt;\n&gt; Sorry for being so unspecific in my=\r\n previous post. I want to develop a network (with certain properties and co=\r\nnstrains) that learns a sequence (or multiple sequences) during its lifetim=\r\ne. I think I will start with supervised learning, that is, the sequence is =\r\npresented to the network (one or several times) and the network should be a=\r\nble to reproduce the sequence when the beginning of the sequence is present=\r\ned. Of course the problem can be defined in various ways and specified in m=\r\nore detail when sequence learning in an agent simulation or in a concrete p=\r\nroblem domain is considered. \n&gt; \n&gt; Therefore, I would like to know which ar=\r\ne the common problem domains that has been used in previous studies to inve=\r\nstigate sequence learning during the lifetime of the network. \n&gt; \n&gt; Best,\n&gt;=\r\n Martin \n&gt; \n&gt; --- In neat@yahoogroups.com, &quot;Ken&quot; &lt;kstanley@&gt; wrote:\n&gt; &gt;\n&gt; &gt;=\r\n \n&gt; &gt; \n&gt; &gt; Hi Martin, could you provide a little more info on what you mean=\r\n by sequence learning?  Do you mean evolving a network to perform a sequenc=\r\ne of actions, or perhaps evolving a network that learns a sequence during i=\r\nts lifetime, or something else (such as learning the predict the next value=\r\n in a sequence)?\n&gt; &gt; \n&gt; &gt; ken\n&gt; &gt; \n&gt; &gt; --- In neat@yahoogroups.com, &quot;martin=\r\n_pyka&quot; &lt;martin.pyka@&gt; wrote:\n&gt; &gt; &gt;\n&gt; &gt; &gt; I would ask the community for good=\r\n sequence learning problems that can be used to train neural networks. I th=\r\nink, a maze-navigation problem is probably a good starting point to study s=\r\nequence learning. But what other problems have been used in the literature =\r\nto study sequence learning?\n&gt; &gt; &gt; \n&gt; &gt; &gt; Best,\n&gt; &gt; &gt; Martin\n&gt; &gt; &gt;\n&gt; &gt;\n&gt;\n\n\n\n"}}