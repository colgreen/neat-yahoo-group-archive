{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":82117382,"authorName":"Jim O&#39;Flaherty, Jr.","from":"&quot;Jim O&#39;Flaherty, Jr.&quot; &lt;jim_oflaherty_jr@...&gt;","profile":"jim_oflaherty_jr","replyTo":"LIST","senderId":"n9KdmR1k22zGOJJLHWYA1WPo83Q_0Asb8RKe-x-DJCZxnEy1Q3nGj2NFEjXnSRMCPmMHJm_jU7B0xAiXaFcXnmy3Awo9yRlOLgX4pvKV4EZJkTqlegDZ-hw","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] Re: simple question","postDate":"1083619108","msgId":736,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDIwMDQwNTAzMjExODI4LjEwNzA0LnFtYWlsQHdlYjE0MzA0Lm1haWwueWFob28uY29tPg==","inReplyToHeader":"PGM3NmEwMys0NWlzQGVHcm91cHMuY29tPg=="},"prevInTopic":735,"nextInTopic":737,"prevInTime":735,"nextInTime":737,"topicId":730,"numMessagesInTopic":13,"msgSnippet":"Ken, I think I may be missing your point somewhere.  I am interested in understanding.  As such, this is how what your wrote occurred to me. ... we care if it","rawEmail":"Return-Path: &lt;jim_oflaherty_jr@...&gt;\r\nX-Sender: jim_oflaherty_jr@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 42901 invoked from network); 3 May 2004 21:18:45 -0000\r\nReceived: from unknown (66.218.66.216)\n  by m24.grp.scd.yahoo.com with QMQP; 3 May 2004 21:18:45 -0000\r\nReceived: from unknown (HELO web14304.mail.yahoo.com) (216.136.173.80)\n  by mta1.grp.scd.yahoo.com with SMTP; 3 May 2004 21:18:44 -0000\r\nMessage-ID: &lt;20040503211828.10704.qmail@...&gt;\r\nReceived: from [205.158.160.209] by web14304.mail.yahoo.com via HTTP; Mon, 03 May 2004 14:18:28 PDT\r\nDate: Mon, 3 May 2004 14:18:28 -0700 (PDT)\r\nTo: neat@yahoogroups.com\r\nIn-Reply-To: &lt;c76a03+45is@...&gt;\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=us-ascii\r\nX-eGroups-Remote-IP: 216.136.173.80\r\nFrom: &quot;Jim O&#39;Flaherty, Jr.&quot; &lt;jim_oflaherty_jr@...&gt;\r\nSubject: Re: [neat] Re: simple question\r\nX-Yahoo-Group-Post: member; u=82117382\r\nX-Yahoo-Profile: jim_oflaherty_jr\r\n\r\nKen,\n\nI think I may be missing your point somewhere.  I am interested in understanding.  As such, this\nis how what your wrote occurred to me.\n\nYou wrote:\n&gt; ...The question is not whether it could perform as well with 25% less structure, but why would\nwe care if it did?\n\nI think this may be a classic example of the theoretical (mathmatically) not effectively dealing\nwith some of the tradeoffs necessary in practical application (actual computation effort spent\ncombined with obtaining better, not best, results).\n\nFor example, please imagine the scenerio where a specimen is produced which required 30% less\n&quot;computational effort&quot; (machine instructions, not network activations) to find a solution 50% the\nsize that of another method.  Theoretically, as long as the resultant specimens behave at a\nsimilar performance level in the target domain *and* this performance level is essentially the\nsole measurement, then I would agree with you.\n\nHowever, if you have finite computational resources, intend to use NEAT to generate sub-components\nto a larger structure *and* you intend to do runs in the ten thousand to million generation range,\nthen doesn&#39;t the &quot;efficiency&quot; of the specimens becomes highly influential in the quantity of\nexperiments one can run and evaluate over-all in limited time?  *Computational* efficiencies can\nmake a slightly less efficient search (with smaller specimens) substantially outperform that of\nanother method (with larger specimens).  At least that is what sure seems to be the case as far as\nI can see, empirically.\n\nConcretely and in the domain of my personal interest thus far, Checkers, I am VERY interested in\nensuring I am maximizing computing resources. I am NOT interested in finding the perfect checker\nplayer.  I am very interested in a resulting phenotype that has the minimal number of genes to\nproduce an effective player.  There are local optima &quot;solutions&quot; that are VERY DESIRABLE and are\n&quot;imperfect&quot; in that they did not find the &quot;optima&quot;.  The found solution given the computation\neffort is good enough.  In other words, I will put up with an &quot;incomplete&quot; or &quot;imperfect&quot; search\nto arrive at a good enough solution.\n\nI get that my &quot;low gene count phenotypes&quot; may lose some robustness the larger gene count\nphenotypes might be able to offer.  However, in *my* world, if I gain only a 0.05% advantage with\nthe more complex type at a cost of more than double the gene count in the phenotype, I would need\nto justify that huge tradeoff, right?  I am not saying it is unjustifiable.  I am just saying the\ntradeoff exists and it would be useful to me to be able to have that choice, right?\n\n\nYou wrote:\n&gt; ...the point of NEAT is that *every* networks you evaluate over the course of all of evolution\nhas a cost.  What&#39;s important is that on average over all of evolution we are minimizing the space\nwe have to search through.\n\nIf I am reading this correctly, your definition of &quot;space to search through&quot; is to minimize the\nnumber of topologies (but not topological complexity) needed to search the space.  Without the\ncomplexity being bounded, won&#39;t this will inevitably lead to an exponential growth of\ncomputational effort without a downward pressure forcing topological efficiency (number of\nnodes/number of connections kept minimized for fittest specimen)?\n\nIf that is true, then given anything but very very simple domains, XOR and Tic-Tac-Toe, I then\nthink your above assertion might not be as useful for practical application.  This is where I\nthink I may be missing something very subtle.  If you have time and patience, please elaborate\nwith concrete examples here.  For whatever reason, I am not getting the theory.  :^)\n\nAs I consider both the topological generation via its genotype of the specimen *and* the\ncomputational effort uniquely required by the resulting phenotype to be the &quot;cost&quot; of a specimen,\nwhat use is there in my minimizing the &quot;search space&quot; (reducing the count of genotypes but not\ncomplexity) only to have enormous specimens (phenotypes with many genes) which have a very high\ncost, as I define cost?\n\nHelp!  What is it I am missing here?\n\n\nJim O&#39;Flaherty\n\n\n--- Kenneth Stanley &lt;kstanley@...&gt; wrote:\n&gt; --- In neat@yahoogroups.com, Derek James &lt;djames@g...&gt; wrote:\n&gt; &gt; The only way to know if you&#39;ve got redundant features would be to \n&gt; use\n&gt; &gt; one of the above approaches, correct?  Has any of this type of\n&gt; &gt; analysis been done?  That is, how do we know that the champions \n&gt; from\n&gt; &gt; experiments such as double pole-balancing or the robot duels could \n&gt; not\n&gt; &gt; perform as well with 25% or more of their topology removed?\n&gt; &gt; \n&gt; \n&gt; I think this is missing the point, though.  This is exactly the \n&gt; perspective I was trying to argue we should change.  The question is \n&gt; not whether it could perform as well with 25% less structure, but \n&gt; why would we care if it did?  The goal of systems like that, that \n&gt; try to prune down at the end of evolution to a small &quot;good&quot; \n&gt; solution, is just to find a final solution that looks good.  But the \n&gt; point of NEAT is that *every* networks you evaluate over the course \n&gt; of all of evolution has a cost.  What&#39;s important is that on average \n&gt; over all of evolution we are minimizing the space we have to search \n&gt; through.  \n&gt; \n&gt; ken\n&gt; \n&gt; \n\n\n\n\t\n\t\t\n__________________________________\nDo you Yahoo!?\nWin a $20,000 Career Makeover at Yahoo! HotJobs  \nhttp://hotjobs.sweepstakes.yahoo.com/careermakeover \n\n"}}