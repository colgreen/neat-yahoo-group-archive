{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":283334584,"authorName":"petar_chervenski","from":"&quot;petar_chervenski&quot; &lt;petar_chervenski@...&gt;","profile":"petar_chervenski","replyTo":"LIST","senderId":"MyeW-Ous5iIbUL4b1t23Rd3w_HWvXTgMM1NUMuxAdUIpcn7wRXJ2DyfsYJAY64aYATfKN4y_TIPxeYc6fllF6NHFCAvIxzY9t1kExUnE__eIVfYZ_80","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Mix of Identity and Sigmoid activation functions","postDate":"1176299121","msgId":3115,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGV2aW9waCtsa25tQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGV2aWF0dSt1M2JnQGVHcm91cHMuY29tPg=="},"prevInTopic":3114,"nextInTopic":3116,"prevInTime":3114,"nextInTime":3116,"topicId":3114,"numMessagesInTopic":8,"msgSnippet":"Hi Shane, I don t know what the identity function is exactly.. I need to learn that. But I suppose you can also try to split the neuron activation into stages,","rawEmail":"Return-Path: &lt;petar_chervenski@...&gt;\r\nX-Sender: petar_chervenski@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 88756 invoked from network); 11 Apr 2007 13:46:38 -0000\r\nReceived: from unknown (66.218.67.35)\n  by m47.grp.scd.yahoo.com with QMQP; 11 Apr 2007 13:46:38 -0000\r\nReceived: from unknown (HELO n5b.bullet.sp1.yahoo.com) (69.147.64.186)\n  by mta9.grp.scd.yahoo.com with SMTP; 11 Apr 2007 13:46:38 -0000\r\nReceived: from [216.252.122.217] by n5.bullet.sp1.yahoo.com with NNFMP; 11 Apr 2007 13:45:22 -0000\r\nReceived: from [66.218.69.1] by t2.bullet.sp1.yahoo.com with NNFMP; 11 Apr 2007 13:45:22 -0000\r\nReceived: from [66.218.66.86] by t1.bullet.scd.yahoo.com with NNFMP; 11 Apr 2007 13:45:22 -0000\r\nDate: Wed, 11 Apr 2007 13:45:21 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;evioph+lknm@...&gt;\r\nIn-Reply-To: &lt;eviatu+u3bg@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0\r\nFrom: &quot;petar_chervenski&quot; &lt;petar_chervenski@...&gt;\r\nSubject: Re: Mix of Identity and Sigmoid activation functions\r\nX-Yahoo-Group-Post: member; u=283334584; y=34ei5VVpPWEj2QQH4nQZvWI4RSgRFRWbqerIinvMb6XOq7Es5N-WoxXCVg\r\nX-Yahoo-Profile: petar_chervenski\r\n\r\nHi Shane, \n\nI don&#39;t know what the identity function is exactly.. I need to =\r\nlearn \nthat. \nBut I suppose you can also try to split the neuron activation=\r\n into \nstages, introduce another function that sums the weighted inputs in =\r\n\ndifferent ways, you know that this is usually addition, but \nmultiplicatio=\r\nn is also promising, since it is like the AND logical \noperation on those i=\r\nnputs. When this is combined with linear and \nother activation functions, i=\r\nt turns NEAT into actually a .. \nsomething like a math equation evolving sy=\r\nstem.. I don&#39;t think this \nwill enhance performace in most tasks, but inter=\r\nesting results may be \nobtained. \nThere may be also a MAX summing function,=\r\n that returns the maximum \ninput of all. This way the neurons can have winn=\r\ner-take-all \nbehaviour, I guess. I have implemented this and I see it gives=\r\n \nresults. \n\nPeter\n\n--- In neat@yahoogroups.com, &quot;shanemcdonaldryan&quot; \n&lt;shan=\r\nemcdonaldryan@...&gt; wrote:\n&gt;\n&gt; I am sure this must have been done but I can&#39;=\r\nt find any papers on \nit.\n&gt; \n&gt; Has anyone tried starting out with an initia=\r\nl population that \nconnects\n&gt; all the inputs to the outputs with a parallel=\r\n set of neurons using\n&gt; identity as the activation function? Then add a new=\r\n type of mutation\n&gt; that can add new neurons with the identity activation f=\r\nunction.\n&gt; \n&gt; Basically what I am proposing is starting with the same initi=\r\nal NEAT\n&gt; configuration of all the inputs connected to the outputs via \nsig=\r\nmoids.\n&gt; But doubling up this configuration with an identical net connected=\r\n\n&gt; via the identity activation function.\n&gt; \n&gt; The whole point of this would=\r\n be to create a hybrid network that is\n&gt; good at capturing the linear aspec=\r\nts (A+B, cross product) of the \ndata\n&gt; and composing it with the non-linear=\r\n aspects.\n&gt; \n&gt; But now that I think about it. I guess a good starting quest=\r\nion \nwould\n&gt; be is a Neural Net using only the  Identity activation functio=\r\nn good\n&gt; at approximating linear functions?\n&gt; \n&gt; Something like this might =\r\nbe good at approximating time series with \na\n&gt; simple linear trend. So you =\r\ndon&#39;t have to detrend the data.\n&gt; \n&gt; Thanks,\n&gt; \n&gt; Shane\n&gt;\n\n\n\n"}}