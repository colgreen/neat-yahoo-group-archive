{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"CYH8yq1mx9CAh5O7eFQQEuynjP0DxU8vav67BAyWFxaWbnlGpGkTBBU8d6loRxoTPkrC3chrBJVjTa8KFKunsMVR07NTB8R1cgtH21dcJ2MX","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: Multiple networks","postDate":"1079507224","msgId":519,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGMzOHRlbyszaDlmQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDFmZjMwMWM0MGJhNyQzNTQ0Mjk4MCQ3MGNiMDEwYUBtYWlsMndvcmxkLmNvbT4="},"prevInTopic":518,"nextInTopic":520,"prevInTime":518,"nextInTime":520,"topicId":514,"numMessagesInTopic":16,"msgSnippet":"Hi Chad, it sounds like a reasonable idea to me.  A lot of the time it s better to combine expertise than to go with only one method. Actually, here is a paper","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 51448 invoked from network); 17 Mar 2004 07:07:04 -0000\r\nReceived: from unknown (66.218.66.216)\n  by m1.grp.scd.yahoo.com with QMQP; 17 Mar 2004 07:07:04 -0000\r\nReceived: from unknown (HELO n4.grp.scd.yahoo.com) (66.218.66.88)\n  by mta1.grp.scd.yahoo.com with SMTP; 17 Mar 2004 07:07:04 -0000\r\nReceived: from [66.218.67.183] by n4.grp.scd.yahoo.com with NNFMP; 17 Mar 2004 07:07:04 -0000\r\nDate: Wed, 17 Mar 2004 07:07:04 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;c38teo+3h9f@...&gt;\r\nIn-Reply-To: &lt;1ff301c40ba7$35442980$70cb010a@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Length: 1661\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-eGroups-Remote-IP: 66.218.66.88\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: Re: Multiple networks\r\nX-Yahoo-Group-Post: member; u=54567749\r\nX-Yahoo-Profile: kenstanley01\r\n\r\nHi Chad, it sounds like a reasonable idea to me.  A lot of the time \nit&#39;s better to combine expertise than to go with only one method.  \nActually, here is a paper on this subject from our research group:\n\nhttp://nn.cs.utexas.edu/pub-view.php?RECORD_KEY(Pubs)=PubID&PubID\n(Pubs)=16\n\nNote that this paper does not use NEAT as its neuroevolution method \n(it came out around the time NEAT was first coming out), but it \nstill addresses the general idea of combining expertise and \nexpressing confidence.  Of course, it would not work exactly the way \nthis paper says if you were to combine evolutionary and non-\nevolutionary approaches at the same time.\n\nken\n\n--- In neat@yahoogroups.com, &quot;Chad Bohannan&quot; &lt;chad@b...&gt; wrote:\n&gt; I&#39;ve been thinking about using &#39;conflicting&#39; networks/controllers\n&gt; operating the same set of inputs and outputs. \n&gt; My thoughts revolve around a weighted summing device that recieves \nboth\n&gt; the ouput and an associated confidence from each network, and the \noutput\n&gt; of the system is a function of the confidence levels ( sudden rise \nin\n&gt; confidence having more meaning than a steady state confidence ) \nand the\n&gt; outputs of the functions.\n&gt; This approach might allow for a coupling of seemingly incompatable\n&gt; networks or controllers, such as a LSTM to recognize features in a\n&gt; landscape, an evolved neural network for obsticle avoidence, and a\n&gt; conventional controller for &#39;after-market&#39; behaviour add-ons, with \nout\n&gt; re-evolving or re-teaching the previous networks.\n&gt; The idea sounds like fun, except that I have no idea how to go \nabout\n&gt; teaching anything how to describe how confident it is in its own \noutput.\n&gt; Any thoughts?\n\n\n"}}