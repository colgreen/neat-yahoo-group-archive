{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":60940451,"authorName":"Jeff Haynes","from":"&quot;Jeff Haynes&quot; &lt;jeff@...&gt;","profile":"jefffhaynes","replyTo":"LIST","senderId":"YYFEyiizB1tLLBnklRXd6Go_HsyunWZFjUw-gXFExlte1DwAIT08_Ie61IAh3tXcwvRMMFJ3i2Z8S_-swASpxgqgdA46c1v5-w","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [neat] Re: Introduction---recurrency question","postDate":"1126067469","msgId":2258,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDIwMDUwOTA3MDQyNzA4Lk04MDE1NkBkZWFyZG9yZmYuY29tPg==","inReplyToHeader":"PGRmbHEydis2NHZxQGVHcm91cHMuY29tPg==","referencesHeader":"PDIwMDUwOTA2MjM0MjU5Lk04MjIyN0BkZWFyZG9yZmYuY29tPiA8ZGZscTJ2KzY0dnFAZUdyb3Vwcy5jb20+"},"prevInTopic":2257,"nextInTopic":0,"prevInTime":2257,"nextInTime":2259,"topicId":2209,"numMessagesInTopic":42,"msgSnippet":"I agree.  The practical, experimental work that you do is much more valuable than idle musings.  It is also, of course, the more arduous of the two; often a","rawEmail":"Return-Path: &lt;jeff@...&gt;\r\nX-Sender: jeff@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 10504 invoked from network); 7 Sep 2005 04:31:22 -0000\r\nReceived: from unknown (66.218.66.216)\n  by m28.grp.scd.yahoo.com with QMQP; 7 Sep 2005 04:31:22 -0000\r\nReceived: from unknown (HELO eagle.deardorff.com) (64.92.206.84)\n  by mta1.grp.scd.yahoo.com with SMTP; 7 Sep 2005 04:31:22 -0000\r\nReceived: from eagle.deardorff.com (jeffie@localhost [127.0.0.1])\n\tby eagle.deardorff.com (8.13.3/8.13.3) with ESMTP id j874V9Y8002630\n\tfor &lt;neat@yahoogroups.com&gt;; Wed, 7 Sep 2005 00:31:09 -0400\r\nTo: neat@yahoogroups.com\r\nDate: Wed, 7 Sep 2005 00:31:09 -0400\r\nMessage-Id: &lt;20050907042708.M80156@...&gt;\r\nIn-Reply-To: &lt;dflq2v+64vq@...&gt;\r\nReferences: &lt;20050906234259.M82227@...&gt; &lt;dflq2v+64vq@...&gt;\r\nX-Mailer: Open WebMail 2.51 20050323\r\nX-OriginatingIP: 69.143.137.249 (jeffie)\r\nMIME-Version: 1.0\r\nContent-Type: text/plain;\n\tcharset=iso-8859-1\r\nX-Spam-Status: No, Not spam. Probably whitelisted.\r\nX-Scanned-By: MIMEDefang 2.51 on 64.92.206.84\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: &quot;Jeff Haynes&quot; &lt;jeff@...&gt;\r\nSubject: Re: [neat] Re: Introduction---recurrency question\r\nX-Yahoo-Group-Post: member; u=60940451; y=C2XPoiJnie_VMuYe44FJqYt0xj3Dv3_2j8_ip29JAUVJUj2-BgM\r\nX-Yahoo-Profile: jefffhaynes\r\n\r\nI agree.  The practical, experimental work that you do is much more valuable\nthan idle musings.  It is also, of course, the more arduous of the two; often\na sign that something is worth doing :)\n\nthanks,\n\nJeff \n\nOn Wed, 07 Sep 2005 04:25:03 -0000, Kenneth Stanley wrote\n&gt; Jeff, I believe we are in agreement.  I just wanted to make sure we \n&gt; don&#39;t stop thinking about the interesting practical issues.  The \n&gt; theoretical side deserves its airing as well, and tells us something \n&gt; about possibility.  I am strongly in agreement with you that we do \n&gt; not want to &quot;inadvertantly close the best doors.&quot;\n&gt; \n&gt; Of course, theoretical arguments can cause doors to close \n&gt; prematurely as well.  Minsky&#39;s famous &quot;Perceptrons&quot; book, which \n&gt; showed through theoretical analysis that a perceptron is limited in \n&gt; what it can compute, shut down the entire field of neural network \n&gt; research for decades.\n&gt; \n&gt; Ultimately neither practical hurdles nor theoretical limitations are \n&gt; the final word on what is impossible in practice.  Just because you \n&gt; couldn&#39;t get your method to solve the problem, or just because a \n&gt; proof showed that a certain type of structure cannot do it, does not \n&gt; mean something else won&#39;t come along that defies expectation.  Hence \n&gt; my advice to keep an open mind.\n&gt; \n&gt; ken\n&gt; \n&gt; --- In neat@yahoogroups.com, &quot;Jeff Haynes&quot; &lt;jeff@d...&gt; wrote:\n&gt; &gt; At the risk of offending the gods, I&#39;m going to humbly add 2 cents.\n&gt; &gt; \n&gt; &gt; While I fully agree with everything you&#39;ve said here, I think \n&gt; there is an\n&gt; &gt; important fundamental truth to all this.  And, it is simply that \n&gt; which I was\n&gt; &gt; trying and failed to convey.\n&gt; &gt; \n&gt; &gt; I think the fact that it is possible to build a network capable of \n&gt; learning\n&gt; &gt; (no further classical training required!) is, in a manner of \n&gt; speaking, the\n&gt; &gt; whole kit and kaboodle.\n&gt; &gt; \n&gt; &gt; Ultimately, isn&#39;t it the creation of a true learning machine that \n&gt; we strive to\n&gt; &gt; accomplish?  I am not in any way purporting to know the best path \n&gt; to this\n&gt; &gt; machine and as to the specifics (such as representation of memory) \n&gt; I have yet\n&gt; &gt; less insight.\n&gt; &gt; \n&gt; &gt; That being said, I think any confusion as to what is ultimately \n&gt; possible, and\n&gt; &gt; what is not, can inadvertantly close the best doors.  At the very \n&gt; least, I do\n&gt; &gt; not believe it is an insignificant conclusion.\n&gt; &gt; \n&gt; &gt; Regards,\n&gt; &gt; \n&gt; &gt; Jeff\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; On Tue, 06 Sep 2005 21:54:07 -0000, Kenneth Stanley wrote\n&gt; &gt; &gt; John (caloyannis) is right when he says the &quot;trick is to find \n&gt; it!&quot;\n&gt; &gt; &gt; \n&gt; &gt; &gt; I think it&#39;s important to keep in mind in discussions such as \n&gt; this \n&gt; &gt; &gt; one that theoretical facts are usually of little practical \n&gt; &gt; &gt; consequence.  That recurrent networks can represent any Turing \n&gt; &gt; &gt; machine is about as practically useful as knowing that you can \n&gt; &gt; &gt; theoretically build a fully-functioning computer out of water \n&gt; pipes \n&gt; &gt; &gt; and water.  The real question with respect to recurrent NN&#39;s is \n&gt; &gt; &gt; what&#39;s reasonably easy to find and represent, not what&#39;s \n&gt; &gt; &gt; theoretically possible.  Also note that what&#39;s easy to find is \n&gt; not \n&gt; &gt; &gt; always so much a property of the neural network alone as a \n&gt; property \n&gt; &gt; &gt; of the search process used to generate it combined with the type \n&gt; of \n&gt; &gt; &gt; neural network.\n&gt; &gt; &gt; \n&gt; &gt; &gt; In any case, my point is that it certainly merits long and deep \n&gt; &gt; &gt; discussion what can or cannot be done with a recurrent network, \n&gt; &gt; &gt; regardless of what is possible in theory, even if all things are \n&gt; &gt; &gt; possible in theory.  We are generally interested in practice, \n&gt; which \n&gt; &gt; &gt; is much more complicated, which is also why it&#39;s fun: There is a \n&gt; lot \n&gt; &gt; &gt; of room for creativity and in fact standard recurrent structures \n&gt; may \n&gt; &gt; &gt; indeed be more difficult to discover than other memory-based \n&gt; &gt; &gt; realizations such as Hebbian networks.  For this reason, we \n&gt; should \n&gt; &gt; &gt; keep an open mind.\n&gt; &gt; &gt; \n&gt; &gt; &gt; ken\n&gt; &gt; &gt; \n&gt; &gt; &gt; --- In neat@yahoogroups.com, &quot;johncaloyannis&quot; \n&gt; &lt;johncaloyannis@y...&gt; \n&gt; &gt; &gt; wrote:\n&gt; &gt; &gt; &gt; Hello everybody\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; I started dabbling in AI abouit twenty years ago on and off. \n&gt; For a\n&gt; &gt; &gt; &gt; while  I thought that genetic programming could be an avenue \n&gt; to \n&gt; &gt; &gt; follow\n&gt; &gt; &gt; &gt; (Koza). However lately I came across the NEAT concept which I \n&gt; found\n&gt; &gt; &gt; &gt; fascinating since it combines the best of both worlds (neural \n&gt; &gt; &gt; networks\n&gt; &gt; &gt; &gt; + evolution).\n&gt; &gt; &gt; &gt; I have followed the discussions you guys have with interest. I \n&gt; &gt; &gt; think\n&gt; &gt; &gt; &gt; that since the recurrent neural networks are intractable\n&gt; &gt; &gt; &gt; mathematically, apart from some very special cases (hopfield), \n&gt; one \n&gt; &gt; &gt; can\n&gt; &gt; &gt; &gt; only speculate and experiment to learn things.\n&gt; &gt; &gt; &gt; Regarding the discussion about the recurrency question and \n&gt; what a \n&gt; &gt; &gt; RNN\n&gt; &gt; &gt; &gt; can or can not do there is a very interesting paper which \n&gt; proves \n&gt; &gt; &gt; that\n&gt; &gt; &gt; &gt; there is a turing machine for every RNN and vice-versa. Since \n&gt; &gt; &gt; anything\n&gt; &gt; &gt; &gt; that is computable can be achieved with a turing machine so \n&gt; can it\n&gt; &gt; &gt; &gt; through an RNN. The trick is to find it :)\n&gt; &gt; &gt; &gt; http://www.uwasa.fi/stes/step96/step96/hyotyniemi1\n&gt; &gt; &gt; \n&gt; &gt; &gt; ------------------------ Yahoo! Groups Sponsor ------------------\n&gt; --~--&gt; \n&gt; &gt; &gt; Get fast access to your favorite Yahoo! Groups. Make Yahoo! your \n&gt; &gt; &gt; home page \n&gt; http://us.click.yahoo.com/dpRU5A/wUILAA/yQLSAA/7brrlB/TM\n&gt; &gt; &gt; -----------------------------------------------------------------\n&gt; ---~-&gt;\n&gt; &gt; &gt; \n&gt; &gt; &gt; Yahoo! Groups Links\n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; -------------------------------------------------------------------\n&gt; ------\n&gt; &gt; www.greghaynes.com - check out my bro&#39;s stuff and order something! \n&gt; thx.\n&gt; \n&gt; ------------------------ Yahoo! Groups Sponsor --------------------~--&gt; \n&gt; Get fast access to your favorite Yahoo! Groups. Make Yahoo! your \n&gt; home page http://us.click.yahoo.com/dpRU5A/wUILAA/yQLSAA/7brrlB/TM\n&gt; --------------------------------------------------------------------~-&gt;\n&gt; \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; \n\n\n-------------------------------------------------------------------------\nwww.greghaynes.com - check out my bro&#39;s stuff and order something! thx.\n\n\n"}}