{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":434634266,"authorName":"Vassilis Vassiliades","from":"Vassilis Vassiliades &lt;vassilisvas@...&gt;","profile":"v.vassiliades","replyTo":"LIST","senderId":"Ep66f82G09iw2X0CnZyF7Zs36OEhsDqWouOHfrGiwprO2tAWvtDCGfQaY8XQvQhTlUXHXV2drbGSI97jGVx6OtFvEnexjTRlwiKS4tVer0yTzys","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [neat] New paper: Automated Generation of Environments to Test the General Learning Capabilities of AI Agents","postDate":"1398790203","msgId":6281,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PENBTnRYaG12dUpHMkxkWXpSRGVXRldpU01HNW1iK3pmQkxWZ0VhQm10dHkyV0ZQaXhFd0BtYWlsLmdtYWlsLmNvbT4=","inReplyToHeader":"PENBK2R1aW1PMjRzYWtPWFNNVnVxYkVleDgremlCbVFIdmVjb1kza3dBZCt6QUI1Wmt3UUBtYWlsLmdtYWlsLmNvbT4=","referencesHeader":"PENBK2R1aW1PMjRzYWtPWFNNVnVxYkVleDgremlCbVFIdmVjb1kza3dBZCt6QUI1Wmt3UUBtYWlsLmdtYWlsLmNvbT4="},"prevInTopic":6279,"nextInTopic":6282,"prevInTime":6280,"nextInTime":6282,"topicId":6279,"numMessagesInTopic":11,"msgSnippet":"Hi Oliver, First, congratulations to you and your co-authors on this interesting paper. I am also interested in adaptive neural networks and I have some ","rawEmail":"Return-Path: &lt;vassilisvas@...&gt;\r\nX-Sender: vassilisvas@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 3098 invoked by uid 102); 29 Apr 2014 16:50:07 -0000\r\nX-Received: from unknown (HELO mtaq5.grp.bf1.yahoo.com) (10.193.84.36)\n  by m2.grp.bf1.yahoo.com with SMTP; 29 Apr 2014 16:50:07 -0000\r\nX-Received: (qmail 12946 invoked from network); 29 Apr 2014 16:50:07 -0000\r\nX-Received: from unknown (HELO mail-pd0-f171.google.com) (209.85.192.171)\n  by mtaq5.grp.bf1.yahoo.com with SMTP; 29 Apr 2014 16:50:07 -0000\r\nX-Received: by mail-pd0-f171.google.com with SMTP id r10so414889pdi.16\n        for &lt;neat@yahoogroups.com&gt;; Tue, 29 Apr 2014 09:50:04 -0700 (PDT)\r\nMIME-Version: 1.0\r\nX-Received: by 10.66.142.132 with SMTP id rw4mr987871pab.6.1398790203817; Tue,\n 29 Apr 2014 09:50:03 -0700 (PDT)\r\nX-Received: by 10.70.50.103 with HTTP; Tue, 29 Apr 2014 09:50:03 -0700 (PDT)\r\nIn-Reply-To: &lt;CA+duimO24sakOXSMVuqbEex8+ziBmQHvecoY3kwAd+zAB5ZkwQ@...&gt;\r\nReferences: &lt;CA+duimO24sakOXSMVuqbEex8+ziBmQHvecoY3kwAd+zAB5ZkwQ@...&gt;\r\nDate: Tue, 29 Apr 2014 19:50:03 +0300\r\nMessage-ID: &lt;CANtXhmvuJG2LdYzRDeWFWiSMG5mb+zfBLVgEaBmtty2WFPixEw@...&gt;\r\nTo: neat@yahoogroups.com\r\nContent-Type: multipart/alternative; boundary=001a11343f9436c5e804f831396e\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Vassilis Vassiliades &lt;vassilisvas@...&gt;\r\nSubject: Re: [neat] New paper: Automated Generation of Environments to Test\n the General Learning Capabilities of AI Agents\r\nX-Yahoo-Group-Post: member; u=434634266; y=zUEJMc9be20kUw5wSiw5FVGkuxJG0hvJcPawQxUIrO7fBZNK_Ijdvw\r\nX-Yahoo-Profile: v.vassiliades\r\n\r\n\r\n--001a11343f9436c5e804f831396e\r\nContent-Type: text/plain; charset=UTF-8\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\nHi Oliver,\n\nFirst, congratulations to you and your co-authors on this inter=\r\nesting\npaper. I am also interested in adaptive neural networks and I have s=\r\nome\nquestions and comments on your paper:\n\n\n1) On page 3 you describe the g=\r\neneralized Hebbian rule, which was\nintroduced by Niv et al., but on page 5 =\r\nyou say that &quot;the genome defines\nthe parameter values for several plasticit=\r\ny rules, or classes, and then\nencodes which rule each connection references=\r\n&quot;... and later... the CPPNs\n&quot;have n+1 outputs that specify the class for a =\r\nconnection&quot;.\n\nIt&#39;s not clear to me whether/how these classes relate to the =\r\ngeneralized\nHebbian rule, or which rule each output of the CPPN corresponds=\r\n to.\n\n\n2) Some clarification: since the environments are deterministic, doe=\r\ns the\nnumber of actions determine the number of states of the MDP? For exam=\r\nple,\nin the configurations where the number of available actions is 16, it =\r\nis\nalso implied that the number of states is 16. Is my interpretation corre=\r\nct?\n\n\n3) On page 3 you say that &quot;the proportion of state transitions that p=\r\nrovide\na reward value is 0.5&quot;. It is not clear to me, however, what the rew=\r\nard\nvalues are. Do all transitions that have a reward value have the *same*=\r\n\nreward value (e.g. equal to 1), or does this value vary?\n\nAlso, regarding =\r\nthe &quot;maximum possible reward maxRx&quot;, do you mean the\n&quot;return (sum of reward=\r\ns) obtained by the optimal policy&quot;? If you have the\n*same* reward value on =\r\nthe transitions (as mentioned above) then it is easy\nto calculate maxRx; if=\r\n the reward values vary then I guess you have to\ncalculate maxRx using dyna=\r\nmic programming; the initial state and the trial\nlength matters, especially=\r\n in the case where you have 16 actions (states?)\nand trial length =3D 4.\n\n\n=\r\n4) Delayed reward: by appropriately setting the parameters of your random\nM=\r\nDP generator you could instantiate environments that have delayed reward.\nF=\r\nor example, consider the case where you have a large discrete state space\na=\r\nnd only one path towards a goal/absorbing state. This path might have a\nsma=\r\nll negative reward on transitions along the way, e.g., -0.1, however,\nthe l=\r\nast transition towards the absorbing state has a huge reward, e.g.,\n+100. M=\r\noreover, transitions from/to other states that are not on this\noptimal path=\r\n, might have a reward value greater than or equal to -0.1, but\nmuch smaller=\r\n than 100, so that this path remains optimal.\n\nThis problem might be diffic=\r\nult for traditional neuroevolution especially\nas the length of the optimal =\r\npath increases (another difficulty dimension\nperhaps?), however, I believe =\r\na novelty search approach is promising here\nbecause what you really need no=\r\nw is better exploration.\n\n\nOn Tue, Apr 29, 2014 at 4:28 AM, Oliver Coleman =\r\n&lt;oliver.coleman@...&gt;wrote:\n\n&gt;\n&gt;\n&gt; Hi all,\n&gt;\n&gt; Jeff Clune, Alan Blair =\r\nand I are pleased to announce a new GECCO paper,\n&gt; &quot;Automated Generation of=\r\n Environments to Test the General Learning\n&gt; Capabilities of AI Agents&quot;\n&gt;\n&gt;=\r\n We&#39;d love to hear your comments and questions.\n&gt;\n&gt; *Video summary:* http:/=\r\n/youtu.be/itbKerV9g6s\n&gt;\n&gt; *PDF:* http://goo.gl/t3tCp2\n&gt;\n&gt; *Abstract*: Algor=\r\nithms for evolving agents that learn during their\n&gt; lifetime have typically=\r\n been evaluated on only a handful of environments.\n&gt; Designing such environ=\r\nments is labour intensive, potentially biased, and\n&gt; provides only a small =\r\nsample size that may prevent accurate general\n&gt; conclusions from being draw=\r\nn. In this paper we introduce a method for\n&gt; automatically generating MDP e=\r\nnvironments which allows the difficulty to be\n&gt; scaled in several ways. We =\r\npresent a case study in which environments are\n&gt; generated that vary along =\r\nthree key dimensions of difficulty: the number of\n&gt; environment configurati=\r\nons, the number of available actions, and the length\n&gt; of each trial. The s=\r\ntudy reveals interesting differences between three\n&gt; neural network models =\r\n-- Fixed-Weight, Plastic-Weight, and Modulated\n&gt; Plasticity -- that would n=\r\not have been obvious without sweeping across\n&gt; these different dimensions. =\r\nOur paper thus introduces a new way of\n&gt; conducting reinforcement learning =\r\nscience: instead of manually designing a\n&gt; few environments, researchers wi=\r\nll be able to automatically generate a\n&gt; range of environments across key d=\r\nimensions of variation. This will allow\n&gt; scientists to more rigorously ass=\r\ness the general learning capabilities of\n&gt; an algorithm, and may ultimately=\r\n improve the rate at which we discover how\n&gt; to create AI with general purp=\r\nose learning.\n&gt;\n&gt; All data and software is available at http://goo.gl/n4D3w=\r\n6\n&gt;\n&gt; T: +61 421 972 953 | E: oliver.coleman@... | W: http://ojcolema=\r\nn.com\n&gt;\n&gt;   \n&gt;\n\r\n--001a11343f9436c5e804f831396e\r\nContent-Type: text/html; charset=UTF-8\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\n&lt;div dir=3D&quot;ltr&quot;&gt;Hi Oliver,&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;First, congratulations to yo=\r\nu and your co-authors on this interesting paper. I am also interested in ad=\r\naptive neural networks and I have some questions and comments on your paper=\r\n:&lt;/div&gt;\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;1) On page 3 you describe the ge=\r\nneralized Hebbian rule, which was introduced by Niv et al., but on page 5 y=\r\nou say that &quot;the genome defines the parameter values for several plast=\r\nicity rules, or classes, and then encodes which rule each connection refere=\r\nnces&quot;... and later... the CPPNs &quot;have n+1 outputs that specify th=\r\ne class for a connection&quot;.&lt;/div&gt;\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;It&#39;s not clea=\r\nr to me whether/how these classes relate to the generalized Hebbian rule, o=\r\nr which rule each output of the CPPN corresponds to.&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;d=\r\niv&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;2) Some clarification: since the environments are determi=\r\nnistic, does the number of actions determine the number of states of the MD=\r\nP? For example, in the configurations where the number of available actions=\r\n is 16, it is also implied that the number of states is 16. Is my interpret=\r\nation correct?&lt;/div&gt;\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;3) On page 3 you sa=\r\ny that &quot;the proportion of state transitions that provide a reward valu=\r\ne is 0.5&quot;. It is not clear to me, however, what the reward values are.=\r\n Do all transitions that have a reward value have the *same* reward value (=\r\ne.g. equal to 1), or does this value vary?&lt;/div&gt;\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Also, =\r\nregarding the &quot;maximum possible reward maxRx&quot;, do you mean the &q=\r\nuot;return (sum of rewards) obtained by the optimal policy&quot;? If you ha=\r\nve the *same* reward value on the transitions (as mentioned above) then it =\r\nis easy to calculate=C2=A0maxRx; if the reward=C2=A0values vary then I gues=\r\ns you have to calculate=C2=A0maxRx=C2=A0using dynamic programming; the init=\r\nial state and the trial length=C2=A0matters, especially in the case where y=\r\nou have 16 actions (states?) and trial length =3D 4.&lt;/div&gt;\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;=\r\ndiv&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;4) Delayed reward: by appropriately setting the paramete=\r\nrs of your random MDP generator you could instantiate environments that hav=\r\ne delayed reward. For example, consider the case where you have a large dis=\r\ncrete state space and only one path towards a goal/absorbing state. This pa=\r\nth might have a small negative reward on transitions along the way, e.g., -=\r\n0.1, however, the last transition towards the absorbing state has a huge re=\r\nward, e.g., +100. Moreover, transitions from/to other states that are not o=\r\nn this optimal path, might have a reward value greater than or equal to -0.=\r\n1, but much smaller than 100, so that this path remains optimal.&lt;/div&gt;\n&lt;div=\r\n&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;This problem might be difficult for traditional neuroevolut=\r\nion especially as the length of the optimal path increases (another difficu=\r\nlty dimension perhaps?), however, I believe a novelty search approach is pr=\r\nomising here because what you really need now is better exploration.&lt;/div&gt;\n=\r\n&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;br&gt;&lt;div class=3D&quot;gmail_quote&quot;&gt;On Tue,=\r\n Apr 29, 2014 at 4:28 AM, Oliver Coleman &lt;span dir=3D&quot;ltr&quot;&gt;&lt;&lt;a href=3D&quot;m=\r\nailto:oliver.coleman@...&quot; target=3D&quot;_blank&quot;&gt;oliver.coleman@...&lt;=\r\n/a&gt;&gt;&lt;/span&gt; wrote:&lt;br&gt;\n&lt;blockquote class=3D&quot;gmail_quote&quot; style=3D&quot;margin=\r\n:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex&quot;&gt;\n\n\n&lt;u&gt;&lt;/u&gt;\n\n\n\n\n\n\n\n=\r\n\n\n \n&lt;div style&gt;\n&lt;span&gt;=C2=A0&lt;/span&gt;\n\n\n&lt;div&gt;\n  &lt;div&gt;\n\n\n    &lt;div&gt;\n      \n    =\r\n  \n      &lt;p&gt;&lt;/p&gt;&lt;div dir=3D&quot;ltr&quot;&gt;&lt;font face=3D&quot;georgia, serif&quot;&gt;Hi all,&lt;/fon=\r\nt&gt;&lt;div&gt;&lt;font face=3D&quot;georgia, serif&quot;&gt;&lt;br&gt;&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;font face=3D&quot;ge=\r\norgia, serif&quot;&gt;Jeff Clune, Alan Blair and I are pleased to announce a new GE=\r\nCCO paper, &quot;Automated Generation of Environments to Test the General L=\r\nearning Capabilities of AI Agents&quot;&lt;/font&gt;&lt;/div&gt;\n\n&lt;div&gt;&lt;font face=3D&quot;ge=\r\norgia, serif&quot;&gt;&lt;br&gt;&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;font face=3D&quot;georgia, serif&quot;&gt;We&#39;d =\r\nlove to hear your comments and questions.&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;font face=3D&quot;ge=\r\norgia, serif&quot;&gt;&lt;br&gt;&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;font face=3D&quot;georgia, serif&quot;&gt;&lt;b&gt;Video =\r\nsummary:&lt;/b&gt;=C2=A0&lt;a href=3D&quot;http://youtu.be/itbKerV9g6s&quot; target=3D&quot;_blank&quot;=\r\n&gt;http://youtu.be/itbKerV9g6s&lt;/a&gt;&lt;/font&gt;&lt;/div&gt;\n\n&lt;div&gt;&lt;font face=3D&quot;georgia, =\r\nserif&quot;&gt;&lt;br&gt;&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;div&gt;&lt;font face=3D&quot;georgia, serif&quot;&gt;&lt;b&gt;PDF:&lt;/b&gt;=\r\n=C2=A0&lt;a href=3D&quot;http://goo.gl/t3tCp2&quot; target=3D&quot;_blank&quot;&gt;http://goo.gl/t3tC=\r\np2&lt;/a&gt;&lt;/font&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;font face=3D&quot;georgia, serif&quot;&gt;&lt;br&gt;\n&lt;/font&gt;&lt;/d=\r\niv&gt;\n&lt;div&gt;&lt;div&gt;&lt;font face=3D&quot;georgia, serif&quot;&gt;&lt;b&gt;Abstract&lt;/b&gt;: Algorithms for=\r\n evolving agents that learn during their lifetime have typically been evalu=\r\nated on only a handful of environments. Designing such environments is labo=\r\nur intensive, potentially biased, and provides only a small sample size tha=\r\nt may prevent accurate general conclusions from being drawn. In this paper =\r\nwe introduce a method for automatically generating MDP environments which a=\r\nllows the difficulty to be scaled in several ways. We present a case study =\r\nin which environments are generated that vary along three key dimensions of=\r\n difficulty: the number of environment configurations, the number of availa=\r\nble actions, and the length of each trial. The study reveals interesting di=\r\nfferences between three neural network models -- Fixed-Weight, Plastic-Weig=\r\nht, and Modulated Plasticity -- that would not have been obvious without sw=\r\neeping across these different dimensions. Our paper thus introduces a new w=\r\nay of conducting reinforcement learning science: instead of manually design=\r\ning a few environments, researchers will be able to automatically generate =\r\na range of environments across key dimensions of variation. This will allow=\r\n scientists to more rigorously assess the general learning capabilities of =\r\nan algorithm, and may ultimately improve the rate at which we discover how =\r\nto create AI with general purpose learning.=C2=A0&lt;/font&gt;&lt;/div&gt;\n\n&lt;/div&gt;&lt;div&gt;=\r\n&lt;font face=3D&quot;georgia, serif&quot;&gt;&lt;br&gt;&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;font face=3D&quot;georgia, =\r\nserif&quot;&gt;All data and software is available at=C2=A0&lt;a href=3D&quot;http://goo.gl/=\r\nn4D3w6&quot; target=3D&quot;_blank&quot;&gt;http://goo.gl/n4D3w6&lt;/a&gt;&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;div&gt;\n&lt;=\r\ndiv dir=3D&quot;ltr&quot;&gt;&lt;div&gt;\n&lt;font face=3D&quot;georgia, serif&quot;&gt;&lt;br&gt;&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;=\r\nfont face=3D&quot;georgia, serif&quot;&gt;T: &lt;a href=3D&quot;tel:%2B61%20421%20972%20953&quot; val=\r\nue=3D&quot;+61421972953&quot; target=3D&quot;_blank&quot;&gt;+61 421 972 953&lt;/a&gt; |=C2=A0E: &lt;a href=\r\n=3D&quot;mailto:oliver.coleman@...&quot; target=3D&quot;_blank&quot;&gt;oliver.coleman@gmail=\r\n.com&lt;/a&gt;=C2=A0|=C2=A0W:=C2=A0&lt;a href=3D&quot;http://ojcoleman.com&quot; target=3D&quot;_bl=\r\nank&quot;&gt;http://ojcoleman.com&lt;/a&gt;&lt;br&gt;\n\n&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;/=\r\ndiv&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;\n&lt;/div&gt;&lt;/div&gt;\n&lt;p&gt;&lt;/p&gt;\n\n    &lt;/div&gt;\n     \n\n    \n    &lt;di=\r\nv style=3D&quot;color:#fff;min-height:0&quot;&gt;&lt;/div&gt;\n\n\n&lt;/div&gt;\n\n\n\n  \n\n\n\n\n\n\n&lt;/blockquot=\r\ne&gt;&lt;/div&gt;&lt;br&gt;&lt;/div&gt;\n\r\n--001a11343f9436c5e804f831396e--\r\n\n"}}