{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":197999825,"authorName":"John Arrowwood","from":"John Arrowwood &lt;jarrowwx@...&gt;","profile":"jarrowwx","replyTo":"LIST","senderId":"ZeOVYAyvOLMs2qnpO1Wu_w6H9mapt6dzSL53DHJf16VpPFWiLczUJOauOgSYhnvc2o5evK9c5coOnBkrmEaKYAaJ6lX--GIsfno","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] Faster evolution for IEX","postDate":"1095280694","msgId":1534,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDUxN2ZhNmYxMDQwOTE1MTMzODM3NGVlNjliQG1haWwuZ21haWwuY29tPg==","inReplyToHeader":"PDYuMS4yLjAuMC4yMDA0MDkxNTE2NDQ1MS4wMjUzMzJiMEBwb3AubWFpbC55YWhvby5jby51az4=","referencesHeader":"PDUxN2ZhNmYxMDQwOTE1MDc1ODZlMzgzOTRkQG1haWwuZ21haWwuY29tPgkgPDYuMS4yLjAuMC4yMDA0MDkxNTE2NDQ1MS4wMjUzMzJiMEBwb3AubWFpbC55YWhvby5jby51az4="},"prevInTopic":1533,"nextInTopic":1535,"prevInTime":1533,"nextInTime":1535,"topicId":1532,"numMessagesInTopic":4,"msgSnippet":"... Sorry...yes, it s black and white.  Repeat the enlargement process once for each color channel then merge to enlarge color photos. ... Not if I go with a","rawEmail":"Return-Path: &lt;jarrowwx@...&gt;\r\nX-Sender: jarrowwx@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 5781 invoked from network); 15 Sep 2004 20:38:15 -0000\r\nReceived: from unknown (66.218.66.172)\n  by m10.grp.scd.yahoo.com with QMQP; 15 Sep 2004 20:38:15 -0000\r\nReceived: from unknown (HELO mproxy.gmail.com) (64.233.170.197)\n  by mta4.grp.scd.yahoo.com with SMTP; 15 Sep 2004 20:38:15 -0000\r\nReceived: by mproxy.gmail.com with SMTP id 74so207476rnk\n        for &lt;neat@yahoogroups.com&gt;; Wed, 15 Sep 2004 13:38:15 -0700 (PDT)\r\nReceived: by 10.38.11.73 with SMTP id 73mr80181rnk;\n        Wed, 15 Sep 2004 13:38:15 -0700 (PDT)\r\nReceived: by 10.38.81.7 with HTTP; Wed, 15 Sep 2004 13:38:14 -0700 (PDT)\r\nMessage-ID: &lt;517fa6f10409151338374ee69b@...&gt;\r\nDate: Wed, 15 Sep 2004 13:38:14 -0700\r\nTo: neat@yahoogroups.com\r\nIn-Reply-To: &lt;6.1.2.0.0.20040915164451.025332b0@...&gt;\r\nMime-Version: 1.0\r\nContent-Type: text/plain; charset=US-ASCII\r\nContent-Transfer-Encoding: 7bit\r\nReferences: &lt;517fa6f104091507586e38394d@...&gt;\n\t &lt;6.1.2.0.0.20040915164451.025332b0@...&gt;\r\nX-eGroups-Remote-IP: 64.233.170.197\r\nFrom: John Arrowwood &lt;jarrowwx@...&gt;\r\nReply-To: John@...\r\nSubject: Re: [neat] Faster evolution for IEX\r\nX-Yahoo-Group-Post: member; u=197999825\r\nX-Yahoo-Profile: jarrowwx\r\n\r\nOn Wed, 15 Sep 2004 16:48:27 +0100, Ian Badcoe &lt;ian_badcoe@...&gt; wrote:\n&gt; \n&gt; At 15:58 15/09/2004, you wrote:\n&gt; \n&gt; &gt;All right, here&#39;s some thoughts.  Anybody have any feedback?\n&gt; &gt;\n&gt; &gt;First, I can settle for a fixed-enlargement scale.  I didn&#39;t want to,\n&gt; &gt;but I really want a better enlargement method, even if it isn&#39;t able\n&gt; &gt;to do arbitrary scale.  Now, having fixed-position outputs, the\n&gt; &gt;problem is reduced to a classification problem.  &quot;Should this pixel be\n&gt; &gt;above or below the center?&quot;  Easily solved, except for the BILLIONS of\n&gt; &gt;training samples...\n&gt; \n&gt; I can&#39;t work out what you meant by &quot;above or below the center&quot;?  Surely\n&gt; it&#39;s not classification, it&#39;s function fitting?  Unless it&#39;s in black and\n&gt; white?\n\nSorry...yes, it&#39;s black and white.  Repeat the enlargement process\nonce for each color channel then merge to enlarge color photos.\n\n&gt; &gt;Second, to get an idea of how many hidden nodes I will need, I can\n&gt; &gt;evolve for the normal input nodes, but with only one output node.\n&gt; &gt;However many hidden nodes it takes for one, that&#39;s how many I start\n&gt; &gt;with for trying to evolve the final solution.\n&gt; \n&gt; I think you need to give a brief summary of what experiment you&#39;re\n&gt; imagining now, because I cannot follow you.  Surely there&#39;s only ever one\n&gt; output node (because you run the network once for each pixel you are asking\n&gt; about...)\n\nNot if I go with a fixed enlargement scale.  \n\nSuppose I&#39;m doing a 4x enlargement.  For every pixel in the input\nimage, I need to know the 4x4 or 16 pixels that make up that position\nin the output image.  If I activate the network once per input pixel,\nI have 16 output nodes in the network.  Of course, I will have to look\nat more than the pixel I&#39;m trying to enlarge, I&#39;ll have to look at the\nsurrounding pixels to get an idea of how to generate those 16 pixels.\n\nThrere are variations I can do.  I can expand the output are to be 8x8\ninstead of 4x4.  Then,  the overlapping area will give me 4 possible\nvalues for each pixel.  This gives me the &#39;panel of experts&#39; approach\nwithout actually having multiple networks.\n\n&gt; &gt;Third, this is not a controller, it is a classification problem.  So,\n&gt; &gt;I can (perhaps) take some shortcuts where connectivity is concerned.\n&gt; &gt;Instead of splitting one connection and then adding more connections\n&gt; &gt;later, I can add a node that is pre-connected to all inputs and all\n&gt; &gt;outputs, but with nearly 0 weights.  Then, it only has to optimize the\n&gt; &gt;weights.  Except then I have to decide if a new node adds to the first\n&gt; &gt;hidden layer or a second (or a third).  Hmm...\n&gt; \n&gt; Except for connections between hidden nodes?\n&gt; \n&gt; &gt;Any other ideas?\n&gt; \n&gt; I need a bit more explanation, I think.  I&#39;m glad you&#39;re still going on\n&gt; with it, however.\n\nWell, the examples I&#39;ve seen of the &#39;state of the art&#39; in enlargement\nis dissapointing compared to the results I saw in one of the papers I\nread.  So I really want the end-result.\n\nSome more thoughts...\n\nPeople generally recommend limiting the number of hidden nodes\n(single-layer), otherwise the network tends to memorize the inputs and\ndoesn&#39;t generalize well, right?  Well, I can&#39;t even COUNT how many\ninput samples I can throw at it.  During training / evolution it may\nnever see the same sample twice.  In order to be successful, it will\nhave no choice but to generalize based on similar inputs, not\nidentical ones.  It will generalize no matter how many hidden nodes I\ngive it.  I can have too few, but I really can&#39;t have too many.  As\nlong as I can fit everything (network plus input image plus output\nimage) in a reasonable amount of ram, the only drawback to a large\nnetwork is computation speed.  I intend to use this in batch\nprocessing, so I&#39;m willing to allow a tradeoff of speed for quality. \nAs long as I&#39;m getting an increase in quality, I&#39;m willing to add more\ntopology.\n\nAdditionally is the idea I had earlier of using a self-organizing map\nfor the first layer.  This relates back to the above thought, since a\nself-organizing map has a fixed topology from the start.  The bigger\nthe map, the more features are available for the rest of the network\nto use to help identify the shape of the sample.  If my understanding\nis correct, the more nodes I use in that map, the less likely that I\nneed another layer of hidden nodes before the output nodes.  Is that\ntrue?\n\nTo make matters more interesting, I could use Pruning both to simplify\nthe SOM connectivity and the final network, allowing the final result\nto go faster...assuming I ever get there! :)\n\n"}}