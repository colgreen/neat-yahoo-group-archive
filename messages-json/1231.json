{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"DZ5m5gsnrZAJscCkc2FEOTQVbq2YkiyYjqzKtFbg80p8TVIPyECpQUBIGijjiHN_Q3LR7DzTL6c2rDnhYiQ0yaYDqftoXMELZzANOLAJ0yEq","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: An ANN design question...","postDate":"1090607715","msgId":1231,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGNkcmxwMys0YWw0QGVHcm91cHMuY29tPg==","inReplyToHeader":"PDIwMDQwNzIzMTQwNDU0LjEyNzQ2LnFtYWlsQHdlYjUyODA2Lm1haWwueWFob28uY29tPg=="},"prevInTopic":1230,"nextInTopic":1232,"prevInTime":1230,"nextInTime":1232,"topicId":1226,"numMessagesInTopic":19,"msgSnippet":"Jim, I ll get back to you on this in a couple days-my  dissertation is due this weekend!!! But a short answer is that at least in my implementation of NEAT the","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 1476 invoked from network); 23 Jul 2004 18:35:41 -0000\r\nReceived: from unknown (66.218.66.172)\n  by m17.grp.scd.yahoo.com with QMQP; 23 Jul 2004 18:35:41 -0000\r\nReceived: from unknown (HELO n30.grp.scd.yahoo.com) (66.218.66.87)\n  by mta4.grp.scd.yahoo.com with SMTP; 23 Jul 2004 18:35:41 -0000\r\nReceived: from [66.218.67.157] by n30.grp.scd.yahoo.com with NNFMP; 23 Jul 2004 18:35:17 -0000\r\nDate: Fri, 23 Jul 2004 18:35:15 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;cdrlp3+4al4@...&gt;\r\nIn-Reply-To: &lt;20040723140454.12746.qmail@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Transfer-Encoding: quoted-printable\r\nContent-Length: 5518\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-eGroups-Remote-IP: 66.218.66.87\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: Re: An ANN design question...\r\nX-Yahoo-Group-Post: member; u=54567749\r\nX-Yahoo-Profile: kenstanley01\r\n\r\nJim, I&#39;ll get back to you on this in a couple days-my  dissertation is\ndue =\r\nthis weekend!!! \n\nBut a short answer is that at least in my implementation =\r\nof NEAT the\ninputs don&#39;t go through any activation function.  Anyway, admit=\r\ntedly I\nskimmed what you wrote so I will read it in more detail after the\nd=\r\nissertation is finished and reply in more detail.\n\nken\n\n--- In neat@yahoogr=\r\noups.com, &quot;Jim O&#39;Flaherty, Jr.&quot;\n &lt;jim_oflaherty_jr@y...&gt; wrote:\n&gt; Ken,\n&gt; \n&gt;=\r\n As I have written previously here, I am working on an optimized ANN\nimplem=\r\nentation in Java which I\n&gt; have named SEMIANN (Sparsely Evaluated Matrix In=\r\nterace Artificial\nNeural Network).\n&gt; \n&gt; In a meeting with Derek and Philip,=\r\n we were reviewing my design and\ncomparing it with the design\n&gt; they are cu=\r\nrrently using derived from your NEAT ANN design.  What\nshowed up was a smal=\r\nl difference\n&gt; in how I am handling the input data versus how it is being h=\r\nandled\nin their NEAT implementation.\n&gt; \n&gt; It is my understanding from the s=\r\nmall number of ANN implementations\nI have seen (around 5)\n&gt; including that =\r\nof David Fogel (author of book titled &quot;Blondie24&quot;\nfrom which I am duplicati=\r\nng\n&gt; experiments), the input data is placed directly into the input node.\n =\r\nThe data is not bounded\n&gt; (other than the actual limits of a float or doubl=\r\ne).  And the input\nnode does *not* have an\n&gt; activation function.  The unbo=\r\nunded data present in the input node\nis then used in the activation\n&gt; of th=\r\ne hidden nodes (simple 3 layer feed forward network).  Any sort\nof altering=\r\n the input data is\n&gt; then handled by the weight attached to that input node=\r\n.  The GA\nprocess will then drift the\n&gt; weights around such that inputs whi=\r\nch are not so valuable are muted\nwith smaller weight values. \n&gt; And inputs =\r\nwhich are important are magnified with higher weight\nvalues.  And all of th=\r\nese weights\n&gt; will eventually form a function over which the input data is\n=\r\n&quot;normalized&quot; based on each input&#39;s\n&gt; relative importance, a sort of first a=\r\npproximation of the input\ndata&#39;s inter-relatedness.\n&gt; \n&gt; In contrast, Phili=\r\np and Derek indicated an input data point entering\ntheir ANN implementation=\r\n is\n&gt; actually being pushed through the input node&#39;s activation function.\n =\r\nThen the &quot;modified&quot; data\n&gt; point is now placed into the input node.  It is =\r\nthen used in the\nactivation of the &quot;hidden&quot; nodes.\n&gt; \n&gt; In talking through =\r\nthe difference, we talked about how that might\nimpact the efficacy of the\n&gt;=\r\n evolving ANN.  In other words, by having the input data go through\nan acti=\r\nvation function without\n&gt; their being a weight involved, it seems the input=\r\n data is being\nskewed, meaningful data is lost \n&gt; with no opportunity for t=\r\nhe GA to compensate prior to the data loss.\n Essentially, some data is\n&gt; lo=\r\nst.  In pure mathematical terms, this implmentation provides a\nweight of 1.=\r\n0 multiplied by the\n&gt; unbounded input value which is then submitted to the =\r\ninput node&#39;s\nactivation function with the\n&gt; result of the function being pl=\r\naced into the input node.\n&gt; \n&gt; My immediate response was this: the skewing =\r\nseems like it would make\nit more difficult for the ANN\n&gt; to generate associ=\r\nations to the inputs that range outside of the\nbounds of the activation\n&gt; f=\r\nunction.  For example, in replicating Fogel&#39;s experiments, I am\nusing the h=\r\nyperbolic tangent \n&gt; bounded -1..1, and the following input values are used=\r\n: a red\nchecker has the value of 1.0, a\n&gt; black check has the value of -1.0=\r\n, a red king has the value of 1.5,\nand a black king has the value\n&gt; of -1.5=\r\n.\n&gt; \n&gt; Now, I know that Fogel was expecting the inputs to be related\ndirect=\r\nly, as a ratio, as he\n&gt; discusses this at some length in his book.  He left=\r\n it up to the\nGA/ANN to work out the optimal\n&gt; ratio relationship.  Additio=\r\nnally, the king&#39;s value was a GA\nparameter which was bounded between\n&gt; 1.0 =\r\nand 3.0 and could randomly change by +/- 0.1 when a parent was\ngenerating a=\r\n descendant.\n&gt; \n&gt; With the approach Philip and Derek have taken (and they s=\r\naid theirs\nis modeled after your design),\n&gt; it seems like the ratio gets pe=\r\nrverted by the activation function on\nthe input node.  So as input\n&gt; values=\r\n fall further and further from the activation function bounds,\nrelationship=\r\ns between inputs\n&gt; outside of the bounds are eventually lost due to\napproxi=\r\nmation/rounding errors in the IEEE float\n&gt; or double.  Or so it seems to me=\r\n.\n&gt; \n&gt; So my questions are this:\n&gt; A) What is the theoretical or mathematic=\r\nal explanation as to why the\ninput values for NEAT are\n&gt; pushed through an =\r\ninput node&#39;s activation function as opposed to\nbeing used directly?\n&gt; B) Do=\r\nes some form of assumption exist in which to provide input to a\nNEAT ANN, t=\r\nhe input data\n&gt; point for each input node must be scaled such that the data=\r\n\npoint&#39;s\nrelevant range of values falls\n&gt; between the upper and lower bound=\r\ns of the input node&#39;s\nactivation\nfunction?\n&gt; C) What kinds of different typ=\r\nes of activation functions on an input\nnode might possibly handle\n&gt; this di=\r\nfferently and/or more effectively?\n&gt; \n&gt; Sidenote: In SEMIANN, I do not allo=\r\nw a connection to have a\ndestination of an input node.  So\n&gt; there is no ne=\r\ned for an activation function at an input node.  And\ninput node is treated =\r\nas just\n&gt; an unbounded data point.  It was my understanding that if there w=\r\nas\nto be feedback to the &quot;input&quot;,\n&gt; it would occur as new nodes and connect=\r\nions around the hidden/output\nnodes from which the\n&gt; particular input node =\r\nwas connected.\n&gt; \n&gt; Well, that sort of took much longer to present than I i=\r\nnitially\nthought.  Hmmm=85\n&gt; \n&gt; Thank you for any clarification(s) you can =\r\noffer on this.\n&gt; \n&gt; \n&gt; Jim O&#39;Flaherty, Jr.\n\n\n"}}