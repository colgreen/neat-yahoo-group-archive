{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":211599040,"authorName":"Jeff Clune","from":"Jeff Clune &lt;jclune@...&gt;","profile":"jeffreyclune","replyTo":"LIST","senderId":"Dk2_PR4BapM55t98CtP-3OIKQzeYCPaTexfkGyLewkN0a5ptkWCx52HiJUhFDeA_dPyF7h7sdr3NMULipGsv56Pk","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [neat] Backpropagation and NEAT","postDate":"1204918922","msgId":3848,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PEMzRjcwNEJBLjIxM0U4JWpjbHVuZUBtc3UuZWR1Pg==","inReplyToHeader":"PDE5YjEwZDUxMDgwMzA3MDYzMnU0ZjhkNjIyM2gxNmIxMzM4MzIwZWM1N2JmQG1haWwuZ21haWwuY29tPg=="},"prevInTopic":3847,"nextInTopic":3852,"prevInTime":3847,"nextInTime":3849,"topicId":3846,"numMessagesInTopic":41,"msgSnippet":"Hello- I believe Nolfi, Parisi and company did work a ways back combining evolution and learning in neural networks. I am not sure if those papers would help ","rawEmail":"Return-Path: &lt;jclune@...&gt;\r\nX-Sender: jclune@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 71944 invoked from network); 7 Mar 2008 19:42:15 -0000\r\nX-Received: from unknown (66.218.67.96)\n  by m56.grp.scd.yahoo.com with QMQP; 7 Mar 2008 19:42:15 -0000\r\nX-Received: from unknown (HELO rv-out-0910.google.com) (209.85.198.189)\n  by mta17.grp.scd.yahoo.com with SMTP; 7 Mar 2008 19:42:15 -0000\r\nX-Received: by rv-out-0910.google.com with SMTP id g11so384992rvb.27\n        for &lt;neat@yahoogroups.com&gt;; Fri, 07 Mar 2008 11:42:14 -0800 (PST)\r\nX-Received: by 10.141.169.9 with SMTP id w9mr891735rvo.179.1204918934655;\n        Fri, 07 Mar 2008 11:42:14 -0800 (PST)\r\nReturn-Path: &lt;jclune@...&gt;\r\nX-Received: from ?192.168.2.2? ( [67.167.130.108])\n        by mx.google.com with ESMTPS id i49sm8403915rne.0.2008.03.07.11.42.10\n        (version=TLSv1/SSLv3 cipher=OTHER);\n        Fri, 07 Mar 2008 11:42:14 -0800 (PST)\r\nUser-Agent: Microsoft-Entourage/12.0.0.071130\r\nDate: Fri, 07 Mar 2008 14:42:02 -0500\r\nTo: &quot;neat@yahoogroups.com&quot; &lt;neat@yahoogroups.com&gt;\r\nMessage-ID: &lt;C3F704BA.213E8%jclune@...&gt;\r\nThread-Topic: [neat] Backpropagation and NEAT\r\nThread-Index: AciAi1CIOcA+D5oMoUqhgVaA2Lkd8A==\r\nIn-Reply-To: &lt;19b10d510803070632u4f8d6223h16b1338320ec57bf@...&gt;\r\nMime-version: 1.0\r\nContent-type: text/plain;\n\tcharset=&quot;US-ASCII&quot;\r\nContent-transfer-encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Jeff Clune &lt;jclune@...&gt;\r\nSubject: Re: [neat] Backpropagation and NEAT\r\nX-Yahoo-Group-Post: member; u=211599040; y=RBE08-Wt-j31lwsDxGW_GffcoxKQohyLX-gq7Y2lKRptILc94Y28\r\nX-Yahoo-Profile: jeffreyclune\r\n\r\nHello-\n\nI believe Nolfi, Parisi and company did work a ways back combining evolution\nand learning in neural networks. I am not sure if those papers would help\nyou, but there were a couple of them. Some of the, if I recall, looked into\nthe Baldwin effect.\n\nGood luck.\n\n\n\n\nCheers,\nJeff Clune\n\nDigital Evolution Lab, Michigan State University\n\njclune@...\n517.214.1060\n\n\n\n\n&gt; From: Derek James &lt;djames@...&gt;\n&gt; Reply-To: &quot;neat@yahoogroups.com&quot; &lt;neat@yahoogroups.com&gt;\n&gt; Date: Fri, 7 Mar 2008 08:32:49 -0600\n&gt; To: &quot;neat@yahoogroups.com&quot; &lt;neat@yahoogroups.com&gt;\n&gt; Subject: Re: [neat] Backpropagation and NEAT\n&gt; \n&gt; Hey Peter,\n&gt; \n&gt; I&#39;ve never used it, but Backpropagation Through Time (BPTT) is a\n&gt; general method for applying backprop to recurrent neural networks.\n&gt; Here&#39;s a paper by Paul Werbos describing how it works:\n&gt; \n&gt; http://axon.cs.byu.edu/~martinez/classes/678/Papers/Werbos_BPTT.pdf\n&gt; \n&gt; I&#39;m not aware of any implementations that work in conjunction with\n&gt; NEAT, though. Others on the list might know more about this technique.\n&gt; Good luck.\n&gt; \n&gt; --Derek\n&gt; \n&gt; On Fri, Mar 7, 2008 at 3:19 AM, petar_chervenski\n&gt; &lt;petar_chervenski@...&gt; wrote:\n&gt;&gt; \n&gt;&gt; \n&gt;&gt; \n&gt;&gt; \n&gt;&gt; \n&gt;&gt; \n&gt;&gt; Hello there.\n&gt;&gt; \n&gt;&gt;  I am looking for any back-propagation algorithm that can work on\n&gt;&gt;  networks with arbitrary topology such as these that NEAT evolves. All\n&gt;&gt;  libraries I found so far either assume layered networks or only feed-\n&gt;&gt;  forward ones.. I am confused. Is there any source code that might help\n&gt;&gt;  me? Any back-prop implementation that can work on NEAT networks such\n&gt;&gt;  that it can easily be integrated. Or maybe some papers on the topic?\n&gt;&gt;  I appreciate any help from the community.\n&gt;&gt; \n&gt;&gt;  Peter\n&gt;&gt; \n&gt;&gt;  \n\n\n\n"}}