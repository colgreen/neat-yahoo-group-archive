{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":283334584,"authorName":"petar_chervenski","from":"&quot;petar_chervenski&quot; &lt;petar_chervenski@...&gt;","profile":"petar_chervenski","replyTo":"LIST","senderId":"xP8f0_INxE7aQRqkT16S1wBXWpYjf_Y_TvvgpT5rj1AzDg9MJ-dXqXULAb0klthLFYB1IKp-n7hGLPVjCsPqYdpM3xilvg2nWkhUZYQgP8lW0sktwi8","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Backpropagation and NEAT","postDate":"1205189436","msgId":3862,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGZyNGR2cysxZnQyQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGZyNDh2aytudnFkQGVHcm91cHMuY29tPg=="},"prevInTopic":3861,"nextInTopic":3863,"prevInTime":3861,"nextInTime":3863,"topicId":3846,"numMessagesInTopic":41,"msgSnippet":"Well I think that encoding the resulting weights back to the genome would somehow hurt the population weight diversity, since most individuals in a species","rawEmail":"Return-Path: &lt;petar_chervenski@...&gt;\r\nX-Sender: petar_chervenski@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 2251 invoked from network); 10 Mar 2008 22:50:37 -0000\r\nX-Received: from unknown (66.218.67.96)\n  by m53.grp.scd.yahoo.com with QMQP; 10 Mar 2008 22:50:37 -0000\r\nX-Received: from unknown (HELO n34b.bullet.mail.sp1.yahoo.com) (66.163.168.148)\n  by mta17.grp.scd.yahoo.com with SMTP; 10 Mar 2008 22:50:37 -0000\r\nX-Received: from [216.252.122.218] by n34.bullet.mail.sp1.yahoo.com with NNFMP; 10 Mar 2008 22:50:37 -0000\r\nX-Received: from [209.73.164.86] by t3.bullet.sp1.yahoo.com with NNFMP; 10 Mar 2008 22:50:37 -0000\r\nX-Received: from [66.218.67.199] by t8.bullet.scd.yahoo.com with NNFMP; 10 Mar 2008 22:50:37 -0000\r\nDate: Mon, 10 Mar 2008 22:50:36 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;fr4dvs+1ft2@...&gt;\r\nIn-Reply-To: &lt;fr48vk+nvqd@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;petar_chervenski&quot; &lt;petar_chervenski@...&gt;\r\nSubject: Re: Backpropagation and NEAT\r\nX-Yahoo-Group-Post: member; u=283334584; y=8htTtzgoljOD3qdxm3l6sOr7m1cnIz82lS3IEIa9tNSXorvhIitV7F7v0w\r\nX-Yahoo-Profile: petar_chervenski\r\n\r\nWell I think that encoding the resulting weights back to the genome \nwould =\r\nsomehow hurt the population weight diversity, since most \nindividuals in a =\r\nspecies represented by a given topology can end up \nin the same local minim=\r\na, thus leaving out a species with the nearly \nsame individuals, i.e. clone=\r\ns. \nThis is why I think that backprop should be applied occasionaly after \n=\r\nlong periods of stagnation, for example the cases where delta-coding \nkicks=\r\n in, when it focuses the search in the most promising areas of \nthe search =\r\nspace. \nI am still trying to re-implement RTRL myself, though.. Then I&#39;ll s=\r\nee \nif it is going to actually enhance performance. \n\nPeter\n\n--- In neat@ya=\r\nhoogroups.com, &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt; wrote:\n&gt;\n&gt; Rafael, thank yo=\r\nu for pointing out the connection to memetic \n&gt; algorithms.  That is good t=\r\no point out that such a combination \nfalls \n&gt; under that category.\n&gt; \n&gt; How=\r\never, there are still those who would argue that the local \nsearch \n&gt; metho=\r\nd should not be encoded back into the genome, that is, that \n&gt; evolution sh=\r\nould simply search for the best starting point from \nwhich \n&gt; a local searc=\r\nh would depart.  Because of the Baldwin Effect, that \nmay \n&gt; even work bett=\r\ner.\n&gt; \n&gt; Personally, I do not know which approach would work better but bot=\r\nh \n&gt; are viable and it is probably domain dependent.\n&gt; \n&gt; ken\n&gt; \n&gt; --- In n=\r\neat@yahoogroups.com, &quot;Rafael C.P.&quot; &lt;kurama.youko.br@&gt; \n&gt; wrote:\n&gt; &gt;\n&gt; &gt; Ken=\r\n, it doesn&#39;t fit pure evolution but it fits memetic \nalgorithms, \n&gt; that\n&gt; =\r\n&gt; consists exactly of evolution alternated with local search \nmethods \n&gt; fo=\r\nr fine\n&gt; &gt; tunning (just few steps). NEAT+BP may become a good memetic \n&gt; a=\r\nlgorithm for\n&gt; &gt; neural networks.\n&gt; &gt; \n&gt; &gt; On Mon, Mar 10, 2008 at 2:19 PM,=\r\n Kenneth Stanley &lt;kstanley@&gt;\n&gt; &gt; wrote:\n&gt; &gt; \n&gt; &gt; &gt;   Peter, I believe that =\r\nbackprop can potentially improve the\n&gt; &gt; &gt; accuracy. It has been shown to w=\r\nork effectively with \nneurevolution\n&gt; &gt; &gt; in classification tasks in the pa=\r\nst. So in principle it could\n&gt; &gt; &gt; help. Of course, there is always the cha=\r\nnce that it will not\n&gt; &gt; &gt; enhance performance as well.\n&gt; &gt; &gt;\n&gt; &gt; &gt; One iss=\r\nue I would also consider is that some people disagree on\n&gt; &gt; &gt; whether the =\r\nchanges to weights from backprop should be encoded \n&gt; back\n&gt; &gt; &gt; into the g=\r\nenome or not. If it is actually encoded back into the\n&gt; &gt; &gt; genome, that is=\r\n &quot;Lamarckian&quot; evolution because in effect what \nthe\n&gt; &gt; &gt; organism learned =\r\nover its lifetime is encoded into its own\n&gt; &gt; &gt; offspring. That is obviousl=\r\ny not how real evolution works.\n&gt; &gt; &gt;\n&gt; &gt; &gt; However, of course, it doesn&#39;t =\r\nhave to work like real evolution \n&gt; and\n&gt; &gt; &gt; some people believe that Lama=\r\nrckian evolution will work better.\n&gt; &gt; &gt; However, there are arguments that =\r\nin fact it works worse \nbecause \n&gt; it\n&gt; &gt; &gt; hurts the diversity of the popu=\r\nlation. Because of the Baldwin\n&gt; &gt; &gt; effect, some would argue that evolutio=\r\nn+backprop is most \npowerful \n&gt; if\n&gt; &gt; &gt; the learned weights are not encode=\r\nd back into the genome. This \n&gt; topic\n&gt; &gt; &gt; is fairly extensive. A lot is w=\r\nritten about the &quot;Baldwin \neffect.&quot;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt; ken\n&gt; &gt; &gt;\n&gt; &gt; &gt; --- I=\r\nn neat@yahoogroups.com &lt;neat%\n&gt; 40yahoogroups.com&gt;, &quot;petar_chervenski&quot;\n&gt; &gt; =\r\n&gt; &lt;petar_chervenski@&gt; wrote:\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; Hi Ken,\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; I am e=\r\nvolving time series predictors, in fact even a \nsimplified\n&gt; &gt; &gt; &gt; version =\r\nof time series predictors, where the network has to \n&gt; answer\n&gt; &gt; &gt; is\n&gt; &gt; =\r\n&gt; &gt; the future value going up or down. The actual output neuron \nis a\n&gt; &gt; &gt;=\r\n &gt; simple step function, but back-prop can be applied if it is \n&gt; turned\n&gt; =\r\n&gt; &gt; &gt; out to a sigmoid with a very steep slope.\n&gt; &gt; &gt; &gt; The networks are al=\r\nlowed to have any topology and they are\n&gt; &gt; &gt; evaluated\n&gt; &gt; &gt; &gt; on the run,=\r\n meaning that on each timestep, an error is being\n&gt; &gt; &gt; &gt; calculated (being=\r\n 0 or 1, depending on the prediction made).\n&gt; &gt; &gt; &gt; First of all, do you th=\r\nink that applying back-prop to these\n&gt; &gt; &gt; networks\n&gt; &gt; &gt; &gt; may bring any a=\r\nccuracy improvement? I know that it is going \nto \n&gt; eat\n&gt; &gt; &gt; &gt; the CPU res=\r\nourses, so it can be applied at regular intervals, \n&gt; say\n&gt; &gt; &gt; &gt; each 50 g=\r\nenerations, to push the networks&#39;s weights in the \nright\n&gt; &gt; &gt; &gt; direction,=\r\n a kind of a hint to the search. I am still thinking\n&gt; &gt; &gt; of &quot;is\n&gt; &gt; &gt; &gt; i=\r\nt worth it?&quot;..\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; Peter\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; --- In=\r\n neat@yahoogroups.com &lt;neat%\n40yahoogroups.com&gt;, &quot;Kenneth \n&gt; Stanley&quot;\n&gt; &gt; &gt;=\r\n &lt;kstanley@&gt; wrote:\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; A number of people have programmed =\r\nbackprop into NEAT. Chris\n&gt; &gt; &gt; &gt; &gt; Christenson did a Masters thesis on com=\r\nbining NEAT and \n&gt; backprop;\n&gt; &gt; &gt; a\n&gt; &gt; &gt; &gt; &gt; paper based on this work is =\r\nactually in the files section of\n&gt; &gt; &gt; this\n&gt; &gt; &gt; &gt; group:\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; =\r\n&gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; \n&gt; \nhttp://f1.grp.yahoofs.com/v1/UP7SR8rDovimxlLlvcmGOziL=\r\nUBIVncb2Tfr7sruo\n&gt; &gt; &gt; B\n&gt; &gt; &gt; &gt; 8b\n&gt; &gt; &gt; &gt; &gt; taAfELU62JLyQ9XCxXF_Akhcmi-\n&gt;=\r\n &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; \n&gt; \nTH4gpVHIikwnzB59ArOMQfPOAzyw25/Evolving_Trainabl=\r\ne_Neural_Networks_6_p\n&gt; &gt; &gt; a\n&gt; &gt; &gt; &gt; ge\n&gt; &gt; &gt; &gt; &gt; s.doc\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; =\r\n&gt; Shimon Whiteson implemented it as part of his NEAT+Q\n&gt; &gt; &gt; reinforcement\n=\r\n&gt; &gt; &gt; &gt; &gt; learning method:\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; \n&gt; \nhttp://staff.science.uva=\r\n.nl/~whiteson/pubs/whitesonaaai06.pdf&lt;http://s\n&gt; taff.science.uva.nl/%7Ewhi=\r\nteson/pubs/whitesonaaai06.pdf&gt;\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; There has been a lot wri=\r\ntten on backprop in NEAT in the \n&gt; archives\n&gt; &gt; &gt; of\n&gt; &gt; &gt; &gt; &gt; this group: =\r\njust search for &quot;backprop&quot; from the yahoo page \nfor\n&gt; &gt; &gt; this\n&gt; &gt; &gt; &gt; &gt; gr=\r\noup and many messages will pop up.\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; In general, if you d=\r\no not allow recurrence then I believe \n&gt; there\n&gt; &gt; &gt; is\n&gt; &gt; &gt; &gt; no\n&gt; &gt; &gt; &gt; =\r\n&gt; special change needed in the traditional backprop algorithm.\n&gt; &gt; &gt; With\n&gt;=\r\n &gt; &gt; &gt; &gt; recurrence you would need something like recurrent backprop \n&gt; lik=\r\ne\n&gt; &gt; &gt; &gt; Derek\n&gt; &gt; &gt; &gt; &gt; suggested. But let&#39;s just say you are evolving no=\r\nnrecurrent\n&gt; &gt; &gt; &gt; networks-\n&gt; &gt; &gt; &gt; &gt; is there a particular problem you ha=\r\nve in mind that comes up\n&gt; &gt; &gt; with\n&gt; &gt; &gt; &gt; &gt; applying backprop to such net=\r\nworks?\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; ken\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; --- In neat@yahoogroups.=\r\ncom &lt;neat%40yahoogroups.com&gt;,\n&gt; &gt; &gt; &quot;petar_chervenski&quot;\n&gt; &gt; &gt; &lt;petar_cherven=\r\nski@&gt;\n&gt; &gt; &gt; &gt; &gt; wrote:\n&gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; Hello there.\n&gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt;=\r\n &gt; &gt; &gt; &gt; I am looking for any back-propagation algorithm that can \n&gt; work\n&gt;=\r\n &gt; &gt; on\n&gt; &gt; &gt; &gt; &gt; &gt; networks with arbitrary topology such as these that NEA=\r\nT\n&gt; &gt; &gt; evolves.\n&gt; &gt; &gt; &gt; All\n&gt; &gt; &gt; &gt; &gt; &gt; libraries I found so far either as=\r\nsume layered networks or\n&gt; &gt; &gt; only\n&gt; &gt; &gt; &gt; feed-\n&gt; &gt; &gt; &gt; &gt; &gt; forward ones.=\r\n. I am confused. Is there any source code \nthat\n&gt; &gt; &gt; might\n&gt; &gt; &gt; &gt; &gt; help\n=\r\n&gt; &gt; &gt; &gt; &gt; &gt; me? Any back-prop implementation that can work on NEAT\n&gt; &gt; &gt; ne=\r\ntworks\n&gt; &gt; &gt; &gt; such\n&gt; &gt; &gt; &gt; &gt; &gt; that it can easily be integrated. Or maybe =\r\nsome papers on \n&gt; the\n&gt; &gt; &gt; &gt; topic?\n&gt; &gt; &gt; &gt; &gt; &gt; I appreciate any help from=\r\n the community.\n&gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; Peter\n&gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;=\r\n\n&gt; &gt; &gt;\n&gt; &gt; &gt;  \n&gt; &gt; &gt;\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; -- \n&gt; &gt; =3D=3D=3D=3D=3D=3D=3D=3D=3D=\r\n\n&gt; &gt; Rafael C.P.\n&gt; &gt; =3D=3D=3D=3D=3D=3D=3D=3D=3D\n&gt; &gt;\n&gt;\n\n\n\n"}}