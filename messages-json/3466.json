{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":275211662,"authorName":"Cesar G. Miguel","from":"&quot;Cesar G. Miguel&quot; &lt;cesargm@...&gt;","profile":"fdital","replyTo":"LIST","senderId":"Se8TzCFKcnfDq-B608tkWTDe5gwQmW0rO63QcgNbRwhYKPWDCNepXR5PbOzOgtvERt6YZb1weyyY0EZYWOw-Ea_KBvzTJro0Qg","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] Re: Different activation methods in NEAT4J","postDate":"1184517210","msgId":3466,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGRkZjEwMDc4MDcwNzE1MDkzM3A3MzFmNmJjZnFlM2NjNGJhYWI2YmFlYWZkQG1haWwuZ21haWwuY29tPg==","inReplyToHeader":"PGY3YWdhaStxNGNuQGVHcm91cHMuY29tPg==","referencesHeader":"PGRkZjEwMDc4MDcwNzEwMTMxM2g1NmFmZjc1Ymk0ZDllM2M3MDM0NGZjMGM2QG1haWwuZ21haWwuY29tPgkgPGY3YWdhaStxNGNuQGVHcm91cHMuY29tPg=="},"prevInTopic":3462,"nextInTopic":3467,"prevInTime":3465,"nextInTime":3467,"topicId":3459,"numMessagesInTopic":6,"msgSnippet":"... It works multiplying a connectivity matrix by the state vector of the network (synchronous updating like in Hopfield networks). ... That s exactly what s","rawEmail":"Return-Path: &lt;cesar.gomes@...&gt;\r\nX-Sender: cesar.gomes@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 95393 invoked from network); 15 Jul 2007 16:33:35 -0000\r\nReceived: from unknown (66.218.67.35)\n  by m53.grp.scd.yahoo.com with QMQP; 15 Jul 2007 16:33:35 -0000\r\nReceived: from unknown (HELO an-out-0708.google.com) (209.85.132.250)\n  by mta9.grp.scd.yahoo.com with SMTP; 15 Jul 2007 16:33:34 -0000\r\nReceived: by an-out-0708.google.com with SMTP id d23so192266and\n        for &lt;neat@yahoogroups.com&gt;; Sun, 15 Jul 2007 09:33:30 -0700 (PDT)\r\nDKIM-Signature: a=rsa-sha1; c=relaxed/relaxed;\n        d=gmail.com; s=beta;\n        h=domainkey-signature:received:received:message-id:date:from:sender:to:subject:in-reply-to:mime-version:content-type:content-transfer-encoding:content-disposition:references:x-google-sender-auth;\n        b=NBKZBSBJBIiiE0vxvwx1Bb0xaQzWFQJOTo1KYEfqIFZo54Pvr1/gqxic8rERTOJNxCiAjztGDYTt2tgUcGaVAOuuJGNvMr9W6dxohGh/6kFmi6VnTik3fI0+b5u6Z6cpdq52xMQt9szBS6tNyMaMgtAsVzTXzoFAWPhDj/XfVrE=\r\nReceived: by 10.100.33.14 with SMTP id g14mr1905047ang.1184517210301;\n        Sun, 15 Jul 2007 09:33:30 -0700 (PDT)\r\nReceived: by 10.100.9.13 with HTTP; Sun, 15 Jul 2007 09:33:30 -0700 (PDT)\r\nMessage-ID: &lt;ddf100780707150933p731f6bcfqe3cc4baab6baeafd@...&gt;\r\nDate: Sun, 15 Jul 2007 13:33:30 -0300\r\nTo: neat@yahoogroups.com\r\nIn-Reply-To: &lt;f7agai+q4cn@...&gt;\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nContent-Disposition: inline\r\nReferences: &lt;ddf100780707101313h56aff75bi4d9e3c70344fc0c6@...&gt;\n\t &lt;f7agai+q4cn@...&gt;\r\nX-Google-Sender-Auth: 15297c9c7336f576\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: &quot;Cesar G. Miguel&quot; &lt;cesargm@...&gt;\r\nSubject: Re: [neat] Re: Different activation methods in NEAT4J\r\nX-Yahoo-Group-Post: member; u=275211662; y=GfXnrAau6mUwXgl3KfG7Qkypxj0sqe-b2hXJEAV9NNB2\r\nX-Yahoo-Profile: fdital\r\n\r\n&gt; Cesar, I&#39;m not sure if I understand completely how your activation\n&gt;  method works\n\nIt works multiplying a connectivity matrix by the state vector of the\nnetwork (synchronous updating like in Hopfield networks).\n\n&gt; If you don&#39;t flush the network between trials, evolution can actually\n&gt; memorize the order that you present them\n\nThat&#39;s exactly what&#39;s happening! It&#39;s not the behavior I want in\nsupervised learning tasks but isn&#39;t it more appropriate for\nreinforcement learning domains?\n\nI think it&#39;s the same activation method you&#39;ve implemented on your\nrobot duel domain.\n\n[]&#39;s\nCesar\n\n&gt;  ken\n&gt;\n&gt;  --- In neat@yahoogroups.com, &quot;Cesar G. Miguel&quot; &lt;cesargm@...&gt; wrote:\n&gt;  &gt;\n&gt;\n&gt;  &gt; Hi there,\n&gt;  &gt;\n&gt;  &gt; I&#39;ve been experimenting with two different methods for neural\n&gt;  network\n&gt;  &gt; update in NEAT4J.\n&gt;  &gt;\n&gt;  &gt; I&#39;ve used the same settings as Matt&#39;s: tahn(x) activation for hidden\n&gt;  &gt; nodes and logistic(x) for the output node.  There are two stop\n&gt;  &gt; criteria: (1) when the error is below 0.1 or (2) the number of\n&gt;  &gt; generations is greater than 100.\n&gt;  &gt;\n&gt;  &gt; For a detailed parameters list, please check:\n&gt;  &gt; http://neat4j.sourceforge.net/documents/config.html\n&gt;  &gt;\n&gt;  &gt; NEAT4J recursively activates the neurons linked to the output layer.\n&gt;  &gt; This is its performance for XOR:\n&gt;  &gt;\n&gt;  &gt; 20 runs:\n&gt;  &gt; -----------------------------------------\n&gt;  &gt;               Gen.      Hidden      Connections\n&gt;  &gt; Avg.:      40.9      3.55          8.75\n&gt;  &gt; Std.:       7.16      1.54          2.45\n&gt;  &gt; -----------------------------------------\n&gt;  &gt;\n&gt;  &gt; This is my modified version of NEAT4J (I removed some classes I\n&gt;  don&#39;t\n&gt;  &gt; need and implemented a synchronous updating method for the neural\n&gt;  &gt; network). The results are:\n&gt;  &gt;\n&gt;  &gt; -----------------------------------------\n&gt;  &gt;               Gen.      Hidden      Connections\n&gt;  &gt; Avg.:      18.95     1.6            4.2\n&gt;  &gt; Std.:       4.95      0.82          1.32\n&gt;  &gt; -----------------------------------------\n&gt;  &gt;\n&gt;  &gt; The only difference here is that the neural network is updated using\n&gt;  &gt; information from the previous step only.\n&gt;  &gt;\n&gt;  &gt; Please note that following this methodology the network has to be\n&gt;  &gt; activated in a &quot;dynamical way&quot;, even for supervised training such as\n&gt;  &gt; XOR. The input patters are presented to the network as follows:\n&gt;  &gt;\n&gt;  &gt; (0,0) at time 1, (0,1) at time 2, (1,0) at time 3 and (1,1) at time\n&gt;  4.\n&gt;  &gt;\n&gt;  &gt; At time 1 the output neuron is activated using the activation from\n&gt;  the\n&gt;  &gt; hidden neuron at time t0 (which is set to zero) and the second input\n&gt;  &gt; value, while the hidden neuron only uses information from the\n&gt;  inputs.\n&gt;  &gt;\n&gt;  &gt; At step 2 the output neuron uses the activation of the hidden neuron\n&gt;  &gt; at time 1 and the second input.\n&gt;  &gt;\n&gt;  &gt; And so on..\n&gt;  &gt;\n&gt;  &gt; In ordinary activation methods for feed-forward networks the same\n&gt;  does\n&gt;  &gt; not happen. We first need to activate the neuron from the first\n&gt;  layer\n&gt;  &gt; and then proceed to the next layer. To achieve the same behavior\n&gt;  we&#39;d\n&gt;  &gt; need to activate the network as many times as the numbers of neurons\n&gt;  &gt; it has.\n&gt;  &gt;\n&gt;  &gt; It seems that evolution can take advantage of this method and thus,\n&gt;  &gt; achieve the expected error value in fewer steps exploring the\n&gt;  &gt; sequential values presented to the inputs.\n&gt;  &gt;\n&gt;  &gt; The winner is attached :-)\n&gt;  &gt;\n&gt;  &gt; Cesar\n&gt;  &gt;\n&gt;\n&gt;\n&gt;\n&gt;                   \n\n"}}