{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":104426122,"authorName":"Mitchell Timin","from":"Mitchell Timin &lt;zenguyuno@...&gt;","profile":"zenguyuno","replyTo":"LIST","senderId":"7-A3ZZKoxAJA_qVnMpVRODhBDTBhToBpFPQyxeZF1Y4gBKfyYDYibK31AbkKg0EvE6TjbvVsTmTNmnFoI1dZZLyC-zU_bSXTcwVL","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] Re: Some Good News and Some Bad News","postDate":"1067661117","msgId":191,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDIwMDMxMTAxMDQzMTU3LjM0ODgucW1haWxAd2ViMjE0MDcubWFpbC55YWhvby5jb20+","inReplyToHeader":"PDNGQTJGRjlDLjMwODA1MDFAZHNsLnBpcGV4LmNvbT4="},"prevInTopic":186,"nextInTopic":193,"prevInTime":190,"nextInTime":192,"topicId":153,"numMessagesInTopic":6,"msgSnippet":"... Right.  Your real target should be the six-card game, and larger.  But you can do the 4-card game on the way there.  It will be interesting to see how many","rawEmail":"Return-Path: &lt;zenguyuno@...&gt;\r\nX-Sender: zenguyuno@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 14313 invoked from network); 1 Nov 2003 04:31:58 -0000\r\nReceived: from unknown (66.218.66.167)\n  by m1.grp.scd.yahoo.com with QMQP; 1 Nov 2003 04:31:58 -0000\r\nReceived: from unknown (HELO web21407.mail.yahoo.com) (216.136.232.77)\n  by mta6.grp.scd.yahoo.com with SMTP; 1 Nov 2003 04:31:57 -0000\r\nMessage-ID: &lt;20031101043157.3488.qmail@...&gt;\r\nReceived: from [64.180.115.78] by web21407.mail.yahoo.com via HTTP; Fri, 31 Oct 2003 20:31:57 PST\r\nDate: Fri, 31 Oct 2003 20:31:57 -0800 (PST)\r\nSubject: Re: [neat] Re: Some Good News and Some Bad News\r\nTo: neat@yahoogroups.com\r\nIn-Reply-To: &lt;3FA2FF9C.3080501@...&gt;\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=us-ascii\r\nFrom: Mitchell Timin &lt;zenguyuno@...&gt;\r\nX-Yahoo-Group-Post: member; u=104426122\r\nX-Yahoo-Profile: zenguyuno\r\n\r\n--- Colin Green &lt;cgreen@...&gt; wrote:\n&gt; Hi Mitchell,\n&gt; \n&gt; I&#39;ve just been catching up on this thread...\n&gt; \n&gt; Mitchell Timin wrote:\n&gt; \n&gt; &gt;--- Kenneth Stanley &lt;kstanley@...&gt; wrote:\n&gt; &gt;&lt;snip&gt;\n&gt; &gt;\n&gt; &gt;What is going on that such a small amount of\n&gt; &gt;information is being\n&gt; &gt;encoded by so many variables?  I also wonder about\n&gt; &gt;  \n&gt; &gt;\n&gt; I agree with Ken on this one. When I was working on\n&gt; prey capture not so \n&gt; long ago I quickly hacked together a NN by hand (I\n&gt; can load/save them as \n&gt; XML files). I was able to hand-craft a basic memory\n&gt; by simply \n&gt; interconnecting two nodes so that each one recieved\n&gt; the output of the \n&gt; other. If an external signal is &#39;injected&#39; into this\n&gt; pair then they will \n&gt; (or can be made to) maintain a high activation,\n&gt; until another external \n&gt; signal cancels out the signal (by using a negative\n&gt; signal). The \n&gt; important thing to note was that I was able to\n&gt; create a very well \n&gt; performing NN with just 4 or 5 neurons, so I can\n&gt; speculate that a really \n&gt; clever (good) network would probably use no more\n&gt; than a dozen (20 at \n&gt; most) neurons. If I see evolved networks with far\n&gt; more neurons then \n&gt; something is probably wrong - probably there are a\n&gt; lot of neurns that \n&gt; simply aren&#39;t doing anything, that is they are not\n&gt; participating in the \n&gt; generating the final output signal.\n&gt; \n&gt; &gt;BTW, I allow the net to settle while holding the\n&gt; input\n&gt; &gt;values.  I let it take up to 16 updates for the\n&gt; &gt;outputs to reach a state where only 1 line is high.\n&gt; \n&gt; &gt;\n&gt; This is interesting. I came across this same problem\n&gt; - how to decide how \n&gt; many re-evaluations (think of them as time steps) of\n&gt; the network to \n&gt; perform for each step in domain time. E.g. each time\n&gt; step the prey has a \n&gt; probability of moving (fixed between 0 and 1) and\n&gt; the agent (controlled \n&gt; by the NN) has a chance to move 1 unit based on it&#39;s\n&gt; output signals. \n&gt; Now, when I feed the input signals in to the NN how\n&gt; many times should I \n&gt; call NN.Recalc() before I read the output values.\n&gt; \n&gt; There are a couple of factors that should be\n&gt; considered:\n&gt; \n&gt; 1) Some of my hand crafted designs were reliant on a\n&gt; certian number of \n&gt; calls to recalc() per cycle. E.g. one design made\n&gt; the agent travel in a \n&gt; circle, to do this I set up a chain of neurons and\n&gt; bounced a signal \n&gt; around the chain.  The 4 output nodes (North east\n&gt; south west) were then \n&gt; connected to points on the chain. Therefore the\n&gt; agent will describe \n&gt; different behaviour if I call recalc()  a different\n&gt; number of times.\n&gt; \n&gt; 2) I always try to keep in mind how CPU intensive a\n&gt; simulation will be. \n&gt; Unnecessary calls to Recalc() waste valuable CPU\n&gt; time.\n&gt; \n&gt; In the end I picked a number of calls to Recal()\n&gt; that I thought was \n&gt; sensible for the size of network I was expecting to\n&gt; evolve - based upon \n&gt; my hand crafted networks. I think I call recalc 3 or\n&gt; 4 times. Anyway \n&gt; it&#39;s a fixed nuber of times, if I changed this then\n&gt; my evolved NN&#39;s \n&gt; would probbaly become useless.\n&gt; \n&gt; I find it interesting that you call recalc() until\n&gt; the output signals \n&gt; &#39;work themselves out&#39;. I hadn&#39;t considered this\n&gt; technique. If however I \n&gt; did implement it, it would almost certainly only\n&gt; call recalc() once sine \n&gt; I just take the highest output value as the selected\n&gt; output, and since \n&gt; the output values are floating point they are rarely\n&gt; equal.\n&gt; \n&gt; &gt;Of course it may not really have settled, just\n&gt; because\n&gt; &gt;1 line out of only 4 outputs is high, but that&#39;s\n&gt; the\n&gt; &gt;way the current code is.  Maybe that&#39;s a serious\n&gt; &gt;defect, and I should require all the outputs to\n&gt; &gt;stabilize.\n&gt; &gt;  \n&gt; &gt;\n&gt; My gut feeling is that this stabilization may be\n&gt; complicating matters. \n&gt; Perhaps try the fixed number of recalc() calls idea?\n&gt; \n&gt; &gt;&gt;I will do that.  I think that neat could evolve a\n&gt; &gt;&gt;perfect player of this 4-card game, although I&#39;m\n&gt; not\n&gt; &gt;&gt;certain.  We can&#39;t be certain until someone sets\n&gt; it up\n&gt; &gt;&gt;and does it.\n&gt; &gt;&gt;\n&gt;  From your description of the problem and knowledge\n&gt; of what NEAT as \n&gt; solved so far I would agree that NEAT is capable of\n&gt; solving this with no \n&gt; problem at all.\n\nRight.  Your real target should be the six-card game,\nand larger.  But you can do the 4-card game on the way\nthere.  It will be interesting to see how many neurons\nyou wind up with.  To play well the network must\nremember which two of the 4 cards were exposed by the\nfirst player, and whether each of those was a 1 or a\n2.\nThen it must use that data to turn over an unexposed\ncard, and then pick the single exposed card that\nmatches it.\n\n=====\nMitchell Timin\nhttp://annevolve.sourceforge.net\n\n__________________________________\nDo you Yahoo!?\nExclusive Video Premiere - Britney Spears\nhttp://launch.yahoo.com/promos/britneyspears/\n\n"}}