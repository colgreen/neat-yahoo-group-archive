{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":211599040,"authorName":"Jeff Clune","from":"Jeff Clune &lt;jclune@...&gt;","profile":"jeffreyclune","replyTo":"LIST","senderId":"w-sjcMK9sj3u_azh5SJnfuB4mSdM0-pKXQcsXcN9bD0GKMaIt90oBl7lL9r4qg3Fu98PePaW98BS_DRaLw4cX5hg","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [neat] Re: Machine Learning and the Long View of AI","postDate":"1209691847","msgId":4031,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PEM0M0ZFNzA3LjIyOTJDJWpjbHVuZUBtc3UuZWR1Pg==","inReplyToHeader":"PGZ2YWNvcSs4MzMwQGVHcm91cHMuY29tPg=="},"prevInTopic":4025,"nextInTopic":0,"prevInTime":4030,"nextInTime":4032,"topicId":3955,"numMessagesInTopic":49,"msgSnippet":"Hello Ken- I am glad you decided to continue the conversation. I fully agree that these are deep and difficult issues. I am also not sure how to solve them, or","rawEmail":"Return-Path: &lt;jclune@...&gt;\r\nX-Sender: jclune@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 33832 invoked from network); 2 May 2008 01:30:51 -0000\r\nX-Received: from unknown (66.218.67.95)\n  by m48.grp.scd.yahoo.com with QMQP; 2 May 2008 01:30:51 -0000\r\nX-Received: from unknown (HELO py-out-1112.google.com) (64.233.166.176)\n  by mta16.grp.scd.yahoo.com with SMTP; 2 May 2008 01:30:51 -0000\r\nX-Received: by py-out-1112.google.com with SMTP id w49so1385987pyg.36\n        for &lt;neat@yahoogroups.com&gt;; Thu, 01 May 2008 18:30:50 -0700 (PDT)\r\nX-Received: by 10.35.86.12 with SMTP id o12mr4757815pyl.13.1209691850474;\n        Thu, 01 May 2008 18:30:50 -0700 (PDT)\r\nReturn-Path: &lt;jclune@...&gt;\r\nX-Received: from ?192.168.2.2? ( [67.167.130.112])\n        by mx.google.com with ESMTPS id a78sm4060349pye.19.2008.05.01.18.30.48\n        (version=TLSv1/SSLv3 cipher=OTHER);\n        Thu, 01 May 2008 18:30:49 -0700 (PDT)\r\nUser-Agent: Microsoft-Entourage/12.1.0.080305\r\nDate: Thu, 01 May 2008 21:30:47 -0400\r\nTo: &quot;neat@yahoogroups.com&quot; &lt;neat@yahoogroups.com&gt;\r\nMessage-ID: &lt;C43FE707.2292C%jclune@...&gt;\r\nThread-Topic: [neat] Re: Machine Learning and the Long View of AI\r\nThread-Index: Acir9CWGz30laMWzAEes8kp+9r7O1g==\r\nIn-Reply-To: &lt;fvacoq+8330@...&gt;\r\nMime-version: 1.0\r\nContent-type: text/plain;\n\tcharset=&quot;US-ASCII&quot;\r\nContent-transfer-encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Jeff Clune &lt;jclune@...&gt;\r\nSubject: Re: [neat] Re: Machine Learning and the Long View of AI\r\nX-Yahoo-Group-Post: member; u=211599040; y=1gm6F_JW_3neGLzX0jdtj-1GXmfiAbrAALOFX01dBS3HkR55aJRm\r\nX-Yahoo-Profile: jeffreyclune\r\n\r\nHello Ken-\n\nI am glad you decided to continue the conversation. I fully agree that these\nare deep and difficult issues. I am also not sure how to solve them, or even\nshed light on them. I wish what follows was more coherent but, as you say,\nthis is complicated stuff.\n\nI think it is interesting to start out be evaluating whether nature solved\nthis problem. I believe it did. It has heavily used bilateral symmetry and\nfive-fold radial symmetry, as you mention. It also created high levels of\nmodularity (e.g neural modules and organs), which at some point involved\nfiguring out how many of such modules to create. One wrinkle, however, is\nthat we need to distinguish between discovering a pre-chosen regularity and\ndiscovering/displaying regularity. Clearly it is harder to match a specific\nregular target than simply to show some type of regularity. While we know\nnature displays regularity, has it matched externally existing regularities?\nI believe it has. Many organisms time their behaviors to match external\npatterns (e.g. the rising and setting of the sun, the emergence of fruit in\nthe spring, etc.). I do not know enough about this area, but one could also\nimagine that mutations might have added body segments and then evolution had\nto modify and modularize the brain accordingly. Furthermore, evolution chose\nhow many neural modules to create to deal with the environments animals\nface. Presumably the number of modules reflects, to some extent,\nregularities in the external environment. So, when you said earlier that no\nanimal has had to create five separate brains, I agree. However, there were\nmany cases where brains had to be tuned to create neural modules to exploit\npre-existing environmental regularities. So, something similar to a process\nof dividing up the brain to match the number of problems out there happened.\n\nIf we can agree that nature solved the problem, then we know the problem is\nsolvable. That is reassuring. How did nature solve it? One very interesting\nthing that has been discussed in this thread is the fact that nature got to\nconduct more search than we can expect to. It has had ~3 billion years, as\nhas been mentioned, but it has also had population sizes that eclipse those\nthat are computationally possible for us.  Are these the main reasons nature\nsolved the problem? I don&#39;t know the answer to this question. Clearly they\nhelp.  If they are a main reason nature solved the problem, it is rather\ndepressing news for EC, as Moore&#39;s law will not get us to natural population\nsizes any time soon. However, there is some evidence that upping population\nsizes a few orders of magnitude is not sufficient. The golem@home project\nwas shut down because, even with those massive number population sizes,\nevolution flatlined pretty early.\n\nAnother explanation is that nature&#39;s populations are much more diverse than\nours. It accomplishes this feat because it can have massive population sizes\nthat are physically separated and occupying different niches. Once again,\nthat nature has a larger computer than we do comes into play. However, we\ncan try to take shortcuts to provide such diversity even with low population\nsizes. Your fitness sharing mechanism is one tactic. Another is an algorithm\ncalled Hierarchical Fair Competition, developed here at MSU, which creates\ndifferent skill levels that organisms compete in (you don&#39;t declare a\nsix-year-old unpromising because he can&#39;t compete against Ronaldinho). There\nare also results that say that when different organisms are rewarded for\ndoing different things (i.e. there are many different fitness resources they\ncan consume) the result is higher levels of diversity.\n\nA different possibility is that regularities are beneficial even when\nemergent. In nearly every case in nature where something extremely\ncomplicated arose (e.g. eyes), we have found that there are tons of\nintermediate steps along the way that were fit. Somehow, bilateral or radial\nsymmetry must have conferred advantages to the blobs long before they were\nhawks. How can we provide such fitness functions? Coevolution is one\npossibility. Rewarding a host of intermediate steps is another. Having lots\nof different niches probably also helps on this front.\n\nNone of the remedies above involves encodings. Since HyperNEAT was a new\nencoding, it&#39;s contribution should be in this arena. We know that natural\nencodings are quite modular and that small mutations can produce large,\ncoordinated, effective changes. This facilitates the production of\nphenotypes that are complicated and regular but show a different regularity\nthan their ancestor. Hornby showed that mutations with his generative\nencoding on average had much larger effects but were also more beneficial\nthan with a direct encoding control. As such, discovering even pre-set\nregularities with his generative encoding is probably easier than with his\ndirect encoding. Certainly the creatures produced by his generative\nencodings were more modular than those produced by the direct encoding\ncontrol. \n\nSimilarly, HyperNEAT should do better than P-NEAT at exploiting regularities\nin the environment. In fact, you showed this in your two 2007 GECCO papers,\nwhere HyperNEAT discovered the externally pre-selected phenotypic motif that\nneeded to be repeated, and did so better and faster than P-NEAT. You may\ncounter that the benefits of discovering that regularity had immediate\nimpacts on fitness, so it does not represent the case of figuring out a\nregularity whose impact on fitness is more removed. That&#39;s true. But it does\nraise the question of whether generative encodings are more likely than\ndirect encodings to produce modular phenotypes even when the fitness\nconsequences of that modularity are less direct. Since HyperNEAT is biased\ntoward producing modular phenotypes, shouldn&#39;t it be more likely than P-NEAT\nto exploit environmental regularities, even if the benefit of the creation\nof the phenotypic modularity necessary to exploit these regularities is less\nimmediate? If we think that claim is true, shouldn&#39;t we test it? If so,\nwouldn&#39;t that test consist of setting up environments with varying amounts\nof regularity and seeing if HyperNEAT is better at exploiting them?\n\nOne could respond that HyperNEAT, in biasing search toward certain regular\npatterns, does all that encodings can do. It creates the bias but is not\nguaranteed to discover the regularity because of the other issues we have\nmentioned. Again, I think it would be nice to first document the gain that\nhas been provided by HyperNEAT because of the bias. The next question is\nwhether the contribution from encodings has been maxed out with HyperNEAT.\nIt seems that being able to evolve the substrate could further improve\nHyperNEAT on this front. For example, I have set up some HyperNEAT\nexperiments where there are two panels of inputs (A and B) and two panels of\noutputs (A&#39; and B&#39;). The fitness function rewards organisms that output in\nA&#39; the pattern that was shown in A (and the same for B&#39; and B). As such,\nthere is a strong pressure to eliminate links between the A inputs and the\nB&#39; outputs (and vice versa). From my *preliminary* results, HyperNEAT is not\nable to eliminate the links between the A and B sides (or even reduce the\nweight values). This is with the .2 cutoff. Maybe evolving the substrate\nsuch that it starts with no links and then complexifies would help, because\nthe fitness function would not be as complicated for the first few links\nadded. I don&#39;t really know. In the long run, though, if we want to produce\nbrains that are modular, the algorithm will need the ability to create\nseparate modules that have a high number of intra-module connections and few\ninter-module connections. I think it is interesting to test how good\nHyperNEAT is at performing that task and trying to make it better on that\nfront.     \n\nIn brief, many of the reasons nature did well with this challenge have\nnothing to do with encodings. However, encodings do have a role to play. Did\nHyperNEAT increase the contribution from the encoding department? Is there\nmore that it can do?\n\nApologies to all for the long, rambling answer. It is a large topic to try\nto cover in an email exchange.\n\n\nCheers,\nJeff Clune\n\nDigital Evolution Lab, Michigan State University\n\njclune@...\n\n\n\n\n&gt; From: Kenneth Stanley &lt;kstanley@...&gt;\n&gt; Reply-To: &quot;neat@yahoogroups.com&quot; &lt;neat@yahoogroups.com&gt;\n&gt; Date: Wed, 30 Apr 2008 18:11:37 -0000\n&gt; To: &quot;neat@yahoogroups.com&quot; &lt;neat@yahoogroups.com&gt;\n&gt; Subject: FW: [neat] Re: Machine Learning and the Long View of AI\n&gt; \n&gt; Jeff,\n&gt; \n&gt; I know we&#39;ve reached a kind of natural conclusion to our discussion,\n&gt; but I think it&#39;s a good opportunity to point out one more interesting\n&gt; issue to think about:\n&gt; \n&gt; To what extent is it possible to improve in general at being &quot;good at\n&gt; discovering and exploiting regularities?&quot;  I mean, let&#39;s really think\n&gt; about what the hope would be with that idea.\n&gt; \n&gt; There is no doubt that an indirect encoding can *represent*\n&gt; regularities. For example, CPPNs can represent symmetry.  But does\n&gt; being able to represent something equate to being good at discovering\n&gt; it, and if not, what else could make something more &quot;discoverable?&quot;\n&gt; \n&gt; What I&#39;m getting at is that I think the issue is fairly deep and\n&gt; complicated (not to imply that you are implying that it is a simple\n&gt; matter either- I just think it&#39;s interesting to discuss these issues).\n&gt; \n&gt; If being good at discovering just means that you tend to discover the\n&gt; right regularity quickly, then it&#39;s easy to see what kind of\n&gt; regularity a particular encoding will be good at discovering.  For\n&gt; example, CPPNs are quite good at discovering bilateral symmetry (in\n&gt; this sense) because all it needs to do is add a single Gaussian node\n&gt; to the CPPN early in evolution, which has relatively high probability.\n&gt; \n&gt; Yet does that mean HyperNEAT is not good at discovering things that\n&gt; would take longer to build up, such as five-fold radial symmetry with\n&gt; two of the repetitions slightly different than the others? Certainly\n&gt; it *can* represent such a structure, but because it does not have a\n&gt; hidden node with that exact pattern in it, it would have to build it\n&gt; up.  So it seems like the &quot;time to discovery&quot; metric is not the full\n&gt; story on how good something is at discovering.  After all, it *should*\n&gt; take longer to discover some regularities than others.\n&gt; \n&gt; So really what we&#39;re talking about here are biases towards certain\n&gt; types of regularities appearing sooner or later in the search.  In\n&gt; that view, we need to be careful about what we mean by good because I\n&gt; am not sure if a system that is fast at discovering a particular\n&gt; bizarre sort of regularity really deserves to be called good in\n&gt; general, when 99% of the time that regularity is going to interfere\n&gt; whenever it is instantiated.\n&gt; \n&gt; Yet the problem is even more complicated than that.  In fact, just\n&gt; because a particular regularity is &quot;discovered&quot; inside the CPPN (or\n&gt; any other encoding) does not mean that the fitness function rewards\n&gt; that discovery.  Therefore, there is a more fundamental problem.  For\n&gt; example, imagine discovering a symmetric blob when we are searching\n&gt; for an airplane.  Well, blobs don&#39;t fly, but symmetry is the right\n&gt; regularity.  But because it can&#39;t fly, the blob doesn&#39;t get rewarded\n&gt; and just dies out, so what then do we say about the goodness of the\n&gt; discovery?\n&gt; \n&gt; Then we might say, well, it should be smarter about things.  It should\n&gt; somehow make sure it doesn&#39;t lose the right regularity when it\n&gt; discovers it.  And in fact there are some protections that can be put\n&gt; in place to maintain diversity and try to help with that issue (like\n&gt; speciation).  But at the end of the day, if you&#39;ve got a blob\n&gt; competing to be an airplane, and it doesn&#39;t get lucky enough to make\n&gt; some more mutations on the path to airplane, it&#39;s not going to cut it,\n&gt; and it is eventually going to die out.\n&gt; \n&gt; So then we might say that what we really need is something that would\n&gt; say, &quot;Ah! Even though I have a useless blob I can see that the\n&gt; discovery of symmetry might be useful and therefore we will hold on to\n&gt; it and see what we can do with it.&quot;  Yet that seems way outside the\n&gt; scope of evolutionary computation.  If you could make that kind of\n&gt; deduction (based on an intelligent insight), why have evolution at\n&gt; all?  You could just build the thing based on intellect and deduction.\n&gt;  Or at least you would have a system well beyond evolutionary\n&gt; computation alone (perhaps not a bad idea if it&#39;s possible).\n&gt; \n&gt; So the point is just that being good at discovering regularities is a\n&gt; fairly tricky topic.  It&#39;s easy to set up expectations that cannot be\n&gt; met.  No Free Lunch is also leaking through the cracks all over the\n&gt; place.  What I think is &quot;good&quot; about CPPNs in terms of discovering\n&gt; regularities is that we are able to encode the primary types of\n&gt; regularities into the activation functions, so we know what we&#39;re\n&gt; building out of and biasing towards, and those regularities are\n&gt; motivated by nature and engineering designs.  Also, we get to forgo\n&gt; the process of developmental unfolding, which can be expensive and\n&gt; harder to bias in a desired way.  So what I&#39;m saying is that part of\n&gt; what&#39;s good about CPPNs is that they can be biased towards different\n&gt; types of patterns relatively easily.  But that does not mean they\n&gt; necessarily discover any arbitrary pattern quickly.  No encoding will\n&gt; ever be able to do that, unless it is under the control of a higher\n&gt; intellect. CPPNs in their present form are based on an assumption\n&gt; about what kinds of regularities are useful.  If the assumptions turn\n&gt; out wrong, the encoding is relatively flexible enough to be biased\n&gt; differently, which is another thing you could say is good about its\n&gt; ability to discover regularities.\n&gt; \n&gt; On the other hand, I think &quot;exploiting&quot; regularities (which you also\n&gt; mention) is easier to pin down.  It means, given that you found\n&gt; symmetry, you can now elaborate on that and make something more\n&gt; complex with even more regularities, but that still respects the\n&gt; initial symmetric regularity.  In other words, we&#39;re not judging on\n&gt; the regularity being good or bad, just that it can be elaborated.  I\n&gt; believe it is easier to see that CPPNs can do that well, since it is\n&gt; directly related to complexification.\n&gt; \n&gt; Anyway, I just thought these would be interesting to consider all the\n&gt; nuances in the discussion of effective pattern discovery.\n&gt; \n&gt; ken\n&gt; \n&gt; \n&gt; --- In neat@yahoogroups.com, Jeff Clune &lt;jclune@...&gt; wrote:\n&gt;&gt; \n&gt;&gt; Hello Ken-\n&gt;&gt; \n&gt;&gt; You make a very strong defense of your position. I have been entirely\n&gt;&gt; persuaded that in the multi-agent paper it was appropriate to use\n&gt; the r(x)\n&gt;&gt; strategy. However, I continue to think that the issue comes down to what\n&gt;&gt; one&#39;s goal is. Your goal here was to evolve sophisticated multi-agent\n&gt;&gt; controllers. Given that goal, you are right that adding the extra\n&gt; challenge\n&gt;&gt; of figuring out where one brain begins and ends is not necessary.\n&gt; However,\n&gt;&gt; were one&#39;s goal to test how good HyperNEAT is at discovering and\n&gt; exploiting\n&gt;&gt; regularities in the problem space, in order to know how well it will\n&gt; perform\n&gt;&gt; when it encounters unanticipated regularities, it could be\n&gt; worthwhile to see\n&gt;&gt; how it does without the injected information.\n&gt;&gt; \n&gt;&gt; While I do think that evolution is cool, and like to show its\n&gt; prowess, I too\n&gt;&gt; am mainly interested in using evolution to produce general AI. My\n&gt;&gt; supposition was that figuring out to what extent our algorithms are\n&gt; able to\n&gt;&gt; exploit problem regularities may facilitate improvements on that font. I\n&gt;&gt; also think such improvements will be necessary steps in the path to\n&gt;&gt; producing evolutionary algorithms capable of creating general AI.\n&gt;&gt; \n&gt;&gt; Nevertheless, I benefited from hearing your perspective and\n&gt; appreciate the\n&gt;&gt; exchange. \n&gt;&gt; \n&gt;&gt; \n&gt;&gt; Cheers,\n&gt;&gt; Jeff Clune\n&gt;&gt; \n&gt;&gt; Digital Evolution Lab, Michigan State University\n&gt;&gt; \n&gt;&gt; jclune@...\n&gt;&gt; \n&gt;&gt; \n&gt;&gt; \n&gt;&gt; ------ Forwarded Message\n&gt;&gt;&gt; From: Kenneth Stanley &lt;kstanley@...&gt;\n&gt;&gt;&gt; Reply-To: &quot;neat@yahoogroups.com&quot; &lt;neat@yahoogroups.com&gt;\n&gt;&gt;&gt; Date: Tue, 29 Apr 2008 02:30:13 -0000\n&gt;&gt;&gt; To: &quot;neat@yahoogroups.com&quot; &lt;neat@yahoogroups.com&gt;\n&gt;&gt;&gt; Subject: [neat] Re: Machine Learning and the Long View of AI\n&gt;&gt;&gt; \n&gt;&gt;&gt; Jeff,\n&gt;&gt;&gt; \n&gt;&gt;&gt; I think it would be interesting to step back and look at the\n&gt;&gt;&gt; assumptions that underly your straw man argument.  In particular, why\n&gt;&gt;&gt; are we using evolution in the first place?\n&gt;&gt;&gt; \n&gt;&gt;&gt; Your straw man implies a world view wherein we are using evolution\n&gt;&gt;&gt; because we like evolution and want it to succeed.  Under that\n&gt;&gt;&gt; philosophy, then indeed, creating a super-powerful neural network\n&gt;&gt;&gt; (DeepNet) by hand and then creating a faux-mutation operator that\n&gt;&gt;&gt; simply makes a neural network turn into DeepNet would be a\n&gt;&gt;&gt; disappointment, because it would mean that evolution didn&#39;t really do\n&gt;&gt;&gt; what we wanted, and we would be being disingenuous (and unimpressed).\n&gt;&gt;&gt; \n&gt;&gt;&gt; However, my world view is different.  I am not using evolution because\n&gt;&gt;&gt; I like evolution and want to prove that it is impressive. (Note that I\n&gt;&gt;&gt; *do* like evolution, but that&#39;s not the reason I use it.)  Rather, I\n&gt;&gt;&gt; am using evolution because I genuinely believe that full-fledged AI\n&gt;&gt;&gt; likely *cannot* be constructed by hand, and that evolution is the best\n&gt;&gt;&gt; alternative.  If someone went ahead and built a general-AI neural\n&gt;&gt;&gt; network by hand, it would simply prove me wrong.  But it would mean\n&gt;&gt;&gt; nothing with respect to how we should go about doing the same with\n&gt;&gt;&gt; evolution (which would however be a moot point at that point anyway\n&gt;&gt;&gt; because why bother when someone figured out a better way?).  So as\n&gt;&gt;&gt; soon as someone did what you said, evolution would be thrown out\n&gt;&gt;&gt; anyway, at least in terms of being the best path to general AI.\n&gt;&gt;&gt; \n&gt;&gt;&gt; So I am looking at things in a kind of reverse perspective from you.\n&gt;&gt;&gt; To me, the point is not to bolster up evolution and show how powerful\n&gt;&gt;&gt; it is.  Rather, the point is that I believe it *is* powerful and\n&gt;&gt;&gt; therefore I am using it.  If I can do something to boost it further,\n&gt;&gt;&gt; good.  What else is there to prove?\n&gt;&gt;&gt; \n&gt;&gt;&gt; However, I am a realist and I doubt that evolution alone will get the\n&gt;&gt;&gt; job done in the long view.  It&#39;s just too gigantic a search space and\n&gt;&gt;&gt; the problem is too poorly specified.  Therefore, I think there will be\n&gt;&gt;&gt; a lot of biases and manipulations along the way.\n&gt;&gt;&gt; \n&gt;&gt;&gt; I&#39;m way off speculating about the far off future here, but my guess is\n&gt;&gt;&gt; that those manipulations will come mostly at the genetic level rather\n&gt;&gt;&gt; than the ANN level.  In other words, the kinds of hacks that you are\n&gt;&gt;&gt; talking about (&quot;building blocks&quot; provided a priori) generally seem to\n&gt;&gt;&gt; be kind of neural &quot;modules&quot; that are built a priori and just dumped\n&gt;&gt;&gt; into the network en masse.  Those are indeed a bit cringe-inducing.\n&gt;&gt;&gt; However, my problem with them is not that they are cringe-inducing.\n&gt;&gt;&gt; Rather, again, I doubt they will really be a big help in the long run.\n&gt;&gt;&gt;  The reason I doubt their utility is because I believe that a massive\n&gt;&gt;&gt; brain needs to be also massively interwoven, such that each internal\n&gt;&gt;&gt; area of each part is entirely accessible and &quot;speaks the language&quot; of\n&gt;&gt;&gt; any other part.  Some ad hoc module thrown in the mix, while perhaps\n&gt;&gt;&gt; helpful in the short run, will never be able to fill that role because\n&gt;&gt;&gt; it was not built along with the rest of the infrastructure.  So that&#39;s\n&gt;&gt;&gt; why I&#39;m against it: Not because it&#39;s cheating, but because it\n&gt; won&#39;t work.\n&gt;&gt;&gt; \n&gt;&gt;&gt; So I think you have to distinguish between that type of hack and the\n&gt;&gt;&gt; kind of thing where we provide sort of &quot;genetically-engineered&quot;\n&gt;&gt;&gt; information, i.e. at the genetic level.  That I do believe will be\n&gt;&gt;&gt; useful, and should be exploited, because those are knobs and\n&gt;&gt;&gt; coordinate frames upon which a castle can be built.  So providing\n&gt;&gt;&gt; coordinate systems that are useful seems to me likely *long-run*\n&gt;&gt;&gt; useful.  It is not the same as telling it how to connect up, and the\n&gt;&gt;&gt; substrate that pops out in the end is going to be as pure as any:\n&gt;&gt;&gt; completely ANN through and through and totally a product of the\n&gt;&gt;&gt; indirect encoding.\n&gt;&gt;&gt; \n&gt;&gt;&gt; Finally, I think you are inferring too much about how much prior\n&gt;&gt;&gt; information I am advocating based on just Multiagent HyperNEAT.  There\n&gt;&gt;&gt; is no animal on earth that has to develop five disconnected brains on\n&gt;&gt;&gt; a single sheet with five compartments.  Expecting evolution to just\n&gt;&gt;&gt; figure out where one brain begins and the other ends seems to me very\n&gt;&gt;&gt; unnatural and bizarre, and also uninteresting.  Statistically\n&gt;&gt;&gt; speaking, it is evident that *any* intelligence would take longer to\n&gt;&gt;&gt; figure that out and solve the problem on average than one that was\n&gt;&gt;&gt; provided such information a priori.  So there&#39;s no surprise in that.\n&gt;&gt;&gt; \n&gt;&gt;&gt; So of course HyperNEAT performs worse without knowing the divisions\n&gt;&gt;&gt; between separate brains on a substrate than when it knows them up\n&gt;&gt;&gt; front.  That doesn&#39;t imply  that HyperNEAT cannot figure it out on its\n&gt;&gt;&gt; own, or that I think it doesn&#39;t matter if HyperNEAT can find\n&gt;&gt;&gt; regularities on its own.  It&#39;s just, it would take a while longer and\n&gt;&gt;&gt; would be less reliable, so why bother waiting?  The spatial divisions\n&gt;&gt;&gt; among the brains is ad hoc (something we simply decided a priori by\n&gt;&gt;&gt; fiat) and thus is not the interesting issue.\n&gt;&gt;&gt; \n&gt;&gt;&gt; I don&#39;t think it will be the same in a lot of non-multiagent tasks\n&gt;&gt;&gt; because this unnatural issue of multiple brains and their positions\n&gt;&gt;&gt; does not come up, and I do believe that HyperNEAT often does discover\n&gt;&gt;&gt; regularities on its own, and that&#39;s a good thing.\n&gt;&gt;&gt; \n&gt;&gt;&gt; Anyway, the broader point is that I will stick to my strong position:\n&gt;&gt;&gt; I do not believe that finding  middle ground or a sweet spot in terms\n&gt;&gt;&gt; of biases and constraints is the important issue in the long view of\n&gt;&gt;&gt; achieving general AI through evolution *unless* you are only doing it\n&gt;&gt;&gt; to prove how cool evolution is.  In contrast, I&#39;m using evolution\n&gt;&gt;&gt; because I think it is the best hope. The funny thing is that we will\n&gt;&gt;&gt; regardless end up agreeing on a lot, because I too don&#39;t like to\n&gt;&gt;&gt; provide big building blocks.  But my reason is that they will end up\n&gt;&gt;&gt; being incapable of building a general AI.  So I think a lot of things\n&gt;&gt;&gt; that look bad also won&#39;t work, so someone who is trying to make\n&gt;&gt;&gt; evolution look good will often see me as sharing their assumptions.\n&gt;&gt;&gt; \n&gt;&gt;&gt; ken\n&gt;&gt;&gt; \n&gt;&gt;&gt; \n&gt;&gt;&gt; --- In neat@yahoogroups.com, Jeff Clune &lt;jclune@&gt; wrote:\n&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt; Ken-\n&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt; Thank you for explaining these arguments at length. I always\n&gt; learn a lot\n&gt;&gt;&gt;&gt; when we have these sorts of discussions, and there is no exception\n&gt;&gt;&gt; in this\n&gt;&gt;&gt;&gt; case. I agree with most of what you write below, especially that\n&gt;&gt;&gt;&gt; constraining/biasing evolution is very important. I guess the only\n&gt;&gt;&gt; place we\n&gt;&gt;&gt;&gt; disagree is that I believe there is a middle ground of constraint\n&gt;&gt;&gt; that we\n&gt;&gt;&gt;&gt; should shoot for, whereas you seem to feel &#39;the more the better&#39;.\n&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt; You write:\n&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt;  Therefore, progress is NE should in part be measured with\n&gt; respect to\n&gt;&gt;&gt;&gt;&gt; progress in constraining the problem to make such a discovery more\n&gt;&gt;&gt;&gt;&gt; likely.  When an NE algorithm is improved to allow us to tell it\n&gt; more\n&gt;&gt;&gt;&gt;&gt; about the world in which its output will be situated, that is good\n&gt;&gt;&gt;&gt;&gt; news for the long view.  In short, we don&#39;t care at all how NE\n&gt;&gt;&gt;&gt;&gt; produced a brain as long as it really does.\n&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt; This reminds me of something that Hod Lipson says repeatedly.\n&gt; Whenever\n&gt;&gt;&gt;&gt; someone evolves something impressive the first question to ask is,\n&gt;&gt;&gt; &quot;How big\n&gt;&gt;&gt;&gt; are your building blocks?&quot; I am going to provide a straw man of your\n&gt;&gt;&gt;&gt; argument. Hopefully the fact that I admit that up front will make it\n&gt;&gt;&gt; less\n&gt;&gt;&gt;&gt; objectionable. Imagine that Kasparov and a neural net engineer\n&gt;&gt;&gt; teamed up and\n&gt;&gt;&gt;&gt; hand-created a neural  net (call it &#39;DeepNet&#39;)  that played chess\n&gt; at a\n&gt;&gt;&gt;&gt; grandmaster level. Now imagine that we create an NE algorithm for\n&gt;&gt;&gt; learning\n&gt;&gt;&gt;&gt; chess playing that was otherwise identical to NEAT, but had one extra\n&gt;&gt;&gt;&gt; mutation operator, which was &#39;clear out the current phenotype and\n&gt;&gt;&gt; replace it\n&gt;&gt;&gt;&gt; with DeepNet&#39;. In this case we would have highly constrained the\n&gt;&gt;&gt; problem to\n&gt;&gt;&gt;&gt; find a good chess playing solution. We would have also successfully\n&gt;&gt;&gt; injected\n&gt;&gt;&gt;&gt; our a priori knowledge of the problem. However, it would be very\n&gt;&gt;&gt;&gt; unimpressive as an accomplishment in the field of evolutionary\n&gt;&gt;&gt; computation.\n&gt;&gt;&gt;&gt; The credit goes to the humans that designed DeepNet, not for the\n&gt;&gt;&gt;&gt; evolutionary algorithm that recreated it.\n&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt; As I said, this is an unfair caricature of your view. However, I\n&gt;&gt;&gt; think it\n&gt;&gt;&gt;&gt; might reveal what I have been trying to say. In my mind, the goal\n&gt; is to\n&gt;&gt;&gt;&gt; provide smaller and smaller building blocks because then we know\n&gt; it is\n&gt;&gt;&gt;&gt; evolution that is doing the work, and not us. There is a sweet spot\n&gt;&gt;&gt; in the\n&gt;&gt;&gt;&gt; middle. If we humans don&#39;t do any work in biasing the search, then\n&gt;&gt;&gt; evolution\n&gt;&gt;&gt;&gt; will perform terribly. But if we provide building blocks that are\n&gt;&gt;&gt; too large,\n&gt;&gt;&gt;&gt; then evolution did not really do the heavy lifting. So, as opposed\n&gt;&gt;&gt; to saying\n&gt;&gt;&gt;&gt; &#39;the more constraint the better,&#39; I think it is interesting to try to\n&gt;&gt;&gt;&gt; provide smaller building blocks while still gaining high levels of\n&gt;&gt;&gt;&gt; performance. As I have said before, I also think that if we make\n&gt;&gt;&gt; progress on\n&gt;&gt;&gt;&gt; this front, the evolutionary algorithm (not its product) will be\n&gt;&gt;&gt; more likely\n&gt;&gt;&gt;&gt; to generalize to solving other problems. The long term goal, of\n&gt;&gt;&gt; course, is\n&gt;&gt;&gt;&gt; to have our algorithms solve problems and create things where we\n&gt; either\n&gt;&gt;&gt;&gt; don&#39;t know how to solve the problems, or can&#39;t be bothered to do\n&gt; so. For\n&gt;&gt;&gt;&gt; example, the NE that produced DeepNet would not do very well at\n&gt; race car\n&gt;&gt;&gt;&gt; driving. But an algorithm that was constrained in a more abstract\n&gt; way to\n&gt;&gt;&gt;&gt; exploit regularities in its environment might do better on both car\n&gt;&gt;&gt; racing\n&gt;&gt;&gt;&gt; and chess. \n&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt; I guess I start from the recognition that evolution produced humans\n&gt;&gt;&gt; without\n&gt;&gt;&gt;&gt; any bias from a conscious entity. How it did that is one of the most\n&gt;&gt;&gt;&gt; fascinating and open questions both in our field and in biology. We\n&gt;&gt;&gt; agree\n&gt;&gt;&gt;&gt; that trying to emulate ways in which natural evolution did things\n&gt;&gt;&gt; like bias\n&gt;&gt;&gt;&gt; itself, and thus allow the evolution of modularity, is the way\n&gt;&gt;&gt; forward for\n&gt;&gt;&gt;&gt; our field. HyperNEAT represents such amazing progress because it\n&gt;&gt;&gt; employed\n&gt;&gt;&gt;&gt; this strategy. But it strikes me that nature was not told a priori\n&gt;&gt;&gt; how many\n&gt;&gt;&gt;&gt; leg modules it should make or learn to control. Nor was it told\n&gt; how many\n&gt;&gt;&gt;&gt; neural modules it should create in the brain. It figured that stuff\n&gt;&gt;&gt; out on\n&gt;&gt;&gt;&gt; its own, and probably performed better as a result because it could\n&gt;&gt;&gt; learn to\n&gt;&gt;&gt;&gt; tailor the number of modules it needed to the regularity of the\n&gt;&gt;&gt; problems it\n&gt;&gt;&gt;&gt; faced. I guess I don&#39;t think we will make it very far towards\n&gt; evolving\n&gt;&gt;&gt;&gt; brains that are generally intelligent if our evolutionary algorithms\n&gt;&gt;&gt; cannot\n&gt;&gt;&gt;&gt; do likewise. It seems that something is majorly lacking if we have\n&gt;&gt;&gt; to tell\n&gt;&gt;&gt;&gt; it each time what the regularities are in the environment, and\n&gt; how to go\n&gt;&gt;&gt;&gt; about exploiting them.\n&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt; Apologies for the straw man argument. I do think there is a lot of\n&gt;&gt;&gt; merit to\n&gt;&gt;&gt;&gt; the general thrust of what you say. I may be overreacting in\n&gt;&gt;&gt; focusing on the\n&gt;&gt;&gt;&gt; extremes\n&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt; Cheers,\n&gt;&gt;&gt;&gt; Jeff Clune\n&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt; Digital Evolution Lab, Michigan State University\n&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt; jclune@\n&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt; From: Kenneth Stanley &lt;kstanley@&gt;\n&gt;&gt;&gt;&gt;&gt; Reply-To: &quot;neat@yahoogroups.com&quot; &lt;neat@yahoogroups.com&gt;\n&gt;&gt;&gt;&gt;&gt; Date: Sun, 27 Apr 2008 21:36:33 -0000\n&gt;&gt;&gt;&gt;&gt; To: &quot;neat@yahoogroups.com&quot; &lt;neat@yahoogroups.com&gt;\n&gt;&gt;&gt;&gt;&gt; Subject: [neat] Re: Machine Learning and the Long View of AI\n&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt; --- In neat@yahoogroups.com, &quot;Derek James&quot; &lt;djames@&gt; wrote:\n&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt;&gt;&gt;  In RL, in contrast, the long view is almost the opposite: They\n&gt;&gt;&gt;&gt;&gt; want to\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;  remove all constraints and still learn nevertheless.\n&gt;&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt;&gt; I&#39;m not sure what you mean by this, Ken. Could you elaborate a\n&gt;&gt;&gt; little?\n&gt;&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt; Sure.  I think the problem is that I can&#39;t find a way to explain my\n&gt;&gt;&gt;&gt;&gt; point concisely.  As I try to explain it, it starts taking up\n&gt; too much\n&gt;&gt;&gt;&gt;&gt; text so I shorten it and then it loses its meaning.  Let me give\n&gt; it a\n&gt;&gt;&gt;&gt;&gt; try again...\n&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt; I think the difference between the goals of RL and NE is an\n&gt;&gt;&gt;&gt;&gt; interesting topic because they are almost always conflated, as\n&gt; if they\n&gt;&gt;&gt;&gt;&gt; are trying to solve the same problem.\n&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt; The RL community (e.g. value-function approaches) is trying to build\n&gt;&gt;&gt;&gt;&gt; something that learns like a natural brain.  They are saying,\n&gt; through\n&gt;&gt;&gt;&gt;&gt; analytic means we can deduce how a brain can learn from sparse\n&gt;&gt;&gt;&gt;&gt; reinforcement and formalize that process in an algorithm.  The\n&gt; hope, I\n&gt;&gt;&gt;&gt;&gt; would think, is to eventually build the &quot;general intelligence&quot; that\n&gt;&gt;&gt;&gt;&gt; aligns with the holy grail of AI.  So each step along the way is an\n&gt;&gt;&gt;&gt;&gt; improvement in that general ability.\n&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt; So if that is your goal, then the benchmarks you choose have to be\n&gt;&gt;&gt;&gt;&gt; designed to measure progress to that goal.  So what they need to\n&gt; do is\n&gt;&gt;&gt;&gt;&gt; show that their designed intelligence can work largely independently\n&gt;&gt;&gt;&gt;&gt; of a priori &quot;cheats&quot; that provide the meat of the solution.\n&gt; Because,\n&gt;&gt;&gt;&gt;&gt; after all, how can it be a general intelligence if it needs you to\n&gt;&gt;&gt;&gt;&gt; tell it something that it is supposed to be able to figure out?\n&gt;  This\n&gt;&gt;&gt;&gt;&gt; perspective, I believe, is aligned with Jeff&#39;s view.\n&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt; However, NE as a long-term pursuit is involved in something\n&gt; different,\n&gt;&gt;&gt;&gt;&gt; even though it can be applied to the same problems.  NE is not an\n&gt;&gt;&gt;&gt;&gt; attempt to formalize how people learn with sparse reinforcement.\n&gt;&gt;&gt;&gt;&gt; Rather, it is an attempt to formalize how evolution can build a\n&gt; brain.\n&gt;&gt;&gt;&gt;&gt;  So RL is formalizing the brain itself and NE is formalizing how\n&gt;&gt;&gt;&gt;&gt; evolution succeeds in creating a brain.  NE is therefore one step\n&gt;&gt;&gt; removed.\n&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt; This difference is ultimately a philosophical difference on the best\n&gt;&gt;&gt;&gt;&gt; approach to creating a full-blown AI.  The instrumental issue is\n&gt;&gt;&gt;&gt;&gt; whether you think it&#39;s easier to build it yourself or to design an\n&gt;&gt;&gt;&gt;&gt; algorithm that can build it.  The confusion and hence conflation of\n&gt;&gt;&gt;&gt;&gt; the two approaches arises in part because they do indeed both aim at\n&gt;&gt;&gt;&gt;&gt; the same long view goal: a general AI.  But they are coming at\n&gt; it from\n&gt;&gt;&gt;&gt;&gt; very different angles.\n&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt; And because of this stark difference, the *metric* of progress\n&gt; should\n&gt;&gt;&gt;&gt;&gt; be quite different.  We cannot measure our progress in building a\n&gt;&gt;&gt;&gt;&gt; general intelligence directly in the same way that we measure our\n&gt;&gt;&gt;&gt;&gt; progress in creating an evolutionary algorithm that itself will\n&gt;&gt;&gt;&gt;&gt; someday output one.\n&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt; This distinction is potentially subtle and confusing so let me\n&gt; try to\n&gt;&gt;&gt;&gt;&gt; make it clearer:  Human brains aren&#39;t designed to build yet more\n&gt; human\n&gt;&gt;&gt;&gt;&gt; brains.  We are good at a lot of things, and we learn generally, but\n&gt;&gt;&gt;&gt;&gt; we do not build 100-trillion part devices that are more complex than\n&gt;&gt;&gt;&gt;&gt; any known object in the universe.  I&#39;m not saying we won&#39;t ever be\n&gt;&gt;&gt;&gt;&gt; able to do it, but if you want to simulate a human brain, your first\n&gt;&gt;&gt;&gt;&gt; thought would not be that it needs to be capable of designing yet\n&gt;&gt;&gt;&gt;&gt; another brain by itself.  Your first thought is about things like\n&gt;&gt;&gt;&gt;&gt; object recognition or pursuit and evasion.\n&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt; In contrast, building brains is exactly what natural evolution did,\n&gt;&gt;&gt;&gt;&gt; and it did it quite well.  Natural evolution does not perform object\n&gt;&gt;&gt;&gt;&gt; recognition; it does not communicate with language; it does not run\n&gt;&gt;&gt;&gt;&gt; away from predators or hunt for prey.  Yet it does build brains that\n&gt;&gt;&gt;&gt;&gt; themselves do those things.  And that is the aspect of it we wish to\n&gt;&gt;&gt;&gt;&gt; harness- a very specific niche kind of skill (though radically\n&gt;&gt;&gt;&gt;&gt; impressive)- not a general skill.\n&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt; So the two pursuits are really quite different.  And therefore they\n&gt;&gt;&gt;&gt;&gt; deserve different metrics to judge their progress with respect\n&gt; to the\n&gt;&gt;&gt;&gt;&gt; long term goal.  That is, unless we conflate them to be the same\n&gt;&gt;&gt;&gt;&gt; thing, which we often do without thinking about it.\n&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt; For example, we could just say, well, both NE and RL are learning\n&gt;&gt;&gt;&gt;&gt; techniques, and after all, we can apply them to the same\n&gt; problems, so\n&gt;&gt;&gt;&gt;&gt; why make a big distinction in how we judge them?  Let&#39;s just compare\n&gt;&gt;&gt;&gt;&gt; them directly on the same benchmarks and get on with it.\n&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt; That&#39;s fine for the short-term view, i.e. let&#39;s just improve our\n&gt;&gt;&gt;&gt;&gt; ability to tackle practical problems, but for the long view, they\n&gt;&gt;&gt;&gt;&gt; cannot be judged in the same way.  If I improve at my ability to\n&gt;&gt;&gt;&gt;&gt; balance on one foot is that a sign that I will be able to build a\n&gt;&gt;&gt;&gt;&gt; brain someday?  If evolution evolves a brain that plays checkers, is\n&gt;&gt;&gt;&gt;&gt; that a sign that evolution *itself* is on the road to performing\n&gt;&gt;&gt;&gt;&gt; object recognition?  These are totally different pursuits.\n&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt; So in that context, how should they be judged with respect to long\n&gt;&gt;&gt;&gt;&gt; term goals?  Well, I think RL deserves to be judged based on its\n&gt;&gt;&gt;&gt;&gt; increasing ability to learn more generally.  And in that sense,\n&gt;&gt;&gt;&gt;&gt; exactly Jeff&#39;s criteria should apply to it: We should be\n&gt; interested in\n&gt;&gt;&gt;&gt;&gt; whether it &quot;needs&quot; a priori information to learn.  In other\n&gt; words, the\n&gt;&gt;&gt;&gt;&gt; less we need to constrain the problem for the learner, the more\n&gt;&gt;&gt;&gt;&gt; impressed we deserve to be.  That shows progress towards more\n&gt; and more\n&gt;&gt;&gt;&gt;&gt; general AI and ML.\n&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt; But if evolution is not *itself* supposed to be a general learner\n&gt;&gt;&gt;&gt;&gt; (rather, we just want it to concentrate on one very specific skill:\n&gt;&gt;&gt;&gt;&gt; brain building), then those considerations are orthogonal to its\n&gt;&gt;&gt;&gt;&gt; greatest promise.  Its promise is to evolve a brain itself, and as\n&gt;&gt;&gt;&gt;&gt; such, neuroevolutionary algorithms deserve to be judged on our\n&gt; ability\n&gt;&gt;&gt;&gt;&gt; to *constrain* the problem so that they can accomplish exactly that.\n&gt;&gt;&gt;&gt;&gt; In other words, the problem NE *algorithms* face is leaps and bounds\n&gt;&gt;&gt;&gt;&gt; beyond what RL algorithms face.  RL algorithms just need to be\n&gt; able to\n&gt;&gt;&gt;&gt;&gt; do as well as brains; NE has to be able to discover brains\n&gt; themselves.\n&gt;&gt;&gt;&gt;&gt;  Therefore, progress is NE should in part be measured with\n&gt; respect to\n&gt;&gt;&gt;&gt;&gt; progress in constraining the problem to make such a discovery more\n&gt;&gt;&gt;&gt;&gt; likely.  When an NE algorithm is improved to allow us to tell it\n&gt; more\n&gt;&gt;&gt;&gt;&gt; about the world in which its output will be situated, that is good\n&gt;&gt;&gt;&gt;&gt; news for the long view.  In short, we don&#39;t care at all how NE\n&gt;&gt;&gt;&gt;&gt; produced a brain as long as it really does.  Will anyone\n&gt; complain if a\n&gt;&gt;&gt;&gt;&gt; human brain pops out of a system that was a priori given the concept\n&gt;&gt;&gt;&gt;&gt; of symmetry?  Rather, we should be glad that such a priori\n&gt; context was\n&gt;&gt;&gt;&gt;&gt; possible to provide in the first place, because it may have\n&gt; saved us a\n&gt;&gt;&gt;&gt;&gt; year of wasted computation in figuring it out needlessly.\n&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt; This distinction is almost completely ignored when NE and RL are\n&gt;&gt;&gt;&gt;&gt; compared directly.  Therefore, the implications of any such\n&gt; comparison\n&gt;&gt;&gt;&gt;&gt; are fuzzy and lacking context with respect to the long view.  I\n&gt; am not\n&gt;&gt;&gt;&gt;&gt; sure if I should care or not if RL solves something better than\n&gt; NE, or\n&gt;&gt;&gt;&gt;&gt; vice versa, because the author doesn&#39;t explain how the result aligns\n&gt;&gt;&gt;&gt;&gt; with the long-term goals of the fields.  Long term goals seem like\n&gt;&gt;&gt;&gt;&gt; unwelcome guests these days in AI, which is why I probably won&#39;t be\n&gt;&gt;&gt;&gt;&gt; writing about any of this in a publication any time soon.\n&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt; ...\n&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt; So Derek what you are saying about NE being good at &quot;hard-wired&quot;\n&gt;&gt;&gt;&gt;&gt; solutions and RL being appropriate for ontogenetic lifetime\n&gt; learning,\n&gt;&gt;&gt;&gt;&gt; while true, is not what I think of as the primary long-view issue.\n&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt; In the long view, NE will be used to evolve structures that do learn\n&gt;&gt;&gt;&gt;&gt; over their lifetime, i.e. not hardwired at all.  The only reason\n&gt; that\n&gt;&gt;&gt;&gt;&gt; it tends to be used to evolve hardwired solutions today is\n&gt; because we\n&gt;&gt;&gt;&gt;&gt; are trying to get a foothold on how to evolve certain types of\n&gt; complex\n&gt;&gt;&gt;&gt;&gt; structures.   Once we get very good at it, focus will naturally\n&gt; shift\n&gt;&gt;&gt;&gt;&gt; to evolving dynamic brains (and of course there is already work\n&gt; along\n&gt;&gt;&gt;&gt;&gt; these lines today, much from Floreano).  I do not even think that we\n&gt;&gt;&gt;&gt;&gt; will need to include stock learning algorithms like Hebbian\n&gt; learning.\n&gt;&gt;&gt;&gt;&gt;  When we achieve our long-term goals, those *themselves* will be\n&gt; left\n&gt;&gt;&gt;&gt;&gt; up to evolution because after all there may be something even\n&gt; better.\n&gt;&gt;&gt;&gt;&gt;  \n&gt;&gt;&gt;&gt;&gt;&gt;&gt; My aim is to design an\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;  algorithm that will output a brain, not to design the brain\n&gt; itself.\n&gt;&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt;&gt; But what kind of brain are you wanting to output?\n&gt;&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt; Note that I&#39;m speaking purely about the long view for these\n&gt; different\n&gt;&gt;&gt;&gt;&gt; fields here.  Of course on a day-to-day basis I am not solely\n&gt; focused\n&gt;&gt;&gt;&gt;&gt; on what will happen 100 years from now.  On a practical day-to-day\n&gt;&gt;&gt;&gt;&gt; basis, of course I want to make NE better capable to tackle problems\n&gt;&gt;&gt;&gt;&gt; that e.g. RL tackles.  So in the short-term context, I just want to\n&gt;&gt;&gt;&gt;&gt; output something that works for the problem at hand.\n&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt; But in the long view, which we were talking about, I think the\n&gt;&gt;&gt;&gt;&gt; ultimate goal would be to output a full-fledged adaptive system with\n&gt;&gt;&gt;&gt;&gt; astronomical complexity and the power and subtlety of human\n&gt; reasoning.\n&gt;&gt;&gt;&gt;&gt;  On that path, constraint is the only hope, unless you want to wait\n&gt;&gt;&gt;&gt;&gt; three billion years and just hope in the meantime that the initial\n&gt;&gt;&gt;&gt;&gt; conditions were set up correctly.  Therefore, demonstrations of the\n&gt;&gt;&gt;&gt;&gt; power of constraint deserve to be judged as evidence of the\n&gt; promise of\n&gt;&gt;&gt;&gt;&gt; and progress towards the long term goal in NE.\n&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt; ken\n&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt; \n&gt;&gt;&gt; \n&gt;&gt;&gt; \n&gt;&gt; \n&gt;&gt; ------ End of Forwarded Message\n&gt;&gt; \n&gt; \n&gt; \n\n\n\n"}}