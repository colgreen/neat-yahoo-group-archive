{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"8wLHAWtQG1hl6QFdMXD9Kx4QcUqMCo93sh60cywjYDJlXyspkwTl0SzRRAnROjKvD0pdCZGmaAd9iJKl6jCml9M1hfRgB4VLo_fnABT8EElG","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Machine Learning and the Long View of AI","postDate":"1209419951","msgId":4001,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGZ2NWhiZit1YmJkQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGZ2M2g3byt2N2s4QGVHcm91cHMuY29tPg=="},"prevInTopic":4000,"nextInTopic":4002,"prevInTime":4000,"nextInTime":4002,"topicId":3955,"numMessagesInTopic":49,"msgSnippet":"Thanks Peter, glad you thought my post was useful.  I agree with the other part of your post as well:  An important area for research with HyperNEAT is how to","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 84168 invoked from network); 28 Apr 2008 21:59:11 -0000\r\nX-Received: from unknown (66.218.67.97)\n  by m36.grp.scd.yahoo.com with QMQP; 28 Apr 2008 21:59:11 -0000\r\nX-Received: from unknown (HELO n33a.bullet.sp1.yahoo.com) (209.131.38.214)\n  by mta18.grp.scd.yahoo.com with SMTP; 28 Apr 2008 21:59:11 -0000\r\nX-Received: from [216.252.122.217] by n33.bullet.sp1.yahoo.com with NNFMP; 28 Apr 2008 21:59:11 -0000\r\nX-Received: from [209.73.164.83] by t2.bullet.sp1.yahoo.com with NNFMP; 28 Apr 2008 21:59:11 -0000\r\nX-Received: from [66.218.67.199] by t7.bullet.scd.yahoo.com with NNFMP; 28 Apr 2008 21:59:11 -0000\r\nDate: Mon, 28 Apr 2008 21:59:11 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;fv5hbf+ubbd@...&gt;\r\nIn-Reply-To: &lt;fv3h7o+v7k8@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: Re: Machine Learning and the Long View of AI\r\nX-Yahoo-Group-Post: member; u=54567749; y=fcH7El-ymIG9uW0cSS4ZSp7V_kObrqebSC2VkaczAvWn05a_yi65\r\nX-Yahoo-Profile: kenstanley01\r\n\r\nThanks Peter, glad you thought my post was useful.  I agree with the\nother =\r\npart of your post as well:  An important area for research with\nHyperNEAT i=\r\ns how to automate the evolution of the substrate\ncomplexity.  Ideally, we w=\r\nould like to see the substrate itself become\nincreasingly complex/dense ove=\r\nr evolutionary time in response to\nevolutionary pressures.  But if that is =\r\ngoing to happen, ideally it\nshould work elegantly and seamlessly, rather th=\r\nan being ad hoc. \n\nI believe it is possible and it will be achieved sooner =\r\nor later.  I\nalso believe that natural evolution proceeded by occasional in=\r\ncreases\nin holistic brain size, so I think it&#39;s a naturally appealing idea\n=\r\nthat would bring us yet another step closer to the power of natural\nevoluti=\r\non. \n\nHowever, I do not believe that the mutations in HyperNEAT are actuall=\r\ny\nmore destructive than those in NEAT.  It is true that HyperNEAT\nmutations=\r\n have holistic effects, but those holistic effects are\norderly, that is, it=\r\n is not the equivalent of randomizing the weights\nof all the connections in=\r\nvolved.  Rather, it is a warping of the\nweight distribution along a dimensi=\r\non of regularity that was selected\nby evolution.   I think there is no a pr=\r\niori reason to believe that\nsuch changes are more or less destructive than =\r\nsingle-weight\nmutations, as long as the magnitude of the overall change is =\r\nkept\nwithin a reasonable limit, just as with any kind of mutation.\n\nken\n\n--=\r\n- In neat@yahoogroups.com, &quot;petar_chervenski&quot; &lt;petar_chervenski@...&gt;\nwrote:=\r\n\n&gt;\n&gt; Great post, Ken! I really enjoyed reading it. It is just all true. \n&gt; =\r\n\n&gt; There is really a difference between building a brain and evolving \n&gt; on=\r\ne. No matter if the brain learns in its lifetime or not. In the \n&gt; case you=\r\n build a brain yourself that doesn&#39;t learn, it is just \n&gt; conventional prog=\r\nramming at all. \n&gt; \n&gt; I think the constraints for evolution should be very =\r\nsharp, so to \n&gt; say, because in EC in general the fitness function is the m=\r\nost \n&gt; important thing as well as the representation/mapping. You can&#39;t jus=\r\nt \n&gt; say &quot;be smart!&quot; to an EC algorithm. You have to model \n&gt; its &quot;environm=\r\nent&quot; as well, and the process of evaluation usually \n&gt; takes a lot of compu=\r\ntation time for the most interesting problems. \n&gt; \n&gt; There is a kind of.. H=\r\nm I guess I can&#39;t express myself in english \n&gt; well. The more complex the t=\r\nask is, the more computation time is \n&gt; required for a proper evaluation. \n=\r\n&gt; I am maybe not saying anything new to you, Ken, but I just mention I \n&gt; u=\r\nnderstand it. \n&gt; \n&gt; HyperNEAT and CPPNs in general opened up an entire new =\r\nfield of \n&gt; research, that is, the evolution of mathematical compositions \n=\r\n&gt; describing phenotypes of any kind. What I think about it is, that \n&gt; muta=\r\ntions are mostly destructive to the networks, while in a robotics \n&gt; experi=\r\nments with direct representation, one weight change is not that \n&gt; bad, so =\r\nto say. But change one weight of a CPPN and you get a totally \n&gt; different =\r\nthing. In HyperNEAT this is not just a minor change, but a \n&gt; total change =\r\nof the network&#39;s behaviour. So there is a great deal of \n&gt; computation time=\r\n required to discover some concepts. In fact the \n&gt; fitness landscape in CP=\r\nPN-based evolution is totally different than \n&gt; other approaches to the sam=\r\ne problem. \n&gt; \n&gt; Another thing is that the geometry itself does not provide=\r\n \n&gt; information about the phenotype complexity at all. I mean that even a \n=\r\n&gt; network of 1000000000 connections can be generated by a connective \n&gt; CPP=\r\nN but the bias is usually towards minimal solutions. I know that \n&gt; complex=\r\nification is a property of the genotype space, but why to \n&gt; waste computat=\r\nion time evaluating individuals with millions of \n&gt; connections that actual=\r\nly are bad solutions? \n&gt; \n&gt; So, you may provide the geometry to the search,=\r\n but you still can&#39;t \n&gt; provide the complexity. You need a priori that the =\r\ncomplexity of the \n&gt; substrate is big enough. \n&gt; \n&gt; That 0.2 treshold is li=\r\nke a hard-coded hack to me. It may be able to \n&gt; represent any kind of conn=\r\nectivity, but I think the effort for \n&gt; discovering it is bigger than disco=\r\nvering the actual regularities at \n&gt; all. \n&gt; \n&gt; There should be a way to ma=\r\np complexity of the genotype to the \n&gt; phenotype, but not in such a constra=\r\nined way. It should be \n&gt; increasing. Did you ever see an animal as simple =\r\nas a worm but big as \n&gt; a whale? OK size doesn&#39;t matter. :) This comparison=\r\n was not a good \n&gt; one. \n&gt; \n&gt; I know there is an option that HyperNEAT can =\r\nevolve the substrate by \n&gt; itself, but how to control it? The dynamics of t=\r\nhe neural networks \n&gt; has to be taken into account. \n&gt; \n&gt; Sorry about my sc=\r\nattered around thoughts. That was just a stream of \n&gt; conciosness. \n&gt; \n&gt; Pe=\r\nter\n&gt; \n&gt; --- In neat@yahoogroups.com, &quot;Kenneth Stanley&quot; &lt;kstanley@&gt; wrote:\n=\r\n&gt; &gt;\n&gt; &gt; --- In neat@yahoogroups.com, &quot;Derek James&quot; &lt;djames@&gt; wrote:\n&gt; &gt; \n&gt; =\r\n&gt; &gt; \n&gt; &gt; &gt; &gt;  In RL, in contrast, the long view is almost the opposite: The=\r\ny\n&gt; &gt; want to\n&gt; &gt; &gt; &gt;  remove all constraints and still learn nevertheless.=\r\n\n&gt; &gt; &gt; \n&gt; &gt; &gt; I&#39;m not sure what you mean by this, Ken. Could you elaborate =\r\na \n&gt; little?\n&gt; &gt; &gt;\n&gt; &gt; \n&gt; &gt; Sure.  I think the problem is that I can&#39;t find=\r\n a way to explain my\n&gt; &gt; point concisely.  As I try to explain it, it start=\r\ns taking up too \n&gt; much\n&gt; &gt; text so I shorten it and then it loses its mean=\r\ning.  Let me give it \n&gt; a\n&gt; &gt; try again...\n&gt; &gt; \n&gt; &gt; I think the difference =\r\nbetween the goals of RL and NE is an\n&gt; &gt; interesting topic because they are=\r\n almost always conflated, as if \n&gt; they\n&gt; &gt; are trying to solve the same pr=\r\noblem.\n&gt; &gt; \n&gt; &gt; The RL community (e.g. value-function approaches) is trying=\r\n to build\n&gt; &gt; something that learns like a natural brain.  They are saying,=\r\n \n&gt; through\n&gt; &gt; analytic means we can deduce how a brain can learn from spa=\r\nrse\n&gt; &gt; reinforcement and formalize that process in an algorithm.  The \n&gt; h=\r\nope, I\n&gt; &gt; would think, is to eventually build the &quot;general intelligence&quot; t=\r\nhat\n&gt; &gt; aligns with the holy grail of AI.  So each step along the way is an=\r\n\n&gt; &gt; improvement in that general ability.\n&gt; &gt; \n&gt; &gt; So if that is your goal,=\r\n then the benchmarks you choose have to be\n&gt; &gt; designed to measure progress=\r\n to that goal.  So what they need to do \n&gt; is\n&gt; &gt; show that their designed =\r\nintelligence can work largely independently\n&gt; &gt; of a priori &quot;cheats&quot; that p=\r\nrovide the meat of the solution.  \n&gt; Because,\n&gt; &gt; after all, how can it be =\r\na general intelligence if it needs you to\n&gt; &gt; tell it something that it is =\r\nsupposed to be able to figure out?  \n&gt; This\n&gt; &gt; perspective, I believe, is =\r\naligned with Jeff&#39;s view.\n&gt; &gt; \n&gt; &gt; However, NE as a long-term pursuit is in=\r\nvolved in something \n&gt; different,\n&gt; &gt; even though it can be applied to the =\r\nsame problems.  NE is not an\n&gt; &gt; attempt to formalize how people learn with=\r\n sparse reinforcement. \n&gt; &gt; Rather, it is an attempt to formalize how evolu=\r\ntion can build a \n&gt; brain.\n&gt; &gt;  So RL is formalizing the brain itself and N=\r\nE is formalizing how\n&gt; &gt; evolution succeeds in creating a brain.  NE is the=\r\nrefore one step \n&gt; removed.\n&gt; &gt; \n&gt; &gt; This difference is ultimately a philos=\r\nophical difference on the best\n&gt; &gt; approach to creating a full-blown AI.  T=\r\nhe instrumental issue is\n&gt; &gt; whether you think it&#39;s easier to build it your=\r\nself or to design an\n&gt; &gt; algorithm that can build it.  The confusion and he=\r\nnce conflation of\n&gt; &gt; the two approaches arises in part because they do ind=\r\need both aim at\n&gt; &gt; the same long view goal: a general AI.  But they are co=\r\nming at it \n&gt; from\n&gt; &gt; very different angles.\n&gt; &gt; \n&gt; &gt; And because of this =\r\nstark difference, the *metric* of progress \n&gt; should\n&gt; &gt; be quite different=\r\n.  We cannot measure our progress in building a\n&gt; &gt; general intelligence di=\r\nrectly in the same way that we measure our\n&gt; &gt; progress in creating an evol=\r\nutionary algorithm that itself will\n&gt; &gt; someday output one.  \n&gt; &gt; \n&gt; &gt; This=\r\n distinction is potentially subtle and confusing so let me try \n&gt; to\n&gt; &gt; ma=\r\nke it clearer:  Human brains aren&#39;t designed to build yet more \n&gt; human\n&gt; &gt;=\r\n brains.  We are good at a lot of things, and we learn generally, but\n&gt; &gt; w=\r\ne do not build 100-trillion part devices that are more complex than\n&gt; &gt; any=\r\n known object in the universe.  I&#39;m not saying we won&#39;t ever be\n&gt; &gt; able to=\r\n do it, but if you want to simulate a human brain, your first\n&gt; &gt; thought w=\r\nould not be that it needs to be capable of designing yet\n&gt; &gt; another brain =\r\nby itself.  Your first thought is about things like\n&gt; &gt; object recognition =\r\nor pursuit and evasion.\n&gt; &gt; \n&gt; &gt; In contrast, building brains is exactly wh=\r\nat natural evolution did,\n&gt; &gt; and it did it quite well.  Natural evolution =\r\ndoes not perform object\n&gt; &gt; recognition; it does not communicate with langu=\r\nage; it does not run\n&gt; &gt; away from predators or hunt for prey.  Yet it does=\r\n build brains that\n&gt; &gt; themselves do those things.  And that is the aspect =\r\nof it we wish to\n&gt; &gt; harness- a very specific niche kind of skill (though r=\r\nadically\n&gt; &gt; impressive)- not a general skill.\n&gt; &gt; \n&gt; &gt; So the two pursuits=\r\n are really quite different.  And therefore they\n&gt; &gt; deserve different metr=\r\nics to judge their progress with respect to \n&gt; the\n&gt; &gt; long term goal.  Tha=\r\nt is, unless we conflate them to be the same\n&gt; &gt; thing, which we often do w=\r\nithout thinking about it.\n&gt; &gt; \n&gt; &gt; For example, we could just say, well, bo=\r\nth NE and RL are learning\n&gt; &gt; techniques, and after all, we can apply them =\r\nto the same problems, \n&gt; so\n&gt; &gt; why make a big distinction in how we judge =\r\nthem?  Let&#39;s just compare\n&gt; &gt; them directly on the same benchmarks and get =\r\non with it.\n&gt; &gt; \n&gt; &gt; That&#39;s fine for the short-term view, i.e. let&#39;s just i=\r\nmprove our\n&gt; &gt; ability to tackle practical problems, but for the long view,=\r\n they\n&gt; &gt; cannot be judged in the same way.  If I improve at my ability to\n=\r\n&gt; &gt; balance on one foot is that a sign that I will be able to build a\n&gt; &gt; b=\r\nrain someday?  If evolution evolves a brain that plays checkers, is\n&gt; &gt; tha=\r\nt a sign that evolution *itself* is on the road to performing\n&gt; &gt; object re=\r\ncognition?  These are totally different pursuits.\n&gt; &gt; \n&gt; &gt; So in that conte=\r\nxt, how should they be judged with respect to long\n&gt; &gt; term goals?  Well, I=\r\n think RL deserves to be judged based on its\n&gt; &gt; increasing ability to lear=\r\nn more generally.  And in that sense,\n&gt; &gt; exactly Jeff&#39;s criteria should ap=\r\nply to it: We should be interested \n&gt; in\n&gt; &gt; whether it &quot;needs&quot; a priori in=\r\nformation to learn.  In other words, \n&gt; the\n&gt; &gt; less we need to constrain t=\r\nhe problem for the learner, the more\n&gt; &gt; impressed we deserve to be.  That =\r\nshows progress towards more and \n&gt; more\n&gt; &gt; general AI and ML.\n&gt; &gt; \n&gt; &gt; But=\r\n if evolution is not *itself* supposed to be a general learner\n&gt; &gt; (rather,=\r\n we just want it to concentrate on one very specific skill:\n&gt; &gt; brain build=\r\ning), then those considerations are orthogonal to its\n&gt; &gt; greatest promise.=\r\n  Its promise is to evolve a brain itself, and as\n&gt; &gt; such, neuroevolutiona=\r\nry algorithms deserve to be judged on our \n&gt; ability\n&gt; &gt; to *constrain* the=\r\n problem so that they can accomplish exactly \n&gt; that. \n&gt; &gt; In other words, =\r\nthe problem NE *algorithms* face is leaps and bounds\n&gt; &gt; beyond what RL alg=\r\norithms face.  RL algorithms just need to be able \n&gt; to\n&gt; &gt; do as well as b=\r\nrains; NE has to be able to discover brains \n&gt; themselves.\n&gt; &gt;  Therefore, =\r\nprogress is NE should in part be measured with respect \n&gt; to\n&gt; &gt; progress i=\r\nn constraining the problem to make such a discovery more\n&gt; &gt; likely.  When =\r\nan NE algorithm is improved to allow us to tell it \n&gt; more\n&gt; &gt; about the wo=\r\nrld in which its output will be situated, that is good\n&gt; &gt; news for the lon=\r\ng view.  In short, we don&#39;t care at all how NE\n&gt; &gt; produced a brain as long=\r\n as it really does.  Will anyone complain \n&gt; if a\n&gt; &gt; human brain pops out =\r\nof a system that was a priori given the concept\n&gt; &gt; of symmetry?  Rather, w=\r\ne should be glad that such a priori context \n&gt; was\n&gt; &gt; possible to provide =\r\nin the first place, because it may have saved \n&gt; us a\n&gt; &gt; year of wasted co=\r\nmputation in figuring it out needlessly.\n&gt; &gt; \n&gt; &gt; This distinction is almos=\r\nt completely ignored when NE and RL are\n&gt; &gt; compared directly.  Therefore, =\r\nthe implications of any such \n&gt; comparison\n&gt; &gt; are fuzzy and lacking contex=\r\nt with respect to the long view.  I am \n&gt; not\n&gt; &gt; sure if I should care or =\r\nnot if RL solves something better than NE, \n&gt; or\n&gt; &gt; vice versa, because th=\r\ne author doesn&#39;t explain how the result aligns\n&gt; &gt; with the long-term goals=\r\n of the fields.  Long term goals seem like\n&gt; &gt; unwelcome guests these days =\r\nin AI, which is why I probably won&#39;t be\n&gt; &gt; writing about any of this in a =\r\npublication any time soon.  \n&gt; &gt; \n&gt; &gt; ...\n&gt; &gt; \n&gt; &gt; So Derek what you are sa=\r\nying about NE being good at &quot;hard-wired&quot;\n&gt; &gt; solutions and RL being appropr=\r\niate for ontogenetic lifetime \n&gt; learning,\n&gt; &gt; while true, is not what I th=\r\nink of as the primary long-view issue.\n&gt; &gt; \n&gt; &gt; In the long view, NE will b=\r\ne used to evolve structures that do learn\n&gt; &gt; over their lifetime, i.e. not=\r\n hardwired at all.  The only reason \n&gt; that\n&gt; &gt; it tends to be used to evol=\r\nve hardwired solutions today is because \n&gt; we\n&gt; &gt; are trying to get a footh=\r\nold on how to evolve certain types of \n&gt; complex\n&gt; &gt; structures.   Once we =\r\nget very good at it, focus will naturally \n&gt; shift\n&gt; &gt; to evolving dynamic =\r\nbrains (and of course there is already work \n&gt; along\n&gt; &gt; these lines today,=\r\n much from Floreano).  I do not even think that we\n&gt; &gt; will need to include=\r\n stock learning algorithms like Hebbian \n&gt; learning.\n&gt; &gt;  When we achieve o=\r\nur long-term goals, those *themselves* will be \n&gt; left\n&gt; &gt; up to evolution =\r\nbecause after all there may be something even \n&gt; better.\n&gt; &gt;  \n&gt; &gt; &gt; &gt; My a=\r\nim is to design an\n&gt; &gt; &gt; &gt;  algorithm that will output a brain, not to desi=\r\ngn the brain \n&gt; itself.\n&gt; &gt; &gt; \n&gt; &gt; &gt; But what kind of brain are you wanting=\r\n to output?\n&gt; &gt; &gt; \n&gt; &gt; \n&gt; &gt; Note that I&#39;m speaking purely about the long vi=\r\new for these \n&gt; different\n&gt; &gt; fields here.  Of course on a day-to-day basis=\r\n I am not solely \n&gt; focused\n&gt; &gt; on what will happen 100 years from now.  On=\r\n a practical day-to-day\n&gt; &gt; basis, of course I want to make NE better capab=\r\nle to tackle problems\n&gt; &gt; that e.g. RL tackles.  So in the short-term conte=\r\nxt, I just want to\n&gt; &gt; output something that works for the problem at hand.=\r\n\n&gt; &gt; \n&gt; &gt; But in the long view, which we were talking about, I think the\n&gt; =\r\n&gt; ultimate goal would be to output a full-fledged adaptive system with\n&gt; &gt; =\r\nastronomical complexity and the power and subtlety of human \n&gt; reasoning.\n&gt;=\r\n &gt;  On that path, constraint is the only hope, unless you want to wait\n&gt; &gt; =\r\nthree billion years and just hope in the meantime that the initial\n&gt; &gt; cond=\r\nitions were set up correctly.  Therefore, demonstrations of the\n&gt; &gt; power o=\r\nf constraint deserve to be judged as evidence of the promise \n&gt; of\n&gt; &gt; and =\r\nprogress towards the long term goal in NE.\n&gt; &gt; \n&gt; &gt; ken\n&gt; &gt;\n&gt;\n\n\n\n"}}