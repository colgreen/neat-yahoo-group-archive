{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":82117382,"authorName":"Jim O&#39;Flaherty, Jr.","from":"&quot;Jim O&#39;Flaherty, Jr.&quot; &lt;jim_oflaherty_jr@...&gt;","profile":"jim_oflaherty_jr","replyTo":"LIST","senderId":"Qjt-C4qaaxYctcgddYvwaf4544fsbmgI_D_pSS-qwG3o62BNahoiciyGjyGFUYVjHMQNXXB_UE7w9eUtPAGnxDbh8RuSUy_Vj_dexMDEr-IO-jRvXKKozSI","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] An ANN design question...","postDate":"1090604456","msgId":1230,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDIwMDQwNzIzMTc0MDU2LjcwNjgyLnFtYWlsQHdlYjUyODAzLm1haWwueWFob28uY29tPg==","inReplyToHeader":"PDIwMDQwNzIzMTcyNjM0Ljk2MjYxLnFtYWlsQHdlYjUyODA5Lm1haWwueWFob28uY29tPg=="},"prevInTopic":1229,"nextInTopic":1231,"prevInTime":1229,"nextInTime":1231,"topicId":1226,"numMessagesInTopic":19,"msgSnippet":"Ken, Further explanation of the assumptions from which I am working. Here is a link: http://ai-depot.com/Articles/47/EANN.html ... function*** is applied at","rawEmail":"Return-Path: &lt;jim_oflaherty_jr@...&gt;\r\nX-Sender: jim_oflaherty_jr@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 82393 invoked from network); 23 Jul 2004 17:40:57 -0000\r\nReceived: from unknown (66.218.66.167)\n  by m24.grp.scd.yahoo.com with QMQP; 23 Jul 2004 17:40:57 -0000\r\nReceived: from unknown (HELO web52803.mail.yahoo.com) (206.190.39.167)\n  by mta6.grp.scd.yahoo.com with SMTP; 23 Jul 2004 17:40:57 -0000\r\nMessage-ID: &lt;20040723174056.70682.qmail@...&gt;\r\nReceived: from [205.158.160.209] by web52803.mail.yahoo.com via HTTP; Fri, 23 Jul 2004 10:40:56 PDT\r\nDate: Fri, 23 Jul 2004 10:40:56 -0700 (PDT)\r\nTo: neat@yahoogroups.com\r\nIn-Reply-To: &lt;20040723172634.96261.qmail@...&gt;\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=us-ascii\r\nX-eGroups-Remote-IP: 206.190.39.167\r\nFrom: &quot;Jim O&#39;Flaherty, Jr.&quot; &lt;jim_oflaherty_jr@...&gt;\r\nSubject: Re: [neat] An ANN design question...\r\nX-Yahoo-Group-Post: member; u=82117382\r\nX-Yahoo-Profile: jim_oflaherty_jr\r\n\r\nKen,\n\nFurther explanation of the assumptions from which I am working.\n\nHere is a link:\nhttp://ai-depot.com/Articles/47/EANN.html\n\nWhich contains this text:\n&gt; An ANN consists of an input layer, an output layer and one or more hidden layers. ***No transfer\nfunction*** is applied at the input layer and direct inputs are transferred as outputs from this\nlayer. The input layer acts like the biological sensory system, providing information about the\nsurrounding environment. Activations are calculated from the next layer onwards (the hidden\nlayers) and fed into higher layers till it reaches the final output layer.\n\n[*** emphasis added]\n\n\nJim\n\n\n--- &quot;Jim O&#39;Flaherty, Jr.&quot; &lt;jim_oflaherty_jr@...&gt; wrote:\n&gt; Ken,\n&gt; \n&gt; In re-reading this, I realized that I did not accurately describe the relationship of the input\n&gt; data to the bounds of the activation function - I realize the activation function is only\n&gt; bounded\n&gt; on the y axis.  The spread of range of x values where the corresponding y value changes the most\n&gt; rapidly (which results in the most distinction) typically occurs when x is between -1..1, which\n&gt; happens to also be the y bounds of the hyperbolic tangent, which is the activation function I\n&gt; almost exclusively use.\n&gt; \n&gt; Anyway, it is when the y value changes are infintesimal (x is less than -10 and x is greater\n&gt; than\n&gt; +10) is where I am concerned about data meaning being lost by the input node having an\n&gt; activation\n&gt; function.\n&gt; \n&gt; Hope this clarifies.\n&gt; \n&gt; \n&gt; Jim\n&gt; \n&gt; \n&gt; --- &quot;Jim O&#39;Flaherty, Jr.&quot; &lt;jim_oflaherty_jr@...&gt; wrote:\n&gt; &gt; Ken,\n&gt; &gt; \n&gt; &gt; As I have written previously here, I am working on an optimized ANN implementation in Java\n&gt; which\n&gt; &gt; I\n&gt; &gt; have named SEMIANN (Sparsely Evaluated Matrix Interace Artificial Neural Network).\n&gt; &gt; \n&gt; &gt; In a meeting with Derek and Philip, we were reviewing my design and comparing it with the\n&gt; design\n&gt; &gt; they are currently using derived from your NEAT ANN design.  What showed up was a small\n&gt; &gt; difference\n&gt; &gt; in how I am handling the input data versus how it is being handled in their NEAT\n&gt; implementation.\n&gt; &gt; \n&gt; &gt; It is my understanding from the small number of ANN implementations I have seen (around 5)\n&gt; &gt; including that of David Fogel (author of book titled &quot;Blondie24&quot; from which I am duplicating\n&gt; &gt; experiments), the input data is placed directly into the input node.  The data is not bounded\n&gt; &gt; (other than the actual limits of a float or double).  And the input node does *not* have an\n&gt; &gt; activation function.  The unbounded data present in the input node is then used in the\n&gt; &gt; activation\n&gt; &gt; of the hidden nodes (simple 3 layer feed forward network).  Any sort of altering the input\n&gt; data\n&gt; &gt; is\n&gt; &gt; then handled by the weight attached to that input node.  The GA process will then drift the\n&gt; &gt; weights around such that inputs which are not so valuable are muted with smaller weight\n&gt; values. \n&gt; &gt; And inputs which are important are magnified with higher weight values.  And all of these\n&gt; &gt; weights\n&gt; &gt; will eventually form a function over which the input data is �normalized� based on each\n&gt; input�s\n&gt; &gt; relative importance, a sort of first approximation of the input data�s inter-relatedness.\n&gt; &gt; \n&gt; &gt; In contrast, Philip and Derek indicated an input data point entering their ANN implementation\n&gt; is\n&gt; &gt; actually being pushed through the input node&#39;s activation function.  Then the &quot;modified&quot; data\n&gt; &gt; point is now placed into the input node.  It is then used in the activation of the &quot;hidden&quot;\n&gt; &gt; nodes.\n&gt; &gt; \n&gt; &gt; In talking through the difference, we talked about how that might impact the efficacy of the\n&gt; &gt; evolving ANN.  In other words, by having the input data go through an activation function\n&gt; &gt; without\n&gt; &gt; their being a weight involved, it seems the input data is being skewed, meaningful data is\n&gt; lost \n&gt; &gt; with no opportunity for the GA to compensate prior to the data loss.  Essentially, some data\n&gt; is\n&gt; &gt; lost.  In pure mathematical terms, this implmentation provides a weight of 1.0 multiplied by\n&gt; the\n&gt; &gt; unbounded input value which is then submitted to the input node&#39;s activation function with the\n&gt; &gt; result of the function being placed into the input node.\n&gt; &gt; \n&gt; &gt; My immediate response was this: the skewing seems like it would make it more difficult for the\n&gt; &gt; ANN\n&gt; &gt; to generate associations to the inputs that range outside of the bounds of the activation\n&gt; &gt; function.  For example, in replicating Fogel&#39;s experiments, I am using the hyperbolic tangent \n&gt; &gt; bounded -1..1, and the following input values are used: a red checker has the value of 1.0, a\n&gt; &gt; black check has the value of -1.0, a red king has the value of 1.5, and a black king has the\n&gt; &gt; value\n&gt; &gt; of -1.5.\n&gt; &gt; \n&gt; &gt; Now, I know that Fogel was expecting the inputs to be related directly, as a ratio, as he\n&gt; &gt; discusses this at some length in his book.  He left it up to the GA/ANN to work out the\n&gt; optimal\n&gt; &gt; ratio relationship.  Additionally, the king&#39;s value was a GA parameter which was bounded\n&gt; between\n&gt; &gt; 1.0 and 3.0 and could randomly change by +/- 0.1 when a parent was generating a descendant.\n&gt; &gt; \n&gt; &gt; With the approach Philip and Derek have taken (and they said theirs is modeled after your\n&gt; &gt; design),\n&gt; &gt; it seems like the ratio gets perverted by the activation function on the input node.  So as\n&gt; &gt; input\n&gt; &gt; values fall further and further from the activation function bounds, relationships between\n&gt; &gt; inputs\n&gt; &gt; outside of the bounds are eventually lost due to approximation/rounding errors in the IEEE\n&gt; float\n&gt; &gt; or double.  Or so it seems to me.\n&gt; &gt; \n&gt; &gt; So my questions are this:\n&gt; &gt; A) What is the theoretical or mathematical explanation as to why the input values for NEAT are\n&gt; &gt; pushed through an input node�s activation function as opposed to being used directly?\n&gt; &gt; B) Does some form of assumption exist in which to provide input to a NEAT ANN, the input data\n&gt; &gt; point for each input node must be scaled such that the data point�s relevant range of values\n&gt; &gt; falls\n&gt; &gt; between the upper and lower bounds of the input node�s activation function?\n&gt; &gt; C) What kinds of different types of activation functions on an input node might possibly\n&gt; handle\n&gt; &gt; this differently and/or more effectively?\n&gt; &gt; \n&gt; &gt; Sidenote: In SEMIANN, I do not allow a connection to have a destination of an input node.  So\n&gt; &gt; there is no need for an activation function at an input node.  And input node is treated as\n&gt; just\n&gt; &gt; an unbounded data point.  It was my understanding that if there was to be feedback to the\n&gt; &gt; �input�,\n&gt; &gt; it would occur as new nodes and connections around the hidden/output nodes from which the\n&gt; &gt; particular input node was connected.\n&gt; &gt; \n&gt; &gt; Well, that sort of took much longer to present than I initially thought.  Hmmm�\n&gt; &gt; \n&gt; &gt; Thank you for any clarification(s) you can offer on this.\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; Jim O�Flaherty, Jr.\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt;  \n&gt; &gt; Yahoo! Groups Links\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt;  \n&gt; &gt; \n&gt; &gt; \n&gt; \n&gt; \n&gt; \n&gt; \n&gt;  \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; \n&gt;  \n&gt; \n&gt; \n\n\n"}}