{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":92207196,"authorName":"Reuben Grinberg","from":"Reuben Grinberg &lt;reuben.grinberg@...&gt;","profile":"ReubGR","replyTo":"LIST","senderId":"PyFjqwa1WE73AKEiX6DHhqitF73m6tbOgUlxOT3wyNYVgcnXmRLq1kzJjdWoiaR9p5OYmmNQXPZDYlwT_t-bnyxtCz-WWVgkd4fKF2icVAFw","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: High rez input (i.e. Video) generalization","postDate":"1102008479","msgId":1749,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDgyRjUzNDFBLTQ0ODctMTFEOS1BREQxLTAwMEE5NTcxNjdBNEB5YWxlLmVkdT4=","inReplyToHeader":"PDE5YjEwZDUxMDQxMTMwMTEwNDc2ZjZhZDY5QG1haWwuZ21haWwuY29tPg==","referencesHeader":"PGNvaTh2Yis2OGc5QGVHcm91cHMuY29tPiA8MTliMTBkNTEwNDExMzAxMTA0NzZmNmFkNjlAbWFpbC5nbWFpbC5jb20+"},"prevInTopic":1748,"nextInTopic":1758,"prevInTime":1748,"nextInTime":1750,"topicId":1743,"numMessagesInTopic":9,"msgSnippet":"Hi Derek, Yes, the pole-balancing approach is a proof-of-concept. I m only going to be using 1 pole. I want to see whether training on high rez video is ","rawEmail":"Return-Path: &lt;reuben.grinberg@...&gt;\r\nX-Sender: reuben.grinberg@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 13040 invoked from network); 2 Dec 2004 17:28:00 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m21.grp.scd.yahoo.com with QMQP; 2 Dec 2004 17:28:00 -0000\r\nReceived: from unknown (HELO pantheon-po05.its.yale.edu) (130.132.50.34)\n  by mta5.grp.scd.yahoo.com with SMTP; 2 Dec 2004 17:28:00 -0000\r\nReceived: from [10.0.1.2] (yale128036070185.student.yale.edu [128.36.70.185])\n\tby pantheon-po05.its.yale.edu (8.12.11/8.12.11) with ESMTP id iB2HRx0g032072\n\tfor &lt;neat@yahoogroups.com&gt;; Thu, 2 Dec 2004 12:28:00 -0500\r\nMime-Version: 1.0 (Apple Message framework v619)\r\nIn-Reply-To: &lt;19b10d51041130110476f6ad69@...&gt;\r\nReferences: &lt;coi8vb+68g9@...&gt; &lt;19b10d51041130110476f6ad69@...&gt;\r\nContent-Type: text/plain; charset=WINDOWS-1252; format=flowed\r\nMessage-Id: &lt;82F5341A-4487-11D9-ADD1-000A957167A4@...&gt;\r\nContent-Transfer-Encoding: quoted-printable\r\nDate: Thu, 2 Dec 2004 12:27:59 -0500\r\nTo: neat@yahoogroups.com\r\nX-Mailer: Apple Mail (2.619)\r\nX-YaleITSMailFilter: Version 1.2a (attachment(s) not renamed)\r\nX-eGroups-Remote-IP: 130.132.50.34\r\nFrom: Reuben Grinberg &lt;reuben.grinberg@...&gt;\r\nSubject: Re: High rez input (i.e. Video) generalization\r\nX-Yahoo-Group-Post: member; u=92207196\r\nX-Yahoo-Profile: ReubGR\r\n\r\nHi Derek,\n\nYes, the pole-balancing approach is a proof-of-concept. I&#39;m only=\r\n going \nto be using 1 pole. I want to see whether training on high rez vide=\r\no is \npossible.\n\nHowever, since I don&#39;t have a physical pole balancing syst=\r\nem (and don&#39;t \nhave the time to evolve a physical system), I&#39;m going to do =\r\nthe \nfollowing:\nTake many still-frames of a block (the car) and a dowel rod=\r\n or a ruler \n(the pole) in many different positions. I&#39;m going to simulate =\r\nthe \nphysics of the system, find the still-frame that corresponds to the \nc=\r\nurrent pole and cart position, and use that frame as the input. A \nsimpler =\r\nway would be to use OpenGL output from the simulator - wouldn&#39;t \nbe as &quot;nea=\r\nt&quot; though ;)\n\nEventually, I&#39;d like it to be able to generalize to different=\r\n lighting \nconditions, backgrounds, pole and cart starting positions and co=\r\nlors, \nand perhaps even to viewing angles and zoom.\n\nTo simplify the proble=\r\nm I could break up the problem: one network to \nget pole angles and cart po=\r\nsition from the video, and then the second, \ntrivial network to do the bala=\r\nncing based on these values. However, \nit&#39;s my hypothesis that the combined=\r\n approach (although it may take \nlonger to train) will yield a smaller netw=\r\nork. There might be cues in \nthe video feed that allow pole-balancing witho=\r\nut having to explicitly \ncalculate these values. Risto (Ken&#39;s advisor) ment=\r\nioned to me that the \npole-balancing problem might be too simple to find di=\r\nfferences between \nthe two approaches.\n\nA lot of vision today is done by ex=\r\nplicitly labeling the world and then \noperating on those labels. However, a=\r\nnimals don&#39;t really operate that \nway.\n\nUsing the roving eye for this appli=\r\ncation might work - I&#39;ll try it out.\n\nThanks,\nReuben\n\nP.S. Just submitted m=\r\ny first CS Grad School application yesterday! \nSeveral more to go...\n\nOn No=\r\nv 30, 2004, at 2:04 PM, Derek James wrote:\n\n&gt; On Tue, 30 Nov 2004 17:00:27 =\r\n-0000, Reuben &lt;reuben.grinberg@...&gt; \n&gt; wrote:\n&gt;  &gt;\n&gt;  &gt; I just recentl=\r\ny discovered NEAT and am about to try to use it to \n&gt; evolve a vision and\n&gt;=\r\n  &gt; control system for pole-balancing. That is, instead of feeding in \n&gt; th=\r\ne angles and cart\n&gt;  &gt; position, I&#39;m\n&gt;  &gt; going to use a &quot;video feed&quot; of th=\r\ne system. I say &quot;video feed&quot; in \n&gt; quotes because I&#39;m going &gt; to\n&gt;  &gt; use a=\r\nn inverted pendulum simulator to keep track of the physics and \n&gt; feed corr=\r\nesponding\n&gt;  &gt; still frames as input.\n&gt;  &gt;\n&gt;  &gt; Over email, Ken told me tha=\r\nt some work they&#39;ve down with low-rez \n&gt; video. It seems that\n&gt;  &gt; the resu=\r\nlts don&#39;t generalize well.\n&gt;\n&gt;  I guess my first question is: Why are you t=\r\naking this particular\n&gt;  approach?=A0 Are you wanting to apply this approac=\r\nh to another\n&gt;  real-world domain, and this is a first cut?\n&gt;\n&gt;  To what ex=\r\ntent are you wanting it to generalize?=A0\n&gt;\n&gt;  We&#39;re currently experimentin=\r\ng with fingerprint classification, so\n&gt;  there might be some overlap in the=\r\n sorts of issues we&#39;re interested\n&gt;  in.=A0 For our domain, we&#39;re applying =\r\nan active vision approach, in\n&gt;  order to drastically reduce the visual inp=\r\nut for a given time step,\n&gt;  and to more closely simulate biological vision=\r\n (there&#39;s plenty in the\n&gt;  message archives not only on this, but on plenty=\r\n of interesting\n&gt;  domains).\n&gt;\n&gt;  You say you&#39;re going to feed in still fra=\r\nmes as input.=A0 Could you be a\n&gt;  little more specific on how you intend t=\r\no do this?=A0 With pole\n&gt;  balancing (by the way, are you going to try sing=\r\nle and double?), you\n&gt;  wouldn&#39;t need to input the entire scene.=A0 Especia=\r\nlly since, if you&#39;re\n&gt;  talking about &quot;hi-resolution&quot;, a given scene could =\r\nhave thousands of\n&gt;  pixels, or more.\n&gt;\n&gt;  The only things you care about i=\r\nn the scene are the angles of the\n&gt;  poles and the position and velocity of=\r\n the cart, right?=A0 So if you set\n&gt;  up your virtual camera so that it is =\r\nviewing the profile of the cart,\n&gt;  you could just feed in pixel values fro=\r\nm small windows on either side\n&gt;  of the starting pole position, and from a=\r\n thin strip along the path of\n&gt;  the cart.=A0 But then, this is hand-pickin=\r\ng what the system sees, and\n&gt;  wouldn&#39;t really be much different from just =\r\ndirectly inputting the\n&gt;  angles and other information.\n&gt;\n&gt;  I would imagin=\r\ne that this would be the way a human would solve the\n&gt;  problem, by moving =\r\nthe cart with their hand, while positioning their\n&gt;  eyes to be in profile =\r\nwith the cart to watch the angle of the pole.\n&gt;  You might want such a syst=\r\nem to be robust to slight changes in the\n&gt;  visual input, but you probably =\r\nwouldn&#39;t need a system that could, for\n&gt;  example, balance the pole(s) by o=\r\nnly looking at a top view of the\n&gt;  scene.=A0 Is this what you&#39;re going for=\r\n?\n&gt;\n&gt;  By the way, these double pole-balancing movies using the ESP techniq=\r\nue\n&gt;  are pretty cool:\n&gt;\n&gt; http://nn.cs.utexas.edu/pages/research/espdemo/\n=\r\n&gt;\n&gt;  Derek\n&gt;\n&gt;  &gt; Ken wrote: &quot;However, we noticed an interesting problem.=\r\n=A0 It is \n&gt; learning which pixel means\n&gt;  &gt; what.=A0 In other words, it is=\r\n not learning an abstraction at all.=A0 \n&gt; It&#39;s just learning off the\n&gt;  &gt; =\r\nspecific pixels.=A0 That means you get really bad generalization \n&gt; perform=\r\nance if you test it on\n&gt;  &gt; stuff it hasn&#39;t seen before. &quot;\n&gt;  &gt;\n&gt;  &gt; Ken - =\r\nwhen you were running NEAT in the car domain, did you try to \n&gt; perturb the=\r\n way the\n&gt;  &gt; video was presented? While it would certainly increase the nu=\r\nmber \n&gt; of generations needed\n&gt;  &gt; to get something acceptable, it might al=\r\nso make a solution that is \n&gt; able to generalize. That\n&gt;  &gt; is, it might ma=\r\nke a solution where there isn&#39;t a pixel-feature \n&gt; correspondance.\n&gt;  &gt;\n&gt;  =\r\n&gt; Any thoughts?\n&gt;  &gt;\n&gt;  &gt; -Reuben\n&gt;  &gt; --------------\n&gt;  &gt; Reuben Grinberg\n=\r\n&gt;  &gt; reuben.grinberg@...\n&gt;  &gt; Trumbull College, Yale University\n&gt;  &gt; C=\r\nomputer Science, Class of &#39;05\n&gt;  &gt;\n&gt;  &gt; Yale Social Robotics Lab - http://g=\r\nundam.cs.yale.edu\n&gt;  &gt;\n&gt;  &gt;\n&gt;  &gt;\n&gt;  &gt; Yahoo! Groups Links\n&gt;  &gt;\n&gt;  &gt;\n&gt;  &gt;\n&gt; =\r\n &gt;\n&gt;  &gt;\n&gt;\n&gt;\n&gt;\n&gt; Yahoo! Groups Sponsor\n&gt;\n&gt; ADVERTISEMENT\n&gt; &lt;111704_1104_g_30=\r\n0250a.gif&gt;\n&gt; &lt;l.gif&gt;\n&gt;\n&gt; Yahoo! Groups Links\n&gt;\n&gt; \t=95 \tTo visit your group =\r\non the web, go to:\n&gt; http://groups.yahoo.com/group/neat/\n&gt; =A0\n&gt; \t=95 \t To =\r\nunsubscribe from this group, send an email to:\n&gt; neat-unsubscribe@yahoogrou=\r\nps.com\n&gt; =A0\n&gt; \t=95 \t Your use of Yahoo! Groups is subject to the Yahoo! Te=\r\nrms of \n&gt; Service.\n&gt;\n&gt;  \n\n"}}