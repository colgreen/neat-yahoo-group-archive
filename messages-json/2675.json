{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"6DYrwIqWaHECAhtZezzTGo3je3K2fmvRuYp-gGaeh2NBr0351dG7UcE24U6GFTLGMAMslsSlsumIJjET8GKFaBIPwNX3mGacqQPD6L9FZn7M","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Image Enlargement domain","postDate":"1153149540","msgId":2675,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGU5ZzlwNCtoNHRpQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDUxN2ZhNmYxMDYwNzEyMTIxMnQ1OGFjMGNjeDliN2JiY2FiNjU2MDljYzdAbWFpbC5nbWFpbC5jb20+"},"prevInTopic":2671,"nextInTopic":2677,"prevInTime":2674,"nextInTime":2676,"topicId":2660,"numMessagesInTopic":7,"msgSnippet":"Hi John, I ll try to answer a couple of your questions below. By the way, I had a student in my class who used NEAT for image enlargement (he called it image","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 27265 invoked from network); 17 Jul 2006 15:19:03 -0000\r\nReceived: from unknown (66.218.67.35)\n  by m33.grp.scd.yahoo.com with QMQP; 17 Jul 2006 15:19:03 -0000\r\nReceived: from unknown (HELO n31.bullet.scd.yahoo.com) (66.94.237.25)\n  by mta9.grp.scd.yahoo.com with SMTP; 17 Jul 2006 15:19:03 -0000\r\nReceived: from [66.218.69.1] by n31.bullet.scd.yahoo.com with NNFMP; 17 Jul 2006 15:19:02 -0000\r\nReceived: from [66.218.66.68] by t1.bullet.scd.yahoo.com with NNFMP; 17 Jul 2006 15:19:02 -0000\r\nDate: Mon, 17 Jul 2006 15:19:00 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;e9g9p4+h4ti@...&gt;\r\nIn-Reply-To: &lt;517fa6f10607121212t58ac0ccx9b7bbcab65609cc7@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: Re: Image Enlargement domain\r\nX-Yahoo-Group-Post: member; u=54567749; y=Iyy_EuB0Nww7kYvH4q4tVUfZezPR-jJnHFFLBVox1coyJ72VdnNn\r\nX-Yahoo-Profile: kenstanley01\r\n\r\nHi John, I&#39;ll try to answer a couple of your questions below.\n\nBy the way, =\r\nI had a student in my class who used NEAT for image \nenlargement (he called=\r\n it &quot;image superresolution&quot;).  He trained on a \nsmall set of pictures, whic=\r\nh it got very good at enlarging.  \nHowever, he didn&#39;t do much work on testi=\r\nng outside the training \nset.  But it seemed to work pretty well because if=\r\n you zoomed in \nreally close on its output you could see artifacts of the l=\r\nittle \ntricks it learned to fill in gaps, i.e. the assumptions it was \nmaki=\r\nng about details at the higher resolution.  And those \nassumptions looked p=\r\nretty smart actually.\n\n--- In neat@yahoogroups.com, &quot;John Arrowwood&quot; &lt;john@=\r\n...&gt; wrote:\n&gt;\n&gt; All,\n&gt; \n&gt; For those that remember, I originally got interes=\r\nted in NEAT in \norder to do\n&gt; image enlargement.  The primary thing that ha=\r\ns stood in the way of \nprogress,\n&gt; besides time, has been the realization t=\r\nhat success depends in \npart on\n&gt; having a properly selected statistically =\r\naccurate training set.  \nThe\n&gt; difficulty there is, you have to go through =\r\nevery possible \ntraining sample\n&gt; (and there are trillions of trillions of =\r\nthem), and categorize \nthem.  This\n&gt; allows the test set to be chosen in a =\r\nevenly distributed manner.  \nThe\n&gt; problem arises in that there are so many=\r\n terabytes of data \nrequired to store\n&gt; all of the possible test samples th=\r\nat we could never actually \nstore them\n&gt; all.  This problem suffers the inv=\r\nerse of the normal problem for \nwhich\n&gt; computer learning is applied:  We h=\r\nave more available training \nsamples than\n&gt; we could ever successfully trai=\r\nn with!\n&gt; \n&gt; An idea came to mind, and I wanted to solicit feedback on the =\r\n\nidea, as well\n&gt; as suggestions on how to implement it.  The idea is to use=\r\n \nartificially\n&gt; generated training samples for the initial training set.  =\r\nAllow me \nto\n&gt; explain:\n&gt; \n&gt; The primary thing that the neural network must=\r\n be able to do in \norder to be\n&gt; useful for image enlargement is to be able=\r\n to recognize and \nestimate the\n&gt; location and sharpness of edges in the so=\r\nurce data, and accurately \nplot\n&gt; those edges in the resultant enlargement.=\r\n  The idea is rather than \nusing\n&gt; real-world test samples, use initially g=\r\nenerated test samples to \nhelp guide\n&gt; the evolution of the complex network=\r\n.\n&gt; \n&gt; Start with a linear division between black and white.  The \ntraining=\r\n sample\n&gt; parameters are &#39;line-angle&#39; and &#39;point&#39; through which it passes. =\r\n \nThe test\n&gt; cases selected for evaluating the genomes could be selected by=\r\n \nchoosing a\n&gt; random degree of rotation for each of the 10-degree incremen=\r\nts, \nfor a total\n&gt; of 36 test cases.  Each is assigned a random point for i=\r\nt to pass \nthrough\n&gt; that is confined to within a particular diameter of th=\r\ne center of \nthe\n&gt; input.  As the average fitness of the population rises, =\r\nthe radius \nof the\n&gt; circle through which each line must pass can be expand=\r\ned, and the \nnumber if\n&gt; divisions of 360 increased, so the total number of=\r\n test cases \nincreases.\n&gt; When the radius of the area has expanded to inclu=\r\nde the full area \ncovered by\n&gt; network outputs, then we can begin adding ad=\r\nditional challenge to \nthe tests\n&gt; by adding curvature to the lines.\n&gt; \n&gt; I=\r\n realize that this description may not be adequate to completely \nconvey th=\r\ne\n&gt; concept to everybody, but hopefully it is enough to get the point \nacro=\r\nss.\n&gt; Here are my questions.\n&gt; \n&gt; Question 1:  What do people think of the =\r\nidea of having a \nprogressively\n&gt; harder and harder task for networks to co=\r\nmplete as they evolve?  I \nbelieve\n&gt; this has been tested before, and I am =\r\nwondering what lessons have \nbeen\n&gt; learned in the past.\n&gt; \n\nThis has been =\r\nshown to work well in many cases so it makes perfect \nsense.  \n\n&gt; Question =\r\n2: Does anybody have an opinion on using testing samples \nthat are\n&gt; quanti=\r\ntatively similar but not identical between generations?  \nNaturally,\n&gt; the =\r\nchampion of one generation may not be the champion of the next\n&gt; generation=\r\n, just because of differences in the testing samples \nused.  Are\n&gt; there an=\r\ny serious rammifications to this?  Anything else I should \nknow\n&gt; about?\n&gt; =\r\n\n\nThe more consistent/deterministic the training set across \ngenerations, t=\r\nhe more effective the evolution.  So as much as you \ncan keep things consis=\r\ntent, I would.  For example, you can train on \nthe same set of examples for=\r\n generations until you reach a \nperformance threshold, and then switch to t=\r\nhe next thing.\n\nHowever, evolution will ultimately sort out noisy evaluatio=\r\nn as long \nas it isn&#39;t too noisy.  If would just take longer.  But on the o=\r\nther \nhand it may lead to more robust solutions.\n\n&gt; Question 3: Does anybod=\r\ny have a suggestion of a way to create the \ntraining\n&gt; samples?  My first i=\r\ndea was to generate SVG and convert the SVG to \nbitmaps\n&gt; via an external t=\r\nool.  ImageMagick can do this, but it does a \nreally crappy\n&gt; job with curv=\r\nes.  Is there a suggestion of another way to convert \nSVG to\n&gt; bitmap?  Or =\r\nis there another way other than SVG?  How would you go \nabout\n&gt; creating bi=\r\ntmap images that are programmatically defined based on\n&gt; parameterized vect=\r\nors?  A library?  A tool?  What existing \ntechnologies can\n&gt; anybody recomm=\r\nend?\n&gt; \n\nI don&#39;t have anything to add to what Colin suggested for this one.=\r\n\n\nken\n\n&gt; Thanks for any inputs you guys might have!\n&gt; \n&gt; -- John\n&gt; \n&gt; -- \n&gt;=\r\n --------------------------------------------------\n&gt; John Arrowwood\n&gt; John=\r\n (at) Hanlons Razor (dot) com\n&gt;\n\n\n\n\n\n"}}