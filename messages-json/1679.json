{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":7192225,"authorName":"Ian Badcoe","from":"Ian Badcoe &lt;ian_badcoe@...&gt;","profile":"ian_badcoe","replyTo":"LIST","senderId":"QOREIhNcXnkSRMTEtgLNKcwF5ICjwxETrE5WPzIhvYYAseorkQu13bPRVsXMCLPugSp1psJBMr-t9vTaLFKALulmRMUSAxgqMIM","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] Neuron functions","postDate":"1099404560","msgId":1679,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDYuMS4yLjAuMC4yMDA0MTEwMjExNTgzMC4wMjUxNDcwOEBwb3AubWFpbC55YWhvby5jby51az4=","inReplyToHeader":"PDQxODY0NDM3LjEwNTA2MDJAZHNsLnBpcGV4LmNvbT4=","referencesHeader":"PDQxODY0NDM3LjEwNTA2MDJAZHNsLnBpcGV4LmNvbT4="},"prevInTopic":1678,"nextInTopic":1683,"prevInTime":1678,"nextInTime":1680,"topicId":1668,"numMessagesInTopic":20,"msgSnippet":"Hi, Whilst I m glad to see this idea getting explored.  I wonder whether you need to think through what the objective is. ... (which I guess was originally","rawEmail":"Return-Path: &lt;ian_badcoe@...&gt;\r\nX-Sender: ian_badcoe@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 31168 invoked from network); 2 Nov 2004 14:04:28 -0000\r\nReceived: from unknown (66.218.66.216)\n  by m2.grp.scd.yahoo.com with QMQP; 2 Nov 2004 14:04:28 -0000\r\nReceived: from unknown (HELO smtp005.mail.ukl.yahoo.com) (217.12.11.36)\n  by mta1.grp.scd.yahoo.com with SMTP; 2 Nov 2004 14:04:28 -0000\r\nReceived: from unknown (HELO ian2k.yahoo.co.uk) (ian?badcoe@212.159.73.108 with login)\n  by smtp005.mail.ukl.yahoo.com with SMTP; 2 Nov 2004 14:04:22 -0000\r\nMessage-Id: &lt;6.1.2.0.0.20041102115830.02514708@...&gt;\r\nX-Sender: ian_badcoe@...\r\nX-Mailer: QUALCOMM Windows Eudora Version 6.1.2.0\r\nDate: Tue, 02 Nov 2004 14:09:20 +0000\r\nTo: neat@yahoogroups.com\r\nIn-Reply-To: &lt;41864437.1050602@...&gt;\r\nReferences: &lt;41864437.1050602@...&gt;\r\nMime-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;us-ascii&quot;; format=flowed\r\nX-eGroups-Remote-IP: 217.12.11.36\r\nFrom: Ian Badcoe &lt;ian_badcoe@...&gt;\r\nSubject: Re: [neat] Neuron functions\r\nX-Yahoo-Group-Post: member; u=7192225\r\nX-Yahoo-Profile: ian_badcoe\r\n\r\nHi,\n         Whilst I&#39;m glad to see this idea getting explored.  I wonder \nwhether you need to think through what the objective is.\n\n         e.g. what I mean is, when we have a statement like:\n\n&gt;Multiply (can also perform division by using input&lt;1.0)\n\n         (which I guess was originally me?) then I realized that the part \nin braces is only true if we also have a means to find the reciprocal of \nthe input.  _BUT_ reciprocal is a very unnatural operator to include, e.g. \nbecause natural neurones have a fixed range of activation (call it 0 - 1) \nand (i) reciprocal would need to output infinity sometimes, and (ii) even \nif we regard it as more like an inverse, outputting 1 continually just \nbecause all your inputs are off doesn&#39;t sound plausible for a biological \nsystem (e.g. burning energy to do nothing).\n\n         So, you might want to think about whether your overall objective \nis &quot;nerve-like&quot; or &quot;maths-like&quot; and select your function set \naccordingly.  Personally I would go with nerve-like, because maths-like \nwould be very like GP and we know GP is very fragile w.r.t mutation.  Now \nonce again I have to say I&#39;m not just backing natural approaches because \nthey are natural.  I just come to this with an intuition and end up[ using \nnature to explain it....\n\n         w.r.t your plan for collector functions and activation functions, \nI like it.  I&#39;m not sure I like so much adding a whole new layer just to \nsupport &quot;leaky integrators&quot; however.\n\n         Did you consider making it an homologous pool of functions.  e.g. \nnot distinguishing collectors from activators but allowing them to be \nplugged in any order.  That way if the system needs linearity through some \nsub-net, it does not need to select a whole load of &quot;linear&quot; activators, it \njust omits the activation functions altogether.  That would cover the leaky \nintegrator as well...\n\n         Alternatively, you could try some sort of single neurone which \ncould cover the whole (or a lot) of what you intend.  But I would only do \nthat if you can come up with a single neurone design which forms a coherent \nwhole and doesn&#39;t look like several unrelated ideas bolded together...\n\nHow about:\n         one type of neurone with several &quot;input channels&quot;, allow multiple \nconnections to each channel and sum them.\n\n         Sum_Channel - like in a standard ANN\n         Act_Channel - makes the neurone more activatable by tightening the \nsigmoid\n         Scale_Channel - multiplies the input\n\n         (scale and act are different in that when act is very low, the \nsigmoid becomes linear, but when scale is very low, the sigmoid just \nbecomes very shallow)\n\n         Wiring Act and Scale just to the bias would be the same as having \nfixed, mutatable &quot;curve shape&quot; parameters on the node.  Wiring them to \nother inputs could give you potentiation/depotentiation (those are where X \ndoes not trigger Y but does increase the ability of Z to trigger Y).\n\n         Just rambling,\n\n                 Ian B\n\np.s. OTOH, mean, median, min and max collectors feel &quot;right&quot; to me, &quot;mode&quot; \nis less intuitive...\n\nAt 14:12 01/11/2004, you wrote:\n\n&gt;I want to start looking into using a range of different functions with\n&gt;neurons in the hope that this will expand NEAT&#39;s ability to find\n&gt;solutions to particular types of problems, e.g. multiplication and\n&gt;vector cross product.\n&gt;\n&gt;To begin with I want to just distinguish between &#39;input collector&#39;\n&gt;functions and activation functions. A collection function describes how\n&gt;the input signals on a neuron are combined, e.g. traditionally we add\n&gt;the inputs, but we could do other things such as multiply them. Input\n&gt;Collection Function seems like a good term to use, I wonder though if\n&gt;there is an existing term I should be using?\n&gt;\n&gt;Once the inputs have been combined we apply the activation function, and\n&gt;again there are all kinds of functions we could use here.\n&gt;\n&gt;The recent discussion on leaky integrator neurons got me thinking about\n&gt;where these would fit into this model. Although they could be described\n&gt;as an activation function I think it might be more useful to extend out\n&gt;neuron model to include a pre-activation function - and to apply the\n&gt;leaky-integrator funtion there.\n&gt;\n&gt;So the complete model of a neuron would be:\n&gt;\n&gt;       &#92;  |  /       [Inputs]\n&gt;         &#92;|/\n&gt;[Input collection function]\n&gt;          |\n&gt;          |\n&gt;[Pre-activation function]\n&gt;          |\n&gt;          |\n&gt;[Activation function]\n&gt;          |\n&gt;    output signal\n&gt;\n&gt;\n&gt;\n&gt;So that now the shape of the activation function is completely\n&gt;independent of how we combine the inputs and whether we use a\n&gt;leaky-integrator function.\n&gt;\n&gt;Some obvious questions then are:\n&gt;\n&gt;1) What set of functions should we provide NEAT  with at each stage?\n&gt;[Collection Functions] - Some of these taken from an ealier post by Ian B.\n&gt;-----------------------\n&gt;Add\n&gt;Multiply (can also perform division by using input&lt;1.0)\n&gt;Logical AND\n&gt;Logical OR\n&gt;Variance\n&gt;Min\n&gt;Max\n&gt;Sum squares\n&gt;\n&gt;[Pre-activation functions]\n&gt;--------------------------\n&gt;Linear (do nothing).\n&gt;Leaky-integrator (probably with fixed parameters)\n&gt;\n&gt;[Activation functions]\n&gt;----------------------\n&gt;Linear\n&gt;Inverse Linear\n&gt;Sigmoid\n&gt;Step\n&gt;\n&gt;or instead of Inverse Linear, perhaps the inverse of each fn should be\n&gt;made available instead?\n&gt;\n&gt;\n&gt;\n&gt;I&#39;d be interested in other people thoughts on this. Does the neuron\n&gt;model make sense? Are there other functions that should be in the above\n&gt;lists, or should I drop some of the functions?\n&gt;\n&gt;Keep in mind that you could (I think) ultimately drop all of the\n&gt;functions (except the sigmoid) and still be able to describe all of the\n&gt;functions using combinations of a standard neuron. The point is that I\n&gt;want to try and take some of the pressure of finding solutions away from\n&gt;NEAT&#39;s topology searching, because certain functions are hard to\n&gt;describe (and therefore discover) using standard neurons - a simple case\n&gt;being multiplication.\n&gt;\n&gt;Colin.\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;Yahoo! Groups Links\n&gt;\n&gt;\n&gt;\n&gt;\n\n\n\nLiving@Home - Open Source Evolving Organisms - \nhttp://livingathome.sourceforge.net/\n\n\n\n\n"}}