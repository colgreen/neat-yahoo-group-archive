{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":37465196,"authorName":"Ken Lloyd","from":"&quot;Ken Lloyd&quot; &lt;kalloyd@...&gt;","profile":"kalloyd2","replyTo":"LIST","senderId":"J0D5y196wcw1hGfBtV-DWZArzg0QzJHgKgE7faDaXOoBcsgU4VZK-eNPGChUNG7Z9cKlroYlf_QE5-O-T9kA7S49RNv-IXMD","spamInfo":{"isSpam":false,"reason":"12"},"subject":"RE: [neat] Yet another question on combining NEAT with backprop (or another non-genetic training algorithm)","postDate":"1205777359","msgId":3885,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDAxY2YwMWM4ODg1YSQwOGIxZTZkMCQ2NTAxYThjMEB3YXR0cDQ+","inReplyToHeader":"PGFiZGQwOTRlMDgwMzE3MDk1Nnk1ZTcwMjQzM2xhYmQxZDA0YzhlZGE3ZTdiQG1haWwuZ21haWwuY29tPg==","referencesHeader":"PGFiZGQwOTRlMDgwMzE3MDk1Nnk1ZTcwMjQzM2xhYmQxZDA0YzhlZGE3ZTdiQG1haWwuZ21haWwuY29tPg=="},"prevInTopic":3882,"nextInTopic":3913,"prevInTime":3884,"nextInTime":3886,"topicId":3882,"numMessagesInTopic":3,"msgSnippet":"Eric, Google FPGA Genetic Algorithm Backpropagation There is much current work being done with FPGA s in MLFN and GA ANN s. Hope this helps. Ken Lloyd _____ ","rawEmail":"Return-Path: &lt;kalloyd@...&gt;\r\nX-Sender: kalloyd@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 56369 invoked from network); 17 Mar 2008 18:09:26 -0000\r\nX-Received: from unknown (66.218.67.94)\n  by m49.grp.scd.yahoo.com with QMQP; 17 Mar 2008 18:09:26 -0000\r\nX-Received: from unknown (HELO wattsys.com) (209.43.123.15)\n  by mta15.grp.scd.yahoo.com with SMTP; 17 Mar 2008 18:09:26 -0000\r\nX-Received: from wattp4 (c-76-113-33-177.hsd1.nm.comcast.net [76.113.33.177])\n\tby wattsys.com (8.11.6/8.11.6) with ESMTP id m2HI9Ps15369\n\tfor &lt;neat@yahoogroups.com&gt;; Mon, 17 Mar 2008 14:09:25 -0400\r\nTo: &lt;neat@yahoogroups.com&gt;\r\nReferences: &lt;abdd094e0803170956y5e702433labd1d04c8eda7e7b@...&gt;\r\nDate: Mon, 17 Mar 2008 12:09:19 -0600\r\nMessage-ID: &lt;01cf01c8885a$08b1e6d0$6501a8c0@wattp4&gt;\r\nMIME-Version: 1.0\r\nContent-Type: multipart/alternative;\n\tboundary=&quot;----=_NextPart_000_01D0_01C88827.BE1776D0&quot;\r\nX-Mailer: Microsoft Office Outlook 11\r\nX-MimeOLE: Produced By Microsoft MimeOLE V6.00.2900.3198\r\nThread-Index: AciIUFLrNXsqXOM5QUqfNGFz3icFIwAAJhzQ\r\nIn-Reply-To: &lt;abdd094e0803170956y5e702433labd1d04c8eda7e7b@...&gt;\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: &quot;Ken Lloyd&quot; &lt;kalloyd@...&gt;\r\nSubject: RE: [neat] Yet another question on combining NEAT with backprop (or another non-genetic training algorithm)\r\nX-Yahoo-Group-Post: member; u=37465196; y=hFF3hJBfoVetQao5xhwTdVtJGfhLzNgy3h-xI6pnhE7P7VE\r\nX-Yahoo-Profile: kalloyd2\r\n\r\n\r\n------=_NextPart_000_01D0_01C88827.BE1776D0\r\nContent-Type: text/plain;\n\tcharset=&quot;us-ascii&quot;\r\nContent-Transfer-Encoding: 7bit\r\n\r\nEric,\n \nGoogle FPGA Genetic Algorithm Backpropagation\n \nThere is much current work being done with FPGA&#39;s in MLFN and GA ANN&#39;s.\nHope this helps.\n \nKen Lloyd\n\n\n  _____  \n\nFrom: neat@yahoogroups.com [mailto:neat@yahoogroups.com] On Behalf Of Eric\nMohlenhoff\nSent: Monday, March 17, 2008 10:57 AM\nTo: neat@yahoogroups.com\nSubject: [neat] Yet another question on combining NEAT with backprop (or\nanother non-genetic training algorithm)\n\n\n\nHey all,\n\nLong-time reader, first time writer here.\n\nThe recent discussion on using NEAT with backprop inspired me to ask\nif anyone has done or knows of any research on a topic similar to the\nfollowing:\n\nThe idea is to use NEAT for developing the topology of a given\nnetwork, and at a later time complete the training with another\nnon-genetic training algorithm such as gradient descent, etc. The\n&#39;NEAT&#39; phase of the network &#39;training&#39; should try to develop a network\ntopology that is _conducive_ to being trained (ideally, quickly) using\na non-genetic algorithm on a limited subset of the domain of a given\nclass of problem. In other words, has anyone ever tried integrating a\nbackprop algorithm into the fitness function of a given NEAT\nimplementation, with the intention of not merging the weight changes\nmade by said backprop algorithm back into the population, but rather\nonly factoring information relating to how well/quickly that network\ncould be trained using backprop (against data set(s) from the problem\nclass, on the limited domain desired) into the fitness score.\n\nIf you need a reason for why this may be useful, consider the\nfollowing scenario. An embedded system with limited memory (making GA\nsomewhat prohibitive), in which the network (including topology) is\nimplemented in hardware (making GA practically _impossible_), but can\nuse fast dedicated multipliers for algorithms such as gradient\ndescent. One desires to fine-tune this network to data sets\nencountered by the system in the field, hopefully by using a backprop\nalgorithm that can be implemented (mostly) in hardware. It would be\ndesirable to run a GA in the lab prior to deployment to determine the\nbest topology to implement in hardware (if possible). The constraints\nof the data/problem domain are known prior to deployment in the field.\n\nIf anyone knows of any research done on this or similar, please let me\nknow. I&#39;ve been doing some research and have found some information\nabout applying gradient descent to NEAT-generated topologies, but\nnever anything about developing a topology that was actually\n_conducive_ to being trained efficiently with a backprop algorithm.\n\nEric\n\n\n \n\n\r\n------=_NextPart_000_01D0_01C88827.BE1776D0\r\nContent-Type: text/html;\n\tcharset=&quot;us-ascii&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\n&lt;!DOCTYPE HTML PUBLIC &quot;-//W3C//DTD HTML 4.01 Transitional//EN&quot; &quot;http://www.=\r\nw3c.org/TR/1999/REC-html401-19991224/loose.dtd&quot;&gt;\n&lt;HTML&gt;&lt;HEAD&gt;\n&lt;META http-eq=\r\nuiv=3DContent-Type content=3D&quot;text/html; charset=3Dus-ascii&quot;&gt;\n&lt;META content=\r\n=3D&quot;MSHTML 6.00.6000.16608&quot; name=3DGENERATOR&gt;&lt;/HEAD&gt;\n&lt;BODY style=3D&quot;BACKGRO=\r\nUND-COLOR: #ffffff&quot;&gt;\n&lt;DIV dir=3Dltr align=3Dleft&gt;&lt;FONT face=3DArial size=3D=\r\n2&gt;&lt;SPAN \nclass=3D781100417-17032008&gt;Eric,&lt;/SPAN&gt;&lt;/FONT&gt;&lt;/DIV&gt;\n&lt;DIV dir=3Dlt=\r\nr align=3Dleft&gt;&lt;FONT face=3DArial size=3D2&gt;&lt;SPAN \nclass=3D781100417-1703200=\r\n8&gt;&lt;/SPAN&gt;&lt;/FONT&gt;&nbsp;&lt;/DIV&gt;\n&lt;DIV dir=3Dltr align=3Dleft&gt;&lt;FONT face=3DArial=\r\n size=3D2&gt;&lt;SPAN \nclass=3D781100417-17032008&gt;Google FPGA Genetic Algorithm \n=\r\nBackpropagation&lt;/SPAN&gt;&lt;/FONT&gt;&lt;/DIV&gt;\n&lt;DIV dir=3Dltr align=3Dleft&gt;&lt;FONT face=\r\n=3DArial size=3D2&gt;&lt;SPAN \nclass=3D781100417-17032008&gt;&lt;/SPAN&gt;&lt;/FONT&gt;&nbsp;&lt;/D=\r\nIV&gt;\n&lt;DIV dir=3Dltr align=3Dleft&gt;&lt;FONT face=3DArial size=3D2&gt;&lt;SPAN \nclass=3D=\r\n781100417-17032008&gt;There is much current work being done with \nFPGA&#39;s&nbsp;=\r\nin MLFN and GA&nbsp;ANN&#39;s.&nbsp; Hope this \nhelps.&lt;/SPAN&gt;&lt;/FONT&gt;&lt;/DIV&gt;\n&lt;DIV=\r\n dir=3Dltr align=3Dleft&gt;&lt;FONT face=3DArial size=3D2&gt;&lt;SPAN \nclass=3D78110041=\r\n7-17032008&gt;&lt;/SPAN&gt;&lt;/FONT&gt;&nbsp;&lt;/DIV&gt;\n&lt;DIV dir=3Dltr align=3Dleft&gt;&lt;FONT fac=\r\ne=3DArial size=3D2&gt;&lt;SPAN \nclass=3D781100417-17032008&gt;Ken Lloyd&lt;/SPAN&gt;&lt;/FONT=\r\n&gt;&lt;/DIV&gt;&lt;BR&gt;\n&lt;BLOCKQUOTE \nstyle=3D&quot;PADDING-LEFT: 5px; MARGIN-LEFT: 5px; BORD=\r\nER-LEFT: #0000ff 2px solid; MARGIN-RIGHT: 0px&quot;&gt;\n  &lt;DIV class=3DOutlookMessa=\r\ngeHeader lang=3Den-us dir=3Dltr align=3Dleft&gt;\n  &lt;HR tabIndex=3D-1&gt;\n  &lt;FONT =\r\nface=3DTahoma size=3D2&gt;&lt;B&gt;From:&lt;/B&gt; neat@yahoogroups.com \n  [mailto:neat@ya=\r\nhoogroups.com] &lt;B&gt;On Behalf Of &lt;/B&gt;Eric \n  Mohlenhoff&lt;BR&gt;&lt;B&gt;Sent:&lt;/B&gt; Monda=\r\ny, March 17, 2008 10:57 AM&lt;BR&gt;&lt;B&gt;To:&lt;/B&gt; \n  neat@yahoogroups.com&lt;BR&gt;&lt;B&gt;Subj=\r\nect:&lt;/B&gt; [neat] Yet another question on \n  combining NEAT with backprop (or=\r\n another non-genetic training \n  algorithm)&lt;BR&gt;&lt;/FONT&gt;&lt;BR&gt;&lt;/DIV&gt;\n  &lt;DIV&gt;&lt;/D=\r\nIV&gt;\n  &lt;DIV id=3Dygrp-text&gt;\n  &lt;P&gt;Hey all,&lt;BR&gt;&lt;BR&gt;Long-time reader, first tim=\r\ne writer here.&lt;BR&gt;&lt;BR&gt;The recent \n  discussion on using NEAT with backprop =\r\ninspired me to ask&lt;BR&gt;if anyone has \n  done or knows of any research on a t=\r\nopic similar to \n  the&lt;BR&gt;following:&lt;BR&gt;&lt;BR&gt;The idea is to use NEAT for dev=\r\neloping the topology \n  of a given&lt;BR&gt;network, and at a later time complete=\r\n the training with \n  another&lt;BR&gt;non-genetic training algorithm such as gra=\r\ndient descent, etc. \n  The&lt;BR&gt;&#39;NEAT&#39; phase of the network &#39;training&#39; should=\r\n try to develop a \n  network&lt;BR&gt;topology that is _conducive_ to being train=\r\ned (ideally, quickly) \n  using&lt;BR&gt;a non-genetic algorithm on a limited subs=\r\net of the domain of a \n  given&lt;BR&gt;class of problem. In other words, has any=\r\none ever tried integrating \n  a&lt;BR&gt;backprop algorithm into the fitness func=\r\ntion of a given \n  NEAT&lt;BR&gt;implementation, with the intention of not mergin=\r\ng the weight \n  changes&lt;BR&gt;made by said backprop algorithm back into the po=\r\npulation, but \n  rather&lt;BR&gt;only factoring information relating to how well/=\r\nquickly that \n  network&lt;BR&gt;could be trained using backprop (against data se=\r\nt(s) from the \n  problem&lt;BR&gt;class, on the limited domain desired) into the =\r\nfitness \n  score.&lt;BR&gt;&lt;BR&gt;If you need a reason for why this may be useful, c=\r\nonsider \n  the&lt;BR&gt;following scenario. An embedded system with limited memor=\r\ny (making \n  GA&lt;BR&gt;somewhat prohibitive)&lt;WBR&gt;, in which the network (includ=\r\ning topology) \n  is&lt;BR&gt;implemented in hardware (making GA practically _impo=\r\nssible_&lt;WBR&gt;), but \n  can&lt;BR&gt;use fast dedicated multipliers for algorithms =\r\nsuch as \n  gradient&lt;BR&gt;descent. One desires to fine-tune this network to da=\r\nta \n  sets&lt;BR&gt;encountered by the system in the field, hopefully by using a =\r\n\n  backprop&lt;BR&gt;algorithm that can be implemented (mostly) in hardware. It w=\r\nould \n  be&lt;BR&gt;desirable to run a GA in the lab prior to deployment to deter=\r\nmine \n  the&lt;BR&gt;best topology to implement in hardware (if possible). The \n =\r\n constraints&lt;BR&gt;of the data/problem domain are known prior to deployment in=\r\n the \n  field.&lt;BR&gt;&lt;BR&gt;If anyone knows of any research done on this or simil=\r\nar, please \n  let me&lt;BR&gt;know. I&#39;ve been doing some research and have found =\r\nsome \n  information&lt;BR&gt;about applying gradient descent to NEAT-generated to=\r\npologies, \n  but&lt;BR&gt;never anything about developing a topology that was \n  =\r\nactually&lt;BR&gt;_conducive_ to being trained efficiently with a backprop \n  alg=\r\norithm.&lt;BR&gt;&lt;BR&gt;Eric&lt;BR&gt;&lt;/P&gt;&lt;/DIV&gt;&lt;!--End group email --&gt;&lt;/BLOCKQUOTE&gt;&lt;/BODY=\r\n&gt;&lt;/HTML&gt;\n\r\n------=_NextPart_000_01D0_01C88827.BE1776D0--\r\n\n"}}