{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":281645563,"authorName":"afcarl2","from":"&quot;afcarl2&quot; &lt;a.carl@...&gt;","profile":"afcarl2","replyTo":"LIST","senderId":"bHfh34JCA1sk-AewkiKDFpDTy5EHyl5zPuheemgj6f6CZqu-iGLRPuS42T1L9oHo1PCBYNH8LzggRacXZ80n9vs","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Backpropagation and NEAT","postDate":"1205179694","msgId":3860,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGZyNDRmZStyc2NrQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGZyM3FpaStvdDhiQGVHcm91cHMuY29tPg=="},"prevInTopic":3859,"nextInTopic":3861,"prevInTime":3859,"nextInTime":3861,"topicId":3846,"numMessagesInTopic":41,"msgSnippet":"Peter, Use of backprop w/ NEAT is an example of hierarchical optimization in which NEAT is used as a global optimizer and backprop is a local non- gradient","rawEmail":"Return-Path: &lt;a.carl@...&gt;\r\nX-Sender: a.carl@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 4554 invoked from network); 10 Mar 2008 20:08:18 -0000\r\nX-Received: from unknown (66.218.67.97)\n  by m47.grp.scd.yahoo.com with QMQP; 10 Mar 2008 20:08:18 -0000\r\nX-Received: from unknown (HELO n32c.bullet.scd.yahoo.com) (66.94.237.10)\n  by mta18.grp.scd.yahoo.com with SMTP; 10 Mar 2008 20:08:18 -0000\r\nX-Received: from [66.218.69.4] by n32.bullet.scd.yahoo.com with NNFMP; 10 Mar 2008 20:08:15 -0000\r\nX-Received: from [66.218.67.199] by t4.bullet.scd.yahoo.com with NNFMP; 10 Mar 2008 20:08:15 -0000\r\nDate: Mon, 10 Mar 2008 20:08:14 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;fr44fe+rsck@...&gt;\r\nIn-Reply-To: &lt;fr3qii+ot8b@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;afcarl2&quot; &lt;a.carl@...&gt;\r\nSubject: Re: Backpropagation and NEAT\r\nX-Yahoo-Group-Post: member; u=281645563; y=sO3d19ePIHlCgWAOIcmqcDTxsRpNHyfc_S-Qe4LfKuF82w\r\nX-Yahoo-Profile: afcarl2\r\n\r\nPeter,\n\nUse of backprop w/ NEAT is an example of hierarchical optimization =\r\nin \nwhich NEAT is used as a global optimizer and backprop is a local non-\ng=\r\nradient based optimizer on weights. \n\nA good &quot;source&quot; for source-code is th=\r\ne Dakota Toolbox ( \nhttp://www.cs.sandia.gov/DAKOTA/software.html ), develo=\r\nped by the \nSandia National Laboratories.\n\nThe encoding of the updated geno=\r\nme derived from local optimization ( \nwhether from backprop or other means =\r\nvia Dakota functionality) \npreserves the fruit of the local optimization, a=\r\nnd in essence re-maps \nthe problem addressed by NEAT to that of finding the=\r\n best starting \npoint for a local optimizer resulting in the best global fi=\r\ntness.\n\nThe awkward point from the NEAT perspective is the concept of speci=\r\nes \nmembership subsequent to genome transfermation as a consequence of \nenc=\r\noding the result of the local optimizer into the individual \norganism. This=\r\n may be addressed by re-defining species membership \nafter each generation.=\r\n Another possiblity is encoding the updated \ngenome as a dormant genome ass=\r\nociated with the organism.\n\n\n--- In neat@yahoogroups.com, &quot;Kenneth Stanley&quot;=\r\n &lt;kstanley@...&gt; wrote:\n&gt;\n&gt; Peter, I believe that backprop can potentially i=\r\nmprove the \n&gt; accuracy.  It has been shown to work effectively with neurevo=\r\nlution \n&gt; in classification tasks in the past.  So in principle it could \n&gt;=\r\n help.  Of course, there is always the chance that it will not \n&gt; enhance p=\r\nerformance as well.\n&gt; \n&gt; One issue I would also consider is that some peopl=\r\ne disagree on \n&gt; whether the changes to weights from backprop should be enc=\r\noded back \n&gt; into the genome or not.  If it is actually encoded back into t=\r\nhe \n&gt; genome, that is &quot;Lamarckian&quot; evolution because in effect what the \n&gt; =\r\norganism learned over its lifetime is encoded into its own \n&gt; offspring.  T=\r\nhat is obviously not how real evolution works.\n&gt; \n&gt; However, of course, it =\r\ndoesn&#39;t have to work like real evolution and \n&gt; some people believe that La=\r\nmarckian evolution will work better.  \n&gt; However, there are arguments that =\r\nin fact it works worse because it \n&gt; hurts the diversity of the population.=\r\n  Because of the Baldwin \n&gt; effect, some would argue that evolution+backpro=\r\np is most powerful \nif \n&gt; the learned weights are not encoded back into the=\r\n genome. This \ntopic \n&gt; is fairly extensive.  A lot is written about the &quot;B=\r\naldwin effect.&quot;\n&gt; \n&gt; ken\n&gt; \n&gt; --- In neat@yahoogroups.com, &quot;petar_chervensk=\r\ni&quot; \n&gt; &lt;petar_chervenski@&gt; wrote:\n&gt; &gt;\n&gt; &gt; Hi Ken, \n&gt; &gt; \n&gt; &gt; I am evolving ti=\r\nme series predictors, in fact even a simplified \n&gt; &gt; version of time series=\r\n predictors, where the network has to \nanswer \n&gt; is \n&gt; &gt; the future value g=\r\noing up or down. The actual output neuron is a \n&gt; &gt; simple step function, b=\r\nut back-prop can be applied if it is \nturned \n&gt; &gt; out to a sigmoid with a v=\r\nery steep slope. \n&gt; &gt; The networks are allowed to have any topology and the=\r\ny are \n&gt; evaluated \n&gt; &gt; on the run, meaning that on each timestep, an error=\r\n is being \n&gt; &gt; calculated (being 0 or 1, depending on the prediction made).=\r\n \n&gt; &gt; First of all, do you think that applying back-prop to these \n&gt; networ=\r\nks \n&gt; &gt; may bring any accuracy improvement? I know that it is going to \neat=\r\n \n&gt; &gt; the CPU resourses, so it can be applied at regular intervals, say \n&gt; =\r\n&gt; each 50 generations, to push the networks&#39;s weights in the right \n&gt; &gt; dir=\r\nection, a kind of a hint to the search. I am still thinking \n&gt; of &quot;is \n&gt; &gt; =\r\nit worth it?&quot;.. \n&gt; &gt; \n&gt; &gt; Peter\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; --- In neat@yahoogroups.=\r\ncom, &quot;Kenneth Stanley&quot; &lt;kstanley@&gt; wrote:\n&gt; &gt; &gt;\n&gt; &gt; &gt; A number of people ha=\r\nve programmed backprop into NEAT.  Chris \n&gt; &gt; &gt; Christenson did a Masters t=\r\nhesis on combining NEAT and \nbackprop; \n&gt; a \n&gt; &gt; &gt; paper based on this work=\r\n is actually in the files section of \n&gt; this \n&gt; &gt; group:\n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; =\r\n\n&gt; \nhttp://f1.grp.yahoofs.com/v1/UP7SR8rDovimxlLlvcmGOziLUBIVncb2Tfr7sruo\n&gt;=\r\n B\n&gt; &gt; 8b\n&gt; &gt; &gt; taAfELU62JLyQ9XCxXF_Akhcmi-\n&gt; &gt; &gt; \n&gt; &gt; \n&gt; \nTH4gpVHIikwnzB59=\r\nArOMQfPOAzyw25/Evolving_Trainable_Neural_Networks_6_p\n&gt; a\n&gt; &gt; ge\n&gt; &gt; &gt; s.do=\r\nc\n&gt; &gt; &gt; \n&gt; &gt; &gt; Shimon Whiteson implemented it as part of his NEAT+Q \n&gt; rein=\r\nforcement \n&gt; &gt; &gt; learning method:\n&gt; &gt; &gt; \n&gt; &gt; &gt; http://staff.science.uva.nl/=\r\n~whiteson/pubs/whitesonaaai06.pdf\n&gt; &gt; &gt; \n&gt; &gt; &gt; There has been a lot written=\r\n on backprop in NEAT in the \narchives \n&gt; of \n&gt; &gt; &gt; this group: just search =\r\nfor &quot;backprop&quot; from the yahoo page for \n&gt; this \n&gt; &gt; &gt; group and many messag=\r\nes will pop up.\n&gt; &gt; &gt; \n&gt; &gt; &gt; In general, if you do not allow recurrence the=\r\nn I believe there \n&gt; is \n&gt; &gt; no \n&gt; &gt; &gt; special change needed in the traditi=\r\nonal backprop algorithm.  \n&gt; With \n&gt; &gt; &gt; recurrence you would need somethin=\r\ng like recurrent backprop \nlike \n&gt; &gt; Derek \n&gt; &gt; &gt; suggested.  But let&#39;s jus=\r\nt say you are evolving nonrecurrent \n&gt; &gt; networks- \n&gt; &gt; &gt; is there a partic=\r\nular problem you have in mind that comes up \n&gt; with \n&gt; &gt; &gt; applying backpro=\r\np to such networks?\n&gt; &gt; &gt; \n&gt; &gt; &gt; ken\n&gt; &gt; &gt; \n&gt; &gt; &gt; --- In neat@yahoogroups.c=\r\nom, &quot;petar_chervenski&quot; \n&gt; &lt;petar_chervenski@&gt; \n&gt; &gt; &gt; wrote:\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;=\r\n Hello there. \n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; I am looking for any back-propagation algor=\r\nithm that can work \n&gt; on \n&gt; &gt; &gt; &gt; networks with arbitrary topology such as =\r\nthese that NEAT \n&gt; evolves. \n&gt; &gt; All \n&gt; &gt; &gt; &gt; libraries I found so far eith=\r\ner assume layered networks or \n&gt; only \n&gt; &gt; feed-\n&gt; &gt; &gt; &gt; forward ones.. I a=\r\nm confused. Is there any source code that \n&gt; might \n&gt; &gt; &gt; help \n&gt; &gt; &gt; &gt; me?=\r\n Any back-prop implementation that can work on NEAT \n&gt; networks \n&gt; &gt; such \n=\r\n&gt; &gt; &gt; &gt; that it can easily be integrated. Or maybe some papers on the \n&gt; &gt; =\r\ntopic? \n&gt; &gt; &gt; &gt; I appreciate any help from the community. \n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt;=\r\n Peter\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt;\n&gt;\n\n\n\n"}}