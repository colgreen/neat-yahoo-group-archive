{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":127853030,"authorName":"Colin Green","from":"Colin Green &lt;cgreen@...&gt;","profile":"alienseedpod","replyTo":"LIST","senderId":"xp60qqi5HmZtsHMuKn4-unzzPm8izq7KXXr2dkF598nb5tmTGNf6cebi5IbMf8JREay4s30iPNWzKmaG_tZ1Q40G83L1c2CgvA","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [neat] 378 days later...","postDate":"1142804496","msgId":2573,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ0MUREMDEwLjMwMjA3QGRzbC5waXBleC5jb20+","inReplyToHeader":"PGR2YzNkcit2Ymh0QGVHcm91cHMuY29tPg==","referencesHeader":"PGR2YzNkcit2Ymh0QGVHcm91cHMuY29tPg=="},"prevInTopic":2571,"nextInTopic":2574,"prevInTime":2572,"nextInTime":2574,"topicId":2571,"numMessagesInTopic":6,"msgSnippet":"Hi Mike, welcome back, ... Take a deep breath..... and relax. Sounds like another bad case of RealLife(TM) ;) ... Yeh not very up-to-date I m afraid. More on","rawEmail":"Return-Path: &lt;cgreen@...&gt;\r\nX-Sender: cgreen@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 19014 invoked from network); 19 Mar 2006 21:41:38 -0000\r\nReceived: from unknown (66.218.67.34)\n  by m35.grp.scd.yahoo.com with QMQP; 19 Mar 2006 21:41:38 -0000\r\nReceived: from unknown (HELO galaxy.systems.pipex.net) (62.241.162.31)\n  by mta8.grp.scd.yahoo.com with SMTP; 19 Mar 2006 21:41:38 -0000\r\nReceived: from [10.0.0.11] (81-86-161-87.dsl.pipex.com [81.86.161.87])\n\tby galaxy.systems.pipex.net (Postfix) with ESMTP id 0459FE00013F\n\tfor &lt;neat@yahoogroups.com&gt;; Sun, 19 Mar 2006 21:41:36 +0000 (GMT)\r\nMessage-ID: &lt;441DD010.30207@...&gt;\r\nDate: Sun, 19 Mar 2006 21:41:36 +0000\r\nUser-Agent: Mozilla Thunderbird 1.0.7 (Windows/20050923)\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: neat@yahoogroups.com\r\nReferences: &lt;dvc3dr+vbht@...&gt;\r\nIn-Reply-To: &lt;dvc3dr+vbht@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: Colin Green &lt;cgreen@...&gt;\r\nSubject: Re: [neat] 378 days later...\r\nX-Yahoo-Group-Post: member; u=127853030; y=o1gXzx2vkkqfHnnW4J1DmtPOK80sLo6lhzZsrzTVGtygZupFRz2q\r\nX-Yahoo-Profile: alienseedpod\r\n\r\nHi Mike, welcome back,\n\nMike Woodhouse wrote:\n\n&gt;Good grief, it&#39;s been just over a year since I last posted here. It&#39;s\n&gt;probably not unfair to say that life got in the way (two job changes,\n&gt;builders ripping house apart, kids being kids, the usual stuff).\n&gt;  \n&gt;\n\nTake a deep breath..... and relax. Sounds like another bad case of \nRealLife(TM) ;) \n\n\n&gt;So far, I&#39;ve downloaded Colin&#39;s latest SharpNEAT source, upgraded it\n&gt;  \n&gt;\n\nYeh not very up-to-date I&#39;m afraid. More on that later.\n\n&gt;SharpNEAT: I hadn&#39;t realised until I started coding against it what a\n&gt;really nice job this is. \n&gt;  \n&gt;\n\nWell it&#39;s very nice of you to say so &lt;blush&gt;, but I get easily \nembarrassed so we should move on now.\n\n&gt;Part of the exercise was to get myself back up to speed in the C#\n&gt;stakes: I hadn&#39;t written any code in .NET for about 18 months. I\n&gt;replaced the progress form graph controls with ZedGraph (from\n&gt;SourceForge) ones as an exercise.\n&gt;  \n&gt;\n\nahh ok, how did that work out? I realise my graphs are [intentionally] \nminimal/lightweight, I take it ZedGraph is a bit more developed?\n\n\n&gt;Network evolution. I speculated last year on the value of starting\n&gt;with a sparsely-connected network (or one with no connections at all).\n&gt;I&#39;m extremely happy with the limited practical experience I&#39;ve had\n&gt;with this approach: the early generations, while fairly stupid,\n&gt;evaluate at a tremendous rate, so complexities that allow some\n&gt;reasonable attempt at the problem are quickly reached. Playing with\n&gt;species numbers via the compatibility threshold is, I&#39;m sure, going to\n&gt;be instructive once I&#39;ve worked out what I think it&#39;s teaching me!\n&gt;  \n&gt;\n\nI don&#39;t think starting population has any real affect on success, at \nleast not in SharpNEAT in it&#39;s current form. As you say, minimal \nnetworks will evaluate quickly and rapidly acqcuire structure. On the \nother hand, complex startin gpopulations will eventually be stripped \ndown to a minimal structure during the first pruning phase. So there&#39;s \nprobably not much in it. Some structure is good because it allows \ndifferentiation into species without elevating the speciation threshold \nwell out of it&#39;s normal range - thus potentially causing it to crash at \na later time, although there are mechanisms in place to alleviate this \nproblem.\n\n\n\n&gt;Architecture. Despite the complete lack of activity on the coding\n&gt;front, I&#39;ve continued to evolve my thinking on this. What I decided to\n&gt;do for this first cut was to evolve a model that would form its own\n&gt;opinions about the relative strengths of teams. \n&gt;\n\nSounds like a sensible starting point.\n\n&gt;I therefore decided to\n&gt;add some outputs to the networks and save them against the team(s)\n&gt;involved inthe match being evaluated. These saved outputs would then\n&gt;form part of the input for the next match that involved that team. I\n&gt;hoped that the more successful networks would learn to calculate a set\n&gt;of &quot;ratings&quot; for each team. Since ratings need to change with results,\n&gt;I also added the team&#39;s most recent result (epxressed as a goal\n&gt;difference) and the network&#39;s most recent prediction to the inputs. I\n&gt;have in the past expressed a liking for the idea of a &quot;confidence&quot;\n&gt;value as a separate output, so I put one of those in too...\n&gt;\n&gt;  \n&gt;\n\nOn the &#39;confidence&#39; output. It is important but does it need to be a \nseperate output? E.g. if you have win and lose outputs - then if they \nare both low then that would effectively be the network giving you a \n&#39;don&#39;t know&#39; or low confidence result - save your money for another day.\n\nThe ratings data is an interesting idea. By reading and writing it on \neach game (in chronological order) this also allows the ratings to \nadjust as the team&#39;s ratings change, e.g. if a key player is injured \nthen you might expect the overall rating of the team to be reduced - the \npoint being that it&#39;s not a constant thing, always changing. I&#39;m not \nsure if this is the best way to do it without thinking about it some \nmore, but the logic seems sound.\n\n\n&gt;The inputs currently look like this:\n&gt;\n&gt;Home team block:\n&gt;    Last result, last predicted win chance & confidence\n&gt;    &quot;memory block&quot; (4 numbers, four because it seemed like a nice number)\n&gt;Away team block:\n&gt;     As above\n&gt;A value to indicate the division in which the match takes place, to\n&gt;give the network a chance to arrive at a sensible number for\n&gt;newly-promoted teams\n&gt;\n&gt;The network is then given a chance to relax over multiple steps (takes\n&gt;about 4 or 5 typically - those that fail to converge within 10 steps\n&gt;are penalised)\n&gt;  \n&gt;\n\nHave you tried a fixed number of activations? If not then it&#39;s worth a \ntry, just stick with 4 or 5 - it should be faster becasue relaxation has \nthe overhead of testing each neuron activation value over and over, so \nif it&#39;s not helping then it&#39;s best avoided.\n\n&gt;Outputs currently look like this:\n&gt;Home win, Away win and Draw probabilities & confidence factors (6 outputs)\n&gt;Home team &quot;memory block&quot; (4)\n&gt;Away team &quot;memory block&quot; (4)\n&gt;\n&gt;Each network evaluation consists of a &quot;calibration&quot; period, consisting\n&gt;of eight years of results being pushed through to develop the model&#39;s\n&gt;&quot;memory&quot;, then a further four seasons for which odds are available are\n&gt;run through. Networks are, by and large, assigned a fitness according\n&gt;to the profit they make.\n&gt;\n&gt;Results to date: The best network I&#39;ve evolved so far managed to take\n&gt;an initial stake of 1,000 and, subject to staking limits of a 50\n&gt;maximum, generate a profit of about 10,000. I&#39;m rather pleased with\n&gt;that :-)\n&gt;\n\nThis suggests overfitting has taken place (see below). Is this over \nseveral seasons, e.g. over 4 years that would be equivalent to a 77% \nreturn each year, which is nice :)\n\n\n&gt; The champion networks tend to have very few hidden nodes (one\n&gt;to four) but a forest of interconnections on the output layer,\n&gt;probably something that stems from &quot;relaxing&quot; over multiple steps.\n&gt;\n&gt;Questions to be answered:\n&gt;1. Is the best network overfitting/memorizing the profitable bets?\n&gt;Quite possibly - I need to source some more recent data for testing.\n&gt;  \n&gt;\n\nThis is nearly always going to be a problem. Having said that, without \ndoing any complex maths we can intuitively see that very simple networks \nsimply won&#39;t have the capacity to memorize large amounts of data (or \noverfit) the training data. Thus your small champion networks almost \ncertainly aren&#39;t overfitting. The litmus test of course is to have a \ntest set of data that wasn&#39;t trained against. If results against the two \nsets begin to diverge then you can assume that overfitting is occuring.\n\n\n\n&gt;2. How robust is the calibration/memory aspect? Would the network\n&gt;arrive at the same result if, say, the first seasons&#39; data were excluded?\n&gt;  \n&gt;\nerr, not sure.\n\n&gt;3. Will I continue to have time to build a front end for the &quot;final\n&gt;solution&quot; network and have it ready to test with real money next\n&gt;season? (I&#39;m prepared to bankroll a prmising candidate to the tune of\n&gt;100 quid or so, for the entertainment if nothing else).\n&gt;  \n&gt;\n\nA) Putting &quot;final solution&quot; in quotes made that sentence far more \nsinister than it otherwise would have done :-|\nB) I think it&#39;s odds on that RealLife(TM) will scupper your plans \nsomewhere along the line. That&#39;s not meant to be a reflection on you \npersonally BTW, I&#39;ll take that bet on anyone any day! (it&#39;s in our DNA \nI&#39;m afraid).\nC) If you&#39;ve had builders in then what&#39;s another 100 quid eh? ;) I think \nthat&#39;s what Plumbers charger an hour these days! On a more serious note, \nwould that be an accumulative strategy whereby only one bet is placed at \na time, the proceeds being used for the next bet? And thus if the first \nfew bets are bust then that&#39;s your 100 quid for this season gone? :(\n\n\n&gt;Well, I came back with a monster. If you&#39;ve reached the end, you have\n&gt;my congratulations (and sympathy for the loss of time it&#39;s caused).\n&gt;I&#39;d welcome any comments, suggestions, or other feedback.\n&gt;  \n&gt;\nThere have been much longer posts believe me. I didn&#39;t really follow \nyour explanation of the inputs and outputs I&#39;m afraid, maybe if I read \nit again tomorrow I&#39;ll absorb it. As a general rule though you always \nneed to use seperate training and test sets in order to evaluate \nperformance on data that wasn&#39;t trained against. You should also try to \nuse multiple test sets in order to geta better picture of variability in \nperformance, e.g. if it varies betweeen really bad and really good then \nthis probably is no use to you, consistently OK is better than \noccasionally brilliant (and occasionally bad).\n\nIf you are using the approach of placing one bet after the other (lets \ncall these serial bets) then I would say that this is a poor way of \nevaluating networks in at search time. The reason being that bad luck \nearly on in the betting sequence has a disproportionate affect on the \noverall success of a network, you should look towards a fairer way of \nevaluating networks (e.g. overall prediction success rate) - you can \nthen use these networks to execute whatever strategy you like for \nplacing real bets and/or evaluation against the test data. E.g. \nevaulation using serial bets on your test data will give you an \nindication of how often your betting series will end up in profit/loss \nin the real world.\n\n\n\nColin.\n\n\nP.S. If you make your fortune then mines a pint. ;)\n\n\n\n"}}