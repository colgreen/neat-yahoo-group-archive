{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":275211662,"authorName":"Cesar G. Miguel","from":"&quot;Cesar G. Miguel&quot; &lt;cesargm@...&gt;","profile":"fdital","replyTo":"LIST","senderId":"URNBYM33wh8r9Zapn_tU3Uxkr2BGlv9Mhq6AThaxuofOqrfpgeLlbKu8BVj8olw0qPheVlqNSvnr5RQXp5FrEXBwbcuIUIv6Ew","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] Re: Different activation methods in NEAT4J","postDate":"1184517924","msgId":3467,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGRkZjEwMDc4MDcwNzE1MDk0NXg2YmY3ZTJhZXg4MzA5YzEzYjFlMzg1OTc3QG1haWwuZ21haWwuY29tPg==","inReplyToHeader":"PGY3NWN0ditrNnF0QGVHcm91cHMuY29tPg==","referencesHeader":"PGRkZjEwMDc4MDcwNzEwMTMxM2g1NmFmZjc1Ymk0ZDllM2M3MDM0NGZjMGM2QG1haWwuZ21haWwuY29tPgkgPGY3NWN0ditrNnF0QGVHcm91cHMuY29tPg=="},"prevInTopic":3466,"nextInTopic":3471,"prevInTime":3466,"nextInTime":3468,"topicId":3459,"numMessagesInTopic":6,"msgSnippet":"Hi Matt, The activation method is still in development and not well documented. And I m not sure how well this method works for supervised learning tasks like","rawEmail":"Return-Path: &lt;cesar.gomes@...&gt;\r\nX-Sender: cesar.gomes@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 40970 invoked from network); 15 Jul 2007 16:45:25 -0000\r\nReceived: from unknown (66.218.66.72)\n  by m38.grp.scd.yahoo.com with QMQP; 15 Jul 2007 16:45:25 -0000\r\nReceived: from unknown (HELO an-out-0708.google.com) (209.85.132.245)\n  by mta14.grp.scd.yahoo.com with SMTP; 15 Jul 2007 16:45:25 -0000\r\nReceived: by an-out-0708.google.com with SMTP id c31so219704ana\n        for &lt;neat@yahoogroups.com&gt;; Sun, 15 Jul 2007 09:45:24 -0700 (PDT)\r\nDKIM-Signature: a=rsa-sha1; c=relaxed/relaxed;\n        d=gmail.com; s=beta;\n        h=domainkey-signature:received:received:message-id:date:from:sender:to:subject:in-reply-to:mime-version:content-type:content-transfer-encoding:content-disposition:references:x-google-sender-auth;\n        b=cbF9/9WoKnmA6xGS4KePpBcG3/rz7nTWhj/pXy9xDhbe5fhSzSV+t+iK+syX4JDQ8usMo3g6ruciE76JMYPYKopI43Jve8+KSMigLI0NxFy94yy0Zo1kPL8959eKnxpLsJbgVlgKrOZa4o1XZ8wXOZi+DQvqZdJGVp30cgIua/0=\r\nReceived: by 10.100.141.13 with SMTP id o13mr1893109and.1184517924437;\n        Sun, 15 Jul 2007 09:45:24 -0700 (PDT)\r\nReceived: by 10.100.9.13 with HTTP; Sun, 15 Jul 2007 09:45:24 -0700 (PDT)\r\nMessage-ID: &lt;ddf100780707150945x6bf7e2aex8309c13b1e385977@...&gt;\r\nDate: Sun, 15 Jul 2007 13:45:24 -0300\r\nTo: neat@yahoogroups.com\r\nIn-Reply-To: &lt;f75ctv+k6qt@...&gt;\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nContent-Disposition: inline\r\nReferences: &lt;ddf100780707101313h56aff75bi4d9e3c70344fc0c6@...&gt;\n\t &lt;f75ctv+k6qt@...&gt;\r\nX-Google-Sender-Auth: 7d64a15abfa43f58\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: &quot;Cesar G. Miguel&quot; &lt;cesargm@...&gt;\r\nSubject: Re: [neat] Re: Different activation methods in NEAT4J\r\nX-Yahoo-Group-Post: member; u=275211662; y=uoXIV17VL5gkmMmomev0MLb1mlHxl_PyO2Uz-l-STuXX\r\nX-Yahoo-Profile: fdital\r\n\r\nHi Matt,\n\nThe activation method is still in development and not well documented.\nAnd I&#39;m not sure how well this method works for supervised learning\ntasks like XOR. If I flush the network (as Ken suggested) the\nperformance might be the same as your original activation method. The\nsame should happen if the inputs are presented at random.\n\nI haven&#39;t done that yet but I&#39;ll send you the sources this week.\n\n&gt;  I have been trying, in vain, to send you my updated code, but I keep\n&gt;  getting it rejected by your mail provider.\n\nI&#39;m using gmail as my main account and I don&#39;t know why it is\nrejecting your attachments. But I think I can set up an ftp server on\nmy machine so that you can upload your code.\n\n[]&#39;s\nCesar\n\nOn 7/12/07, Matt Simmerson &lt;m.simmerson@...&gt; wrote:\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt; Hi Cesar\n&gt;\n&gt;  Your results are very interesting.  Is it possible you could send me\n&gt;  your synchronous update code, and I will integrate it into NEAT4J,\n&gt;  also, does it work for recurrent connections?\n&gt;\n\n&gt;\n&gt;  Cheers\n&gt;\n&gt;  Matt\n&gt;\n&gt;\n&gt;  --- In neat@yahoogroups.com, &quot;Cesar G. Miguel&quot; &lt;cesargm@...&gt; wrote:\n&gt;  &gt;\n&gt;  &gt; Hi there,\n&gt;  &gt;\n&gt;  &gt; I&#39;ve been experimenting with two different methods for neural network\n&gt;  &gt; update in NEAT4J.\n&gt;  &gt;\n&gt;  &gt; I&#39;ve used the same settings as Matt&#39;s: tahn(x) activation for hidden\n&gt;  &gt; nodes and logistic(x) for the output node.  There are two stop\n&gt;  &gt; criteria: (1) when the error is below 0.1 or (2) the number of\n&gt;  &gt; generations is greater than 100.\n&gt;  &gt;\n&gt;  &gt; For a detailed parameters list, please check:\n&gt;  &gt; http://neat4j.sourceforge.net/documents/config.html\n&gt;  &gt;\n&gt;  &gt; NEAT4J recursively activates the neurons linked to the output layer.\n&gt;  &gt; This is its performance for XOR:\n&gt;  &gt;\n&gt;  &gt; 20 runs:\n&gt;  &gt; -----------------------------------------\n&gt;  &gt;               Gen.      Hidden      Connections\n&gt;  &gt; Avg.:      40.9      3.55          8.75\n&gt;  &gt; Std.:       7.16      1.54          2.45\n&gt;  &gt; -----------------------------------------\n&gt;  &gt;\n&gt;  &gt; This is my modified version of NEAT4J (I removed some classes I don&#39;t\n&gt;  &gt; need and implemented a synchronous updating method for the neural\n&gt;  &gt; network). The results are:\n&gt;  &gt;\n&gt;  &gt; -----------------------------------------\n&gt;  &gt;               Gen.      Hidden      Connections\n&gt;  &gt; Avg.:      18.95     1.6            4.2\n&gt;  &gt; Std.:       4.95      0.82          1.32\n&gt;  &gt; -----------------------------------------\n&gt;  &gt;\n&gt;  &gt; The only difference here is that the neural network is updated using\n&gt;  &gt; information from the previous step only.\n&gt;  &gt;\n&gt;  &gt; Please note that following this methodology the network has to be\n&gt;  &gt; activated in a &quot;dynamical way&quot;, even for supervised training such as\n&gt;  &gt; XOR. The input patters are presented to the network as follows:\n&gt;  &gt;\n&gt;  &gt; (0,0) at time 1, (0,1) at time 2, (1,0) at time 3 and (1,1) at time 4.\n&gt;  &gt;\n&gt;  &gt; At time 1 the output neuron is activated using the activation from the\n&gt;  &gt; hidden neuron at time t0 (which is set to zero) and the second input\n&gt;  &gt; value, while the hidden neuron only uses information from the inputs.\n&gt;  &gt;\n&gt;  &gt; At step 2 the output neuron uses the activation of the hidden neuron\n&gt;  &gt; at time 1 and the second input.\n&gt;  &gt;\n&gt;  &gt; And so on..\n&gt;  &gt;\n&gt;  &gt; In ordinary activation methods for feed-forward networks the same does\n&gt;  &gt; not happen. We first need to activate the neuron from the first layer\n&gt;  &gt; and then proceed to the next layer. To achieve the same behavior we&#39;d\n&gt;  &gt; need to activate the network as many times as the numbers of neurons\n&gt;  &gt; it has.\n&gt;  &gt;\n&gt;  &gt; It seems that evolution can take advantage of this method and thus,\n&gt;  &gt; achieve the expected error value in fewer steps exploring the\n&gt;  &gt; sequential values presented to the inputs.\n&gt;  &gt;\n&gt;  &gt; The winner is attached :-)\n&gt;  &gt;\n&gt;  &gt; Cesar\n&gt;  &gt;\n&gt;\n&gt;\n&gt;\n&gt;                   \n\n"}}