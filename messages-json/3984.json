{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":281645563,"authorName":"afcarl2","from":"&quot;afcarl2&quot; &lt;a.carl@...&gt;","profile":"afcarl2","replyTo":"LIST","senderId":"Hcw3W7ARreJ4VCdn_Sn5XoDH-gOJxASRmNlLMiXrKMtTVgEUDdH7VqSQbwalFUXhDsHxP00ciyRuKowwpHNRFDc","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: Another New Paper:  Multiagent HyperNEAT","postDate":"1209061695","msgId":3984,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGZ1cWpnMCtybG5tQGVHcm91cHMuY29tPg==","inReplyToHeader":"PEM0MzY0NDI3LjIyNjg1JWpjbHVuZUBtc3UuZWR1Pg=="},"prevInTopic":3983,"nextInTopic":3986,"prevInTime":3983,"nextInTime":3985,"topicId":3955,"numMessagesInTopic":49,"msgSnippet":"As much as it may pain me, in his defense, I believe addressing the issue of seeding (i.e. reuse), is required to learn how to exploit that geometry, and its","rawEmail":"Return-Path: &lt;a.carl@...&gt;\r\nX-Sender: a.carl@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 18369 invoked from network); 24 Apr 2008 18:28:18 -0000\r\nX-Received: from unknown (66.218.67.94)\n  by m53.grp.scd.yahoo.com with QMQP; 24 Apr 2008 18:28:18 -0000\r\nX-Received: from unknown (HELO n2c.bullet.mail.re1.yahoo.com) (69.147.103.233)\n  by mta15.grp.scd.yahoo.com with SMTP; 24 Apr 2008 18:28:17 -0000\r\nX-Received: from [68.142.237.90] by n2.bullet.mail.re1.yahoo.com with NNFMP; 24 Apr 2008 18:28:17 -0000\r\nX-Received: from [66.218.69.3] by t6.bullet.re3.yahoo.com with NNFMP; 24 Apr 2008 18:28:17 -0000\r\nX-Received: from [66.218.66.74] by t3.bullet.scd.yahoo.com with NNFMP; 24 Apr 2008 18:28:17 -0000\r\nDate: Thu, 24 Apr 2008 18:28:15 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;fuqjg0+rlnm@...&gt;\r\nIn-Reply-To: &lt;C4364427.22685%jclune@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: &quot;afcarl2&quot; &lt;a.carl@...&gt;\r\nSubject: Re: Another New Paper:  Multiagent HyperNEAT\r\nX-Yahoo-Group-Post: member; u=281645563; y=E_rfiErEzvBn2_cPy5pbjCD5gqIOQKQKYU7JBQFLeTvMuw\r\nX-Yahoo-Profile: afcarl2\r\n\r\nAs much as it may pain me, in his defense, I believe addressing the \nissue =\r\nof seeding (i.e. reuse), is required to &quot;learn how to exploit \nthat geometr=\r\ny, and its regularities&quot;. There is no need to re-learn \nhow to tie one&#39;s sh=\r\noelaces every time before progressing on to \nsomething with more utility. M=\r\ny only criticism, if any, would be that \nwhile there are a number of infras=\r\ntructure modifications which would \nfacilitate the objectives and promote s=\r\ncalability, it appears that \nonce again the infrastructure has been held co=\r\nnstant. Expression of \ngreater complexity requires a broader vocabulary to =\r\nwork with.\n\n--- In neat@yahoogroups.com, Jeff Clune &lt;jclune@...&gt; wrote:\n&gt;\n&gt;=\r\n &gt; Perhaps one way to explain why we thought about that first is to\n&gt; &gt; con=\r\nsider that one important philosophical motivation behind \nHyperNEAT\n&gt; &gt; is =\r\nthat machine learning needs a way for humans to convey to the\n&gt; &gt; learner a=\r\n priori known domain geometry.  In effect, we are running\n&gt; &gt; away from the=\r\n black box of No Free Lunch (which is a nasty trap) \nby\n&gt; &gt; finding new way=\r\ns to convey critical a priori domain information.\n&gt; &gt; While arguments can b=\r\ne made that because certain techniques align \nwith\n&gt; &gt; certain problem clas=\r\nses we should not pay too much heed to NFL, \nwhy\n&gt; &gt; would we purposefully =\r\nmove *towards* the black box when we don&#39;t \nhave\n&gt; &gt; to?  The real exciteme=\r\nnt, I think, is to find very general \ntechniques\n&gt; &gt; for conveying to the l=\r\nearner standard kinds of a priori practical\n&gt; &gt; information (or bias), e.g.=\r\n geometry.\n&gt; \n&gt; Hi Ken. I think it is indeed cool that you demonstrate an e=\r\nasy way \nto inject\n&gt; user knowledge in a way that appropriately biases the =\r\nalgorithm \ntoward\n&gt; better solutions. However, in my opinion, the reason we=\r\n are \ninterested in\n&gt; machine learning is because it can solve the problems=\r\n we *don&#39;t* \nknow how to\n&gt; solve. The reason we use simple toy problems is =\r\nbecause we know \nwhat the\n&gt; expected solutions should look like, and we wan=\r\nt to demonstrate \nthat our\n&gt; algorithms can find them. That gives us some c=\r\nonfidence that, when \nwe start\n&gt; showing our algorithms problems where we d=\r\non&#39;t know what the \nsolutions\n&gt; should look like, they will discover good s=\r\nolutions to these \nproblems.\n&gt; Specifically, with regards to generative enc=\r\nodings, one touted \nbenefit is\n&gt; their ability to exploit regularities that=\r\n exist in the problem \ndomain. It\n&gt; is the hope that on complex domains the=\r\nre will be many such \nregularities\n&gt; that can be exploited. It is implied t=\r\nhat humans will not always \nknow what\n&gt; those regularities are. Even if we =\r\ndid, it would be nice if we did \nnot have\n&gt; to tell the algorithm about eac=\r\nh type of regularity. The hope is \nthat\n&gt; generative encodings can discover=\r\n them and exploit them without our \naid. So,\n&gt; while it is nice to be able =\r\nto inject knowledge, it is also \nimportant to\n&gt; devise algorithms that don&#39;=\r\nt require such knowledge. Clearly the \nlogical\n&gt; extremes of either positio=\r\nn are untenable: it is uninteresting to \ntell the\n&gt; network how to do every=\r\nthing but one trivial thing and have it \nlearn that,\n&gt; and NFL tells us we =\r\ncan&#39;t have it be a jack of all trades. But I \nthink there\n&gt; are reasons to =\r\nexplore the intermediate ranges of both positions.\n&gt; \n&gt; &gt; That said, if you=\r\n really did start without the repeating \ncoordinate\n&gt; &gt; frames, I am guessi=\r\nng it would perform worse as you predict, \nthough I\n&gt; &gt; don&#39;t know by how m=\r\nuch.  It is probably worth doing just to see \nwhat\n&gt; &gt; happens.  Yet my per=\r\nsonal view is that there would not be a very \ndeep\n&gt; &gt; insight to gain from=\r\n such a result.  After all, why would we \nexpect it\n&gt; &gt; to consistently dis=\r\ncover the right regularity simply by chance \nevery\n&gt; &gt; time?  Remember that=\r\n early in evolution, simply discovering this\n&gt; &gt; regularity may not even be=\r\n rewarded; just because it somehow gets\n&gt; &gt; lucky and figures out exactly t=\r\nhe right repeating frame of \nreference,\n&gt; &gt; that does not necessarily mean =\r\nthat within those coordinate \nframes it\n&gt; &gt; is doing anything useful (i.e. =\r\nit could be a repetition of a bad\n&gt; &gt; policy), so the discovery is likely t=\r\no go unnoticed and die out, \njust\n&gt; &gt; as easily as it might be leveraged an=\r\nd elaborated properly.\n&gt; \n&gt; I agree that this is a challenging problem, but=\r\n I think that it is \nvery\n&gt; important for us to figure out algorithms that =\r\ncan cope with it. \nClearly\n&gt; nature figured out ways to deal with this issu=\r\ne. How can we set up\n&gt; algorithms such that if the population early on disc=\r\novers a bauplan \nthat\n&gt; keeps it trapped on a local peak, it can eventually=\r\n discover a \nbauplan that\n&gt; gives it access to a higher peak?  To me this i=\r\ns a fundamental \nissue for our\n&gt; field. If we cannot improve upon it, we wi=\r\nll be stuck evolving \nrelatively\n&gt; trivial solutions and never evolve thing=\r\ns as impressive as jaguars \nor poets.\n&gt; I am surprised you don&#39;t think rese=\r\narch on this front is important \nor would\n&gt; provide deep insights. Or, is i=\r\nt that you think we can&#39;t make \nprogress here,\n&gt; so documenting a further f=\r\nailure isn&#39;t deep?\n&gt; &gt; \n&gt; &gt; This problem is related to that discussion we h=\r\nad a while back \nabout\n&gt; &gt; &quot;target-based evolution&quot; and the phenomena of Pi=\r\ncbreeder.  Often \nthe\n&gt; &gt; stepping stones (such as discovering the right ba=\r\nsic regularity) \nare\n&gt; &gt; not recognized by the ultimate objective function,=\r\n so it&#39;s pretty \nmuch\n&gt; &gt; up to luck to find them and keep them around long=\r\n enough to take\n&gt; &gt; advantage of them.  My feeling is that it is not fruitf=\r\nul for any\n&gt; &gt; indirect encoding to try to solve that problem, because it i=\r\ns not \na\n&gt; &gt; problem with the encoding per say but rather with the way fitn=\r\ness \nis\n&gt; &gt; assigned.\n&gt; \n&gt; Fitness is one part of the mix. But there are al=\r\nso issues of \npopulation\n&gt; diversity, representation flexibility and evolva=\r\nbility, etc. Any of \nthese\n&gt; could assist in allowing deep switches in baup=\r\nlan.\n&gt;  \n&gt; I&#39;d like to say one last thing. One of the great innovations of =\r\n\nHyperNEAT\n&gt; was that you provided the geometry of the problem such that th=\r\ne \nalgorithm\n&gt; could exploit it. That strikes me as a bit different than te=\r\nlling \nthe\n&gt; algorithm *how* to go about exploiting that geometry. I think =\r\nthat\n&gt; distinction is important, and might be being conflated a bit here. \n=\r\nI agree\n&gt; unequivocally that providing access to the geometry is important,=\r\n \nbut it\n&gt; seems to me that it is an interesting field of research to figur=\r\ne \nout how to\n&gt; create algorithms that learn how to exploit that geometry, =\r\nand its\n&gt; regularities, on their own.\n&gt; \n&gt; \n&gt; Cheers,\n&gt; Jeff Clune\n&gt; \n&gt; Dig=\r\nital Evolution Lab, Michigan State University\n&gt; \n&gt; jclune@...\n&gt; \n&gt; \n&gt; \n&gt; \n&gt;=\r\n &gt; \n&gt; &gt; --- In neat@yahoogroups.com, Jeff Clune &lt;jclune@&gt; wrote:\n&gt; &gt;&gt; \n&gt; &gt;&gt;=\r\n Hello-\n&gt; &gt;&gt; \n&gt; &gt;&gt; I enjoyed reading this. Thanks for posting it.\n&gt; &gt;&gt; \n&gt; &gt;=\r\n&gt; A question: how did HyperNEAT perform when you did not provide it\n&gt; &gt; wit=\r\nh the\n&gt; &gt;&gt; repeating coordinate frame for each agent? As you mention in the=\r\n\n&gt; &gt; paper, this\n&gt; &gt;&gt; is something that HyperNEAT could learn on its own. I=\r\n assume from\n&gt; &gt; the fact\n&gt; &gt;&gt; that you added it that HyperNEAT was not doi=\r\nng a good job of\n&gt; &gt; learning this.\n&gt; &gt;&gt; \n&gt; &gt;&gt; If that assumption is right,=\r\n how bad was it at learning this \nproblem\n&gt; &gt;&gt; decomposition? One of the to=\r\nuted benefits of HyperNEAT, and \ngenerative\n&gt; &gt;&gt; encodings in general, is t=\r\nhe ability to evolve a module and \nreuse it\n&gt; &gt; many\n&gt; &gt;&gt; times (potentiall=\r\ny with variation).  Here the modularity of the\n&gt; &gt; problem was\n&gt; &gt;&gt; cleanly=\r\n divided, and should have been relatively easy for \nHyperNEAT to\n&gt; &gt;&gt; disco=\r\nver. Do you find it disconcerting that it couldn&#39;t do so?\n&gt; &gt;&gt; \n&gt; &gt;&gt; \n&gt; &gt;&gt; =\r\n\n&gt; &gt;&gt; Cheers,\n&gt; &gt;&gt; Jeff Clune\n&gt; &gt;&gt; \n&gt; &gt;&gt; Digital Evolution Lab, Michigan St=\r\nate University\n&gt; &gt;&gt; \n&gt; &gt;&gt; jclune@\n&gt; &gt;&gt; \n&gt; &gt;&gt; \n&gt; &gt;&gt; \n&gt; &gt;&gt; \n&gt; &gt;&gt;&gt; From: Kenne=\r\nth Stanley &lt;kstanley@&gt;\n&gt; &gt;&gt;&gt; Reply-To: &quot;neat@yahoogroups.com&quot; &lt;neat@yahoogr=\r\noups.com&gt;\n&gt; &gt;&gt;&gt; Date: Wed, 16 Apr 2008 22:48:44 -0000\n&gt; &gt;&gt;&gt; To: &quot;neat@yahoo=\r\ngroups.com&quot; &lt;neat@yahoogroups.com&gt;\n&gt; &gt;&gt;&gt; Subject: [neat] Another New Paper:=\r\n  Multiagent HyperNEAT\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt; David D&#39;Ambrosio and I discuss the pote=\r\nntial for HyperNEAT\n&gt; &gt;&gt;&gt; controlling multiple heterogeneous agents in this=\r\n new\n&gt; &gt;&gt;&gt; paper, &quot;Generative Encoding for Multiagent Learning,&quot; to appear =\r\n\nat\n&gt; &gt;&gt;&gt; GECCO 2008:\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt; http://eplex.cs.ucf.edu/index.php?\n&gt; &gt;&gt;&gt;=\r\n option=3Dcom_content&task=3Dview&id=3D14&Itemid=3D28#dambrosio.gecco08\n&gt; &gt;=\r\n&gt;&gt; \n&gt; &gt;&gt;&gt; Direct Link:\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt; http://eplex.cs.ucf.edu/papers/dambrosi=\r\no_gecco08.pdf\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt; We also have a nice sample of videos that depict=\r\n various evolved\n&gt; &gt;&gt;&gt; teams in action:\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt; http://eplex.cs.ucf.ed=\r\nu/multiagenthyperneat\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt; The interesting idea in this paper is th=\r\nat just as a single\n&gt; &gt;&gt;&gt; connective CPPN can encode how a single network v=\r\naries over \nspace,\n&gt; &gt;&gt;&gt; it can also encode how a *set* of networks (each r=\r\nepresenting \nthe\n&gt; &gt;&gt;&gt; policy of one agent on the team) varies over space. =\r\n In this \nway,\n&gt; &gt;&gt;&gt; HyperNEAT can learn an expression that encodes how pol=\r\nicies vary\n&gt; &gt;&gt;&gt; over the team geometry.  For example, in a soccer team age=\r\nnts \nvary\n&gt; &gt;&gt;&gt; from defensive to offensive as you move away from the goal.=\r\n  \nPart of\n&gt; &gt;&gt;&gt; the power of this approach is that it means basic skills c=\r\nan be\n&gt; &gt;&gt;&gt; learned and shared among the whole team, since the CPPN encodes=\r\n \nhow\n&gt; &gt;&gt;&gt; those skills vary across the field.\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt; ken\n&gt; &gt;&gt;&gt; \n&gt; &gt;=\r\n&gt; \n&gt; &gt; \n&gt; &gt;\n&gt;\n\n\n\n"}}