{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":186151300,"authorName":"gbravoescobar","from":"&quot;gbravoescobar&quot; &lt;gbravoescobar@...&gt;","profile":"gbravoescobar","replyTo":"LIST","senderId":"B4xy2arWgEvj5HNA-s-SQbPydXqDTnhpYY4s6Nr8fEg4sel8h6k7D7B6kLUhNPIruhdTmzSbHM3yjUkDsxhmnefqDnCVof6qXi_e4aZu1A","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: Symmetry, concepts and data buses in the brain","postDate":"1106045684","msgId":1828,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGNzaXB0ayttc2RiQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDYuMi4wLjE0LjAuMjAwNTAxMTQxNzE4MDIuMDI3ZDE5MzBAcG9wLm1haWwueWFob28uY28udWs+"},"prevInTopic":1826,"nextInTopic":1829,"prevInTime":1827,"nextInTime":1829,"topicId":1698,"numMessagesInTopic":40,"msgSnippet":"Hi Ian: Something that I asked in the past about Neural Networks evolution was the some you say at the final of the mail:  is it enough to use the actual kind","rawEmail":"Return-Path: &lt;gbravoescobar@...&gt;\r\nX-Sender: gbravoescobar@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 30166 invoked from network); 18 Jan 2005 10:54:59 -0000\r\nReceived: from unknown (66.218.66.216)\n  by m5.grp.scd.yahoo.com with QMQP; 18 Jan 2005 10:54:59 -0000\r\nReceived: from unknown (HELO n14a.bulk.scd.yahoo.com) (66.94.237.28)\n  by mta1.grp.scd.yahoo.com with SMTP; 18 Jan 2005 10:54:59 -0000\r\nReceived: from [66.218.69.5] by n14.bulk.scd.yahoo.com with NNFMP; 18 Jan 2005 10:54:46 -0000\r\nReceived: from [66.218.66.79] by mailer5.bulk.scd.yahoo.com with NNFMP; 18 Jan 2005 10:54:46 -0000\r\nDate: Tue, 18 Jan 2005 10:54:44 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;csiptk+msdb@...&gt;\r\nIn-Reply-To: &lt;6.2.0.14.0.20050114171802.027d1930@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Transfer-Encoding: quoted-printable\r\nContent-Length: 8671\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Remote-IP: 66.94.237.28\r\nFrom: &quot;gbravoescobar&quot; &lt;gbravoescobar@...&gt;\r\nSubject: Re: Symmetry, concepts and data buses in the brain\r\nX-Yahoo-Group-Post: member; u=186151300\r\nX-Yahoo-Profile: gbravoescobar\r\n\r\n\nHi Ian:\n\n    Something that I asked in the past about Neural Networks evol=\r\nution\nwas the some you say at the final of the mail:  is it enough to use\nt=\r\nhe actual kind of Neurons and structures to represent whatever class\nof res=\r\nponse?. \n\nThe answer I found in the mathematical approach of Neural Network=\r\ns.\nThey argued that a neural network is a &quot;differentiate program&quot; with\nvari=\r\nables and conditional sentences like a hand-coded program. The\ndifference i=\r\ns in the use of sigmoid functions that don&#39;t let the\nsystems to memorize th=\r\ne variables for a long time. \n\nTo solve this and to let to neural networks =\r\nto be a &quot;Turing machine&quot;,\nSchmidhuber have introduced the LSTM cells. (see\n=\r\nhttp://www.idsia.ch/~juergen/rnn.html) .  \n\n The use of  LSTM cells (Long S=\r\nhort-Term Memory cells) in a Neural\nNetwork let the network to use &quot;memorie=\r\ns&quot;, &quot;conditional statements&quot;\n(&quot;routed events&quot; has more meaning) to extract =\r\nor store that&#39;s memories\ndepending of such conditional statements . \n\nThe a=\r\nnalysis of evolved LSTM-NEAT networks or the pure LSTM (with\nother learning=\r\n algorithm) show that they map some events across time\nin the right memorie=\r\ns (LSTM cells) giving some sort of &quot;mind state&quot;.\nThis &quot;mind state&quot; can be a=\r\ns complex as the number of LSTM cells it\nhas. This was evident in the amazi=\r\nng experiment of Hotchreiter (see\nhttp://www.ph.tn.tudelft.nl/PRInfo/report=\r\ns/msg00684.html).  Although,\nwe have to point that it was &quot;amazing&quot;, too, t=\r\nhe learning time: 5\nmonths!!!!&quot;. \n\nSo, It could be possible that  with some=\r\n kind of evolution pressure \nwe could guide Neural Networks to create a Neu=\r\nral Processor with a\nseparation between Memories, buses, &quot;Conditional event=\r\ns&quot; and\nprocessing units. This would help us to understand a litle bit more\n=\r\nthe solutions (something that in today solutions,where such concepts\nare mi=\r\nxed, is realy difficult)\n\n Besides,Instead of let the systems to create wha=\r\ntever useful modules,\nsuch separation could help the systems to reduce sear=\r\nching space, and\ntherefore to find more elaborated solutions.\n\n  Germ=E1n.\n=\r\n\n\n\n--- In neat@yahoogroups.com, Ian Badcoe &lt;ian_badcoe@y...&gt; wrote:\n&gt; At 17=\r\n:02 14/01/2005, you wrote:\n&gt; \n&gt; \n&gt; &gt;--- Ian Badcoe &lt;ian_badcoe@y...&gt; wrote:=\r\n\n&gt; &gt;(about symettry)\n&gt; &gt;&lt;snip&gt;\n&gt; &gt;\n&gt; &gt;My suggestion is that a symettry dete=\r\ncting network\n&gt; &gt;might be evolved whose only function is to reduce many\n&gt; &gt;=\r\ninputs to few inputs by detecting symettry.  The\n&gt; &gt;outputs of this ANN wou=\r\nld be the inputs to the main\n&gt; &gt;ANN that is being evolved.\n&gt; &gt;\n&gt; &gt;For examp=\r\nle, consider a board game played on a\n&gt; &gt;rectangular board of NxN squares. =\r\n The initial ANN\n&gt; &gt;receives all the inputs but maps them into a set of\n&gt; &gt;=\r\noutputs that is equivalent to a rotation of the board\n&gt; &gt;so that most of th=\r\ne pieces are in the lower left\n&gt; &gt;corner.  This pre-processing does not hav=\r\ne to be done\n&gt; &gt;by an ANN, but it could be.  The net effect would be\n&gt; &gt;as =\r\nif the player viewed the board from various\n&gt; &gt;positions until he found the=\r\n one with the pieces\n&gt; &gt;arranged most toward the lower left.  This greatly\n=\r\n&gt; &gt;reduces the variety of configurations that must be\n&gt; &gt;considered, althou=\r\ngh they are still considerable.\n&gt; &gt;This might allow tic-tac-toe to be solve=\r\nd by\n&gt; &gt;neuroevolution.\n&gt; &gt;\n&gt; &gt;I hope I have not been too laconic; I will e=\r\nxpand if\n&gt; &gt;asked.\n&gt; \n&gt; Hi,\n&gt;          Let&#39;s call it &quot;admirably succinct&quot; r=\r\nather than laconic.  I get \n&gt; exactly what you mean.  I have considered thi=\r\ns approach in the past, in \n&gt; fact that may even have been when we discusse=\r\nd it before.\n&gt; \n&gt;          This is what I think of as the Mapping -&gt; Proces=\r\nsing -&gt;\nDemapping \n&gt; approach, e.g. an outer subsystem (ANN or other) exami=\r\nnes the data,\ndecides \n&gt; on an orientation, rotates the data to use that or=\r\nientation and\npasses the \n&gt; result to the inner subsystem.  The inner does =\r\nits magic and then\npasses \n&gt; the results back to the outer.  The outer then=\r\n has to remove the\nmapping to \n&gt; restore &quot;world coordinates&quot;, before the an=\r\nswer will make sense to the \n&gt; output layer.\n&gt; \n&gt;          There are two pr=\r\noblems with this approach:\n&gt; \n&gt; A) Demapping.  Obviously if we simply provi=\r\nde the outer subsystem\nthen we \n&gt; can guarantee that mapping is the inverse=\r\n of demapping.  However, if we \n&gt; wish to evolve them then selecting them t=\r\no do that (without applying a \n&gt; separate and very specific selection press=\r\nure just to this\nsubsystem) is \n&gt; very improbable in the face of mutation (=\r\nremember that the inner system \n&gt; cannot operate at all unless the outer is=\r\n working).  Similarly, we\nwould \n&gt; like it to be free to evolve different m=\r\nappings whilst retaining the \n&gt; inverse relationship but that is equally un=\r\nlikely.\n&gt; \n&gt; B) Multipliciy of viewpoint.  Given a chess board, exploitatio=\r\nn of the \n&gt; symmetry doesn&#39;t just mean picking a single location and\ntransf=\r\norming the \n&gt; view into it.  Rather, the need is to consider the board unde=\r\nr multiple \n&gt; transforms and find the one that enables the inner subsystem =\r\nto\nreturn the \n&gt; most positive result.  You can view this in two ways (both=\r\n of which \n&gt; probably happen in nature), first, that the inner system feeds=\r\n back\nto the \n&gt; outer saying &quot;now change the transform to&quot; -- this is essen=\r\ntially the \n&gt; roving eye approach.  Second, we can imagine the outer system=\r\n providing \n&gt; multiple transformations of the board and multiple copies of =\r\nthe inner \n&gt; system being used to analyze them in parallel.  This is what I=\r\n call an \n&gt; &quot;omnipresent eye&quot; approach.  The advantages of an omnipresent e=\r\nye\nare that \n&gt; (i) it looks further afield than a roving eye and does not r=\r\nely on\nthere \n&gt; being a path across the board to the region of interest, an=\r\nd (ii) that \n&gt; since multiple views of the board are processed simultaneous=\r\nly, data\nfrom \n&gt; different views can be used in the same calculation (eithe=\r\nr passing\nit to \n&gt; other views &quot;how dangerous is my neighbour&#39;s view?&quot;; or =\r\nanalyzing it\nin an \n&gt; overview subsystem &quot;which view is most confident?&quot;)\n&gt;=\r\n \n&gt;          Both these limitations illustrate what I am wondering \n&gt; about=\r\n.  Because, although we can fairly easily see some situations\nunder \n&gt; whic=\r\nh they can be overcome.  We can also see how difficult they\nwould be \n&gt; und=\r\ner other conditions.  e.g.\n&gt; \n&gt; 1) In a &quot;custom&quot; system where we have some =\r\nevolving components, but \n&gt; hand-coded systems for other parts, then we cou=\r\nld easily set up the \n&gt; hand-coded portion so that multiple viewpoints were=\r\n processed and/or\ndata \n&gt; mapped on input is correspondingly demapped on ou=\r\ntput.\n&gt; \n&gt; 2) In a system where everything evolved, if we were processing o=\r\nnly a \n&gt; single problem, then it would be very difficult for the system to\n=\r\nevolve a \n&gt; [network] where mapping and demapping were kept compatible \n&gt; _=\r\nby_the_process_of_evolution_ (e.g. relying on, when mapping\nmutates, that \n=\r\n&gt; demapping should mutate in the complementary manner).\n&gt; \n&gt; 3) BUT, a syst=\r\nem which had evolved (over a far longer period) to\nhandle a \n&gt; variety of &quot;=\r\nsymmetric&quot; problems, would/could have developed a general \n&gt; purpose map/de=\r\nmap system (a MaDem -- like a MoDem).  Such a system is \n&gt; related to the i=\r\ndea of a routing system (e.g. because remapping a \n&gt; chess-board is equaval=\r\nent to rerouting the contents of one square to \n&gt; another square).  And a r=\r\nouting system would also be required in\norder to \n&gt; apply the madem system =\r\nto more than one other system.  Such a system\nwould \n&gt; automatically keep m=\r\napping and demapping in synchrony, because it\nwould be \n&gt; built to *underst=\r\nand* them as two sides of the same coin.  It would be \n&gt; soft-configurable =\r\nwith a mapping and loading the mapping would also\ndefine \n&gt; the correspondi=\r\nng demapping.\n&gt; \n&gt; --\n&gt; \n&gt;          So what I am getting at is an interest =\r\nin 3 which is deeply\nlinked \n&gt; to ideas of modularity (and thus probably re=\r\nquiring indirect\nrepresentation).\n&gt; \n&gt;          I&#39;m interested in (1) and p=\r\nart of the document I&#39;m currently \n&gt; working on is a specification for a sy=\r\nstem which can do some aspects\nof it.\n&gt; \n&gt;          I&#39;m interested in any c=\r\nonfirmation/commentary that my analysis \n&gt; that (2) forms a &quot;hard&quot; problem =\r\nfor (short term) evolution and hence \n&gt; implies the existence of (3) in nat=\r\nural thinkers.\n&gt; \n&gt;          And I&#39;m especially interested in any informati=\r\non or ideas\nabout \n&gt; whether you agree there&#39;s an implication that natural =\r\nthinkers\nimplement \n&gt; some sort of systems akin to (3), what sort of level =\r\nthey might be \n&gt; implemented at (e.g. is a &quot;router&quot; a 10, 10,000 or 10,000,=\r\n000 neurone \n&gt; structure), what sort of neural structures might be able to =\r\ndo it, and \n&gt; whether it can be done with the sort of artificial neurones t=\r\nhat we\nuse at \n&gt; the moment, or whether some new properties are required.\n&gt;=\r\n \n&gt;          Ian Badcoe\n&gt; \n&gt; \n&gt; \n&gt; Living@Home - Open Source Evolving Organ=\r\nisms - \n&gt; http://livingathome.sourceforge.net/\n\n\n\n\n"}}