{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":115403844,"authorName":"John Arrowwood","from":"&quot;John Arrowwood&quot; &lt;jarrowwx@...&gt;","profile":"jarrowwx","replyTo":"LIST","senderId":"lC-KVefadPlKjnwoTdr9yt9JD3QYBmGlbJjsf6kdu9mPbmwn1sKkh9bWwhaD2lurrOroNm59OooZsxF_lBIMgmndXAwTFdZenEg1ukSH","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] Computation Time","postDate":"1087581884","msgId":1097,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PEJBWTItRjc0cEp4VjBIdkJnVXAwMDAwMzE0OUBob3RtYWlsLmNvbT4="},"prevInTopic":1095,"nextInTopic":1104,"prevInTime":1096,"nextInTime":1098,"topicId":845,"numMessagesInTopic":99,"msgSnippet":"... Which is what this particular test was focused on. ... It can.  But when the amount of work being done is large enough, it helps to do both.  I know, I","rawEmail":"Return-Path: &lt;jarrowwx@...&gt;\r\nX-Sender: jarrowwx@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 92985 invoked from network); 18 Jun 2004 18:04:46 -0000\r\nReceived: from unknown (66.218.66.216)\n  by m25.grp.scd.yahoo.com with QMQP; 18 Jun 2004 18:04:46 -0000\r\nReceived: from unknown (HELO hotmail.com) (65.54.247.74)\n  by mta1.grp.scd.yahoo.com with SMTP; 18 Jun 2004 18:04:46 -0000\r\nReceived: from mail pickup service by hotmail.com with Microsoft SMTPSVC;\n\t Fri, 18 Jun 2004 11:04:45 -0700\r\nReceived: from 64.122.44.102 by by2fd.bay2.hotmail.msn.com with HTTP;\n\tFri, 18 Jun 2004 18:04:44 GMT\r\nX-Originating-Email: [jarrowwx@...]\r\nX-Sender: jarrowwx@...\r\nTo: neat@yahoogroups.com\r\nBcc: \r\nDate: Fri, 18 Jun 2004 11:04:44 -0700\r\nMime-Version: 1.0\r\nContent-Type: text/plain; format=flowed\r\nMessage-ID: &lt;BAY2-F74pJxV0HvBgUp00003149@...&gt;\r\nX-OriginalArrivalTime: 18 Jun 2004 18:04:45.0258 (UTC) FILETIME=[BC97FAA0:01C4555E]\r\nX-eGroups-Remote-IP: 65.54.247.74\r\nFrom: &quot;John Arrowwood&quot; &lt;jarrowwx@...&gt;\r\nReply-To: john@...\r\nSubject: Re: [neat] Computation Time\r\nX-Yahoo-Group-Post: member; u=115403844\r\nX-Yahoo-Profile: jarrowwx\r\n\r\n&gt;From: Ian Badcoe &lt;ian_badcoe@...&gt;\n\n&gt;For small amounts of data, they have to be read into the cache once, but\n&gt;once they are in the cache they are fast.  In that case speed will depend \n&gt;mostly on the detail of the cpu instructions and their order.\n\nWhich is what this particular test was focused on.\n\n&gt;For large amounts of data (meaning bigger than the cache but particularly\n&gt;when _much_ bigger than the cache) the order in which data is accessed\n&gt;becomes more important than instruction order.\n\nIt can.  But when the amount of work being done is large enough, it helps to \ndo both.  I know, I proved it for myself with one of my programs.\n\n&gt;Since you said &quot;billions&quot; of accesses, memory speed will be critical to\n&gt;your program, so that is what I concentrated on.\n\nWhich is why I am organizing it so that I work on the same bit of data over \nand over again until I have done everything I can with it.  The same 9x9 \ninput pixels will be evaluated with 1,005,720 different combinations of \n&#39;area&#39; parameters and expected results.  Then I will evaluate the next \nnetwork on those same inputs and combinations and expected results.  End \nresult?  Small set of working data at a time so I am always within the \ncache.  So, instruction order helps, too.\n\nWhich brings up an important question:  Suppose I have something like this:\n\nan array of function pointers to custom activation functions\nan array of inputs and expected outputs\n\nIf the size of the activation function is such that it can fit in the code \ncache, then it is better to have that be iterated in the outer loop, and the \ninputs/outputs iterated in the inner loop.\n\nIf the function is too big to stay in the code cache anyway, then it is \nfaster to reverse that, right?  Let the data stay in the data cache, and \ncall all the different functions with that data.\n\nSo, how big is too big?  /proc/cpuinfo says my laptop&#39;s cache is 256kb, and \nmy servers have 512kb.  Is that shared for both code and data?  How big of a \nnetwork would I need in order for the generated activation function (no \nloops) to exceed the size of the cache?  At what point would the cost of a \nloop counter and weight lookups be less than the unrolled loop?  Is it as I \nsuspect, only for networks so big that worrying about it isn&#39;t worth the \neffort?\n\n&gt;Processing a small amount of data 1 billion times is not the same as\n&gt;processing a lot of data once.\n&gt;\n&gt;The reason memory access patters matter is because (i) uncached memory\n&gt;access is slower than CPU processing (ii) the cache loads an entire &quot;line&quot;\n&gt;of contiguous memory in one &quot;burst&quot; -- this is 32 bytes (8 floats, 4 \n&gt;doubles).\n\nA good point.  I usually do my memory access linearly anyway, even though I \ndidn&#39;t know why I should.  It just seems &#39;cleaner&#39; to do it that way.  And, \nit allows greater optimization of the inner loop if you do.\n\n&gt;Thus this:\n&gt;\n&gt;int a[1000000000];\n&gt;int i;\n&gt;\n&gt;for(i = 0; i &lt; 1000000000; i += 100) {\n&gt;    for(j = 0; j &lt; 100; j++) {\n&gt;      a[i+j] = 1;\n&gt;    }\n&gt;}\n&gt;\n&gt;Will be far faster than this:\n&gt;\n&gt;for(j = 0; j &lt; 100; j++) {\n&gt;    for(i = 0; i &lt; 1000000000; i += 100) {\n&gt;      a[i+j] = 1;\n&gt;    }\n&gt;}\n\nA silly example, since it walks off the end of the array, and could be done \nwithout an inner loop!  :)   But I understand the point.\n\nA better example is extracting a rectangle from a larger rectangle.  You \nwant to have the Y loop on the outside, and the X loop on the inside, for \nprecisely that reason.  But if your memory is stored in a one-dimensional \narray (which technically it always is), you have to calculate the offset \nbased on x,y every time.  And even if you store it in a 2-dimensional array, \nyou just push that job off to the compiler to do transparently for you.  \nNow, if you instead calculate the offset of the first point in the row, you \ncan access the next data item by just incrementing your pointer!   The \npointer increment is much faster than the calculated memory offset.\n\n&gt;That&#39;s what I was getting at, but all your examples will fit in the cache,\n&gt;so none of this will apply.\n&gt;\n&gt;Big-data programs are different to small-data programs.\n\nYes.  And I&#39;ve spent large amounts of time optimizing big-data programs.  \nOne program went from 12 minutes to under 3 minutes on a very fast computer. \n  So it really does make orders-of-magnitude difference, if what you are \ndoing is sufficently large.  Good point.\n\n&gt;Now, you can rightly say that if you process one network at a time, it will\n&gt;all fit in the cache and you need not worry, but I was thinking that (i)\n&gt;your input data (the image) is quite large, and (ii) if you are storing it,\n&gt;then your output data is also quite large, and as they get accessed during\n&gt;network evaluations\n\nNah.  I have a collection of around 3000 images.  But for network evaluation \npurposes, I&#39;m going to randomly select one image, randomly grab one 144x144 \nchunk out of it, and use that as the fitness evaluation for all networks.  \nThen, I&#39;ll repeat.  So long as the relative rankings of the networks change, \nI&#39;ll keep pulling chunks at random.  As soon as two successive evaluations \nof all networks result in the cumulative relative fitness ordering being the \nsame, I&#39;m done evaluating for that generation.\n\nNow, once I find a working network and it is time to implement it for actual \nimage enlargement, that&#39;s different.  That&#39;s going to be a whole other \noptimization problem! :)\n\n&gt;And finally, I deliberately neglected before to mention the other cost,\n&gt;which is that any process which wants to exploit coherent memory access\n&gt;patterns will have to pay the price of rearranging its data into coherent\n&gt;order.  If you can arrange to do that rearrangement rarely, then coherent\n&gt;access will save you time, but otherwise you&#39;ll spend longer getting ready\n&gt;than doing the work.\n\nYeah.  Especially if it is more costly to rearrange than it is to just \naccess it in a less linear fashion.  And if you are only accessing it ONCE, \nthere is NO benefit to rearranging.\n\n&gt;p.s. you do know those numeric constants you wrote are far too high\n&gt;precision?  About 12 sig-figs in the limit of double.\n\nTell it to my C compiler.  I generated those constants from a C program that \njust calculates random values and then uses printf to dump them.  So, \naccording to my C compiler, the result of the random number calculation did \nin fact have that much precision. :)\n\nBTW:  Thanks for sharing your insight into optimization.  It really is \nappreciated.  And helpful!\n\n\n\n"}}