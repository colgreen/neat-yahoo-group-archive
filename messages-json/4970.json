{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":211599040,"authorName":"Jeff Clune","from":"Jeff Clune &lt;jclune@...&gt;","profile":"jeffreyclune","replyTo":"LIST","senderId":"EO2j_Jd92tlT6-x9o5XeIAqKyIsVXhp4y4oPY3cpGw9usZsmIjVid9pBA13UxhkiLs95K2lnQig5fS-Gsb0E7TeF","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [neat] Re: HyperNEAT Tutorial?","postDate":"1259774918","msgId":4970,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PEM3M0MwREY2LjJFRDYzJWpjbHVuZUBtc3UuZWR1Pg==","inReplyToHeader":"PGhlczZocCt0dXNvQGVHcm91cHMuY29tPg=="},"prevInTopic":4963,"nextInTopic":4985,"prevInTime":4969,"nextInTime":4971,"topicId":4884,"numMessagesInTopic":21,"msgSnippet":"Hello all- I like the idea of a tutorial, especially with respect to the many HyperNEAT (NEAT) parameters. For example, I just had a question internally about","rawEmail":"Return-Path: &lt;jclune@...&gt;\r\nX-Sender: jclune@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 36386 invoked from network); 2 Dec 2009 17:28:46 -0000\r\nX-Received: from unknown (98.137.34.46)\n  by m1.grp.sp2.yahoo.com with QMQP; 2 Dec 2009 17:28:46 -0000\r\nX-Received: from unknown (HELO mail-gx0-f213.google.com) (209.85.217.213)\n  by mta3.grp.sp2.yahoo.com with SMTP; 2 Dec 2009 17:28:46 -0000\r\nX-Received: by gxk5 with SMTP id 5so366293gxk.19\n        for &lt;neat@yahoogroups.com&gt;; Wed, 02 Dec 2009 09:28:45 -0800 (PST)\r\nX-Received: by 10.100.50.3 with SMTP id x3mr406463anx.138.1259774925145;\n        Wed, 02 Dec 2009 09:28:45 -0800 (PST)\r\nReturn-Path: &lt;jclune@...&gt;\r\nX-Received: from ?10.0.1.2? (c-71-206-125-104.hsd1.mi.comcast.net [71.206.125.104])\n        by mx.google.com with ESMTPS id 34sm638172yxf.47.2009.12.02.09.28.41\n        (version=TLSv1/SSLv3 cipher=RC4-MD5);\n        Wed, 02 Dec 2009 09:28:42 -0800 (PST)\r\nUser-Agent: Microsoft-Entourage/12.13.0.080930\r\nDate: Wed, 02 Dec 2009 12:28:38 -0500\r\nTo: &quot;neat@yahoogroups.com&quot; &lt;neat@yahoogroups.com&gt;\r\nMessage-ID: &lt;C73C0DF6.2ED63%jclune@...&gt;\r\nThread-Topic: [neat] Re: HyperNEAT Tutorial?\r\nThread-Index: AcpzdOIWIVOhUvhNx0S9M6GHQIO9FA==\r\nIn-Reply-To: &lt;hes6hp+tuso@...&gt;\r\nMime-version: 1.0\r\nContent-type: text/plain;\n\tcharset=&quot;ISO-8859-1&quot;\r\nContent-transfer-encoding: quoted-printable\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Jeff Clune &lt;jclune@...&gt;\r\nSubject: Re: [neat] Re: HyperNEAT Tutorial?\r\nX-Yahoo-Group-Post: member; u=211599040; y=HE6Bbep253_PDwLlwUkbOAPYfEIE68KoZHH7oSyFMVPBR2R-xcU6\r\nX-Yahoo-Profile: jeffreyclune\r\n\r\nHello all-\n\nI like the idea of a tutorial, especially with respect to the m=\r\nany HyperNEAT\n(NEAT) parameters. For example, I just had a question interna=\r\nlly about what\nthe SpeciesSizeTarget would be, and had to search around for=\r\n a while to find\nthe thread on the NEAT list where I had previously asked a=\r\nbout this\nparameter (and Ken provided a helpful answer, pasted below).\n\nIt =\r\nwould be great to capture this knowledge in a wiki, instead of just in\nthis=\r\n forum, and let people add their own thoughts. That way, a researcher\ncould=\r\n read a paragraph or three about the parameter they are wondering\nabout, wh=\r\nich can help them understand it and possibly optimize it for their\nproblem.=\r\n It would probably also highlight which parameters we have not yet\nhad disc=\r\nussions about, so people can fill in their own rules of thumb for\nthat para=\r\nmeter. \n\nHere was Ken&#39;s reply about SpeciesSizeTarget:\n&gt;&gt;&gt; I haven&#39;t seen a=\r\nny explicit studies on population size in NEAT\n&gt;&gt;&gt; specifically, but there =\r\nhave been such studies for genetic algorithms\n&gt;&gt;&gt; in general. I think Kenne=\r\nth De Jong&#39;s textbook, &quot;Evolutionary\n&gt;&gt;&gt; Computation: A Unified Perspective=\r\n&quot; examines it.\n&gt;&gt;&gt;\n&gt;&gt;&gt; However, of course as people have pointed out in NEA=\r\nT it&#39;s not just a\n&gt;&gt;&gt; question of a healthy population but of healthy speci=\r\nes, since each\n&gt;&gt;&gt; species is like its own little population. In other word=\r\ns, the\n&gt;&gt;&gt; species need enough internal diversity to drive their own\n&gt;&gt;&gt; ex=\r\nplorations. My own rule of thumb has been a minimum of 15 members\n&gt;&gt;&gt; per s=\r\npecies (on average) to keep them healthy. I&#39;m sure 30 is even\n&gt;&gt;&gt; safer.\n&gt;&gt;=\r\n&gt;\n&gt;&gt;&gt; Another thing to note is that the population size you need can vary\n&gt;=\r\n&gt;&gt; wildly by problem and it can be surprising. For pole balancing, it\n&gt;&gt;&gt; t=\r\nurns out that you can solve it fastest with a minuscule population of\n&gt;&gt;&gt; s=\r\nize under 20 total (that&#39;s for the whole population!). That is\n&gt;&gt;&gt; because =\r\nsmaller populations are greedier (they concentrate resources\n&gt;&gt;&gt; on a small=\r\n part of the search space) and apparently pole balancing\n&gt;&gt;&gt; benefits great=\r\nly from greedy search. However, that is a specific\n&gt;&gt;&gt; property of pole bal=\r\nancing and I would not expect it to carry over to\n&gt;&gt;&gt; more significant doma=\r\nins. Surely in some domains having a population\n&gt;&gt;&gt; over 1000 would lead to=\r\n the most interesting results, if only you can\n&gt;&gt;&gt; afford the luxury of the=\r\n added computational cost of a large population.\n\n\n\n\nCheers,\nJeff Clune\n\nDi=\r\ngital Evolution Lab, Michigan State University\njclune@...\nwww.msu.edu/~=\r\njclune\n\n\n\n\n&gt; From: Ken &lt;kstanley@...&gt;\n&gt; Reply-To: &quot;neat@yahoogrou=\r\nps.com&quot; &lt;neat@yahoogroups.com&gt;\n&gt; Date: Sat, 28 Nov 2009 22:00:25 -0000\n&gt; To=\r\n: &quot;neat@yahoogroups.com&quot; &lt;neat@yahoogroups.com&gt;\n&gt; Subject: [neat] Re: Hyper=\r\nNEAT Tutorial?\n&gt; \n&gt; \n&gt; \n&gt; Anthony,\n&gt; \n&gt; We are hoping to make the HyperNEAT=\r\n Users Page at\n&gt; http://eplex.cs.ucf.edu/hyperNEATpage/HyperNEAT.html into =\r\na place where people\n&gt; can have questions like yours answered.  The &quot;Introd=\r\nuction / What is\n&gt; HyperNEAT?&quot; section on that page is intended to provide =\r\nsome general answers\n&gt; to beginners without having to read a research paper=\r\n.  Did that section help\n&gt; you at all?  I&#39;d like to make the page as useful=\r\n as possible and we will\n&gt; continue to improve it.\n&gt; \n&gt; To answer your ques=\r\ntion, HyperNEAT is a significant step beyond NEAT so it\n&gt; involves a lot of=\r\n new ideas that aren&#39;t part of the original NEAT.  Some of\n&gt; those concepts=\r\n can theoretically be applied on top of non-NEAT methods.  For\n&gt; example, i=\r\nn the following paper, NEAT is substituted with GP to create a\n&gt; &quot;HyperGP,&quot;=\r\n in which GP evolves the CPPN:\n&gt; \n&gt; Buk Z., Koutn=EDk J., =8Anorek M., NEAT=\r\n in HyperNEAT Substituted with Genetic\n&gt; Programming, In: ICANNGA 2009.\n&gt; h=\r\nttp://cig.felk.cvut.cz/research/publications/hypergp.pdf\n&gt; \n&gt; That said, Hy=\r\nperNEAT addresses a limitation of NEAT, which is a limitation of\n&gt; all dire=\r\nct encodings:  In NEAT, there is one gene for every connection in the\n&gt; net=\r\nwork.  Even with complexification, that kind of representation cannot hope\n=\r\n&gt; to scale to networks with millions or more connections, because such netw=\r\norks\n&gt; would have millions or more genes, which is an astronomical search s=\r\npace.\n&gt; \n&gt; However, there are in fact 100 trillion connections in the human=\r\n brain, which\n&gt; means that in principle it is possible to evolve such struc=\r\ntures.  Yet there\n&gt; are only about 30,000 genes in the human genome, which =\r\nsuggests that any\n&gt; evolutionary approach to evolving large-scale neural ne=\r\ntworks must encode the\n&gt; connection weights in a compressed description, wh=\r\nich is called an indirect\n&gt; encoding.\n&gt; \n&gt; In HyperNEAT, the indirect encod=\r\ning is the CPPN, which encodes the\n&gt; connectivity of a network as a pattern=\r\n across its geometry.  HyperNEAT\n&gt; combines the idea of indirect encoding w=\r\nith a strong notion of geometry and\n&gt; builds on our understanding of encodi=\r\nng patterns to produce an algorithm that\n&gt; encodes large-scale topographies=\r\n (i.e. connection weights across a geometry).\n&gt; Thus it extends NEAT by giv=\r\ning it the power of indirect encoding, thereby\n&gt; greatly expanding the scop=\r\ne of networks it can evolve.\n&gt; \n&gt; Of course NEAT is still there under the h=\r\nood.  NEAT is evolving the CPPNs,\n&gt; which in turn encode neural networks (c=\r\nalled substrates in HyperNEAT).  The\n&gt; CPPNs themselves are still complexif=\r\nying.  However, that complexification is\n&gt; no longer literally adding one c=\r\nonnection at a time to a neural network.\n&gt; Rather it is adding *information=\r\n* to the encoding, so that it can encode more\n&gt; complex *holistic* connecti=\r\nvity patterns.  In other words, HyperNEAT means\n&gt; that evolution is no long=\r\ner limited by the dimensionality of the inputs and\n&gt; outputs but rather can=\r\n search for the correct implicit problem complexity,\n&gt; whatever that may be=\r\n, inside the CPPN.  The substrate (which the CPPN encodes)\n&gt; will then have=\r\n as many connections as it needs, in principle up to millions or\n&gt; even tri=\r\nllions (with enough CPU power).\n&gt; \n&gt; It is true that geometry may be vague =\r\nor difficult to decide in some problems.\n&gt; Those may be more difficult for =\r\nusers to approach with HyperNEAT.  Yet I think\n&gt; most if not all problems c=\r\nan ultimately be posed within some geometry, even if\n&gt; it is abstract or co=\r\nnceptual.  As Jeff Clune has shown\n&gt; (https://www.msu.edu/~jclune/webfiles/=\r\npublications/Clune-HyperNEATSensitivityT\n&gt; oGeometry.pdf), geometry does no=\r\nt need to be perfect, or necessarily even\n&gt; close to perfect, for HyperNEAT=\r\n to still find some regularities to exploit, so\n&gt; while it may be an imperf=\r\nect art, geometry is still ultimately a useful tool\n&gt; for conveying exploit=\r\nable information about a domain.\n&gt; \n&gt; I hope that provides some insight,\n&gt; =\r\n\n&gt; ken\n&gt; \n&gt; \n&gt; --- In neat@yahoogroups.com, &quot;Anthony Ison&quot; &lt;anthony.ison@..=\r\n.&gt; wrote:\n&gt;&gt; \n&gt;&gt; I&#39;d like to add my vote to some kind of non-research style=\r\n tutorial -\n&gt;&gt; especially towards the HyperNEAT methodology.\n&gt;&gt; \n&gt;&gt;  \n&gt;&gt; \n&gt;=\r\n&gt; Is there somewhere I can get an overview of how the CPPN is actually\n&gt;&gt; u=\r\nsed and what it does?  I&#39;ve looked through a number of papers on the\n&gt;&gt; mai=\r\nn HyperNEAT site, but I feel like I&#39;m missing something.  I have read\n&gt;&gt; th=\r\nat HyperNEAT is the future of NEAT - does this apply to problems where\n&gt;&gt; t=\r\nhere is no useful input geometry?  I understand how NEAT grows a\n&gt;&gt; network=\r\n by adding nodes and connections and overall I love the concept.\n&gt;&gt; It make=\r\ns sense that a learning network can adjust itself to improve its\n&gt;&gt; perform=\r\nance.  What I don&#39;t really understand is how a CPPN is involved\n&gt;&gt; in this =\r\nprocess.\n&gt;&gt; \n&gt;&gt;  \n&gt;&gt; \n&gt;&gt; Is anyone able to give a short overview on what Hy=\r\nperNEAT offers over\n&gt;&gt; NEAT?\n&gt;&gt; \n&gt;&gt;  \n&gt;&gt; \n&gt;&gt; Cheers,\n&gt;&gt; \n&gt;&gt; Anthony\n&gt;&gt; \n&gt;&gt; =\r\n \n&gt;&gt; \n&gt;&gt;  \n&gt;&gt; \n&gt;&gt; From: dkuppitz [mailto:daniel_kuppitz@...]\n&gt;&gt; Sent: Monda=\r\ny, 23 November 2009 9:08 AM\n&gt;&gt; To: neat@yahoogroups.com\n&gt;&gt; Subject: [neat] =\r\nRe: HyperNEAT Tutorial?\n&gt;&gt; \n&gt;&gt;  \n&gt;&gt; \n&gt;&gt;   \n&gt;&gt; \n&gt;&gt; Hello Ken,\n&gt;&gt; \n&gt;&gt; here&#39;s =\r\njust another vote for a tutorial, with the difference that I\n&gt;&gt; would prefe=\r\nr it for HyperSharpNEAT.\n&gt;&gt; \n&gt;&gt; It would be great to see something like a H=\r\nOL (Hands on Labs) where a\n&gt;&gt; new experiment is created from the scratch. P=\r\narameters should be\n&gt;&gt; explained in detail, for example: Which impact has t=\r\nhe parameter\n&gt;&gt; Treshold, which impact has WeightRange, etc.? How are the v=\r\nalues for\n&gt;&gt; each parameter determined, what is taken into account when you=\r\n set the\n&gt;&gt; values? There are so many unanswered questions for those who ar=\r\ne new to\n&gt;&gt; HyperNEAT and I think most people (including me) have a really =\r\ngreat\n&gt;&gt; interest in this topic, but not the time to read (and understand) =\r\nall\n&gt;&gt; the technical papers. So any tutorial should target beginners and\n&gt;&gt;=\r\n explain things that have become self-evident for intermediates.\n&gt;&gt; \n&gt;&gt; I t=\r\nhink one such &quot;official&quot; tutorial that explains every step in detail\n&gt;&gt; sho=\r\nuld be enough, more will surely follow from the growing community.\n&gt;&gt; \n&gt;&gt; C=\r\nheers,\n&gt;&gt; Daniel\n&gt;&gt; \n&gt;&gt; --- In neat@yahoogroups.com &lt;mailto:neat%40yahoogro=\r\nups.com&gt; , &quot;Ken&quot;\n&gt;&gt; &lt;kstanley@&gt; wrote:\n&gt;&gt;&gt; \n&gt;&gt;&gt; \n&gt;&gt;&gt; \n&gt;&gt;&gt; Andrei, which ver=\r\nsion of HyperNEAT are you interested in and what\n&gt;&gt; references have you loo=\r\nked at so far? We can potentially improve the\n&gt;&gt; documentation based on you=\r\nr comments (and a tutorial is a good idea),\n&gt;&gt; but first I want to understa=\r\nnd which &quot;comment-less examples&quot; you are\n&gt;&gt; referring to.\n&gt;&gt;&gt; \n&gt;&gt;&gt; Note tha=\r\nt several experiments with complete source code are available\n&gt;&gt; in two exi=\r\nsting HyperNEAT releases of which I am aware. These and a\n&gt;&gt; variety of pub=\r\nlications from several groups are linked from the\n&gt;&gt; HyperNEAT Users Page, =\r\nwhich also provides a brief introduction:\n&gt;&gt;&gt; \n&gt;&gt;&gt; http://eplex.cs.ucf.edu/=\r\nhyperNEATpage/HyperNEAT.html\n&gt;&gt;&gt; \n&gt;&gt;&gt; I understand you may have already bee=\r\nn through this site and its\n&gt;&gt; associated software and papers, but I wanted=\r\n to point it out in case you\n&gt;&gt; had not been aware of it.\n&gt;&gt;&gt; \n&gt;&gt;&gt; We want =\r\nto make the algorithm as accessible as possible so your\n&gt;&gt; comments are app=\r\nreciated.\n&gt;&gt;&gt; \n&gt;&gt;&gt; ken\n&gt;&gt;&gt; \n&gt;&gt;&gt; --- In neat@yahoogroups.com &lt;mailto:neat%40=\r\nyahoogroups.com&gt; , &quot;Andrei&quot;\n&gt;&gt; &lt;andrei.rusu@&gt; wrote:\n&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt; Can anyone =\r\nplease recommend some HyperNEAT documentation, a\n&gt;&gt; tutorial, diagram, some=\r\n clue, or anything that does not mean reverse\n&gt;&gt; engineering the comment-le=\r\nss examples ?\n&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt; Thanks! Andrei.\n&gt;&gt;&gt;&gt; \n&gt;&gt;&gt; \n&gt;&gt; \n&gt; \n&gt; \n\n\n\n"}}