{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":467342474,"authorName":"Jeff Clune","from":"Jeff Clune &lt;jeffclune@...&gt;","profile":"jeffreyclune","replyTo":"LIST","senderId":"ca2pbaUE-D837xqAQXxg53Eu5G17EbTi7zPzg7ra4M1AHdMQ8oyz88bwOreqrpx7o07ZlUVKAA5yPh6TTghC7mUZfHOmKp8j-g","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [neat] Re: Models of brains, what should we borrow from biology?","postDate":"1342859156","msgId":5831,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDc0NEJBN0EyLUE2MzAtNEREMC1CRDdDLTcxRDkyMzI0QjhFQkBjb3JuZWxsLmVkdT4=","inReplyToHeader":"PENBK2R1aW1OMXQyek5MT1QyY0FFMUhzZmNtOFF2ZllrTU53Q2dVX2sxNUpTNU1ZTS1nQUBtYWlsLmdtYWlsLmNvbT4=","referencesHeader":"PENBK2R1aW1QWlp3MEJXbjg2Z3hXM2FoSmNVY18yVVhlUkNULWFxMjlpcHpXOWhrUExZQUBtYWlsLmdtYWlsLmNvbT4gPEMyRTE3MjVELTNEMUMtNDE1QS05QkNGLUIyOEVDQjAxQzRBRkBjb3JuZWxsLmVkdT4gPENBK2R1aW1OMXQyek5MT1QyY0FFMUhzZmNtOFF2ZllrTU53Q2dVX2sxNUpTNU1ZTS1nQUBtYWlsLmdtYWlsLmNvbT4="},"prevInTopic":5830,"nextInTopic":5832,"prevInTime":5830,"nextInTime":5832,"topicId":5801,"numMessagesInTopic":16,"msgSnippet":"... No problem. I m actually on my honeymoon in remote jungles of Indonesia right now, so please excuse my delayed response. ... My instincts tell me that you","rawEmail":"Return-Path: &lt;jclune@...&gt;\r\nX-Sender: jclune@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 66822 invoked from network); 21 Jul 2012 10:47:35 -0000\r\nX-Received: from unknown (98.137.34.46)\n  by m9.grp.sp2.yahoo.com with QMQP; 21 Jul 2012 10:47:35 -0000\r\nX-Received: from unknown (HELO mail-pb0-f52.google.com) (209.85.160.52)\n  by mta3.grp.sp2.yahoo.com with SMTP; 21 Jul 2012 10:47:35 -0000\r\nX-Received: by pbbro8 with SMTP id ro8so8370484pbb.11\n        for &lt;neat@yahoogroups.com&gt;; Sat, 21 Jul 2012 03:47:35 -0700 (PDT)\r\nX-Received: by 10.66.75.97 with SMTP id b1mr17794065paw.15.1342867655017;\n        Sat, 21 Jul 2012 03:47:35 -0700 (PDT)\r\nReturn-Path: &lt;jclune@...&gt;\r\nX-Received: from n90.nc0a858.nn ([110.232.87.85])\n        by mx.google.com with ESMTPS id ka5sm5832881pbb.37.2012.07.21.03.47.25\n        (version=TLSv1/SSLv3 cipher=OTHER);\n        Sat, 21 Jul 2012 03:47:34 -0700 (PDT)\r\nContent-Type: text/plain; charset=windows-1252\r\nMime-Version: 1.0 (Apple Message framework v1084)\r\nIn-Reply-To: &lt;CA+duimN1t2zNLOT2cAE1Hsfcm8QvfYkMNwCgU_k15JS5MYM-gA@...&gt;\r\nDate: Sat, 21 Jul 2012 16:25:56 +0800\r\nContent-Transfer-Encoding: quoted-printable\r\nMessage-Id: &lt;744BA7A2-A630-4DD0-BD7C-71D92324B8EB@...&gt;\r\nReferences: &lt;CA+duimPZZw0BWn86gxW3ahJcUc_2UXeRCT-aq29ipzW9hkPLYA@...&gt; &lt;C2E1725D-3D1C-415A-9BCF-B28ECB01C4AF@...&gt; &lt;CA+duimN1t2zNLOT2cAE1Hsfcm8QvfYkMNwCgU_k15JS5MYM-gA@...&gt;\r\nTo: &lt;neat@yahoogroups.com&gt;\r\nX-Mailer: Apple Mail (2.1084)\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Jeff Clune &lt;jeffclune@...&gt;\r\nSubject: Re: [neat] Re: Models of brains, what should we borrow from biology?\r\nX-Yahoo-Group-Post: member; u=467342474; y=tJuwzaKOzBnVwaBht0QA-5TXnTsrnlAG9zrzjsYFZdRmqNKRiIDE\r\nX-Yahoo-Profile: jeffreyclune\r\n\r\n&gt; Thanks for taking the time to write your thoughts. :)\n&gt; \n\nNo problem. I&#39;m=\r\n actually on my honeymoon in remote jungles of Indonesia right now, so plea=\r\nse excuse my delayed response. \n\n&gt; Perhaps I did not describe my intentions=\r\n clearly: I don&#39;t intend to evolve the parameters of the EA, but rather the=\r\n parameters of some neural network components or functions (eq the paramete=\r\nrs for synaptic plasticity update rules).\n&gt; \n\nMy instincts tell me that you=\r\n will run into problems any time there is a tradeoff between exploration an=\r\nd exploitation. If such tradeoffs exist within the parameters you talk abou=\r\nt, which they likely do, you may find worse performance than picking these =\r\nparameters yourself. Note that I am talking about evolving within a run (e.=\r\ng. evolving a different parameter for each organism on its genome during th=\r\ne course of the run). If you fix parameters for an entire run, and then hav=\r\ne an outer loop that is evolving those parameters, that works just fine. Gu=\r\ns Eiben and his students have done some interesting work showing that. In a=\r\nny case, I recommend that you compare your meta-evolution results to experi=\r\nments where you manually or algorithmically select good parameters and fix =\r\nthem instead of evolving them. \n\n&gt; Yes, I&#39;ve been wondering how best to go =\r\nabout testing the usefulness of multiple functional properties in a model. =\r\nI&#39;m aware of Miller and Khan&#39;s work. They certainly did throw just about ev=\r\nerything into the pot. And it was hard to draw many solid conclusions as a =\r\nresult. I think the best approach I&#39;ve come up with so far (and I&#39;m open to=\r\n criticism or other ideas) is to create a model with (computationally tract=\r\nable abstractions of) most or all of the biological phenomena/properties fo=\r\nr which there is evidence of a role in learning, and/or which are computati=\r\nonally cheap, and then perform an ablative study where each property is dis=\r\nabled in turn: if disabling a property reduces the efficacy of the model in=\r\n some learning task or other (in terms of evolvability, quality of solution=\r\ns, or some other metric), then that property is useful (I believe Stanley a=\r\nnd Miikkulainen used this approach with their NEAT algorithm). This way one=\r\n doesn&#39;t have to try every combination of properties, and avoids the proble=\r\nm where one property might only be useful in combination with one or more o=\r\nther properties (which would be a problem if only one property is enabled a=\r\nt a time).\n&gt; \n\nThat is a great strategy. You don&#39;t completely eliminate int=\r\neraction effects (one mechanism may be very helpful, but only in the absenc=\r\ne of other mechanisms), but it is a good first crack at the problem. You ma=\r\ny actually want to look at algorithms that have been developed for SVM feat=\r\nure selection: they have the same problem of figuring out which features ar=\r\ne helpful, and there are often many interaction effects. I believe the best=\r\n current policy involves both adding and subtracting features in a certain =\r\norder, but I forget the details of the algorithm and its name. I could find=\r\n it out for you if you like, or maybe someone else on the list knows more a=\r\nbout this subject. \n\n&gt; Comparing the performance of the new model against o=\r\nther existing models on the same learning tasks would help demonstrate whet=\r\nher the new model as a whole is an improvement or not, although like you sa=\r\ny this doesn&#39;t avoid the issue of &quot;interaction effect[s] with one of the fe=\r\natures in your backdrop&quot;. Perhaps testing against many kinds of learning ta=\r\nsks would help alleviate this effect?\n&gt; \n\nKen Stanley has strong opinions o=\r\nn why comparing different algorithms on one or a few tasks tells us very li=\r\nttle, which he may want to chime in with. I generally agree with him that i=\r\nt is not terribly informative, although I tend to think it is still somewha=\r\nt valuable, while he thinks it is mostly worthless! (Sorry if I am incorrec=\r\ntly paraphrasing you Ken). Ken is right that different algorithms perform v=\r\nery differently on different problems, so a few tests provides too small a =\r\nsample size to learn much. Moreover, every researcher inadvertently knows t=\r\nheir own algorithm much better than what they are comparing against, so the=\r\ny keep tuning their algorithm to the benchmarks being used until they win, =\r\nreducing the value of the comparison. There&#39;s no great alternative, in my o=\r\npinion, so I still do it...but  I increasingly agree with Ken that our time=\r\n as scientists can better be spent on other chores (such as showing the new=\r\n, interesting, properties of our new algorithms...an example being HyperNEA=\r\nT genomes scaling up to very large networks without substantial performance=\r\n drops). \n\n&gt; [I&#39;m wondering if all of this is a little off-topic for this l=\r\nist...]\n&gt; \n\nIt&#39;s not at all off-topic, in my opinion! In fact, I&#39;ve had sim=\r\nilar conversations (e.g. with Ken Stanley and Julian Miller) at many evolut=\r\nionary conferences. \n\nBest of luck, and I look forward to hearing more abou=\r\nt your work as you begin it. \nJeff\n\n\n&gt; Cheers,\n&gt; Oliver\n&gt; \n&gt; On 11 July 201=\r\n2 16:12, Jeff Clune &lt;jeffclune@...&gt; wrote:\n&gt; Hello Oliver,\n&gt; \n&gt; I&#39;m=\r\n much delayed in reading all of this as I have been insanely busy lately, b=\r\nut I have a few thoughts that might help you out:\n&gt; \n&gt; 1) Be careful with m=\r\neta-evolution (evolving the parameters of evolutionary algorithms). It soun=\r\nds good in theory, but can be tricky in practice because evolution is short=\r\n-sighted and conservative, preferring exploitation over exploration, which =\r\ncan be very harmful vis a vis long-term adaptation. Check out my PLoS Compu=\r\ntational Biology paper for a smoking gun on this front (the evolution of mu=\r\ntation rates). You may face the exact same problem if you go down this road=\r\n. [Note, however, that using a divergent search algorithm like novelty sear=\r\nch may allow you to take better advantage of meta-evolution: see Joel and K=\r\nen&#39;s 2012 alife review article on that subject.]\n&gt; \n&gt; 2) Another reason peo=\r\nple do not throw all of the biology into the soup to see what happens is be=\r\ncause scientifically you end up in an impenetrable quagmire where you can&#39;t=\r\n figure out what is going on and you end up not learning much/anything. The=\r\n scientific method demands keeping all else equal, and if you have a lot of=\r\n variables you don&#39;t perfectly understand, it takes years to figure out wha=\r\nt is going on if you are lucky! Even if you keep all else equal, if you are=\r\n doing so against a backdrop that involves a lot of complexity you don&#39;t un=\r\nderstand, any difference you see may be due to an interaction effect with o=\r\nne of the features in your backdrop...and that may invalidate generalizing =\r\nyour result to other backdrops of interest. In my limited experience, I hav=\r\ne found that most new scientists want to throw a million things into their =\r\nmodel--especially biologically motivated phenomena--to see what happens, an=\r\nd as they grow older/more jaded/wiser/more experienced/gun shy/etc. they in=\r\ncreasingly keep things as simple as possible. In fact, a pretty good heuris=\r\ntic for good hypothesis-testing science is to keep things absolutely as sim=\r\nple as possible while allowing the question to be asked. However, that may =\r\nnot be a good heuristic for more exploratory science where you just set out=\r\n and see what you discover.\n&gt; \n&gt; 3) You should check out Julian Miller&#39;s pa=\r\npers on evolving a checkers player (with his student M. Khan, I believe). O=\r\nr, better, email/Skype him (he&#39;s an extremely nice guy and I&#39;m sure he woul=\r\nd be happy to talk to you). He decided in the last few years that he is run=\r\nning out of time as a scientist and has tenure and he has spent years keepi=\r\nng things as simple as possible, and he now just wants to do what he origin=\r\nally wanted to do when he started: throw as much biology in the soup as pos=\r\nsible and see if a golem crawls out. He has incorporated a ton of biologica=\r\nlly inspired low-level mechanisms in evolving neural networks. From what I =\r\nrecall, however, it did become very difficult to figure out which ingredien=\r\nts were essential and exactly what was going on because of all the involved=\r\n complexity. He may have updated results since I last checked in, however. =\r\nSo, you may benefit the actual work that he has done on this front and, mor=\r\ne generally, from his opinions on the general scientific approach you are p=\r\nroposing.\n&gt; \n&gt; I hope that helps. Best of luck, and I look forward to heari=\r\nng what you learn!\n&gt; \n&gt; Best regards,\n&gt; Jeff Clune\n&gt; \n&gt; Postdoctoral Fellow=\r\n\n&gt; Cornell University\n&gt; jeffclune@...\n&gt; jeffclune.com\n&gt; \n&gt; On May 1=\r\n1, 2012, at 6:34 AM, Oliver Coleman wrote:\n&gt; \n&gt; &gt; Hi Ken,\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; Yes, =\r\nI&#39;m pretty sure that not all of the phenomena I listed are important; and t=\r\nhat a good starting point in general is to assume that they are not. I also=\r\n agree with your argument that a lot of the low-level phenomena we see may =\r\nbe a result of implementation with particular physical systems (and I would=\r\n add perhaps as a result of evolutionary happenstance). The CPPN is a parti=\r\ncularly compelling example of significant abstraction of developmental proc=\r\nesses, producing many of the same features of the end result of development=\r\nal processes. One thing it does abstract away, in the context of plastic ne=\r\ntworks, is the effect of external input on the developmental process (which=\r\n may or may not be an issue depending on details of implementation, problem=\r\n domain, etc...).\n&gt; &gt;\n&gt; &gt; Perhaps we could also assume that, rather than so=\r\nme specific set of functions being the only workable set, what matters is h=\r\naving a workable combination of functions, and that there are many possible=\r\n combinations that would work equally well. In this framework we could assu=\r\nme that biological neural networks represent at least a reasonably good com=\r\nbination of low-level functions, and so we could use this combination as a =\r\nguide (but of course this doesn&#39;t answer what functions in this combination=\r\n are actually important, or what things can be abstracted away). Also, some=\r\n combinations may be workable, but are far harder to evolve solutions with,=\r\n or require much larger networks, etc (eg evolving networks incorporating n=\r\neuromodulation of synaptic plasticity can be much easier for some tasks tha=\r\nn for those without this type of neuromodulation).\n&gt; &gt;\n&gt; &gt; I&#39;m intending to=\r\n run some experiments to explore these questions (which phenomena are impor=\r\ntant, acceptable level of abstraction, etc), but of course to try and thoro=\r\nughly explore all of these functions in many combinations would be a massiv=\r\ne undertaking, and is not my main interest, so at some point I will have to=\r\n pick a model and run with it after only a few, hopefully well chosen, expe=\r\nriments... Perhaps one approach is to create flexible parameterised version=\r\ns of these functions, and let evolution determine what combination is right=\r\n (like your approach described in &quot;Evolving adaptive neural networks with a=\r\nnd without adaptive synapses&quot;, but perhaps more flexible and applied to mor=\r\ne functions).\n&gt; &gt;\n&gt; &gt; Do you mind if I post/quote some/all of this discussi=\r\non in the comments of my blog post?\n&gt; &gt;\n&gt; &gt; Cheers,\n&gt; &gt; Oliver\n&gt; &gt;\n&gt; &gt;\n&gt; \n&gt;=\r\n \n&gt; \n&gt; ------------------------------------\n&gt; \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n=\r\n&gt; \n&gt; \n&gt; \n&gt; \n\n\n"}}