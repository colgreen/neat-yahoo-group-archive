{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":283334584,"authorName":"petar_chervenski","from":"&quot;petar_chervenski&quot; &lt;petar_chervenski@...&gt;","profile":"petar_chervenski","replyTo":"LIST","senderId":"C1LzHwrEYE2xPtUg07T5Kk9YmJdPtZX7l9p_2PsqFBF4gjsA5663aHA79zb0owg1LWq9tHjxKaNiX7O0fA47oLjaM8RirT4djseFtvNKLsRG9F5LJpA","spamInfo":{"isSpam":false,"reason":"6"},"subject":"FW: [neat] Re: Machine Learning and the Long View of AI","postDate":"1209595791","msgId":4025,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGZ2YXQyZitvYmtkQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGZ2YWNvcSs4MzMwQGVHcm91cHMuY29tPg=="},"prevInTopic":4023,"nextInTopic":4031,"prevInTime":4024,"nextInTime":4026,"topicId":3955,"numMessagesInTopic":49,"msgSnippet":"Hi there, I remember one thread where we discussed Alife/CPPNs and Ken suggested a filter, that is applied before any individual is born. If is does not pass a","rawEmail":"Return-Path: &lt;petar_chervenski@...&gt;\r\nX-Sender: petar_chervenski@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 74128 invoked from network); 30 Apr 2008 22:49:52 -0000\r\nX-Received: from unknown (66.218.67.97)\n  by m45.grp.scd.yahoo.com with QMQP; 30 Apr 2008 22:49:52 -0000\r\nX-Received: from unknown (HELO n35a.bullet.mail.sp1.yahoo.com) (66.163.168.129)\n  by mta18.grp.scd.yahoo.com with SMTP; 30 Apr 2008 22:49:52 -0000\r\nX-Received: from [216.252.122.218] by n35.bullet.mail.sp1.yahoo.com with NNFMP; 30 Apr 2008 22:49:52 -0000\r\nX-Received: from [66.218.69.1] by t3.bullet.sp1.yahoo.com with NNFMP; 30 Apr 2008 22:49:51 -0000\r\nX-Received: from [66.218.67.197] by t1.bullet.scd.yahoo.com with NNFMP; 30 Apr 2008 22:49:51 -0000\r\nDate: Wed, 30 Apr 2008 22:49:51 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;fvat2f+obkd@...&gt;\r\nIn-Reply-To: &lt;fvacoq+8330@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;petar_chervenski&quot; &lt;petar_chervenski@...&gt;\r\nSubject: FW: [neat] Re: Machine Learning and the Long View of AI\r\nX-Yahoo-Group-Post: member; u=283334584; y=SV2q0vQLzi6ojy5Ig4ftK56zW_Sye-GHIxVUXSgneQLh19uEeYVUURzQqQ\r\nX-Yahoo-Profile: petar_chervenski\r\n\r\nHi there, \n\nI remember one thread where we discussed Alife/CPPNs and Ken \ns=\r\nuggested a filter, that is applied before any individual is born. If \nis do=\r\nes not pass a test, it is not born and a new trial is then made \nand so on.=\r\n This ensures that only valid individuals will appear. I \nbelieve that such=\r\n a filter may be applied to any kind of substrate. \nThis would blow up the =\r\ncomputation time required because most \nindividuals spawned would be bad on=\r\nes but defining and implementing \nsuch a filter ensures that the right thin=\r\ng is going on. \n\nPeter\n\n--- In neat@yahoogroups.com, &quot;Kenneth Stanley&quot; &lt;kst=\r\nanley@...&gt; wrote:\n&gt;\n&gt; Jeff,\n&gt; \n&gt; I know we&#39;ve reached a kind of natural con=\r\nclusion to our discussion,\n&gt; but I think it&#39;s a good opportunity to point o=\r\nut one more \ninteresting\n&gt; issue to think about:  \n&gt; \n&gt; To what extent is i=\r\nt possible to improve in general at being &quot;good \nat\n&gt; discovering and explo=\r\niting regularities?&quot;  I mean, let&#39;s really \nthink\n&gt; about what the hope wou=\r\nld be with that idea.  \n&gt; \n&gt; There is no doubt that an indirect encoding ca=\r\nn *represent*\n&gt; regularities. For example, CPPNs can represent symmetry.  B=\r\nut does\n&gt; being able to represent something equate to being good at \ndiscov=\r\nering\n&gt; it, and if not, what else could make something more &quot;discoverable?&quot;=\r\n\n&gt; \n&gt; What I&#39;m getting at is that I think the issue is fairly deep and\n&gt; co=\r\nmplicated (not to imply that you are implying that it is a simple\n&gt; matter =\r\neither- I just think it&#39;s interesting to discuss these \nissues).\n&gt; \n&gt; If be=\r\ning good at discovering just means that you tend to discover \nthe\n&gt; right r=\r\negularity quickly, then it&#39;s easy to see what kind of\n&gt; regularity a partic=\r\nular encoding will be good at discovering.  For\n&gt; example, CPPNs are quite =\r\ngood at discovering bilateral symmetry (in\n&gt; this sense) because all it nee=\r\nds to do is add a single Gaussian node\n&gt; to the CPPN early in evolution, wh=\r\nich has relatively high \nprobability.\n&gt; \n&gt; Yet does that mean HyperNEAT is =\r\nnot good at discovering things that\n&gt; would take longer to build up, such a=\r\ns five-fold radial symmetry \nwith\n&gt; two of the repetitions slightly differe=\r\nnt than the others? Certainly\n&gt; it *can* represent such a structure, but be=\r\ncause it does not have a\n&gt; hidden node with that exact pattern in it, it wo=\r\nuld have to build it\n&gt; up.  So it seems like the &quot;time to discovery&quot; metric=\r\n is not the full\n&gt; story on how good something is at discovering.  After al=\r\nl, it \n*should*\n&gt; take longer to discover some regularities than others.\n&gt; =\r\n\n&gt; So really what we&#39;re talking about here are biases towards certain\n&gt; typ=\r\nes of regularities appearing sooner or later in the search.  In\n&gt; that view=\r\n, we need to be careful about what we mean by good because \nI\n&gt; am not sure=\r\n if a system that is fast at discovering a particular\n&gt; bizarre sort of reg=\r\nularity really deserves to be called good in\n&gt; general, when 99% of the tim=\r\ne that regularity is going to interfere\n&gt; whenever it is instantiated.\n&gt; \n&gt;=\r\n Yet the problem is even more complicated than that.  In fact, just\n&gt; becau=\r\nse a particular regularity is &quot;discovered&quot; inside the CPPN (or\n&gt; any other =\r\nencoding) does not mean that the fitness function rewards\n&gt; that discovery.=\r\n  Therefore, there is a more fundamental problem.  \nFor\n&gt; example, imagine =\r\ndiscovering a symmetric blob when we are searching\n&gt; for an airplane.  Well=\r\n, blobs don&#39;t fly, but symmetry is the right\n&gt; regularity.  But because it =\r\ncan&#39;t fly, the blob doesn&#39;t get rewarded\n&gt; and just dies out, so what then =\r\ndo we say about the goodness of the\n&gt; discovery?\n&gt; \n&gt; Then we might say, we=\r\nll, it should be smarter about things.  It \nshould\n&gt; somehow make sure it d=\r\noesn&#39;t lose the right regularity when it\n&gt; discovers it.  And in fact there=\r\n are some protections that can be \nput\n&gt; in place to maintain diversity and=\r\n try to help with that issue (like\n&gt; speciation).  But at the end of the da=\r\ny, if you&#39;ve got a blob\n&gt; competing to be an airplane, and it doesn&#39;t get l=\r\nucky enough to make\n&gt; some more mutations on the path to airplane, it&#39;s not=\r\n going to cut \nit,\n&gt; and it is eventually going to die out.\n&gt; \n&gt; So then we=\r\n might say that what we really need is something that \nwould\n&gt; say, &quot;Ah! Ev=\r\nen though I have a useless blob I can see that the\n&gt; discovery of symmetry =\r\nmight be useful and therefore we will hold on \nto\n&gt; it and see what we can =\r\ndo with it.&quot;  Yet that seems way outside the\n&gt; scope of evolutionary comput=\r\nation.  If you could make that kind of\n&gt; deduction (based on an intelligent=\r\n insight), why have evolution at\n&gt; all?  You could just build the thing bas=\r\ned on intellect and \ndeduction.\n&gt;  Or at least you would have a system well=\r\n beyond evolutionary\n&gt; computation alone (perhaps not a bad idea if it&#39;s po=\r\nssible).\n&gt; \n&gt; So the point is just that being good at discovering regularit=\r\nies is \na\n&gt; fairly tricky topic.  It&#39;s easy to set up expectations that can=\r\nnot \nbe\n&gt; met.  No Free Lunch is also leaking through the cracks all over t=\r\nhe\n&gt; place.  What I think is &quot;good&quot; about CPPNs in terms of discovering\n&gt; r=\r\negularities is that we are able to encode the primary types of\n&gt; regulariti=\r\nes into the activation functions, so we know what we&#39;re\n&gt; building out of a=\r\nnd biasing towards, and those regularities are\n&gt; motivated by nature and en=\r\ngineering designs.  Also, we get to forgo\n&gt; the process of developmental un=\r\nfolding, which can be expensive and\n&gt; harder to bias in a desired way.  So =\r\nwhat I&#39;m saying is that part of\n&gt; what&#39;s good about CPPNs is that they can =\r\nbe biased towards different\n&gt; types of patterns relatively easily.  But tha=\r\nt does not mean they\n&gt; necessarily discover any arbitrary pattern quickly. =\r\n No encoding \nwill\n&gt; ever be able to do that, unless it is under the contro=\r\nl of a higher\n&gt; intellect. CPPNs in their present form are based on an assu=\r\nmption\n&gt; about what kinds of regularities are useful.  If the assumptions \n=\r\nturn\n&gt; out wrong, the encoding is relatively flexible enough to be biased\n&gt;=\r\n differently, which is another thing you could say is good about its\n&gt; abil=\r\nity to discover regularities.\n&gt; \n&gt; On the other hand, I think &quot;exploiting&quot; =\r\nregularities (which you also\n&gt; mention) is easier to pin down.  It means, g=\r\niven that you found\n&gt; symmetry, you can now elaborate on that and make some=\r\nthing more\n&gt; complex with even more regularities, but that still respects t=\r\nhe\n&gt; initial symmetric regularity.  In other words, we&#39;re not judging on\n&gt; =\r\nthe regularity being good or bad, just that it can be elaborated.  I\n&gt; beli=\r\neve it is easier to see that CPPNs can do that well, since it is\n&gt; directly=\r\n related to complexification.\n&gt; \n&gt; Anyway, I just thought these would be in=\r\nteresting to consider all \nthe\n&gt; nuances in the discussion of effective pat=\r\ntern discovery.\n&gt; \n&gt; ken\n&gt; \n&gt; \n&gt; --- In neat@yahoogroups.com, Jeff Clune &lt;j=\r\nclune@&gt; wrote:\n&gt; &gt;\n&gt; &gt; Hello Ken-\n&gt; &gt; \n&gt; &gt; You make a very strong defense o=\r\nf your position. I have been \nentirely\n&gt; &gt; persuaded that in the multi-agen=\r\nt paper it was appropriate to use\n&gt; the r(x)\n&gt; &gt; strategy. However, I conti=\r\nnue to think that the issue comes down \nto what\n&gt; &gt; one&#39;s goal is. Your goa=\r\nl here was to evolve sophisticated multi-\nagent\n&gt; &gt; controllers. Given that=\r\n goal, you are right that adding the extra\n&gt; challenge\n&gt; &gt; of figuring out =\r\nwhere one brain begins and ends is not necessary.\n&gt; However,\n&gt; &gt; were one&#39;s=\r\n goal to test how good HyperNEAT is at discovering and\n&gt; exploiting\n&gt; &gt; reg=\r\nularities in the problem space, in order to know how well it \nwill\n&gt; perfor=\r\nm\n&gt; &gt; when it encounters unanticipated regularities, it could be\n&gt; worthwhi=\r\nle to see\n&gt; &gt; how it does without the injected information.\n&gt; &gt; \n&gt; &gt; While =\r\nI do think that evolution is cool, and like to show its\n&gt; prowess, I too\n&gt; =\r\n&gt; am mainly interested in using evolution to produce general AI. My\n&gt; &gt; sup=\r\nposition was that figuring out to what extent our algorithms \nare\n&gt; able to=\r\n\n&gt; &gt; exploit problem regularities may facilitate improvements on that \nfont=\r\n. I\n&gt; &gt; also think such improvements will be necessary steps in the path \nt=\r\no\n&gt; &gt; producing evolutionary algorithms capable of creating general AI.\n&gt; &gt;=\r\n \n&gt; &gt; Nevertheless, I benefited from hearing your perspective and\n&gt; appreci=\r\nate the\n&gt; &gt; exchange. \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; Cheers,\n&gt; &gt; Jeff Clune\n&gt; &gt; \n&gt; &gt; Digita=\r\nl Evolution Lab, Michigan State University\n&gt; &gt; \n&gt; &gt; jclune@\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n=\r\n&gt; &gt; ------ Forwarded Message\n&gt; &gt; &gt; From: Kenneth Stanley &lt;kstanley@&gt;\n&gt; &gt; &gt; =\r\nReply-To: &quot;neat@yahoogroups.com&quot; &lt;neat@yahoogroups.com&gt;\n&gt; &gt; &gt; Date: Tue, 29=\r\n Apr 2008 02:30:13 -0000\n&gt; &gt; &gt; To: &quot;neat@yahoogroups.com&quot; &lt;neat@yahoogroups=\r\n.com&gt;\n&gt; &gt; &gt; Subject: [neat] Re: Machine Learning and the Long View of AI\n&gt; =\r\n&gt; &gt; \n&gt; &gt; &gt; Jeff,\n&gt; &gt; &gt; \n&gt; &gt; &gt; I think it would be interesting to step back =\r\nand look at the\n&gt; &gt; &gt; assumptions that underly your straw man argument.  In=\r\n \nparticular, why\n&gt; &gt; &gt; are we using evolution in the first place?\n&gt; &gt; &gt; \n&gt;=\r\n &gt; &gt; Your straw man implies a world view wherein we are using \nevolution\n&gt; =\r\n&gt; &gt; because we like evolution and want it to succeed.  Under that\n&gt; &gt; &gt; phi=\r\nlosophy, then indeed, creating a super-powerful neural \nnetwork\n&gt; &gt; &gt; (Deep=\r\nNet) by hand and then creating a faux-mutation operator \nthat\n&gt; &gt; &gt; simply =\r\nmakes a neural network turn into DeepNet would be a\n&gt; &gt; &gt; disappointment, b=\r\necause it would mean that evolution didn&#39;t \nreally do\n&gt; &gt; &gt; what we wanted,=\r\n and we would be being disingenuous (and \nunimpressed).\n&gt; &gt; &gt; \n&gt; &gt; &gt; Howeve=\r\nr, my world view is different.  I am not using evolution \nbecause\n&gt; &gt; &gt; I l=\r\nike evolution and want to prove that it is impressive. (Note \nthat I\n&gt; &gt; &gt; =\r\n*do* like evolution, but that&#39;s not the reason I use it.)  \nRather, I\n&gt; &gt; &gt;=\r\n am using evolution because I genuinely believe that full-\nfledged AI\n&gt; &gt; &gt;=\r\n likely *cannot* be constructed by hand, and that evolution is \nthe best\n&gt; =\r\n&gt; &gt; alternative.  If someone went ahead and built a general-AI \nneural\n&gt; &gt; =\r\n&gt; network by hand, it would simply prove me wrong.  But it would \nmean\n&gt; &gt; =\r\n&gt; nothing with respect to how we should go about doing the same \nwith\n&gt; &gt; &gt;=\r\n evolution (which would however be a moot point at that point \nanyway\n&gt; &gt; &gt;=\r\n because why bother when someone figured out a better way?).  So \nas\n&gt; &gt; &gt; =\r\nsoon as someone did what you said, evolution would be thrown out\n&gt; &gt; &gt; anyw=\r\nay, at least in terms of being the best path to general AI.\n&gt; &gt; &gt; \n&gt; &gt; &gt; So=\r\n I am looking at things in a kind of reverse perspective from \nyou.\n&gt; &gt; &gt; T=\r\no me, the point is not to bolster up evolution and show how \npowerful\n&gt; &gt; &gt;=\r\n it is.  Rather, the point is that I believe it *is* powerful and\n&gt; &gt; &gt; the=\r\nrefore I am using it.  If I can do something to boost it \nfurther,\n&gt; &gt; &gt; go=\r\nod.  What else is there to prove?\n&gt; &gt; &gt; \n&gt; &gt; &gt; However, I am a realist and =\r\nI doubt that evolution alone will \nget the\n&gt; &gt; &gt; job done in the long view.=\r\n  It&#39;s just too gigantic a search \nspace and\n&gt; &gt; &gt; the problem is too poorl=\r\ny specified.  Therefore, I think there \nwill be\n&gt; &gt; &gt; a lot of biases and m=\r\nanipulations along the way.\n&gt; &gt; &gt; \n&gt; &gt; &gt; I&#39;m way off speculating about the =\r\nfar off future here, but my \nguess is\n&gt; &gt; &gt; that those manipulations will c=\r\nome mostly at the genetic level \nrather\n&gt; &gt; &gt; than the ANN level.  In other=\r\n words, the kinds of hacks that \nyou are\n&gt; &gt; &gt; talking about (&quot;building blo=\r\ncks&quot; provided a priori) generally \nseem to\n&gt; &gt; &gt; be kind of neural &quot;modules=\r\n&quot; that are built a priori and just \ndumped\n&gt; &gt; &gt; into the network en masse.=\r\n  Those are indeed a bit cringe-\ninducing.\n&gt; &gt; &gt; However, my problem with t=\r\nhem is not that they are cringe-\ninducing.\n&gt; &gt; &gt; Rather, again, I doubt the=\r\ny will really be a big help in the \nlong run.\n&gt; &gt; &gt;  The reason I doubt the=\r\nir utility is because I believe that a \nmassive\n&gt; &gt; &gt; brain needs to be als=\r\no massively interwoven, such that each \ninternal\n&gt; &gt; &gt; area of each part is=\r\n entirely accessible and &quot;speaks the \nlanguage&quot; of\n&gt; &gt; &gt; any other part.  S=\r\nome ad hoc module thrown in the mix, while \nperhaps\n&gt; &gt; &gt; helpful in the sh=\r\nort run, will never be able to fill that role \nbecause\n&gt; &gt; &gt; it was not bui=\r\nlt along with the rest of the infrastructure.  So \nthat&#39;s\n&gt; &gt; &gt; why I&#39;m aga=\r\ninst it: Not because it&#39;s cheating, but because it\n&gt; won&#39;t work.\n&gt; &gt; &gt; \n&gt; &gt;=\r\n &gt; So I think you have to distinguish between that type of hack \nand the\n&gt; =\r\n&gt; &gt; kind of thing where we provide sort of &quot;genetically-engineered&quot;\n&gt; &gt; &gt; i=\r\nnformation, i.e. at the genetic level.  That I do believe will \nbe\n&gt; &gt; &gt; us=\r\neful, and should be exploited, because those are knobs and\n&gt; &gt; &gt; coordinate=\r\n frames upon which a castle can be built.  So \nproviding\n&gt; &gt; &gt; coordinate s=\r\nystems that are useful seems to me likely *long-run*\n&gt; &gt; &gt; useful.  It is n=\r\not the same as telling it how to connect up, \nand the\n&gt; &gt; &gt; substrate that =\r\npops out in the end is going to be as pure as \nany:\n&gt; &gt; &gt; completely ANN th=\r\nrough and through and totally a product of the\n&gt; &gt; &gt; indirect encoding.\n&gt; &gt;=\r\n &gt; \n&gt; &gt; &gt; Finally, I think you are inferring too much about how much prior\n=\r\n&gt; &gt; &gt; information I am advocating based on just Multiagent \nHyperNEAT.  The=\r\nre\n&gt; &gt; &gt; is no animal on earth that has to develop five disconnected \nbrain=\r\ns on\n&gt; &gt; &gt; a single sheet with five compartments.  Expecting evolution to \n=\r\njust\n&gt; &gt; &gt; figure out where one brain begins and the other ends seems to \nm=\r\ne very\n&gt; &gt; &gt; unnatural and bizarre, and also uninteresting.  Statistically\n=\r\n&gt; &gt; &gt; speaking, it is evident that *any* intelligence would take \nlonger to=\r\n\n&gt; &gt; &gt; figure that out and solve the problem on average than one that \nwas\n=\r\n&gt; &gt; &gt; provided such information a priori.  So there&#39;s no surprise in \nthat.=\r\n\n&gt; &gt; &gt; \n&gt; &gt; &gt; So of course HyperNEAT performs worse without knowing the \ndi=\r\nvisions\n&gt; &gt; &gt; between separate brains on a substrate than when it knows the=\r\nm \nup\n&gt; &gt; &gt; front.  That doesn&#39;t imply  that HyperNEAT cannot figure it out=\r\n \non its\n&gt; &gt; &gt; own, or that I think it doesn&#39;t matter if HyperNEAT can find=\r\n\n&gt; &gt; &gt; regularities on its own.  It&#39;s just, it would take a while \nlonger a=\r\nnd\n&gt; &gt; &gt; would be less reliable, so why bother waiting?  The spatial \ndivis=\r\nions\n&gt; &gt; &gt; among the brains is ad hoc (something we simply decided a \nprior=\r\ni by\n&gt; &gt; &gt; fiat) and thus is not the interesting issue.\n&gt; &gt; &gt; \n&gt; &gt; &gt; I don&#39;=\r\nt think it will be the same in a lot of non-multiagent \ntasks\n&gt; &gt; &gt; because=\r\n this unnatural issue of multiple brains and their \npositions\n&gt; &gt; &gt; does no=\r\nt come up, and I do believe that HyperNEAT often does \ndiscover\n&gt; &gt; &gt; regul=\r\narities on its own, and that&#39;s a good thing.\n&gt; &gt; &gt; \n&gt; &gt; &gt; Anyway, the broad=\r\ner point is that I will stick to my strong \nposition:\n&gt; &gt; &gt; I do not believ=\r\ne that finding  middle ground or a sweet spot in \nterms\n&gt; &gt; &gt; of biases and=\r\n constraints is the important issue in the long \nview of\n&gt; &gt; &gt; achieving ge=\r\nneral AI through evolution *unless* you are only \ndoing it\n&gt; &gt; &gt; to prove h=\r\now cool evolution is.  In contrast, I&#39;m using \nevolution\n&gt; &gt; &gt; because I th=\r\nink it is the best hope. The funny thing is that we \nwill\n&gt; &gt; &gt; regardless =\r\nend up agreeing on a lot, because I too don&#39;t like to\n&gt; &gt; &gt; provide big bui=\r\nlding blocks.  But my reason is that they will \nend up\n&gt; &gt; &gt; being incapabl=\r\ne of building a general AI.  So I think a lot of \nthings\n&gt; &gt; &gt; that look ba=\r\nd also won&#39;t work, so someone who is trying to make\n&gt; &gt; &gt; evolution look go=\r\nod will often see me as sharing their \nassumptions.\n&gt; &gt; &gt; \n&gt; &gt; &gt; ken\n&gt; &gt; &gt; =\r\n\n&gt; &gt; &gt; \n&gt; &gt; &gt; --- In neat@yahoogroups.com, Jeff Clune &lt;jclune@&gt; wrote:\n&gt; &gt; =\r\n&gt;&gt; \n&gt; &gt; &gt;&gt; Ken-\n&gt; &gt; &gt;&gt; \n&gt; &gt; &gt;&gt; Thank you for explaining these arguments at =\r\nlength. I always\n&gt; learn a lot\n&gt; &gt; &gt;&gt; when we have these sorts of discussio=\r\nns, and there is no \nexception\n&gt; &gt; &gt; in this\n&gt; &gt; &gt;&gt; case. I agree with most=\r\n of what you write below, especially \nthat\n&gt; &gt; &gt;&gt; constraining/biasing evol=\r\nution is very important. I guess the \nonly\n&gt; &gt; &gt; place we\n&gt; &gt; &gt;&gt; disagree i=\r\ns that I believe there is a middle ground of \nconstraint\n&gt; &gt; &gt; that we\n&gt; &gt; =\r\n&gt;&gt; should shoot for, whereas you seem to feel &#39;the more the \nbetter&#39;.\n&gt; &gt; &gt;=\r\n&gt; \n&gt; &gt; &gt;&gt; You write:\n&gt; &gt; &gt;&gt; \n&gt; &gt; &gt;&gt;&gt;  Therefore, progress is NE should in p=\r\nart be measured with\n&gt; respect to\n&gt; &gt; &gt;&gt;&gt; progress in constraining the prob=\r\nlem to make such a discovery \nmore\n&gt; &gt; &gt;&gt;&gt; likely.  When an NE algorithm is=\r\n improved to allow us to tell \nit\n&gt; more\n&gt; &gt; &gt;&gt;&gt; about the world in which i=\r\nts output will be situated, that is \ngood\n&gt; &gt; &gt;&gt;&gt; news for the long view.  =\r\nIn short, we don&#39;t care at all how NE\n&gt; &gt; &gt;&gt;&gt; produced a brain as long as i=\r\nt really does.\n&gt; &gt; &gt;&gt; \n&gt; &gt; &gt;&gt; This reminds me of something that Hod Lipson =\r\nsays repeatedly.\n&gt; Whenever\n&gt; &gt; &gt;&gt; someone evolves something impressive the=\r\n first question to ask \nis,\n&gt; &gt; &gt; &quot;How big\n&gt; &gt; &gt;&gt; are your building blocks?=\r\n&quot; I am going to provide a straw man \nof your\n&gt; &gt; &gt;&gt; argument. Hopefully the=\r\n fact that I admit that up front will \nmake it\n&gt; &gt; &gt; less\n&gt; &gt; &gt;&gt; objectiona=\r\nble. Imagine that Kasparov and a neural net engineer\n&gt; &gt; &gt; teamed up and\n&gt; =\r\n&gt; &gt;&gt; hand-created a neural  net (call it &#39;DeepNet&#39;)  that played \nchess\n&gt; a=\r\nt a\n&gt; &gt; &gt;&gt; grandmaster level. Now imagine that we create an NE algorithm \nf=\r\nor\n&gt; &gt; &gt; learning\n&gt; &gt; &gt;&gt; chess playing that was otherwise identical to NEAT=\r\n, but had \none extra\n&gt; &gt; &gt;&gt; mutation operator, which was &#39;clear out the cur=\r\nrent phenotype \nand\n&gt; &gt; &gt; replace it\n&gt; &gt; &gt;&gt; with DeepNet&#39;. In this case we =\r\nwould have highly constrained \nthe\n&gt; &gt; &gt; problem to\n&gt; &gt; &gt;&gt; find a good ches=\r\ns playing solution. We would have also \nsuccessfully\n&gt; &gt; &gt; injected\n&gt; &gt; &gt;&gt; =\r\nour a priori knowledge of the problem. However, it would be \nvery\n&gt; &gt; &gt;&gt; un=\r\nimpressive as an accomplishment in the field of evolutionary\n&gt; &gt; &gt; computat=\r\nion.\n&gt; &gt; &gt;&gt; The credit goes to the humans that designed DeepNet, not for \nt=\r\nhe\n&gt; &gt; &gt;&gt; evolutionary algorithm that recreated it.\n&gt; &gt; &gt;&gt; \n&gt; &gt; &gt;&gt; As I sai=\r\nd, this is an unfair caricature of your view. However, \nI\n&gt; &gt; &gt; think it\n&gt; =\r\n&gt; &gt;&gt; might reveal what I have been trying to say. In my mind, the \ngoal\n&gt; i=\r\ns to\n&gt; &gt; &gt;&gt; provide smaller and smaller building blocks because then we \nkn=\r\now\n&gt; it is\n&gt; &gt; &gt;&gt; evolution that is doing the work, and not us. There is a =\r\nsweet \nspot\n&gt; &gt; &gt; in the\n&gt; &gt; &gt;&gt; middle. If we humans don&#39;t do any work in b=\r\niasing the search, \nthen\n&gt; &gt; &gt; evolution\n&gt; &gt; &gt;&gt; will perform terribly. But =\r\nif we provide building blocks that \nare\n&gt; &gt; &gt; too large,\n&gt; &gt; &gt;&gt; then evolut=\r\nion did not really do the heavy lifting. So, as \nopposed\n&gt; &gt; &gt; to saying\n&gt; =\r\n&gt; &gt;&gt; &#39;the more constraint the better,&#39; I think it is interesting to \ntry to=\r\n\n&gt; &gt; &gt;&gt; provide smaller building blocks while still gaining high \nlevels of=\r\n\n&gt; &gt; &gt;&gt; performance. As I have said before, I also think that if we \nmake\n&gt;=\r\n &gt; &gt; progress on\n&gt; &gt; &gt;&gt; this front, the evolutionary algorithm (not its pro=\r\nduct) will \nbe\n&gt; &gt; &gt; more likely\n&gt; &gt; &gt;&gt; to generalize to solving other prob=\r\nlems. The long term goal, of\n&gt; &gt; &gt; course, is\n&gt; &gt; &gt;&gt; to have our algorithms=\r\n solve problems and create things where \nwe\n&gt; either\n&gt; &gt; &gt;&gt; don&#39;t know how =\r\nto solve the problems, or can&#39;t be bothered to \ndo\n&gt; so. For\n&gt; &gt; &gt;&gt; example=\r\n, the NE that produced DeepNet would not do very well at\n&gt; race car\n&gt; &gt; &gt;&gt; =\r\ndriving. But an algorithm that was constrained in a more \nabstract\n&gt; way to=\r\n\n&gt; &gt; &gt;&gt; exploit regularities in its environment might do better on \nboth ca=\r\nr\n&gt; &gt; &gt; racing\n&gt; &gt; &gt;&gt; and chess. \n&gt; &gt; &gt;&gt; \n&gt; &gt; &gt;&gt; I guess I start from the r=\r\necognition that evolution produced \nhumans\n&gt; &gt; &gt; without\n&gt; &gt; &gt;&gt; any bias fr=\r\nom a conscious entity. How it did that is one of \nthe most\n&gt; &gt; &gt;&gt; fascinati=\r\nng and open questions both in our field and in \nbiology. We\n&gt; &gt; &gt; agree\n&gt; &gt;=\r\n &gt;&gt; that trying to emulate ways in which natural evolution did \nthings\n&gt; &gt; =\r\n&gt; like bias\n&gt; &gt; &gt;&gt; itself, and thus allow the evolution of modularity, is t=\r\nhe way\n&gt; &gt; &gt; forward for\n&gt; &gt; &gt;&gt; our field. HyperNEAT represents such amazin=\r\ng progress because \nit\n&gt; &gt; &gt; employed\n&gt; &gt; &gt;&gt; this strategy. But it strikes =\r\nme that nature was not told a \npriori\n&gt; &gt; &gt; how many\n&gt; &gt; &gt;&gt; leg modules it =\r\nshould make or learn to control. Nor was it told\n&gt; how many\n&gt; &gt; &gt;&gt; neural m=\r\nodules it should create in the brain. It figured that \nstuff\n&gt; &gt; &gt; out on\n&gt;=\r\n &gt; &gt;&gt; its own, and probably performed better as a result because it \ncould\n=\r\n&gt; &gt; &gt; learn to\n&gt; &gt; &gt;&gt; tailor the number of modules it needed to the regular=\r\nity of the\n&gt; &gt; &gt; problems it\n&gt; &gt; &gt;&gt; faced. I guess I don&#39;t think we will ma=\r\nke it very far towards\n&gt; evolving\n&gt; &gt; &gt;&gt; brains that are generally intellig=\r\nent if our evolutionary \nalgorithms\n&gt; &gt; &gt; cannot\n&gt; &gt; &gt;&gt; do likewise. It see=\r\nms that something is majorly lacking if we \nhave\n&gt; &gt; &gt; to tell\n&gt; &gt; &gt;&gt; it ea=\r\nch time what the regularities are in the environment, and\n&gt; how to go\n&gt; &gt; &gt;=\r\n&gt; about exploiting them.\n&gt; &gt; &gt;&gt; \n&gt; &gt; &gt;&gt; Apologies for the straw man argumen=\r\nt. I do think there is a \nlot of\n&gt; &gt; &gt; merit to\n&gt; &gt; &gt;&gt; the general thrust o=\r\nf what you say. I may be overreacting in\n&gt; &gt; &gt; focusing on the\n&gt; &gt; &gt;&gt; extre=\r\nmes\n&gt; &gt; &gt;&gt; \n&gt; &gt; &gt;&gt; \n&gt; &gt; &gt;&gt; \n&gt; &gt; &gt;&gt; Cheers,\n&gt; &gt; &gt;&gt; Jeff Clune\n&gt; &gt; &gt;&gt; \n&gt; &gt; &gt;&gt;=\r\n Digital Evolution Lab, Michigan State University\n&gt; &gt; &gt;&gt; \n&gt; &gt; &gt;&gt; jclune@\n&gt; =\r\n&gt; &gt;&gt; \n&gt; &gt; &gt;&gt; \n&gt; &gt; &gt;&gt; \n&gt; &gt; &gt;&gt; \n&gt; &gt; &gt;&gt;&gt; From: Kenneth Stanley &lt;kstanley@&gt;\n&gt; &gt;=\r\n &gt;&gt;&gt; Reply-To: &quot;neat@yahoogroups.com&quot; &lt;neat@yahoogroups.com&gt;\n&gt; &gt; &gt;&gt;&gt; Date: =\r\nSun, 27 Apr 2008 21:36:33 -0000\n&gt; &gt; &gt;&gt;&gt; To: &quot;neat@yahoogroups.com&quot; &lt;neat@ya=\r\nhoogroups.com&gt;\n&gt; &gt; &gt;&gt;&gt; Subject: [neat] Re: Machine Learning and the Long Vi=\r\new of AI\n&gt; &gt; &gt;&gt;&gt; \n&gt; &gt; &gt;&gt;&gt; --- In neat@yahoogroups.com, &quot;Derek James&quot; &lt;djame=\r\ns@&gt; wrote:\n&gt; &gt; &gt;&gt;&gt; \n&gt; &gt; &gt;&gt;&gt;&gt; \n&gt; &gt; &gt;&gt;&gt;&gt;&gt;  In RL, in contrast, the long view =\r\nis almost the opposite: \nThey\n&gt; &gt; &gt;&gt;&gt; want to\n&gt; &gt; &gt;&gt;&gt;&gt;&gt;  remove all constra=\r\nints and still learn nevertheless.\n&gt; &gt; &gt;&gt;&gt;&gt; \n&gt; &gt; &gt;&gt;&gt;&gt; I&#39;m not sure what you=\r\n mean by this, Ken. Could you elaborate \na\n&gt; &gt; &gt; little?\n&gt; &gt; &gt;&gt;&gt;&gt; \n&gt; &gt; &gt;&gt;&gt; =\r\n\n&gt; &gt; &gt;&gt;&gt; Sure.  I think the problem is that I can&#39;t find a way to \nexplain =\r\nmy\n&gt; &gt; &gt;&gt;&gt; point concisely.  As I try to explain it, it starts taking up\n&gt; =\r\ntoo much\n&gt; &gt; &gt;&gt;&gt; text so I shorten it and then it loses its meaning.  Let m=\r\ne \ngive\n&gt; it a\n&gt; &gt; &gt;&gt;&gt; try again...\n&gt; &gt; &gt;&gt;&gt; \n&gt; &gt; &gt;&gt;&gt; I think the difference=\r\n between the goals of RL and NE is an\n&gt; &gt; &gt;&gt;&gt; interesting topic because the=\r\ny are almost always conflated, as\n&gt; if they\n&gt; &gt; &gt;&gt;&gt; are trying to solve the=\r\n same problem.\n&gt; &gt; &gt;&gt;&gt; \n&gt; &gt; &gt;&gt;&gt; The RL community (e.g. value-function appro=\r\naches) is trying \nto build\n&gt; &gt; &gt;&gt;&gt; something that learns like a natural bra=\r\nin.  They are saying,\n&gt; through\n&gt; &gt; &gt;&gt;&gt; analytic means we can deduce how a =\r\nbrain can learn from sparse\n&gt; &gt; &gt;&gt;&gt; reinforcement and formalize that proces=\r\ns in an algorithm.  The\n&gt; hope, I\n&gt; &gt; &gt;&gt;&gt; would think, is to eventually bui=\r\nld the &quot;general \nintelligence&quot; that\n&gt; &gt; &gt;&gt;&gt; aligns with the holy grail of A=\r\nI.  So each step along the way \nis an\n&gt; &gt; &gt;&gt;&gt; improvement in that general a=\r\nbility.\n&gt; &gt; &gt;&gt;&gt; \n&gt; &gt; &gt;&gt;&gt; So if that is your goal, then the benchmarks you c=\r\nhoose have \nto be\n&gt; &gt; &gt;&gt;&gt; designed to measure progress to that goal.  So wh=\r\nat they need \nto\n&gt; do is\n&gt; &gt; &gt;&gt;&gt; show that their designed intelligence can =\r\nwork largely \nindependently\n&gt; &gt; &gt;&gt;&gt; of a priori &quot;cheats&quot; that provide the m=\r\neat of the solution. \n&gt; Because,\n&gt; &gt; &gt;&gt;&gt; after all, how can it be a general=\r\n intelligence if it needs \nyou to\n&gt; &gt; &gt;&gt;&gt; tell it something that it is supp=\r\nosed to be able to figure \nout?\n&gt;  This\n&gt; &gt; &gt;&gt;&gt; perspective, I believe, is =\r\naligned with Jeff&#39;s view.\n&gt; &gt; &gt;&gt;&gt; \n&gt; &gt; &gt;&gt;&gt; However, NE as a long-term pursu=\r\nit is involved in something\n&gt; different,\n&gt; &gt; &gt;&gt;&gt; even though it can be appl=\r\nied to the same problems.  NE is \nnot an\n&gt; &gt; &gt;&gt;&gt; attempt to formalize how p=\r\neople learn with sparse \nreinforcement.\n&gt; &gt; &gt;&gt;&gt; Rather, it is an attempt to=\r\n formalize how evolution can build \na\n&gt; brain.\n&gt; &gt; &gt;&gt;&gt;  So RL is formalizin=\r\ng the brain itself and NE is formalizing \nhow\n&gt; &gt; &gt;&gt;&gt; evolution succeeds in=\r\n creating a brain.  NE is therefore one \nstep\n&gt; &gt; &gt; removed.\n&gt; &gt; &gt;&gt;&gt; \n&gt; &gt; &gt;=\r\n&gt;&gt; This difference is ultimately a philosophical difference on \nthe best\n&gt; =\r\n&gt; &gt;&gt;&gt; approach to creating a full-blown AI.  The instrumental issue \nis\n&gt; &gt;=\r\n &gt;&gt;&gt; whether you think it&#39;s easier to build it yourself or to \ndesign an\n&gt; =\r\n&gt; &gt;&gt;&gt; algorithm that can build it.  The confusion and hence \nconflation of\n=\r\n&gt; &gt; &gt;&gt;&gt; the two approaches arises in part because they do indeed both \naim =\r\nat\n&gt; &gt; &gt;&gt;&gt; the same long view goal: a general AI.  But they are coming at\n&gt;=\r\n it from\n&gt; &gt; &gt;&gt;&gt; very different angles.\n&gt; &gt; &gt;&gt;&gt; \n&gt; &gt; &gt;&gt;&gt; And because of thi=\r\ns stark difference, the *metric* of progress\n&gt; should\n&gt; &gt; &gt;&gt;&gt; be quite diff=\r\nerent.  We cannot measure our progress in \nbuilding a\n&gt; &gt; &gt;&gt;&gt; general intel=\r\nligence directly in the same way that we measure \nour\n&gt; &gt; &gt;&gt;&gt; progress in c=\r\nreating an evolutionary algorithm that itself \nwill\n&gt; &gt; &gt;&gt;&gt; someday output =\r\none.\n&gt; &gt; &gt;&gt;&gt; \n&gt; &gt; &gt;&gt;&gt; This distinction is potentially subtle and confusing =\r\nso let me\n&gt; try to\n&gt; &gt; &gt;&gt;&gt; make it clearer:  Human brains aren&#39;t designed t=\r\no build yet \nmore\n&gt; human\n&gt; &gt; &gt;&gt;&gt; brains.  We are good at a lot of things, =\r\nand we learn \ngenerally, but\n&gt; &gt; &gt;&gt;&gt; we do not build 100-trillion part devi=\r\nces that are more \ncomplex than\n&gt; &gt; &gt;&gt;&gt; any known object in the universe.  =\r\nI&#39;m not saying we won&#39;t \never be\n&gt; &gt; &gt;&gt;&gt; able to do it, but if you want to =\r\nsimulate a human brain, \nyour first\n&gt; &gt; &gt;&gt;&gt; thought would not be that it ne=\r\neds to be capable of designing \nyet\n&gt; &gt; &gt;&gt;&gt; another brain by itself.  Your =\r\nfirst thought is about things \nlike\n&gt; &gt; &gt;&gt;&gt; object recognition or pursuit a=\r\nnd evasion.\n&gt; &gt; &gt;&gt;&gt; \n&gt; &gt; &gt;&gt;&gt; In contrast, building brains is exactly what n=\r\natural \nevolution did,\n&gt; &gt; &gt;&gt;&gt; and it did it quite well.  Natural evolution=\r\n does not perform \nobject\n&gt; &gt; &gt;&gt;&gt; recognition; it does not communicate with=\r\n language; it does \nnot run\n&gt; &gt; &gt;&gt;&gt; away from predators or hunt for prey.  =\r\nYet it does build \nbrains that\n&gt; &gt; &gt;&gt;&gt; themselves do those things.  And tha=\r\nt is the aspect of it we \nwish to\n&gt; &gt; &gt;&gt;&gt; harness- a very specific niche ki=\r\nnd of skill (though radically\n&gt; &gt; &gt;&gt;&gt; impressive)- not a general skill.\n&gt; &gt;=\r\n &gt;&gt;&gt; \n&gt; &gt; &gt;&gt;&gt; So the two pursuits are really quite different.  And \ntherefo=\r\nre they\n&gt; &gt; &gt;&gt;&gt; deserve different metrics to judge their progress with resp=\r\nect\n&gt; to the\n&gt; &gt; &gt;&gt;&gt; long term goal.  That is, unless we conflate them to b=\r\ne the \nsame\n&gt; &gt; &gt;&gt;&gt; thing, which we often do without thinking about it.\n&gt; &gt;=\r\n &gt;&gt;&gt; \n&gt; &gt; &gt;&gt;&gt; For example, we could just say, well, both NE and RL are \nlea=\r\nrning\n&gt; &gt; &gt;&gt;&gt; techniques, and after all, we can apply them to the same\n&gt; pr=\r\noblems, so\n&gt; &gt; &gt;&gt;&gt; why make a big distinction in how we judge them?  Let&#39;s =\r\njust \ncompare\n&gt; &gt; &gt;&gt;&gt; them directly on the same benchmarks and get on with =\r\nit.\n&gt; &gt; &gt;&gt;&gt; \n&gt; &gt; &gt;&gt;&gt; That&#39;s fine for the short-term view, i.e. let&#39;s just i=\r\nmprove \nour\n&gt; &gt; &gt;&gt;&gt; ability to tackle practical problems, but for the long =\r\nview, \nthey\n&gt; &gt; &gt;&gt;&gt; cannot be judged in the same way.  If I improve at my a=\r\nbility \nto\n&gt; &gt; &gt;&gt;&gt; balance on one foot is that a sign that I will be able t=\r\no \nbuild a\n&gt; &gt; &gt;&gt;&gt; brain someday?  If evolution evolves a brain that plays =\r\n\ncheckers, is\n&gt; &gt; &gt;&gt;&gt; that a sign that evolution *itself* is on the road to=\r\n \nperforming\n&gt; &gt; &gt;&gt;&gt; object recognition?  These are totally different pursu=\r\nits.\n&gt; &gt; &gt;&gt;&gt; \n&gt; &gt; &gt;&gt;&gt; So in that context, how should they be judged with re=\r\nspect to \nlong\n&gt; &gt; &gt;&gt;&gt; term goals?  Well, I think RL deserves to be judged =\r\nbased on \nits\n&gt; &gt; &gt;&gt;&gt; increasing ability to learn more generally.  And in t=\r\nhat \nsense,\n&gt; &gt; &gt;&gt;&gt; exactly Jeff&#39;s criteria should apply to it: We should b=\r\ne\n&gt; interested in\n&gt; &gt; &gt;&gt;&gt; whether it &quot;needs&quot; a priori information to learn.=\r\n  In other\n&gt; words, the\n&gt; &gt; &gt;&gt;&gt; less we need to constrain the problem for t=\r\nhe learner, the \nmore\n&gt; &gt; &gt;&gt;&gt; impressed we deserve to be.  That shows progr=\r\ness towards more\n&gt; and more\n&gt; &gt; &gt;&gt;&gt; general AI and ML.\n&gt; &gt; &gt;&gt;&gt; \n&gt; &gt; &gt;&gt;&gt; But=\r\n if evolution is not *itself* supposed to be a general \nlearner\n&gt; &gt; &gt;&gt;&gt; (ra=\r\nther, we just want it to concentrate on one very specific \nskill:\n&gt; &gt; &gt;&gt;&gt; b=\r\nrain building), then those considerations are orthogonal to \nits\n&gt; &gt; &gt;&gt;&gt; gr=\r\neatest promise.  Its promise is to evolve a brain itself, \nand as\n&gt; &gt; &gt;&gt;&gt; s=\r\nuch, neuroevolutionary algorithms deserve to be judged on our\n&gt; ability\n&gt; &gt;=\r\n &gt;&gt;&gt; to *constrain* the problem so that they can accomplish \nexactly that.\n=\r\n&gt; &gt; &gt;&gt;&gt; In other words, the problem NE *algorithms* face is leaps and \nboun=\r\nds\n&gt; &gt; &gt;&gt;&gt; beyond what RL algorithms face.  RL algorithms just need to be\n&gt;=\r\n able to\n&gt; &gt; &gt;&gt;&gt; do as well as brains; NE has to be able to discover brains=\r\n\n&gt; themselves.\n&gt; &gt; &gt;&gt;&gt;  Therefore, progress is NE should in part be measure=\r\nd with\n&gt; respect to\n&gt; &gt; &gt;&gt;&gt; progress in constraining the problem to make su=\r\nch a discovery \nmore\n&gt; &gt; &gt;&gt;&gt; likely.  When an NE algorithm is improved to a=\r\nllow us to tell \nit\n&gt; more\n&gt; &gt; &gt;&gt;&gt; about the world in which its output will=\r\n be situated, that is \ngood\n&gt; &gt; &gt;&gt;&gt; news for the long view.  In short, we d=\r\non&#39;t care at all how NE\n&gt; &gt; &gt;&gt;&gt; produced a brain as long as it really does.=\r\n  Will anyone\n&gt; complain if a\n&gt; &gt; &gt;&gt;&gt; human brain pops out of a system that=\r\n was a priori given the \nconcept\n&gt; &gt; &gt;&gt;&gt; of symmetry?  Rather, we should be=\r\n glad that such a priori\n&gt; context was\n&gt; &gt; &gt;&gt;&gt; possible to provide in the f=\r\nirst place, because it may have\n&gt; saved us a\n&gt; &gt; &gt;&gt;&gt; year of wasted computa=\r\ntion in figuring it out needlessly.\n&gt; &gt; &gt;&gt;&gt; \n&gt; &gt; &gt;&gt;&gt; This distinction is al=\r\nmost completely ignored when NE and RL \nare\n&gt; &gt; &gt;&gt;&gt; compared directly.  The=\r\nrefore, the implications of any such\n&gt; comparison\n&gt; &gt; &gt;&gt;&gt; are fuzzy and lac=\r\nking context with respect to the long view.  \nI\n&gt; am not\n&gt; &gt; &gt;&gt;&gt; sure if I =\r\nshould care or not if RL solves something better \nthan\n&gt; NE, or\n&gt; &gt; &gt;&gt;&gt; vic=\r\ne versa, because the author doesn&#39;t explain how the result \naligns\n&gt; &gt; &gt;&gt;&gt; =\r\nwith the long-term goals of the fields.  Long term goals seem \nlike\n&gt; &gt; &gt;&gt;&gt;=\r\n unwelcome guests these days in AI, which is why I probably \nwon&#39;t be\n&gt; &gt; &gt;=\r\n&gt;&gt; writing about any of this in a publication any time soon.\n&gt; &gt; &gt;&gt;&gt; \n&gt; &gt; &gt;=\r\n&gt;&gt; ...\n&gt; &gt; &gt;&gt;&gt; \n&gt; &gt; &gt;&gt;&gt; So Derek what you are saying about NE being good at=\r\n &quot;hard-\nwired&quot;\n&gt; &gt; &gt;&gt;&gt; solutions and RL being appropriate for ontogenetic l=\r\nifetime\n&gt; learning,\n&gt; &gt; &gt;&gt;&gt; while true, is not what I think of as the prima=\r\nry long-view \nissue.\n&gt; &gt; &gt;&gt;&gt; \n&gt; &gt; &gt;&gt;&gt; In the long view, NE will be used to =\r\nevolve structures that \ndo learn\n&gt; &gt; &gt;&gt;&gt; over their lifetime, i.e. not hard=\r\nwired at all.  The only \nreason\n&gt; that\n&gt; &gt; &gt;&gt;&gt; it tends to be used to evolv=\r\ne hardwired solutions today is\n&gt; because we\n&gt; &gt; &gt;&gt;&gt; are trying to get a foo=\r\nthold on how to evolve certain types of\n&gt; complex\n&gt; &gt; &gt;&gt;&gt; structures.   Onc=\r\ne we get very good at it, focus will \nnaturally\n&gt; shift\n&gt; &gt; &gt;&gt;&gt; to evolving=\r\n dynamic brains (and of course there is already \nwork\n&gt; along\n&gt; &gt; &gt;&gt;&gt; these=\r\n lines today, much from Floreano).  I do not even think \nthat we\n&gt; &gt; &gt;&gt;&gt; wi=\r\nll need to include stock learning algorithms like Hebbian\n&gt; learning.\n&gt; &gt; &gt;=\r\n&gt;&gt;  When we achieve our long-term goals, those *themselves* will \nbe\n&gt; left=\r\n\n&gt; &gt; &gt;&gt;&gt; up to evolution because after all there may be something even\n&gt; be=\r\ntter.\n&gt; &gt; &gt;&gt;&gt;  \n&gt; &gt; &gt;&gt;&gt;&gt;&gt; My aim is to design an\n&gt; &gt; &gt;&gt;&gt;&gt;&gt;  algorithm that =\r\nwill output a brain, not to design the brain\n&gt; itself.\n&gt; &gt; &gt;&gt;&gt;&gt; \n&gt; &gt; &gt;&gt;&gt;&gt; B=\r\nut what kind of brain are you wanting to output?\n&gt; &gt; &gt;&gt;&gt;&gt; \n&gt; &gt; &gt;&gt;&gt; \n&gt; &gt; &gt;&gt;&gt;=\r\n Note that I&#39;m speaking purely about the long view for these\n&gt; different\n&gt; =\r\n&gt; &gt;&gt;&gt; fields here.  Of course on a day-to-day basis I am not solely\n&gt; focus=\r\ned\n&gt; &gt; &gt;&gt;&gt; on what will happen 100 years from now.  On a practical day-\nto-=\r\nday\n&gt; &gt; &gt;&gt;&gt; basis, of course I want to make NE better capable to tackle \npr=\r\noblems\n&gt; &gt; &gt;&gt;&gt; that e.g. RL tackles.  So in the short-term context, I just =\r\n\nwant to\n&gt; &gt; &gt;&gt;&gt; output something that works for the problem at hand.\n&gt; &gt; &gt;=\r\n&gt;&gt; \n&gt; &gt; &gt;&gt;&gt; But in the long view, which we were talking about, I think the\n=\r\n&gt; &gt; &gt;&gt;&gt; ultimate goal would be to output a full-fledged adaptive \nsystem wi=\r\nth\n&gt; &gt; &gt;&gt;&gt; astronomical complexity and the power and subtlety of human\n&gt; re=\r\nasoning.\n&gt; &gt; &gt;&gt;&gt;  On that path, constraint is the only hope, unless you wan=\r\nt \nto wait\n&gt; &gt; &gt;&gt;&gt; three billion years and just hope in the meantime that t=\r\nhe \ninitial\n&gt; &gt; &gt;&gt;&gt; conditions were set up correctly.  Therefore, demonstra=\r\ntions \nof the\n&gt; &gt; &gt;&gt;&gt; power of constraint deserve to be judged as evidence =\r\nof the\n&gt; promise of\n&gt; &gt; &gt;&gt;&gt; and progress towards the long term goal in NE.\n=\r\n&gt; &gt; &gt;&gt;&gt; \n&gt; &gt; &gt;&gt;&gt; ken\n&gt; &gt; &gt;&gt;&gt; \n&gt; &gt; &gt;&gt;&gt; \n&gt; &gt; &gt;&gt; \n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; \n&gt; &gt; -----=\r\n- End of Forwarded Message\n&gt; &gt;\n&gt;\n\n\n\n"}}