{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"3On1sZ38u0f7jvsY6kvve7msVDZ3Hv3DzCCKGBj8_SmTtzgyfrZDrIyzEyL338rlUEUhBKwk8DocdigqUu-PK0ln_G31oO2RSfZtcqDZiyyS","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: A Few Thoughts on HyperNEAT","postDate":"1177905652","msgId":3219,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGYxM3BsbCs5YWxsQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGYxM25rbCtoMm50QGVHcm91cHMuY29tPg=="},"prevInTopic":3218,"nextInTopic":3220,"prevInTime":3218,"nextInTime":3220,"topicId":3214,"numMessagesInTopic":27,"msgSnippet":"Sorry you felt frustrated, Andy.  I didn t intend to avoid that issue; I just had felt you did a good job explaining your position and didn t have a lot to","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 902 invoked from network); 30 Apr 2007 04:02:31 -0000\r\nReceived: from unknown (66.218.67.33)\n  by m45.grp.scd.yahoo.com with QMQP; 30 Apr 2007 04:02:31 -0000\r\nReceived: from unknown (HELO n31.bullet.scd.yahoo.com) (66.94.237.25)\n  by mta7.grp.scd.yahoo.com with SMTP; 30 Apr 2007 04:02:31 -0000\r\nReceived: from [209.73.164.83] by n31.bullet.scd.yahoo.com with NNFMP; 30 Apr 2007 04:00:55 -0000\r\nReceived: from [66.218.66.87] by t7.bullet.scd.yahoo.com with NNFMP; 30 Apr 2007 04:00:55 -0000\r\nDate: Mon, 30 Apr 2007 04:00:52 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;f13pll+9all@...&gt;\r\nIn-Reply-To: &lt;f13nkl+h2nt@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: Re: A Few Thoughts on HyperNEAT\r\nX-Yahoo-Group-Post: member; u=54567749; y=s-yHibDgufHVJmGUOAtw03GJv1HYamKHKz93EwjqecOc3yDHbnDR\r\nX-Yahoo-Profile: kenstanley01\r\n\r\nSorry you felt frustrated, Andy.  I didn&#39;t intend to avoid that \nissue; I j=\r\nust had felt you did a good job explaining your position \nand didn&#39;t have a=\r\n lot to add.\n\nI agree that special hidden node activation functions that \nc=\r\nontain &quot;multiple unique inputs&quot; could be useful.  I would not go as \nfar as=\r\n saying they are necessary, but certainly it&#39;s an interesting \navenue for r=\r\nesearch.  Some special transformations such as rotations \nor translations m=\r\night employ nodes like that, and could be quite \nuseful for some problems.\n=\r\n\nThere is always a trade-off between adding new operators and blowing \nup t=\r\nhe search space.  The way these trade against each other depends \non the pr=\r\noblem.  Generally I would prefer a small, well-selected set \nof functions t=\r\nhat are well-motivated.  Gaussians, for example, \nnaturally encode symmetry=\r\n, so they make sense to include.  At the \nsame time, there is some number a=\r\nfter which throwing too many \nfunctions into the mix is overkill, since aft=\r\ner all we know that \nsigmoids alone can approximate any function.  Other fu=\r\nnctions \nintroduce a further bias, so the aim should not be to throw in as =\r\n\nmany functions as possible, but rather to throw in ones that offer \nthe mo=\r\nst reasonable bias.  As long as that is the aim, we can begin \nto ask for a=\r\nny proposed function what kind of bias it adds that might \nbe useful.\n\nTher=\r\ne isn&#39;t any particular reason why public work has not addressed \nyour conce=\r\nrns.  The real problem is just that HyperNEAT has opened up \nso many potent=\r\nial avenues for exploration that at least our group \nalone can&#39;t cover all =\r\nof them at once.  HyperNEAT has only been \ninvented in the last year, so it=\r\n is going to take some time to \nexhaust the possibilities.  \n\nIn any case, =\r\nI do think your suggested research direction deserves a \nlook.\n\nken\n\n\n\n\n---=\r\n In neat@yahoogroups.com, &quot;afcarl2&quot; &lt;a.carl@...&gt; wrote:\n&gt;\n&gt; First, it is so=\r\nmewhat frustrating receiving responses which cherry-\n&gt; pick issues raised, =\r\ndoubly so when they appear to be tangents. \nThere \n&gt; is no disparaging inte=\r\nnt regarding whether or not hidden nodes are \n&gt; presently addressed in the =\r\nsubstrate network of hyperNEAT. The \n&gt; fixation on geometry dictating subst=\r\nrate topology seems limiting, \n&gt; though probably motivated by a legitimate =\r\nclass of useful problems \n&gt; ill-suited to conventional NEAT. &quot;Geometry&quot; is =\r\na subset of \nvariables \n&gt; taken together to describe a state, such as coord=\r\ninates plus a time \n&gt; stamp can express a trajectory. There are any number =\r\nof additional \n&gt; descriptors and/or parameters, whether or not expressly re=\r\npresented \n&gt; as inputs, which may be considered &quot;geometry&quot; in a higher orde=\r\nr \n&gt; design space.\n&gt; \n&gt; My primary objective is to address how the employed=\r\n hypercube \n&gt; functional vocabulary effects the search space requirements o=\r\nf \nuseful \n&gt; problems looking for an answer. And the features required to \n=\r\naddress \n&gt; said functional vocabulary. There is a door that appears to be \n=\r\n&gt; unopened, though it is shouting to be cracked. As I observe the \n&gt; public=\r\n work performed, it keeps avoiding the issue, for reasons I \ncan \n&gt; only im=\r\nagine are associated with added complexity, that for \nmultiple \n&gt; reasons, =\r\nnodes need to employ multiple unique inputs for hidden and \n&gt; output nodes,=\r\n each of which maintain independent summation of \n&gt; connection inputs, and =\r\nmultiple unique outputs for sensor and \nhidden \n&gt; nodes. \n&gt; \n&gt; The fundamen=\r\ntal properties of the NEAT approach, combined with the \n&gt; hypercube methodo=\r\nlogy of seeking higher or more global levels of \n&gt; mathematical expression,=\r\n assuming an adequately rich functional \n&gt; vocabulary, is both applicable a=\r\nnd extensible to a much larger set \nof \n&gt; useful problems looking for an an=\r\nswer than are currently being \n&gt; represented for HyperNEAT.\n&gt; \n&gt; That&#39;s the=\r\n point. Please respond to that point.\n&gt; \n&gt; Thanks,\n&gt;    Andy Carl\n&gt; \n&gt; \n&gt; \n=\r\n&gt; --- In neat@yahoogroups.com, &quot;Kenneth Stanley&quot; &lt;kstanley@&gt; wrote:\n&gt; &gt;\n&gt; &gt;=\r\n Oh well, I was trying to avoid getting too technical about No \nFree \n&gt; &gt; L=\r\nunch because I figured that would be veering too far off the \npoint \n&gt; &gt; th=\r\nat people were discussing.  But yes if you want to get into the \n&gt; &gt; detail=\r\ns of course there are a number of considerations.  That&#39;s \nwhy \n&gt; I \n&gt; &gt; sa=\r\nid it &quot;may&quot; be better for evolving very large scale brains.  \n&gt; &gt; The &quot;may&quot;=\r\n hinges on the issues you bring up and others.  However, \n&gt; &gt; even the oppo=\r\nrtunity to be better is an advantage over having no \n&gt; such \n&gt; &gt; opportunit=\r\ny.  The opportunity of course can be squandered with \nthe \n&gt; &gt; wrong a prio=\r\nri knowledge.   Yet part of my point is that it will \n&gt; &gt; often be the case=\r\n that the natural geometry of a task is all you \n&gt; need \n&gt; &gt; to provide a p=\r\nowerful bias (or at least a bias that is better \nthan \n&gt; &gt; nothing), so it =\r\nwill often be possible to seize the opportunity \n&gt; &gt; without a great deal o=\r\nf effort.\n&gt; &gt; \n&gt; &gt; It is true too that NEAT and other neural network algori=\r\nthms do \n&gt; &gt; indeed allow for some inclusion of prior knowledge through the=\r\nir \n&gt; &gt; input/output encoding.  However, note how I phrased my \n&gt; &gt; claim: =\r\n&quot;HyperNEAT is not subject to the No Free Lunch theorem \nwhen \n&gt; &gt; comparing=\r\n to algorithms that do not allow injecting such a priori \n&gt; &gt; knowledge.&quot;  =\r\nThat is, among algorithms that allow you to decide \non \n&gt; an \n&gt; &gt; input enc=\r\noding in the traditional way, the provision of such \n&gt; encoding \n&gt; &gt; does n=\r\not give one algorithm a leg up over another since they all \n&gt; &gt; allow for s=\r\nuch knowledge to be included.  HyperNEAT, on the other \n&gt; &gt; hand, allows a =\r\nnew kind of knowledge (i.e. geometry) to be \nincluded \n&gt; &gt; and therefore do=\r\nes have a potential leg up on that class of \n&gt; &gt; algorithms.  Of course it =\r\ndepends on how well the user takes \n&gt; &gt; advantage of the opportunity, but t=\r\nhe opportunity is now there.  \n&gt; This \n&gt; &gt; fact does indeed mean that state=\r\nments about HyperNEAT vs. other \n&gt; &gt; neuroevolution (or even machine learni=\r\nng) algorithms can cite an \n&gt; &gt; opportunity to genuinely be better on avera=\r\nge, which in effect \n&gt; brings \n&gt; &gt; it outside NFL in one particular sense. =\r\n \n&gt; &gt; \n&gt; &gt; In a way, this claim is nothing surprising, because it only says=\r\n \n&gt; that \n&gt; &gt; the algorithm is biased to a certain type of problem, which i=\r\ns \n&gt; &gt; exactly what Wolpert said algorithms need to be to have any real \n&gt; =\r\n&gt; advantage.  However, when you consider what type of problem we \nare \n&gt; &gt; =\r\ntalking about, it&#39;s a pretty significant observation.\n&gt; &gt; \n&gt; &gt; ken\n&gt; &gt; \n&gt; &gt;=\r\n \n&gt; &gt; --- In neat@yahoogroups.com, Joseph Reisinger &lt;joeraii@&gt; wrote:\n&gt; &gt; &gt;=\r\n\n&gt; &gt; &gt; &gt; While it may be viewed as a weakness that the user must \ndecide \n&gt;=\r\n the\n&gt; &gt; &gt; &gt; node layout, my view is that it is actually quite a bonus, \n&gt; =\r\n&gt; because it\n&gt; &gt; &gt; &gt; means we have an opportuntiy to inject intuitive \nrela=\r\ntionships \n&gt; &gt; into\n&gt; &gt; &gt; &gt; the learning process from the get-go.  The most=\r\n interesting\n&gt; &gt; &gt; &gt; consequence of this ability is that HyperNEAT is not s=\r\nubject \nto \n&gt; &gt; the\n&gt; &gt; &gt; &gt; No Free Lunch theorem when comparing to algorit=\r\nhms that do \nnot \n&gt; &gt; allow\n&gt; &gt; &gt; &gt; injecting such a priori knowledge, whic=\r\nh justifies the \n&gt; expectation\n&gt; &gt; &gt; &gt; that HyperNEAT actually may be &quot;bett=\r\ner&quot; for evolving very-\nlarge-\n&gt; &gt; scale\n&gt; &gt; &gt; &gt; brains.\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; No =\r\nFree Lunch is a theorem showing that no single black-box \n&gt; search\n&gt; &gt; &gt; &gt; =\r\nmethod can be better than any other when averaged over all \n&gt; &gt; possible\n&gt; =\r\n&gt; &gt; &gt; problems.  However, HyperNEAT escapes this trap because it is \nno\n&gt; &gt;=\r\n &gt; &gt; longer a black box algorithm, thanks to the ability to inject \na\n&gt; &gt; &gt;=\r\n &gt; priori relationships at the start.  In many cases this a \npriori\n&gt; &gt; &gt; &gt;=\r\n knowledge is very simple to include because it follows \ndirectly \n&gt; &gt; from=\r\n\n&gt; &gt; &gt; &gt; the obvious geometry of the task (such as a visual field or \ngame\n=\r\n&gt; &gt; &gt; &gt; board being Cartesian in an self-evident arrangement).  Yet \nthe\n&gt; =\r\n&gt; &gt; &gt; significance of such knowledge (as opposed to not having it) \nis\n&gt; &gt; =\r\n&gt; &gt; priceless.  So having the capacity to arrange sensors and \n&gt; outputs \n&gt;=\r\n &gt; in\n&gt; &gt; &gt; &gt; the way you want is quite a powerful new capability.\n&gt; &gt; &gt; \n&gt;=\r\n &gt; &gt; Hi Ken,\n&gt; &gt; &gt; \n&gt; &gt; &gt; I&#39;d be really careful in how you word this statem=\r\nent. The point \n&gt; you \n&gt; &gt; are \n&gt; &gt; &gt; trying to make, I think, is that for =\r\nany specific problem you&#39;d \n&gt; &gt; want to \n&gt; &gt; &gt; solve with HyperNEAT, the ex=\r\nperimenter will always inject the \n&gt; &gt; &gt; /appropriate/ prior knowledge for =\r\nthat problem, and thus \n&gt; HyperNEAT \n&gt; &gt; will \n&gt; &gt; &gt; /never/ be used in a b=\r\nlack-box setting (e.g. in situations \nwhere \n&gt; no \n&gt; &gt; prior \n&gt; &gt; &gt; knowled=\r\nge is available). In this case, yes, NFL no longer holds \n&gt; &gt; because, \n&gt; &gt;=\r\n &gt; in a sense, you are not applying the same algorithm to each \n&gt; &gt; problem=\r\n. \n&gt; &gt; &gt; Rather, you are peeking at the problem, and then selecting the \n&gt; =\r\nmost \n&gt; &gt; &gt; appropriate settings for HyperNEAT to solve that problem.\n&gt; &gt; &gt;=\r\n \n&gt; &gt; &gt; Since your argument relies on the use of an &quot;oracle&quot; \nexperimenter =\r\n\n&gt; &gt; to know \n&gt; &gt; &gt; the correct properties of the problem to use as prior \n=\r\nknowledge, \n&gt; I \n&gt; &gt; think \n&gt; &gt; &gt; you are being a little disingenuous. What=\r\n if the experimenter \n&gt; &gt; messes up \n&gt; &gt; &gt; and puts in the wrong prior know=\r\nledge? Or, even worse, puts in \n&gt; &gt; almost \n&gt; &gt; &gt; correct prior knowledge t=\r\nhat s/he assumes generalizes from \nother \n&gt; &gt; problems. \n&gt; &gt; &gt; In this case=\r\n, without any other limitations placed on the space \n&gt; of \n&gt; &gt; &gt; problems w=\r\ne may be trying, we are back to NFL land. Based on \nthis \n&gt; I \n&gt; &gt; don&#39;t \n&gt;=\r\n &gt; &gt; think you are justified in saying HyperNEAT is not a black box \n&gt; &gt; al=\r\ngorithm.\n&gt; &gt; &gt; \n&gt; &gt; &gt; Furthermore, I would argue that HyperNEAT&#39;s ability t=\r\no inject \n&gt; prior \n&gt; &gt; &gt; knowledge is not something special to that algorit=\r\nhm. You can \ndo \n&gt; &gt; this in \n&gt; &gt; &gt; NEAT as well, albeit to a less spectacu=\r\nlar extent, by \n&gt; manipulating \n&gt; &gt; the \n&gt; &gt; &gt; input coding. The same probl=\r\nem can be made arbitrarily \ndifficult \n&gt; by \n&gt; &gt; using \n&gt; &gt; &gt; &quot;dumb&quot; input =\r\ncodings, i.e. input codings that you might use if \n&gt; you \n&gt; &gt; didn&#39;t \n&gt; &gt; &gt;=\r\n have any prior knowledge of the problem.  Nate has done some \n&gt; &gt; interest=\r\ning \n&gt; &gt; &gt; work in this area.\n&gt; &gt; &gt; \n&gt; &gt; &gt; In any case, I do agree with you=\r\nr original point: HyperNEAT \n&gt; &gt; probably has a \n&gt; &gt; &gt; bias which makes it =\r\nbetter for large scale problems, given the \n&gt; &gt; appropriate \n&gt; &gt; &gt; prior kn=\r\nowledge, and NEAT probably has a bias towards simpler \n&gt; &gt; problems. I \n&gt; &gt;=\r\n &gt; would leave out the appeal to &quot;injecting prior knowledge&quot; and \n&gt; just \n&gt;=\r\n &gt; say: \n&gt; &gt; &gt; &quot;HyperNEAT is expected to perform better on the class of lar=\r\nge-\n&gt; &gt; scale \n&gt; &gt; &gt; problems with regular structure.&quot; This statement can b=\r\ne made \nNFL-\n&gt; &gt; proof in \n&gt; &gt; &gt; and of itsel0f, simply by ensuring that th=\r\nat class of problems \nis \n&gt; &gt; not \n&gt; &gt; &gt; closed-under-permutation, which it=\r\n probably isn&#39;t: See, for \n&gt; &gt; instance, Marc \n&gt; &gt; &gt; Toussaint&#39;s work on co=\r\nmpressible search landscapes (i.e. \nproblems \n&gt; &gt; with \n&gt; &gt; &gt; regular struc=\r\nture) and how the class of such landscapes is not \n&gt; CUP \n&gt; &gt; and \n&gt; &gt; &gt; th=\r\nus NFL does not apply.\n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; &gt; Ok, phew. Let me recap my argume=\r\nnt just in case I lost anyone: \n&gt; Ken, \n&gt; &gt; I \n&gt; &gt; &gt; think your intuition h=\r\nere about HyperNEAT is perfectly correct, \n&gt; but \n&gt; &gt; your \n&gt; &gt; &gt; justifica=\r\ntion and statement that HyperNEAT does not fall under \n&gt; the \n&gt; &gt; purview \n=\r\n&gt; &gt; &gt; of NFL is tenuous at best.\n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; &gt; -- Joe\n&gt; &gt; &gt; \n&gt; &gt; &gt; --=\r\n \n&gt; &gt; &gt; \n&gt; &gt; &gt; Joseph Reisinger\n&gt; &gt; &gt; http://www.cs.utexas.edu/~joeraii\n&gt; &gt;=\r\n &gt;\n&gt; &gt;\n&gt;\n\n\n\n"}}