{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Ken","from":"&quot;Ken&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"t-Mm57SFZNIEZehiszNKYp9Raof9thX7wnEZqOgIuwgmSG6UlFMlUWrS_dieCfTJBh97U2RKBCuCcJWsmYKiAwlMWoue","spamInfo":{"isSpam":false,"reason":"3"},"subject":"Re: Models of brains, what should we borrow from biology?","postDate":"1343773756","msgId":5840,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGp2OW03cys1cmlrQGVHcm91cHMuY29tPg==","inReplyToHeader":"PEE4REU1Mzc4LTgzQTUtNDVERC04MEM0LTNEQzEwRENFMDE5NUB6dWtrZXNwaWprZXJzLm5sPg=="},"prevInTopic":5837,"nextInTopic":0,"prevInTime":5839,"nextInTime":5841,"topicId":5801,"numMessagesInTopic":16,"msgSnippet":"Hi Evert, thanks for sharing the article.   As you can guess, I agree with a lot of what it says and I liked it.  But my concerns go even farther in the sense","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 48043 invoked from network); 31 Jul 2012 22:29:18 -0000\r\nX-Received: from unknown (98.137.34.44)\n  by m14.grp.sp2.yahoo.com with QMQP; 31 Jul 2012 22:29:18 -0000\r\nX-Received: from unknown (HELO ng12-vm5.bullet.mail.gq1.yahoo.com) (98.136.219.148)\n  by mta1.grp.sp2.yahoo.com with SMTP; 31 Jul 2012 22:29:18 -0000\r\nX-Received: from [98.137.0.82] by ng12.bullet.mail.gq1.yahoo.com with NNFMP; 31 Jul 2012 22:29:18 -0000\r\nX-Received: from [98.137.34.72] by tg2.bullet.mail.gq1.yahoo.com with NNFMP; 31 Jul 2012 22:29:18 -0000\r\nDate: Tue, 31 Jul 2012 22:29:16 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;jv9m7s+5rik@...&gt;\r\nIn-Reply-To: &lt;A8DE5378-83A5-45DD-80C4-3DC10DCE0195@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 2:3:4:0:0\r\nFrom: &quot;Ken&quot; &lt;kstanley@...&gt;\r\nSubject: Re: Models of brains, what should we borrow from biology?\r\nX-Yahoo-Group-Post: member; u=54567749; y=zXUxQukEnGCD78sCVWdEoPtxffPYs0ictU_gI1EkWsRtD2-wYCbu\r\nX-Yahoo-Profile: kenstanley01\r\n\r\n\n\nHi Evert, thanks for sharing the article.   As you can guess, I agree wit=\r\nh a lot of what it says and I liked it.  But my concerns go even farther in=\r\n the sense that this article still voices a concern with the accuracy of co=\r\nmparative results whereas my question is: Even if we could be sure that the=\r\n results of any comparison were accurate, what good would that be if the lo=\r\nng-term potential of a research direction has no correlation with whether i=\r\nt compares well or poorly with some arbitrary alternative method?  In other=\r\n words, if method X performs worse than method Y, method X still might be e=\r\nntirely more innovative and lead to many more new ideas than dead-end metho=\r\nd Y.  So even a lack of accuracy isn&#39;t the deepest issue.  \n\nMy opinion (wh=\r\nich in part echoes the article) is that we spend significantly more effort =\r\nthan we should on trying to prove to each other that our comparisons are ac=\r\ncurate as opposed to spending time on discussing the future potential creat=\r\ned by the ideas behind our methods.  \n\n\nBest,\n\nken\n\n\n--- In neat@yahoogroup=\r\ns.com, Evert Haasdijk &lt;evert@...&gt; wrote:\n&gt;\n&gt; All,\n&gt; \n&gt; An interesting paper=\r\n on the subject of the use(lesness) of performance comparisons in our resea=\r\nrch  was published by Hooker:\n&gt; \n&gt; \n&gt; Hooker J (1995) Testing heuristics: W=\r\ne have it all wrong. Journal of Heuristics 1:33=9642, URL\n&gt; http://dx.doi.o=\r\nrg/10.1007/BF02430364, 10.1007/BF02430364\n&gt; \n&gt; Just in case you hadn&#39;t read=\r\n that yet=85\n&gt; \n&gt; Cheers,\n&gt; \n&gt; Evert\n&gt; \n&gt; \n&gt; On 26 Jul 2012, at 00:15, Oliv=\r\ner Coleman wrote:\n&gt; \n&gt; &gt; \n&gt; &gt; Hi Ken,\n&gt; &gt; \n&gt; &gt; I agree overall that qualita=\r\ntive results are much more interesting and necessary than quantitative resu=\r\nlts at least at this stage of advancement in AI (I&#39;m strongly swayed by you=\r\nr arguments; what have you got to say Jeff?).\n&gt; &gt; \n&gt; &gt; However, perhaps com=\r\nparative performance results can help to provide insight into the character=\r\nistics of one approach versus another and provide useful information to hel=\r\np improve them, for example if two approaches fail on different kinds of ta=\r\nsks perhaps we can look at how they each succeed in different ways in order=\r\n to improve one or both approaches. In the search space of AI algorithms, c=\r\nomparative results can provide information about which algorithms should ha=\r\nve a crossover operator applied to them to produce potentially better ones =\r\n(the apple and orange may combine to produce a new inspirational fruit ;)).=\r\n Perhaps this is too vague to be useful... \n&gt; &gt; Oliver\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; P.S. I=\r\n&#39;ve quoted your comment in a reply on my blog, I&#39;m assuming that your permi=\r\nssion to post your comments on my blog earlier in this discussion applies t=\r\no ongoing replies, let me know if this is not okay...\n&gt; &gt; \n&gt; &gt; On 25 July 2=\r\n012 17:34, Ken &lt;kstanley@...&gt; wrote:\n&gt; &gt;  \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; Hi Jeff and O=\r\nliver, nice discussion and definitely relevant to the group. Jeff mentioned=\r\n my &quot;strong opinions&quot; about algorithm comparisons, so I thought it can&#39;t hu=\r\nrt to follow up on what Jeff said:\n&gt; &gt; \n&gt; &gt; &quot;Ken Stanley has strong opinion=\r\ns on why comparing different\n&gt; &gt; algorithms on one or a few tasks tells us =\r\nvery little, which he may \n&gt; &gt; want to chime in with. I generally agree wit=\r\nh him that it is not \n&gt; &gt; terribly informative, although I tend to think it=\r\n is still somewhat \n&gt; &gt; tvaluable, while he thinks it is mostly worthless! =\r\n(Sorry if I am \n&gt; &gt; tincorrectly paraphrasing you Ken). Ken is right that d=\r\nifferent \n&gt; &gt; algorithms perform very differently on different problems, so=\r\n a few tests provides too small a sample size to learn much. Moreover, ever=\r\ny researcher inadvertently knows their own algorithm much better than what =\r\nthey are comparing against, so they keep tuning their algorithm to the benc=\r\nhmarks being used until they win, reducing the value of the comparison. The=\r\nre&#39;s no great alternative, in my opinion, so I still do it...but I increasi=\r\nngly agree with Ken that our time as scientists can better be spent on othe=\r\nr chores (such as showing the new, interesting, properties of our new algor=\r\nithms...an example being HyperNEAT genomes scaling up to very large network=\r\ns without substantial performance drops).&quot;\n&gt; &gt; \n&gt; &gt; I agree with these conc=\r\nerns but as Jeff hints I&#39;d go farther with it. The problem here is more fun=\r\ndamental than simply that it&#39;s hard to tell which algorithm is &quot;better&quot; fro=\r\nm a few comparisons. The problem is that it&#39;s not even clear what &quot;better&quot; =\r\nmeans no matter how many comparisons there are. Quantitative comparisons im=\r\nply that &quot;better&quot; means that an algorithms scores better on average on some=\r\n performance metric. But for those who are pursuing revolutionary advances =\r\nin AI, I&#39;m skeptical that it really matters which algorithm scores better e=\r\nven across many benchmarks.\n&gt; &gt; \n&gt; &gt; The reason is that to me &quot;better&quot; shou=\r\nld mean &quot;leads to the most new algorithms in the future.&quot; In other words, i=\r\nt has little or nothing to do with performance. &quot;Better&quot; means creating a f=\r\noundation for new ideas and a new research direction. We know it when we se=\r\ne it. We&#39;re talking about primitive AI algorithms here that are about 3 inc=\r\nhes into a 10-million-kilometer marathon to the pinnacle of AI. If you&#39;re l=\r\nooking at two different algorithms then in effect you&#39;re comparing two diff=\r\nerent points in the vast space of all possible algorithms. Given that there=\r\n are probably light years of advances to go in the direction of either one =\r\nof them, why would you cut the path of either one of them off regardless of=\r\n the &quot;results&quot; if both of them are interesting ideas?\n&gt; &gt; \n&gt; &gt; If you were =\r\nrunning an evolutionary algorithm with diversity maintenance of some kind, =\r\nthen how one arbitrary point in the search space compares to another would =\r\nhardly matter. So why do we care about apple-and-oranges comparisons in AI?=\r\n \n&gt; &gt; \n&gt; &gt; I think it has become a convenient way to avoid the sobering rea=\r\nlity that most algorithms don&#39;t have any exciting ideas behind them. So the=\r\n only thing you can do is look at a pointless comparison. For those algorit=\r\nhms that do have interesting ideas behind them, I don&#39;t even need a compari=\r\nson to know they&#39;re interesting, and even if they perform worse than someth=\r\ning else, the last thing I want to do is throw out an interesting idea. Who=\r\n knows where it might lead?\n&gt; &gt; \n&gt; &gt; So yes comparisons are very overrated.=\r\n One type of comparison I do think can be useful once in a while is to comp=\r\nare an algorithm with a variant of itself (which includes ablations). That =\r\ncan give a sense of what a new ingredient adds. But even then, if the idea =\r\nisn&#39;t inspirational, the performance gain won&#39;t matter much in the long run=\r\n. Because in the long run we aren&#39;t interested in performance gains but rat=\r\nher in stepping stones to new frontiers. These things (i.e. performance and=\r\n where an idea leads) are not correlated in any complex search space and th=\r\nerefore we should should not be running the whole field of AI research like=\r\n a naive giant hill-climbing algorithm. The irony here is that the world&#39;s =\r\ngreatest experts in search are doing exactly that at the meta-level (i.e. a=\r\nt the level of how the community searches for new algorithms) by focusing s=\r\no intently on comparative performance results.\n&gt; &gt; \n&gt; &gt; The one other kind =\r\nof performance result I think is useful is when an algorithm does something=\r\n completely unprecedented. Of course, in that case, you don&#39;t need a compar=\r\nison because there&#39;s nothing to compare with. Though that won&#39;t stop tradit=\r\nionalists from clamoring for a comparison anyway.\n&gt; &gt; \n&gt; &gt; ken\n&gt; &gt; \n&gt; &gt; \n&gt; =\r\n&gt; \n&gt; &gt; \n&gt; &gt;\n&gt;\n\n\n\n"}}