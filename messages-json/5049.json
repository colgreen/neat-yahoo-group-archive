{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":211599040,"authorName":"Jeff Clune","from":"Jeff Clune &lt;jclune@...&gt;","profile":"jeffreyclune","replyTo":"LIST","senderId":"K-EHvPuZr4xvMIW1wA91xshxhZ9MCLntH8rWak9zDqGA4ROU-phYAVd186sngD7Nr7nWzkoX8O0Ol1PXd-IXKCqA","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [neat] Does evolving mutation rates work?","postDate":"1263413671","msgId":5049,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PEM3NzM5M0Q3LjJGN0M2JWpjbHVuZUBtc3UuZWR1Pg==","inReplyToHeader":"PDRCNEJEODIwLjgwOTAzMDFAdnQuZWR1Pg=="},"prevInTopic":5048,"nextInTopic":0,"prevInTime":5048,"nextInTime":5050,"topicId":4984,"numMessagesInTopic":16,"msgSnippet":"Hello Wesley- Thanks for the response and the cites. I will check them out. You are right that I was investigating algorithms that evolve the mutation rate","rawEmail":"Return-Path: &lt;jclune@...&gt;\r\nX-Sender: jclune@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 27183 invoked from network); 13 Jan 2010 20:14:58 -0000\r\nX-Received: from unknown (66.196.94.107)\n  by m12.grp.re1.yahoo.com with QMQP; 13 Jan 2010 20:14:58 -0000\r\nX-Received: from unknown (HELO qw-out-1920.google.com) (74.125.92.147)\n  by mta3.grp.re1.yahoo.com with SMTP; 13 Jan 2010 20:14:58 -0000\r\nX-Received: by qw-out-1920.google.com with SMTP id 14so492631qwa.30\n        for &lt;neat@yahoogroups.com&gt;; Wed, 13 Jan 2010 12:14:42 -0800 (PST)\r\nX-Received: by 10.224.106.226 with SMTP id y34mr1540573qao.303.1263413677407;\n        Wed, 13 Jan 2010 12:14:37 -0800 (PST)\r\nReturn-Path: &lt;jclune@...&gt;\r\nX-Received: from ?35.9.42.98? (host-23389.dhcp.egr.msu.edu [35.9.42.98])\n        by mx.google.com with ESMTPS id 23sm6426471iwn.15.2010.01.13.12.14.33\n        (version=TLSv1/SSLv3 cipher=RC4-MD5);\n        Wed, 13 Jan 2010 12:14:35 -0800 (PST)\r\nUser-Agent: Microsoft-Entourage/12.13.0.080930\r\nDate: Wed, 13 Jan 2010 15:14:31 -0500\r\nTo: &quot;neat@yahoogroups.com&quot; &lt;neat@yahoogroups.com&gt;\r\nMessage-ID: &lt;C77393D7.2F7C6%jclune@...&gt;\r\nThread-Topic: [neat] Does evolving mutation rates work?\r\nThread-Index: AcqUjQPjeLm/Ku+wqUGHBAV6A8h9PA==\r\nIn-Reply-To: &lt;4B4BD820.8090301@...&gt;\r\nMime-version: 1.0\r\nContent-type: text/plain;\n\tcharset=&quot;ISO-8859-1&quot;\r\nContent-transfer-encoding: quoted-printable\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Jeff Clune &lt;jclune@...&gt;\r\nSubject: Re: [neat] Does evolving mutation rates work?\r\nX-Yahoo-Group-Post: member; u=211599040; y=bXVbW8n87LNtUWHAZOIRSSp6dW5DCpSFFoOnm5vW4-_y7szyxOih\r\nX-Yahoo-Profile: jeffreyclune\r\n\r\nHello Wesley-\n\nThanks for the response and the cites. I will check them out=\r\n.\n\nYou are right that I was investigating algorithms that evolve the mutati=\r\non\nrate (specifically, in my paper it was the percent chance that each site=\r\n in\nthe genome is mutated).\n\nNevertheless, I cannot think of a reason why t=\r\nhe argument presented in our\npaper (along with the data) would not also app=\r\nly to evolving the standard\ndeviation. Briefly, our argument is that evolut=\r\nion is short-sighted. If a\npopulation is not immediately generating benefic=\r\nial mutations (because the\nfitness landscape is rugged and beneficial mutat=\r\nions are rare), then the\naverage fitness of organisms with a high mutation =\r\nrate is lower than the\naverage fitness of organisms with a low mutation rat=\r\ne. The same should be\ntrue with a larger standard deviation. If the gene va=\r\nlues in an organism are\nlocally optimal (on an local peak), then the farthe=\r\nr you move away from\nthose values, the worse your fitness will be (until yo=\r\nu hit the rare\nbeneficial mutation, double mutation, etc.). So, unless the =\r\nlandscape is\nsuch that beneficial mutations are common, in the short run th=\r\ne average\nfitness of organisms that have smaller standard deviations will b=\r\ne higher\nthan organisms that have larger standard deviations, which means t=\r\nhat\nnatural selection will favor the smaller standard deviation.\n\nThat, int=\r\nuitively, is an argument that I think makes sense and is supported\nby the d=\r\nata in our paper. What do you think? If the papers you cite\ndemonstrate tha=\r\nt evolving mutation rates can work (and I look forward to\nreading them to s=\r\nee if they do), how do you think they escape this argument?\n\nKind regards,\n=\r\nJeff Clune\n\nDigital Evolution Lab, Michigan State University\njclune@...=\r\n\nwww.msu.edu/~jclune\n\n\n\n\n&gt; From: Wesley Tansey &lt;tansey@...&gt;\n&gt; Reply-To: =\r\n&quot;neat@yahoogroups.com&quot; &lt;neat@yahoogroups.com&gt;\n&gt; Date: Mon, 11 Jan 2010 18:0=\r\n2:08 -0800\n&gt; To: &quot;neat@yahoogroups.com&quot; &lt;neat@yahoogroups.com&gt;\n&gt; Subject: R=\r\ne: [neat] Does evolving mutation rates work?\n&gt; \n&gt; Hi Jeff,\n&gt; \n&gt; Below are a=\r\n few references on self-adaptation:\n&gt; \n&gt; Fogel DB, Fogel LJ, and Atmar JW (=\r\n1991) &quot;Meta-Evolutionary Programming,&quot;\n&gt; Proc. of 25th Asilomar Conference =\r\non Signals, Systems & Computers, R.R.\n&gt; Chen (ed.), Pacific Grove, CA, pp. =\r\n540-545.\n&gt; \n&gt; Fogel DB, Fogel GB, and Ohkura K (2001) &quot;Multiple-Vector Self=\r\n-Adaptation\n&gt; in Evolutionary Algorithms,&quot; BioSystems, Vol. 61:2-3, pp. 155=\r\n-162.\n&gt; \n&gt; Saravanan N and Fogel DB (1996) &quot;An Empirical Comparison of Meth=\r\nods for\n&gt; Correlated Mutations under Self-Adaptation,&quot; Evolutionary Program=\r\nming V,\n&gt; L.J. Fogel, P.J. Angeline, T. B=E4ck (eds.), MIT Press, Cambridge=\r\n, MA, pp.\n&gt; 479-485.\n&gt; \n&gt; Angeline PJ, Fogel DB, Fogel LJ (1996) &quot;A Compari=\r\nson of Self-Adaptation\n&gt; Methods for Finite State Machines in a Dynamic Env=\r\nironment,&quot;\n&gt; Evolutionary Programming V, L.J. Fogel, P.J. Angeline, T. B=E4=\r\nck (eds.),\n&gt; MIT Press, Cambridge, MA, pp. 441-449.\n&gt; \n&gt; Chellapilla K and =\r\nFogel DB (1997) &quot;Exploring Self-Adaptive Methods to\n&gt; Improve the Efficienc=\r\ny of Generating Approximate Solutions to Traveling\n&gt; Salesman Problems Usin=\r\ng Evolutionary Programming,&quot; Evolutionary\n&gt; Programming VI, P.J. Angeline, =\r\nR.G. Reynolds, J.R. McDonnell, and R.\n&gt; Eberhart (eds.), Springer, Berlin, =\r\npp. 361-371.\n&gt; \n&gt; Personally, I&#39;ve found that self-adaptation works better =\r\non just about\n&gt; every fitness landscape where I&#39;ve explicitly compared the =\r\ntwo.\n&gt; \n&gt; Regarding your paper, I think you&#39;re doing a different kind of SA=\r\n than\n&gt; my version. If I understand correctly (and please correct me if I m=\r\nissed\n&gt; something), you&#39;re taking bit strings and evolving the probability =\r\nthat\n&gt; a child will be mutated. This seems like it would converge the\n&gt; pop=\r\nulation too quickly, and thus your results make sense.\n&gt; \n&gt; The type of SA =\r\nthat I typically work with is evolving the standard\n&gt; deviation for Gaussia=\r\nn mutation. All child parameters are mutated. In\n&gt; pseudo code, I create ch=\r\nildren as follows:\n&gt; \n&gt; For each parent, p\n&gt; Create a copy of p and store i=\r\nt in child c\n&gt; For each gene, g in c,\n&gt; g.Sigma =3D Max(0, g.Sigma * Math.E=\r\nxp(GaussianMutation(0,1)))\n&gt; g.Value =3D GaussianMutation(g.Value, g.Sigma)=\r\n\n&gt; \n&gt; I think if you try this approach on some of the fitness functions in =\r\nYao\n&gt; et al&#39;s &quot;Evolutionary programming made faster&quot; (\n&gt; http://citeseerx.i=\r\nst.psu.edu/viewdoc/download?doi=3D10.1.1.45.1830&rep=3Drep1&type\n&gt; =3Dpdf \n=\r\n&gt; ), you&#39;ll be pleasantly surprised. :)\n&gt; \n&gt; Sincerely,\n&gt; Wesley\n&gt; \n&gt; Jeff =\r\nClune wrote:\n&gt;&gt; \n&gt;&gt; Hello Wesley-\n&gt;&gt; \n&gt;&gt; Sorry for the delayed reply, but I=\r\n am catching up on this list and saw\n&gt;&gt; your\n&gt;&gt; comment (below).\n&gt;&gt; \n&gt;&gt; Out=\r\n of curiosity, do you have any evidence that self-adaptation of (i.e.,\n&gt;&gt; e=\r\nvolving) mutation rates works? In the below paper we found that evolution\n&gt;=\r\n&gt; is not able to optimize its mutation rate for long term adaptation.\n&gt;&gt; In=\r\nstead,\n&gt;&gt; it just tries to lower the mutation rate as much as possible.\n&gt;&gt; =\r\n\n&gt;&gt; That may help evolution home-in on precise answers, in the sense of\n&gt;&gt; =\r\ngetting\n&gt;&gt; to the very top of a current (and usually suboptimal) fitness pe=\r\nak, but it\n&gt;&gt; greatly harms the discovery of other, better fitness peaks. I=\r\nn other\n&gt;&gt; words,\n&gt;&gt; self-adaptation was found in our paper to be extremely=\r\n greedy and\n&gt;&gt; effectively eliminate meaningful adaptation.\n&gt;&gt; \n&gt;&gt; Here is =\r\nthe cite and link:\n&gt;&gt; \n&gt;&gt; Clune J, Misevic D, Ofria C, Lenski RE, Elena SF,=\r\n and Sanju=E1n, R (2008)\n&gt;&gt; Natural selection fails to optimize mutation ra=\r\ntes for long-term\n&gt;&gt; adaptation\n&gt;&gt; on rugged fitness landscapes.\n&gt;&gt; \n&gt;&gt; PLo=\r\nS Computational Biology 4(9): e1000187.\n&gt;&gt; \n&gt;&gt; https://www.msu.edu/~jclune/=\r\nwebfiles/publications/Clune-EvolvingMutationRate\n&gt;&gt; &lt;https://www.msu.edu/%7=\r\nEjclune/webfiles/publications/Clune-EvolvingMutationRa\n&gt;&gt; te&gt;\n&gt;&gt; s-PLoSCB-2=\r\n008.pdf\n&gt;&gt; \n&gt;&gt; I know the Evolutionary Strategies people like self-adaptati=\r\non, but I have\n&gt;&gt; never seen evidence that it works when evolution is contr=\r\nolling the\n&gt;&gt; mutation\n&gt;&gt; rate. Note that evolution is not in control in sc=\r\nhemes like the 1/5th\n&gt;&gt; rule,\n&gt;&gt; CMA, EDA, etc.\n&gt;&gt; \n&gt;&gt; Cheers,\n&gt;&gt; Jeff\n&gt;&gt; \n=\r\n&gt;&gt;&gt; This is particularly true if networks are self-adaptive, though I don&#39;t=\r\n\n&gt;&gt;&gt; think any NEAT framework supports self-adaptation. Not to hijack the\n&gt;=\r\n&gt; thread,\n&gt;&gt;&gt; but I don&#39;t see any reason why it couldn&#39;t be incorporated. A=\r\nll that&#39;s\n&gt;&gt;&gt; necessary is for every gene to have its own sigma to be used =\r\nin the\n&gt;&gt; mutation\n&gt;&gt;&gt; operator; the custom sigma is mutated as (C# code):\n=\r\n&gt;&gt;&gt; \n&gt;&gt;&gt; Sigma =3D Math.Max(0, Sigma * Math.Exp(GaussianMutation(0,1)));\n&gt;&gt;=\r\n&gt; //GaussianMutation(mean, stdev)\n&gt;&gt;&gt; \n&gt;&gt;&gt; Self-adaptation helps evolution =\r\nhome-in on precise answers better\n&gt;&gt; than using\n&gt;&gt;&gt; a fixed sigma for mutat=\r\nion.\n&gt;&gt;&gt; \n&gt;&gt;&gt; Wesley\n&gt;&gt;&gt; \n&gt;&gt;&gt; On Tue, Dec 8, 2009 at 1:58 PM, Colin Green\n&gt;=\r\n&gt;&gt; &lt;colin.green1@...\n&gt;&gt; &lt;mailto:colin.green1%40googlemail.com&gt;&gt;w=\r\nrote:\n&gt;&gt;&gt; \n&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt; 2009/12/8 snapmedown &lt;snapmedown@...\n&gt;&gt; &lt;=\r\nmailto:snapmedown%40yahoo.com&gt; &lt;snapmedown%40yahoo.com&gt;&gt;\n&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt;=\r\n What are the thoughts on a neural network that has inputs and outputs\n&gt;&gt;&gt;&gt;=\r\n digitized? Perhaps 8\n&gt;&gt;&gt;&gt;&gt; bits or even less? This would reduce the search=\r\n space, but encourage\n&gt;&gt;&gt;&gt; larger structures.\n&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt; Hi,\n&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt; Str=\r\nictly speaking the signals and weights and already digitized in\n&gt;&gt;&gt;&gt; that t=\r\nhey&#39;re represented by a 32 bit floating point binary\n&gt;&gt;&gt;&gt; representation (o=\r\nr 64bit for double precision). However your question\n&gt;&gt;&gt;&gt; essentially then =\r\nbecomes how much precision is necessary for a given\n&gt;&gt;&gt;&gt; problem domain? an=\r\nd that&#39;s certainly an interesting question both from\n&gt;&gt;&gt;&gt; the perspective o=\r\nf the maths of neural networks but also at the\n&gt;&gt;&gt;&gt; implementation level wh=\r\nere an 8 bit based ANN is is a lot less\n&gt;&gt;&gt;&gt; computing resource hungry comp=\r\nared to a 64bit one of the same\n&gt;&gt;&gt;&gt; structural size/complexity.\n&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;=\r\n You might be interested in the short article I wrote abotu\n&gt;&gt;&gt;&gt; implementi=\r\nng neural nets with integer maths and /fixed/ point\n&gt;&gt;&gt;&gt; arithmetic:\n&gt;&gt;&gt;&gt; \n=\r\n&gt;&gt;&gt;&gt; http://sharpneat.sourceforge.net/integer_network.html\n&gt;&gt; &lt;http://sharp=\r\nneat.sourceforge.net/integer_network.html&gt;\n&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt; In terms of pushing N=\r\nEAT to it&#39;s limits I think something like 16bit\n&gt;&gt;&gt;&gt; maths running on CUDA =\r\nor equivalent platforms is a logical goal in the\n&gt;&gt;&gt;&gt; near term. There&#39;s po=\r\ntentially a three orders of magnitude speedup to\n&gt;&gt;&gt;&gt; be achieved there.\n&gt;&gt;=\r\n&gt;&gt; \n&gt;&gt;&gt;&gt; Colin.\n&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt; \n&gt;&gt; \n&gt;&gt; \n&gt; \n&gt; \n&gt; -------------------------------=\r\n-----\n&gt; \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; \n\n\n\n"}}