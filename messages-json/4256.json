{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":8147458,"authorName":"Christian","from":"&quot;Christian&quot; &lt;Christian.Hofmann@...&gt;","profile":"chhofchhof","replyTo":"LIST","senderId":"PVULBZdqeumteHZKm92YjeXIyhCruBTcj9cpvG6ZUdd7Y4jQwYeFUujO9eT8DlPq2P5aCOAzbZcgj5ucQLMT4mGF5xcVh-iofwMok0w","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Evaluating population with large training data set","postDate":"1218231842","msgId":4256,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGc3aWVuMitmdWg1QGVHcm91cHMuY29tPg==","inReplyToHeader":"PGJhOTE5ZDBlMDgwODA3MDIyMW82MDIyNDAyMnI3YTM2ZTRkZmJlY2IyNDc5QG1haWwuZ21haWwuY29tPg=="},"prevInTopic":4255,"nextInTopic":4257,"prevInTime":4255,"nextInTime":4257,"topicId":4253,"numMessagesInTopic":11,"msgSnippet":"Hello Sandor, thank you very much for your detailed post. Testing the younger generations with less training data sounds very good. But currently I am using","rawEmail":"Return-Path: &lt;Christian.Hofmann@...&gt;\r\nX-Sender: Christian.Hofmann@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 29666 invoked from network); 8 Aug 2008 21:44:02 -0000\r\nX-Received: from unknown (66.218.67.94)\n  by m57.grp.scd.yahoo.com with QMQP; 8 Aug 2008 21:44:02 -0000\r\nX-Received: from unknown (HELO n40b.bullet.mail.sp1.yahoo.com) (66.163.168.154)\n  by mta15.grp.scd.yahoo.com with SMTP; 8 Aug 2008 21:44:02 -0000\r\nX-Received: from [216.252.122.216] by n40.bullet.mail.sp1.yahoo.com with NNFMP; 08 Aug 2008 21:44:02 -0000\r\nX-Received: from [66.218.69.6] by t1.bullet.sp1.yahoo.com with NNFMP; 08 Aug 2008 21:44:02 -0000\r\nX-Received: from [66.218.67.199] by t6.bullet.scd.yahoo.com with NNFMP; 08 Aug 2008 21:44:02 -0000\r\nDate: Fri, 08 Aug 2008 21:44:02 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;g7ien2+fuh5@...&gt;\r\nIn-Reply-To: &lt;ba919d0e0808070221o60224022r7a36e4dfbecb2479@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;Christian&quot; &lt;Christian.Hofmann@...&gt;\r\nSubject: Re: Evaluating population with large training data set\r\nX-Yahoo-Group-Post: member; u=8147458; y=4Pux-BKguPKKZibqdIaaQgP95xyB8ueOUYafEIIyzPoDN6TAXA\r\nX-Yahoo-Profile: chhofchhof\r\n\r\nHello Sandor,\n\nthank you very much for your detailed post.\n\nTesting the you=\r\nnger generations with less training data sounds very\ngood. But currently I =\r\nam using (Hyper)SharpNeat and there is no way to\nget the genome age. I only=\r\n have the network itself and not the genom.\nBut I think I could implement i=\r\nt.\n\nYou write that you don&#39;t want to use completely new random data (out\nof=\r\n training set) as you think every genome should have the same\nchance. I don=\r\n&#39;t think so. When you take one random training data set\nand per random you =\r\nhave bad luck and some bad samples, every\ngeneration is trained wrong. But =\r\nwhen you take a completely new random\ntraining data set then bad genomes wi=\r\nll get erased soon. Also you have\na much bigger deviation.\n\nSo my next ques=\r\ntion is how big the training part of all sample data\nshould be so I should =\r\nhave a good average.\n\nIf I have one million training data, mabye 5 percent =\r\nis enough. When I\nonly have 1000, I will need them all.\n\nBy choosing better=\r\n samples I could decrement the needed value rate\neven further. \n\nYour advic=\r\ne about logging the fitness variance sounds very good. But\nwhat is a high v=\r\nariance? Let&#39;s say  my fitness range is from 0 to 1.\nIs, for example, 5% st=\r\nd. deviation a level you raise or lower sample\ndata amount?\n\nI am looking f=\r\nor something to give every training data set based on\ninputs and outputs a =\r\nspecial number. So I can choose the samples that\nare most critical for the =\r\ntest.\n\nWhat&#39;s about that:\n\nI just sum up all input and output values. After=\r\n that I can choose the\nsample data with the most different sums. Maybe that=\r\n will work. But\nwhen input 1 has the same value as input 2 in a different t=\r\nest data\n(and vice versa with some other inputs) they will get the same sum=\r\n.\n\nSo they are different but would seen as identical. So this apoach is\nnot=\r\n good.\n\nMaybe someone has an additional one?\n\nKind regards,\n\nChristian\n\n---=\r\n In neat@yahoogroups.com, &quot;Sandor Murakozi&quot; &lt;smurakozi@...&gt; wrote:\n&gt;\n&gt; Hi C=\r\nhristian,\n&gt; \n&gt; I did some experiments with similar stuff a while ago. Some =\r\nof them were\n&gt; related to Neat, others were &quot;plain&quot; GA, but the situation w=\r\nas the same:\n&gt; evaluation is expensive, training data set is huge. I&#39;ve tri=\r\ned idea\n#1 and\n&gt; #3, but not the correlation based.\n&gt; \n&gt; In my experience b=\r\noth of them could really speed up the\n&gt; optimization/learning process, with=\r\nout degrading the results too much:\n&gt; \n&gt; #1: When a new genotype was create=\r\nd I tested it with a small subset\nof the\n&gt; data (typically 5-10%), then in =\r\neach generations I&#39;ve tested all\nsurviving\n&gt; genotypes a bit more (+3-5%). =\r\nThe fitness was always projected to\nthe same\n&gt; basis (usually I used someth=\r\ning like the average fitness for the tested\n&gt; cases).\n&gt; a) using first 10, =\r\n15.. % of data - if the dataset is not uniformly\n&gt; distributed then it can =\r\nbias the search quite heavily, so I wouldn&#39;t\n&gt; recommend it. An especially =\r\nbad case is if you have the hardest\ncases at the\n&gt; beginning. With such a s=\r\ncenario if a genotype survives the first\ngenerations\n&gt; it will have better =\r\nand better results -&gt; new genotypes will have a\npretty\n&gt; hard job to compet=\r\ne with them. Having the easier cases first is a bit\n&gt; better, but it can al=\r\nso lead to strange problems.\n&gt; b) using random samples for each genotypes. =\r\nThis is a bit better,\nbut luck\n&gt; has a too much influence on the youngest g=\r\nenotypes (with not enough\ntests).\n&gt; c) using the same random sample for eva=\r\nluations in the same\ngeneration. For\n&gt; me this worked usually the best.\n&gt; \n=\r\n&gt; #3: There are (at least) three ways to do it:\n&gt; a) Take a small, fixed su=\r\nbset of test data, train on that and\nvalidate your\n&gt; result using the remai=\r\nning (or full) set. This also works, but I had\na bit\n&gt; worse results then w=\r\nith #1. But separating the test set to a\ntraining and\n&gt; validation set is a=\r\n good idea in general, it can help a lot to avoid\n&gt; overfitting.\n&gt; b) Take =\r\na subset, work on it for a while. When the fitness reaches a\nplateau\n&gt; (or =\r\nyou reach a specified number of generations) increase the size\nof the\n&gt; tra=\r\nining set (it might be even better if you also replace it with new\n&gt; cases/=\r\ntake a new random sample). In my experience it did not\nincrease the\n&gt; final=\r\n fitness significantly, but the time needed for the whole\nprocess was\n&gt; muc=\r\nh more then for the other experiments.\n&gt; c) You can also group your genotyp=\r\nes based on their age. Younger\ngenotypes\n&gt; compete (mainly) with other youn=\r\ngsters... Genotypes are moved to another\n&gt; group when they reach a specific=\r\n age (or when they have extremely high\n&gt; fitness, compared to its mates, to=\r\n avoid too dominant genotypes in any\n&gt; groups). Each groups are evaluated t=\r\no a different extent (similar to\nand can\n&gt; be combined with #1).\n&gt; I didn&#39;t=\r\n play too much with it, but somewhere (on this list, maybe)\nI read a\n&gt; pape=\r\nr about this approach and they claimed it works pretty well on some\n&gt; types=\r\n of problems.\n&gt; \n&gt; One more trick that I sometimes used: if the problem you=\r\n try to solve is\n&gt; extremely hard then getting a head start can be problema=\r\ntic if you\nsimply\n&gt; start with a totally random population. In this case I =\r\njust added a\n&gt; threshold to the fitness  of genotypes in the initial popula=\r\ntion.\nFitness\n&gt; was determined on a very small subset (1-5%) of the whole t=\r\nest data. The\n&gt; purpose of this test was just to kill definitely bad genoty=\r\npes without\n&gt; spending too much time on evaluation.\n&gt; I created new genotyp=\r\nes until I got enough of them that passed the\n&gt; threshold.\n&gt; In some cases =\r\nit helps, in others it does not - depends heavily on\nthe kind\n&gt; of problem =\r\nyou are working on.\n&gt; \n&gt; You can also keep some statistics about the test r=\r\nesults of each\n&gt; individuals. If there are enough tests already, and varian=\r\nce of the\nresults\n&gt; is low, then you don&#39;t have to push further evaluations=\r\n so\naggressively. If\n&gt; variance is high, then your results are not really t=\r\nrustworthy -&gt;\nyou need\n&gt; more test. Just like with the previous trick it ma=\r\ny hep a bit, but it\n&gt; depends on the data you use.\n&gt; \n&gt; Please let us know =\r\nyour results (or some more details about your\nproblem),\n&gt; I&#39;m really curiou=\r\ns to know how these methods work on other problems.\n&gt; \n&gt; Cheers,\n&gt; Sandor\n&gt;=\r\n \n&gt; On Wed, Aug 6, 2008 at 11:45 PM, Christian &lt;Christian.Hofmann@...&gt;\nwrot=\r\ne:\n&gt; \n&gt; &gt;   Hello,\n&gt; &gt;\n&gt; &gt; I want to discuss with you about this topic. The=\r\n main bottleneck of\n&gt; &gt; NEAT is the evaluation of every created network to =\r\nget the fitness.\n&gt; &gt; But what if you have big networks and very much traini=\r\nng data? I have\n&gt; &gt; over one million training datasets that I need to evalu=\r\nate for every\n&gt; &gt; network. I need about one minute per network.\n&gt; &gt;\n&gt; &gt; I h=\r\nave thought about some solutions regarding this problem. Maybe you\n&gt; &gt; have=\r\n already tried one of these or you think that I cannot do some of\n&gt; &gt; them =\r\nor should favorite one of them.\n&gt; &gt; First I need to say that I calculate th=\r\ne fitness based on fitness\n&gt; &gt; values divided by the number of used sample =\r\ndata. So it is the same\n&gt; &gt; fitness level regarding of using 10% or 100% of=\r\n the training data.\n&gt; &gt;\n&gt; &gt; 1) You could evaluate only (for example) 10% of=\r\n the training data. And\n&gt; &gt; calculate the overall fitness based on this dat=\r\na. Then if the fitness\n&gt; &gt; is higher than the previous highest 10% fitness =\r\ndata, calculate the\n&gt; &gt; remaining 90% and return the value for all 100% tra=\r\niningdata. If the\n&gt; &gt; fitness is lower than the previous highest 10% fitnes=\r\ns data you can\n&gt; &gt; return the fitness data for these 10%.\n&gt; &gt;\n&gt; &gt; I don&#39;t k=\r\nnow if it is better to choose just the first 10% of training\n&gt; &gt; data (so e=\r\nvery network is getting the same data) or just picking the\n&gt; &gt; 10% training=\r\n data by random from the complete training data set. I\n&gt; &gt; don&#39;t know if 10=\r\n% is the right number or if you should use some\n&gt; &gt; additional steps (10%, =\r\n25%,50%, 75%, 100%). I really don&#39;t know.\n&gt; &gt;\n&gt; &gt; 2) Instead of choosing ju=\r\nst 10% if the data we could choose the\n&gt; &gt; training data that correlate lik=\r\ne 0 or -1 with the other training\n&gt; &gt; data. This way we train with data tha=\r\nt is the most different. But\n&gt; &gt; how get this correlation? I don&#39;t know if =\r\nthere is an algorithm.\n&gt; &gt; Specialty if you have 100 input and 300 output n=\r\neurons =85\n&gt; &gt;\n&gt; &gt; 3) Just pick 10% training data randomly from the complet=\r\ne training\n&gt; &gt; data set without calculating the other 90% ever. This way ev=\r\nery\n&gt; &gt; training dataset will get chosen one time.\n&gt; &gt;\n&gt; &gt; Do you have some=\r\n experience in this area? Maybe there are some other\n&gt; &gt; ways. I would real=\r\nly appreciate to hear them :-)\n&gt; &gt;\n&gt; &gt; Kind regards,\n&gt; &gt;\n&gt; &gt; Christian\n&gt; &gt;\n=\r\n&gt; &gt;  \n&gt; &gt;\n&gt;\n\n\n\n"}}