{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":281645563,"authorName":"afcarl2","from":"&quot;afcarl2&quot; &lt;a.carl@...&gt;","profile":"afcarl2","replyTo":"LIST","senderId":"IkkuWfGIdrBquCiLhnbCT_VGWwhVhO_vUvIawACs4Xa1oLfn8YU1mT7CTYjJ5c91rEAueE0_XIwNcupgM9Y8Nyc","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Backpropagation and NEAT","postDate":"1205193675","msgId":3863,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGZyNGk0YitsbHVyQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGZyNGR2cysxZnQyQGVHcm91cHMuY29tPg=="},"prevInTopic":3862,"nextInTopic":3865,"prevInTime":3862,"nextInTime":3864,"topicId":3846,"numMessagesInTopic":41,"msgSnippet":"If most individuals in a species represented by a given topology ended up in the same local minimia , one could argue that the subject specie s logical end","rawEmail":"Return-Path: &lt;a.carl@...&gt;\r\nX-Sender: a.carl@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 45931 invoked from network); 11 Mar 2008 00:01:17 -0000\r\nX-Received: from unknown (66.218.67.97)\n  by m43.grp.scd.yahoo.com with QMQP; 11 Mar 2008 00:01:17 -0000\r\nX-Received: from unknown (HELO n37a.bullet.mail.sp1.yahoo.com) (66.163.168.131)\n  by mta18.grp.scd.yahoo.com with SMTP; 11 Mar 2008 00:01:17 -0000\r\nX-Received: from [216.252.122.218] by n37.bullet.mail.sp1.yahoo.com with NNFMP; 11 Mar 2008 00:01:17 -0000\r\nX-Received: from [66.218.69.6] by t3.bullet.sp1.yahoo.com with NNFMP; 11 Mar 2008 00:01:17 -0000\r\nX-Received: from [66.218.66.85] by t6.bullet.scd.yahoo.com with NNFMP; 11 Mar 2008 00:01:17 -0000\r\nDate: Tue, 11 Mar 2008 00:01:15 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;fr4i4b+llur@...&gt;\r\nIn-Reply-To: &lt;fr4dvs+1ft2@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;afcarl2&quot; &lt;a.carl@...&gt;\r\nSubject: Re: Backpropagation and NEAT\r\nX-Yahoo-Group-Post: member; u=281645563; y=r2_oyet7IHrjqBDd0LhGfI3uT1CIoesSQ0m9lAcOkukpgQ\r\nX-Yahoo-Profile: afcarl2\r\n\r\nIf &quot;most individuals in a species represented by a given topology&quot; \nended u=\r\np in &quot;the same local minimia&quot;, one could argue that the \nsubject specie&#39;s l=\r\nogical end point was the same local minimia, and \nthat the cost of maintain=\r\ning more than one organism was \ncomputationally wasteful. Better to know so=\r\noner and breed additional \norganisms of differing topology so as to maintai=\r\nn the population size \nand maximize the population&#39;s &quot;effective&quot; diversity.=\r\n\n\nPaying more for the same answer does not make it a better answer.\n\n--- In=\r\n neat@yahoogroups.com, &quot;petar_chervenski&quot; \n&lt;petar_chervenski@...&gt; wrote:\n&gt;\n=\r\n&gt; Well I think that encoding the resulting weights back to the genome \n&gt; wo=\r\nuld somehow hurt the population weight diversity, since most \n&gt; individuals=\r\n in a species represented by a given topology can end up \n&gt; in the same loc=\r\nal minima, thus leaving out a species with the \nnearly \n&gt; same individuals,=\r\n i.e. clones. \n&gt; This is why I think that backprop should be applied occasi=\r\nonaly \nafter \n&gt; long periods of stagnation, for example the cases where del=\r\nta-\ncoding \n&gt; kicks in, when it focuses the search in the most promising ar=\r\neas of \n&gt; the search space. \n&gt; I am still trying to re-implement RTRL mysel=\r\nf, though.. Then I&#39;ll \nsee \n&gt; if it is going to actually enhance performanc=\r\ne. \n&gt; \n&gt; Peter\n&gt; \n&gt; --- In neat@yahoogroups.com, &quot;Kenneth Stanley&quot; &lt;kstanle=\r\ny@&gt; wrote:\n&gt; &gt;\n&gt; &gt; Rafael, thank you for pointing out the connection to mem=\r\netic \n&gt; &gt; algorithms.  That is good to point out that such a combination \n&gt;=\r\n falls \n&gt; &gt; under that category.\n&gt; &gt; \n&gt; &gt; However, there are still those wh=\r\no would argue that the local \n&gt; search \n&gt; &gt; method should not be encoded ba=\r\nck into the genome, that is, that \n&gt; &gt; evolution should simply search for t=\r\nhe best starting point from \n&gt; which \n&gt; &gt; a local search would depart.  Bec=\r\nause of the Baldwin Effect, that \n&gt; may \n&gt; &gt; even work better.\n&gt; &gt; \n&gt; &gt; Per=\r\nsonally, I do not know which approach would work better but \nboth \n&gt; &gt; are =\r\nviable and it is probably domain dependent.\n&gt; &gt; \n&gt; &gt; ken\n&gt; &gt; \n&gt; &gt; --- In ne=\r\nat@yahoogroups.com, &quot;Rafael C.P.&quot; &lt;kurama.youko.br@&gt; \n&gt; &gt; wrote:\n&gt; &gt; &gt;\n&gt; &gt; =\r\n&gt; Ken, it doesn&#39;t fit pure evolution but it fits memetic \n&gt; algorithms, \n&gt; =\r\n&gt; that\n&gt; &gt; &gt; consists exactly of evolution alternated with local search \n&gt; =\r\nmethods \n&gt; &gt; for fine\n&gt; &gt; &gt; tunning (just few steps). NEAT+BP may become a =\r\ngood memetic \n&gt; &gt; algorithm for\n&gt; &gt; &gt; neural networks.\n&gt; &gt; &gt; \n&gt; &gt; &gt; On Mon,=\r\n Mar 10, 2008 at 2:19 PM, Kenneth Stanley &lt;kstanley@&gt;\n&gt; &gt; &gt; wrote:\n&gt; &gt; &gt; \n&gt;=\r\n &gt; &gt; &gt;   Peter, I believe that backprop can potentially improve the\n&gt; &gt; &gt; &gt;=\r\n accuracy. It has been shown to work effectively with \n&gt; neurevolution\n&gt; &gt; =\r\n&gt; &gt; in classification tasks in the past. So in principle it could\n&gt; &gt; &gt; &gt; h=\r\nelp. Of course, there is always the chance that it will not\n&gt; &gt; &gt; &gt; enhance=\r\n performance as well.\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; One issue I would also consider is th=\r\nat some people disagree \non\n&gt; &gt; &gt; &gt; whether the changes to weights from bac=\r\nkprop should be \nencoded \n&gt; &gt; back\n&gt; &gt; &gt; &gt; into the genome or not. If it is=\r\n actually encoded back into \nthe\n&gt; &gt; &gt; &gt; genome, that is &quot;Lamarckian&quot; evolu=\r\ntion because in effect what \n&gt; the\n&gt; &gt; &gt; &gt; organism learned over its lifeti=\r\nme is encoded into its own\n&gt; &gt; &gt; &gt; offspring. That is obviously not how rea=\r\nl evolution works.\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; However, of course, it doesn&#39;t have to w=\r\nork like real \nevolution \n&gt; &gt; and\n&gt; &gt; &gt; &gt; some people believe that Lamarcki=\r\nan evolution will work \nbetter.\n&gt; &gt; &gt; &gt; However, there are arguments that i=\r\nn fact it works worse \n&gt; because \n&gt; &gt; it\n&gt; &gt; &gt; &gt; hurts the diversity of the=\r\n population. Because of the Baldwin\n&gt; &gt; &gt; &gt; effect, some would argue that e=\r\nvolution+backprop is most \n&gt; powerful \n&gt; &gt; if\n&gt; &gt; &gt; &gt; the learned weights a=\r\nre not encoded back into the genome. \nThis \n&gt; &gt; topic\n&gt; &gt; &gt; &gt; is fairly ext=\r\nensive. A lot is written about the &quot;Baldwin \n&gt; effect.&quot;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt;=\r\n &gt; &gt; ken\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; --- In neat@yahoogroups.com &lt;neat%\n&gt; &gt; 40yahoogrou=\r\nps.com&gt;, &quot;petar_chervenski&quot;\n&gt; &gt; &gt; &gt; &lt;petar_chervenski@&gt; wrote:\n&gt; &gt; &gt; &gt; &gt;\n&gt; =\r\n&gt; &gt; &gt; &gt; Hi Ken,\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; I am evolving time series predictors, i=\r\nn fact even a \n&gt; simplified\n&gt; &gt; &gt; &gt; &gt; version of time series predictors, wh=\r\nere the network has to \n&gt; &gt; answer\n&gt; &gt; &gt; &gt; is\n&gt; &gt; &gt; &gt; &gt; the future value go=\r\ning up or down. The actual output neuron \n&gt; is a\n&gt; &gt; &gt; &gt; &gt; simple step func=\r\ntion, but back-prop can be applied if it is \n&gt; &gt; turned\n&gt; &gt; &gt; &gt; &gt; out to a =\r\nsigmoid with a very steep slope.\n&gt; &gt; &gt; &gt; &gt; The networks are allowed to have=\r\n any topology and they are\n&gt; &gt; &gt; &gt; evaluated\n&gt; &gt; &gt; &gt; &gt; on the run, meaning =\r\nthat on each timestep, an error is being\n&gt; &gt; &gt; &gt; &gt; calculated (being 0 or 1=\r\n, depending on the prediction made).\n&gt; &gt; &gt; &gt; &gt; First of all, do you think t=\r\nhat applying back-prop to these\n&gt; &gt; &gt; &gt; networks\n&gt; &gt; &gt; &gt; &gt; may bring any ac=\r\ncuracy improvement? I know that it is going \n&gt; to \n&gt; &gt; eat\n&gt; &gt; &gt; &gt; &gt; the CP=\r\nU resourses, so it can be applied at regular \nintervals, \n&gt; &gt; say\n&gt; &gt; &gt; &gt; &gt;=\r\n each 50 generations, to push the networks&#39;s weights in the \n&gt; right\n&gt; &gt; &gt; =\r\n&gt; &gt; direction, a kind of a hint to the search. I am still \nthinking\n&gt; &gt; &gt; &gt;=\r\n of &quot;is\n&gt; &gt; &gt; &gt; &gt; it worth it?&quot;..\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; Peter\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt;=\r\n &gt; &gt;\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; --- In neat@yahoogroups.com &lt;neat%\n&gt; 40yahoogroups=\r\n.com&gt;, &quot;Kenneth \n&gt; &gt; Stanley&quot;\n&gt; &gt; &gt; &gt; &lt;kstanley@&gt; wrote:\n&gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; =\r\n&gt; &gt; &gt; A number of people have programmed backprop into NEAT. \nChris\n&gt; &gt; &gt; &gt;=\r\n &gt; &gt; Christenson did a Masters thesis on combining NEAT and \n&gt; &gt; backprop;\n=\r\n&gt; &gt; &gt; &gt; a\n&gt; &gt; &gt; &gt; &gt; &gt; paper based on this work is actually in the files sec=\r\ntion \nof\n&gt; &gt; &gt; &gt; this\n&gt; &gt; &gt; &gt; &gt; group:\n&gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt;\n&gt; =\r\n&gt; &gt; &gt; \n&gt; &gt; \n&gt; \nhttp://f1.grp.yahoofs.com/v1/UP7SR8rDovimxlLlvcmGOziLUBIVncb=\r\n2Tfr7sruo\n&gt; &gt; &gt; &gt; B\n&gt; &gt; &gt; &gt; &gt; 8b\n&gt; &gt; &gt; &gt; &gt; &gt; taAfELU62JLyQ9XCxXF_Akhcmi-\n&gt; =\r\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; \n&gt; &gt; \n&gt; \nTH4gpVHIikwnzB59ArOMQfPOAzyw25/Evolvin=\r\ng_Trainable_Neural_Networks_6_p\n&gt; &gt; &gt; &gt; a\n&gt; &gt; &gt; &gt; &gt; ge\n&gt; &gt; &gt; &gt; &gt; &gt; s.doc\n&gt; =\r\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; Shimon Whiteson implemented it as part of his NEAT+Q\n=\r\n&gt; &gt; &gt; &gt; reinforcement\n&gt; &gt; &gt; &gt; &gt; &gt; learning method:\n&gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; =\r\n\n&gt; &gt; \n&gt; \nhttp://staff.science.uva.nl/~whiteson/pubs/whitesonaaai06.pdf&lt;http=\r\n://s\n&gt; &gt; taff.science.uva.nl/%7Ewhiteson/pubs/whitesonaaai06.pdf&gt;\n&gt; &gt; &gt; &gt; &gt;=\r\n &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; There has been a lot written on backprop in NEAT in the \n&gt; &gt;=\r\n archives\n&gt; &gt; &gt; &gt; of\n&gt; &gt; &gt; &gt; &gt; &gt; this group: just search for &quot;backprop&quot; fro=\r\nm the yahoo \npage \n&gt; for\n&gt; &gt; &gt; &gt; this\n&gt; &gt; &gt; &gt; &gt; &gt; group and many messages w=\r\nill pop up.\n&gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; In general, if you do not allow recurre=\r\nnce then I believe \n&gt; &gt; there\n&gt; &gt; &gt; &gt; is\n&gt; &gt; &gt; &gt; &gt; no\n&gt; &gt; &gt; &gt; &gt; &gt; special c=\r\nhange needed in the traditional backprop \nalgorithm.\n&gt; &gt; &gt; &gt; With\n&gt; &gt; &gt; &gt; &gt;=\r\n &gt; recurrence you would need something like recurrent \nbackprop \n&gt; &gt; like\n&gt;=\r\n &gt; &gt; &gt; &gt; Derek\n&gt; &gt; &gt; &gt; &gt; &gt; suggested. But let&#39;s just say you are evolving \n=\r\nnonrecurrent\n&gt; &gt; &gt; &gt; &gt; networks-\n&gt; &gt; &gt; &gt; &gt; &gt; is there a particular problem =\r\nyou have in mind that comes \nup\n&gt; &gt; &gt; &gt; with\n&gt; &gt; &gt; &gt; &gt; &gt; applying backprop =\r\nto such networks?\n&gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; ken\n&gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; --- I=\r\nn neat@yahoogroups.com &lt;neat%40yahoogroups.com&gt;,\n&gt; &gt; &gt; &gt; &quot;petar_chervenski&quot;=\r\n\n&gt; &gt; &gt; &gt; &lt;petar_chervenski@&gt;\n&gt; &gt; &gt; &gt; &gt; &gt; wrote:\n&gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt;=\r\n Hello there.\n&gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; I am looking for any back-propaga=\r\ntion algorithm that \ncan \n&gt; &gt; work\n&gt; &gt; &gt; &gt; on\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; networks with a=\r\nrbitrary topology such as these that NEAT\n&gt; &gt; &gt; &gt; evolves.\n&gt; &gt; &gt; &gt; &gt; All\n&gt; =\r\n&gt; &gt; &gt; &gt; &gt; &gt; libraries I found so far either assume layered networks \nor\n&gt; &gt;=\r\n &gt; &gt; only\n&gt; &gt; &gt; &gt; &gt; feed-\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; forward ones.. I am confused. Is th=\r\nere any source code \n&gt; that\n&gt; &gt; &gt; &gt; might\n&gt; &gt; &gt; &gt; &gt; &gt; help\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; me=\r\n? Any back-prop implementation that can work on NEAT\n&gt; &gt; &gt; &gt; networks\n&gt; &gt; &gt;=\r\n &gt; &gt; such\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; that it can easily be integrated. Or maybe some pap=\r\ners \non \n&gt; &gt; the\n&gt; &gt; &gt; &gt; &gt; topic?\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; I appreciate any help from =\r\nthe community.\n&gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; Peter\n&gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt;\n=\r\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;  \n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; &gt; -- \n&gt; &gt; &gt; =\r\n=3D=3D=3D=3D=3D=3D=3D=3D=3D\n&gt; &gt; &gt; Rafael C.P.\n&gt; &gt; &gt; =3D=3D=3D=3D=3D=3D=3D=\r\n=3D=3D\n&gt; &gt; &gt;\n&gt; &gt;\n&gt;\n\n\n\n"}}