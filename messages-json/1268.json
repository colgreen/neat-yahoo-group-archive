{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"UeUTxh8rqPcZICpUaXM3beQLrY5mqz3E0JZaiSJq99R1un89FH3PWGxg1S7PnXx7QT8Ih1KlYWcrIUyUAYc7wI48Y4hRmgJN8H56_Fn8dcPe","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: Stephen Thaler","postDate":"1091049833","msgId":1268,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGNlOTVoOSs3MGwzQGVHcm91cHMuY29tPg==","inReplyToHeader":"PEJBWTItRjE4Q2w5Rm9wWUM5OEIwMDAyYTczY0Bob3RtYWlsLmNvbT4="},"prevInTopic":1261,"nextInTopic":1281,"prevInTime":1267,"nextInTime":1269,"topicId":1257,"numMessagesInTopic":9,"msgSnippet":"John, thanks for the unhyped explanation of Thaler.  Admittedly, I know nothing about it otherwise, but from your description it sounds like simple hill","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 55673 invoked from network); 28 Jul 2004 21:24:04 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m13.grp.scd.yahoo.com with QMQP; 28 Jul 2004 21:24:04 -0000\r\nReceived: from unknown (HELO n28.grp.scd.yahoo.com) (66.218.66.84)\n  by mta5.grp.scd.yahoo.com with SMTP; 28 Jul 2004 21:24:03 -0000\r\nReceived: from [66.218.67.188] by n28.grp.scd.yahoo.com with NNFMP; 28 Jul 2004 21:23:55 -0000\r\nDate: Wed, 28 Jul 2004 21:23:53 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;ce95h9+70l3@...&gt;\r\nIn-Reply-To: &lt;BAY2-F18Cl9FopYC98B0002a73c@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Length: 6573\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-eGroups-Remote-IP: 66.218.66.84\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: Re: Stephen Thaler\r\nX-Yahoo-Group-Post: member; u=54567749\r\nX-Yahoo-Profile: kenstanley01\r\n\r\nJohn, thanks for the unhyped explanation of Thaler.  Admittedly, I \nknow nothing about it otherwise, but from your description it sounds \nlike simple hill climbing, i.e. make a tweak, see if you improved, \nand if not go back to where you were.  Is there any real \ndifference?  Hill climbing is known to get stuck on local \noptima...well that&#39;s an understatenent...it *does* get stuck on \nlocal optima.  That&#39;s one reason having a population is helpful.\nSo I am just wondering how Thaler&#39;s thing differs from hill climbing?\n\nI agree with what Derek later said...that site is so full of hype \nand hyperbole that I can hardly believe this guy is really on the \nball and not just a nut.  But then again, they say genius is \ndifficult recognize...\n\nken\n\n--- In neat@yahoogroups.com, &quot;John Arrowwood&quot; &lt;jarrowwx@h...&gt; wrote:\n&gt; &gt;From: Tyler Streeter &lt;tylerstreeter@y...&gt;\n&gt; &gt;\n&gt; &gt; &gt; And what if the network does come to depend on that\n&gt; &gt; &gt; noise?  Read up on\n&gt; &gt; &gt; Stephen Thaler and his Creativity Machine.\n&gt; &gt; &gt; http://www.imagination-engines.com/thaler.htm\n&gt; &gt;\n&gt; &gt;I remember hearing about this &quot;Creativity Machine&quot; a\n&gt; &gt;while ago.  I was just reading some things on his\n&gt; &gt;site.  He seems to think that his Creative Machine\n&gt; &gt;paradigm is the ultimate solution for everything under\n&gt; &gt;the sun.  It sounds like a lot of hype, but I guess\n&gt; &gt;time will tell.  He does have some recent applications\n&gt; &gt;that sound pretty good.\n&gt; &gt;\n&gt; &gt;In a few places he talks about the inferiority of GAs\n&gt; &gt;to his technique, usually worded in annoying ways.\n&gt; &gt;Here&#39;s a sample:\n&gt; \n&gt; [snip]\n&gt; \n&gt; Funny thing is, his technique is functionally equivalent to the \n&gt; weight-mutation aspect of NEAT.  He randomly tweaks a few weights \nby a small \n&gt; amount, to see if it makes things better.\n&gt; \n&gt; Where it differs is that it judges the effect immediately, rather \nthan over \n&gt; a longer evaluation.  And the changing (evolving) continues \nindefinitely, \n&gt; not just until it has a solution.  Which creates an interesting \nand \n&gt; potential useful behavior.\n&gt; \n&gt; You have this fixed topology.  And the network has some built-in \nability to \n&gt; judge its own fitness (for example, is the walker balanced?).  \nWith every \n&gt; activation, it makes SMALL tweaks to some small portion of the \nweights.  \n&gt; Then it activates.  Did its fitness improve?  If not, the tweaks \nare rolled \n&gt; back.  If so, they remain.  What this means is that it adapts to \nits present \n&gt; situation, at the expense of the generalized solution.  When the \nsituation \n&gt; changes, it then re-adapts to that one.  If the topology \nfacilitates it, \n&gt; then this constant weight shifting allows it to be as good as it \ncan be at \n&gt; the current task, with a delay while it comes up to speed.  This \nas opposed \n&gt; to dividing your error equally among all situations, which is what \na \n&gt; fixed-weight network would be forced to do.\n&gt; \n&gt; But what his work does show is that randomness can be a valuable \ntool for \n&gt; searching for an optimal solution.  Which of course, NEAT already \nproved.  \n&gt; :)\n&gt; \n&gt; &gt;So far I haven&#39;t found any good methods for robot\n&gt; &gt;control that are as &quot;self-training&quot; as a GA since the\n&gt; &gt;feedback is just so sparse, but apparently Thaler&#39;s\n&gt; &gt;method has been used to train robots.  I guess I&#39;m\n&gt; &gt;open to new ideas...\n&gt; \n&gt; Personally, I see NEAT and Thaler&#39;s approaches being able to co-\nexist.  \n&gt; Thaler requires an appropriate topology and works best if that \ntopology is \n&gt; already tuned to the generalized solution.  NEAT evolves that \nstarting \n&gt; configuration.  But as part of the evaluation cycle, you could \nhave the \n&gt; weight tweaking &#39;creativity machine&#39; component.  So the overall \nfitness \n&gt; evaluation given back to NEAT is a function of both whether or not \nthe \n&gt; topology was able to solve the problem, and how far it had to \nchange the \n&gt; weights to get there.  Or something to that effect.\n&gt; \n&gt; Combine that with competitive co-evolution and see what you get. :)\n&gt; \n&gt; &gt;I&#39;m wondering if anyone has any more info on this.\n&gt; &gt;I&#39;d like to get some info from an unbiased source\n&gt; &gt;(i.e. not the super-hyped stuff from the Imagination\n&gt; &gt;Engines website).\n&gt; \n&gt; All I can offer is my analysis of what it is and why it works.  It \nworks \n&gt; exactly the same as the means by which NEAT optimizes a single \nnetwork \n&gt; topology...weight mutation, rewarding the more fit mutations and \ndiscarding \n&gt; the less fit ones.  The only difference is that they are rewarded \nor \n&gt; punished immediately, rather than evaluating their fitness over a \nlarge \n&gt; number of possible situations.  Both approaches have their place, \nand might \n&gt; be quite powerful if combined.\n&gt; \n&gt; Which gives me an idea...I imagine a mechanism that helps the \nnetwork \n&gt; remember where it came from, so it is more likely to go back to it \nas a \n&gt; fall-back:\n&gt; \n&gt; Have the connection weight be a series of numbers, a minimum of \n3.  The \n&gt; actual weight is the sum of all of them.  The first number is the \none that \n&gt; is randomly changed if it is perturbed.  That number is propagated \nto the \n&gt; next number based on fitness change.  If a rise in fitness has \nrecently been \n&gt; detected, then a portion of that perturbation is propagated to the \nnext \n&gt; number proportionate to how much of a rise there was.  So let&#39;s \nsay there \n&gt; was a good rise in fitness, so 25% of the first number is \npropagated to the \n&gt; second.  The first number is lowered by 25%, and the second \nincreased by the \n&gt; amount that the first was reduced.  The net weight remains \nunchanged. \n&gt; Likewise, a smaller portion of the 2nd number is propagated to the \nthird.  \n&gt; The last number is the &#39;base weight&#39;.  As long as fitness appears \nto be \n&gt; improving, changes propagate back and modify the base weight, \nslowly.\n&gt; \n&gt; Whereas, when there is a reduction in fitness, something else \nhappens.  All \n&gt; except the base weight are reduced by some percentage.  So the \noverall value \n&gt; tends to return back to its base value.  This allows the weight to \n&gt; temporarily adjust to a value that gives it greater fitness, and \nthen go \n&gt; back to what it was when that mutation is no longer beneficial, \nbut without \n&gt; having to find its way back to that base weight through random \nchance.  The \n&gt; longer the chain of weight propagation, the longer you can be at a \nnew \n&gt; weight value before that new value becomes permanent.  Or you can \nget the \n&gt; same effect with three numbers, but altering how fast the \nperturbations \n&gt; propagate back to the base weight.\n&gt; \n&gt; Now if only I was working on controllers, I might actually \nimplement it and \n&gt; see what the effect is...\n&gt; \n&gt; -- John\n\n\n"}}