{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":130984297,"authorName":"joel278","from":"&quot;joel278&quot; &lt;lehman.154@...&gt;","profile":"joel278","replyTo":"LIST","senderId":"h5vUtWIirtfMDKb34u1jHnaAin6Eyk_9-sSpiNHDK9syhbb_2ueFcsABp0mi_bQDqVra9LjmbI01A0PIGXfhFORIOoP1","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Introducing a New Approach to Search: Novelty Search (New Paper)","postDate":"1210865508","msgId":4069,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGcwaGwxNCtzYTNsQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGcwZ2thMCsxMHFxbkBlR3JvdXBzLmNvbT4="},"prevInTopic":4066,"nextInTopic":4072,"prevInTime":4068,"nextInTime":4070,"topicId":4038,"numMessagesInTopic":26,"msgSnippet":"Jeff, Reading your post led to an interesting insight concerning constraints in the behavioral space. If the behavior space is large and unconstrained, then I","rawEmail":"Return-Path: &lt;lehman.154@...&gt;\r\nX-Sender: lehman.154@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 54917 invoked from network); 15 May 2008 15:31:49 -0000\r\nX-Received: from unknown (66.218.67.97)\n  by m43.grp.scd.yahoo.com with QMQP; 15 May 2008 15:31:49 -0000\r\nX-Received: from unknown (HELO n20b.bullet.sp1.yahoo.com) (69.147.64.134)\n  by mta18.grp.scd.yahoo.com with SMTP; 15 May 2008 15:31:49 -0000\r\nX-Received: from [216.252.122.216] by n20.bullet.sp1.yahoo.com with NNFMP; 15 May 2008 15:31:49 -0000\r\nX-Received: from [209.73.164.86] by t1.bullet.sp1.yahoo.com with NNFMP; 15 May 2008 15:31:49 -0000\r\nX-Received: from [66.218.66.88] by t8.bullet.scd.yahoo.com with NNFMP; 15 May 2008 15:31:49 -0000\r\nDate: Thu, 15 May 2008 15:31:48 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;g0hl14+sa3l@...&gt;\r\nIn-Reply-To: &lt;g0gka0+10qqn@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;joel278&quot; &lt;lehman.154@...&gt;\r\nSubject: Re: Introducing a New Approach to Search: Novelty Search (New Paper)\r\nX-Yahoo-Group-Post: member; u=130984297; y=-ohfUb1VmkSrxaXVDuWLmDR1iHgUmQuK0S2gzVoP5pkG403HxWHn\r\nX-Yahoo-Profile: joel278\r\n\r\nJeff,\n\nReading your post led to an interesting insight concerning constrain=\r\nts\nin the behavioral space.\n\nIf the behavior space is large and unconstrain=\r\ned, then I agree that\nnovelty search may not do well and may resemble exhau=\r\nstive search;\nhowever, if behavior space is unconstrained then novelty sear=\r\nch may be\nunnecessary! Constraints in behavior space mean that some behavio=\r\nrs\nmay be very difficult to reach in the underlying search space\n(genotype =\r\nspace). If behaviors are easy to reach (no constraints),\nthen an objective-=\r\nbased method may do fine or it may be that\nexhaustive search is the best we=\r\n can do in general (e.g. evolving\npictures, where the amount of viable pict=\r\nures is limitless, and\ngradient-based methods to evolve to a certain pictur=\r\ne fail).\n\nIt seems like there also is a relation between constraints in the=\r\n\nbehavioral space and deception in the objective function. If the\nobjective=\r\n function is deceptive, this means that there exists a local\noptima such th=\r\nat from the behavior that maximizes the objective\nfunction there is no simp=\r\nle mutation on the underlying genotype(s)\nthat will yield a behavior that y=\r\nields further increase in the\nobjective function. That is, behavior space i=\r\ns constrained such that\nfrom the optima there are no connections in behavio=\r\nr space that lead\nto behaviors with higher fitness.\n\nIt may be the case tha=\r\nt when a domain has its behavioral space\nconstrained to a degree that fitne=\r\nss-based search will be deceived,\nthat those same constraints may make nove=\r\nlty search a feasible\napproach by limiting the degree of exhaustive search =\r\nthrough the\nbehavior space.\n\nJoel\n\n--- In neat@yahoogroups.com, &quot;Kenneth St=\r\nanley&quot; &lt;kstanley@...&gt; wrote:\n&gt;\n&gt; Jeff,\n&gt; \n&gt; Thanks for asking these questio=\r\nns.  These are likely the kinds of\n&gt; questions we are going to face over ti=\r\nme so it&#39;s a good chance to work\n&gt; on our response.  I&#39;m going to start wit=\r\nh a technical response but\n&gt; move to philosophical.  It&#39;s a bit of an essay=\r\n but hopefully worth the\n&gt; read.\n&gt; \n&gt; Your first set of questions regards t=\r\nhe relationship between novelty\n&gt; search (NS) and exhaustive search of beha=\r\nvioral space.  Do we think\n&gt; these are effectively the same thing?\n&gt; \n&gt; I t=\r\nhink it&#39;s a tricky subject but in the end I would not equate NS to\n&gt; exhaus=\r\ntive search.  While NS indeed does attempt to essentially visit\n&gt; every beh=\r\navior in behavioral space, the way it does it is not the same\n&gt; as what is =\r\nusually meant by exhaustive search.  We usually think of an\n&gt; exhaustive se=\r\narch as a purposeful enumeration, where if I just try\n&gt; every combination o=\r\nf something, one will be the answer.  A key feature\n&gt; of this type of searc=\r\nh is that it does not require or utilize\n&gt; information or feedback of any k=\r\nind as a guide.  Rather, it simply\n&gt; makes sure it does not visit the same =\r\npoint twice.  (It is similar to\n&gt; random search except that random search m=\r\night indeed visit the same\n&gt; point twice.)\n&gt; \n&gt; One clear difference betwee=\r\nn such a search and NS is that we cannot\n&gt; purposefully enumerate all possi=\r\nble behaviors because our search space\n&gt; is the genotype space, which maps =\r\nindirectly to the behavior space. \n&gt; Therefore, there is actually no way to=\r\n simply say &quot;list all possible\n&gt; behaviors&quot; and try each one in succession.=\r\n  We must seek them out.\n&gt; \n&gt; Furthermore, in novelty search there is *info=\r\nrmation* guiding the\n&gt; order of points we visit (the information is the nov=\r\nelty measure). \n&gt; The points also have an inherent order (perhaps quite use=\r\nful) induced\n&gt; by the genetic encoding (hence the promise of combining it w=\r\nith\n&gt; HyperNEAT). In other words, we are not simply enumerating in an\n&gt; arb=\r\nitrary non-overlapping order.  Rather, we follow the most promising\n&gt; trail=\r\ns that seem to be leading to something new.\n&gt; \n&gt; One significant consequenc=\r\ne of this fact is that the actual search\n&gt; space (which is the genotype spa=\r\nce) will almost certainly never need\n&gt; to be exhaustively searched.  So we =\r\neffectively avoided wasting our\n&gt; time examining all kinds of genetic combi=\r\nnations that would have\n&gt; produced redundant behaviors.  In its usual meani=\r\nng, exhaustive search\n&gt; would be just such a search through genotype space.=\r\n  So certainly it\n&gt; is not normal exhaustive search.\n&gt; \n&gt; Yet if you still =\r\nwant to talk about exhaustively searching behavior\n&gt; space, it is still not=\r\n a typical exhaustive search because of the fact\n&gt; it proceeds in an order =\r\nguided by information- not typical of\n&gt; exhaustive search.  Another corolla=\r\nry is that it is impossible to\n&gt; perform a typical exhaustive search of beh=\r\navior space because we have\n&gt; no method to enumerate it:  It must be explor=\r\ned.\n&gt; \n&gt; Yet you might still say, fine, semantically perhaps it is somethin=\r\ng\n&gt; different, but still, in spirit are you not essentially doing\n&gt; somethi=\r\nng equally inefficient, i.e. looking for absolutely everything?  \n&gt; \n&gt; And =\r\nit is true that in effect we are searching for every behavior, or\n&gt; at leas=\r\nt every behavior that looks distinguishable from each other\n&gt; with respect =\r\nto the novelty metric and in an order from simplicity to\n&gt; high complexity.=\r\n  Yet therein lies the fundamental insight: While such\n&gt; a search might sou=\r\nnd bad, it is a lot better to look at everything (in\n&gt; some meaningful orde=\r\nr) and eventually run across what you want\n&gt; (perhaps after a very long tim=\r\ne) than to look for one specific thing\n&gt; and *never* find it.\n&gt; \n&gt; We are i=\r\nndeed suggesting that this rather tortured choice (between\n&gt; looking for so=\r\nmething without hope and looking for everything) is\n&gt; sometimes the real ch=\r\noice that we face with the most ambitious\n&gt; problems.  In other words, obje=\r\nctive-based fitness is literally doomed\n&gt; when it comes to evolving many in=\r\ncredibly complex systems from\n&gt; scratch.  It will simply never happen becau=\r\nse at high levels of\n&gt; complexity, there is going to be massive deception a=\r\nt many levels, and\n&gt; fitness becomes as bad as random search in such a prob=\r\nlem.  In fact,\n&gt; as our results show, you don&#39;t even have to make the probl=\r\nem all that\n&gt; hard to cause fitness to crumble into something as bad as ran=\r\ndom\n&gt; search.  What do you think will happen if you are trying to evolve a\n=\r\n&gt; neural network into the mind of a quantum physicist based on how\n&gt; brilli=\r\nant it is at quantum physics?  The answer: Nothing.\n&gt; \n&gt; A more down-to-ear=\r\nth example is the car that I evolved on Picbreeder.\n&gt;  It could never have =\r\nevolved if evolving a car had been the\n&gt; *objective* because it was precede=\r\nd by an alien face (evolved by\n&gt; someone else).  It just so happens that th=\r\ne alien face is a stepping\n&gt; stone to a car, but if we had judged the alien=\r\n face on it &quot;car-ness,&quot;\n&gt; it would have failed miserably and been deleted. =\r\n Hence if we were\n&gt; looking for a car we would never have found one.\n&gt; \n&gt; I=\r\nn nature, if we had bred flatworms (the earliest chordates) based on\n&gt; thei=\r\nr humanity, they too would have never evolved into amphibians,\n&gt; then repti=\r\nles, then mammals, as they did.  The objective in long-run\n&gt; problems is to=\r\ntally blind to the necessary stepping stones.  Unless we\n&gt; know them a prio=\r\nri, we are lost.\n&gt; \n&gt; Why do you suppose we have never seen a flowering of =\r\ndiversity and\n&gt; complexity such as seen in nature in evolutionary computati=\r\non?  In 30\n&gt; or 40 years of this field, which is inspired by nature, we hav=\r\ne never\n&gt; seen something that comes even close to even nature&#39;s modest\n&gt; ac=\r\nhievements.  The problem is that we have been blinded by objectives.\n&gt;    A=\r\ns soon as we set an objective, the stepping stones to it often\n&gt; vanish int=\r\no thin air.  There is no escape from this very sobering\n&gt; fact.  It will ha=\r\nve to be accepted, though it will be hard to accept.\n&gt; \n&gt; Novelty search cu=\r\nres this problem, although admittedly in a painful\n&gt; way, because it forces=\r\n us to let go of our natural desire to *control*\n&gt; what is happening.  We f=\r\neel compelled to demand to the search\n&gt; algorithm that it go in a certain d=\r\nirection.  It is difficult to\n&gt; relinquish this feeling of control, yet if =\r\nwe recognize that in the\n&gt; end the setting of such goals is its own worst e=\r\nnemy, then we can\n&gt; begin to be liberated from this longstanding compulsion=\r\n.\n&gt; \n&gt; (Note that I am not saying fitness is always useless; obviously that=\r\n\n&gt; is not the case.  I am talking about very ambitious problems, beyond\n&gt; t=\r\nhe kind we have been able to solve yet- though even the &quot;hard maze&quot;\n&gt; is bo=\r\nrdering on such a problem.)\n&gt; \n&gt; You point out that novelty search may get =\r\n&quot;lost&quot; in a kind of endless\n&gt; offshoot in behavior space.  And yes, in some=\r\n situations, it very well\n&gt; may.  Yet I believe it will not do so in many i=\r\nnteresting domains. \n&gt; For example, if there was indeed a long offshoot of =\r\nthe maze, first,\n&gt; since there is a time limit for each robot, it would mak=\r\ne little\n&gt; difference and would quickly be filled up by novelty search.  If=\r\n there\n&gt; are no obstacles in the offshoot, it would be filled especially fa=\r\nst.\n&gt;  But at the same time, novelty search is not likely to go off in only=\r\n\n&gt; one direction anyway.  Because it is a population, it will go off in\n&gt; m=\r\nany directions at once.  While some parts may shoot down the\n&gt; diversionary=\r\n offshoot, at the same time others will begin to flow down\n&gt; the paths we w=\r\nant.  If it happens to take the wrong paths, it will\n&gt; fill that areas firs=\r\nt and eventually be pushed back into the areas we\n&gt; care about (only if eac=\r\nh individual was given infinite time could it\n&gt; be stuck in one area foreve=\r\nr)\n&gt; \n&gt; In the end, of course there is a chance it will fail to go the righ=\r\nt\n&gt; way in reasonable time; after all, it has no objective!  In fact, in\n&gt; =\r\nthe hard maze experiment, it did once fail to find a solution in 40\n&gt; runs.=\r\n  Yet look at how much more consistent it is than fitness-based\n&gt; search.  =\r\nSo the point is not that this is a guarantee (there will\n&gt; never be one).  =\r\nRather, it is a profoundly different kind of search,\n&gt; which is likely to r=\r\neach places that fitness-based search can never\n&gt; hope to touch.  So it ope=\r\nns up a whole new world of possibilities to\n&gt; evolutionary computation.  Th=\r\nat does not mean it solves everything; of\n&gt; course it may still not always =\r\ngive us what we want.  There is no\n&gt; method that will ever always give you =\r\nwhat you want.  Yet objective\n&gt; fitness is often even worse.  That is the s=\r\nobering moral of the story.\n&gt; \n&gt; So in my view, NS is actually suited for e=\r\nxactly those &quot;large spaces&quot;\n&gt; that you suggest it will have trouble in.  An=\r\nd by large I mean LARGE.\n&gt;  Those are the ones where objective-based fitnes=\r\ns has no hope.  These\n&gt; are spaces like life on earth, where it would be fu=\r\ntile (and silly) to\n&gt; start with a single cell and select offspring based o=\r\nn relative\n&gt; humanity.  The only reason we got to humans in nature is becau=\r\nse\n&gt; nobody said we had to get there.  It&#39;s almost paradoxical, but if you\n=\r\n&gt; accept it, it is an exciting liberation.  If we let go of the\n&gt; compulsio=\r\nn to be in control, we may find something we did not expect\n&gt; that is quite=\r\n significant.\n&gt; \n&gt; So actually the idea of running fitness-based search and=\r\n then novelty\n&gt; search is less exciting to us although we raise it as a pra=\r\nctical\n&gt; matter.  In some domains, objective fitness is simply impotent, an=\r\nd\n&gt; the compulsion to have some shred of guidance to hang onto is a false\n&gt;=\r\n comfort.  We will have to let go, and then, strangely, we will end up\n&gt; wh=\r\nere we want to be.  It&#39;s like when your grandparents told you that\n&gt; if you=\r\n stop worrying so much about making things work out, they will\n&gt; work out o=\r\nn their own.  Did you believe them?  Maybe there is more\n&gt; wisdom in it tha=\r\nn there appeared to be.\n&gt; \n&gt; ken\n&gt; \n&gt; \n&gt; --- In neat@yahoogroups.com, Jeff =\r\nClune &lt;jclune@&gt; wrote:\n&gt; &gt;\n&gt; &gt; Hello. \n&gt; &gt; \n&gt; &gt; Thanks for the thought prov=\r\noking paper. Here is my main question\n&gt; regarding\n&gt; &gt; the work: How would y=\r\nou differentiate the NSA (novelty search\nalgorithm)\n&gt; &gt; from exhaustive sea=\r\nrch in the behavior space?\n&gt; &gt; \n&gt; &gt; If you think there is a relevant differ=\r\nence between the NSA and\n&gt; exhaustive\n&gt; &gt; behavioral search, have you tried=\r\n using the former as a control and\n&gt; seeing\n&gt; &gt; how they compare?\n&gt; &gt; \n&gt; &gt; =\r\nIf the NSA is effectively exhaustive search in behavior space, it\n&gt; seems t=\r\no\n&gt; &gt; me that, while it will work well (and better than a GA) in small,\n&gt; d=\r\neceptive\n&gt; &gt; landscapes, it will not perform very well in large search spac=\r\nes.\n&gt; Imagine,\n&gt; &gt; for example, the paper&#39;s &quot;medium map&quot; but with a huge (n=\r\near\n&gt; infinite) open\n&gt; &gt; area to the left of the starting condition. It cou=\r\nld easily get lost\n&gt; over\n&gt; &gt; there indefinitely.\n&gt; &gt; \n&gt; &gt; You mention that=\r\n it helps to have the domain constrain the search\nspace.\n&gt; &gt; Does this just=\r\n mean that the (behavioral) search space has to be small\n&gt; &gt; enough that it=\r\n can an exhaustive search can deal with it? How will\n&gt; it fare\n&gt; &gt; in the m=\r\nuch larger search spaces of real-world problems, like\ncheckers?\n&gt; &gt; \n&gt; &gt; Us=\r\ning the NSA as something to bail out objective-search when it\n&gt; stagnates i=\r\ns\n&gt; &gt; an interesting suggestion. But at that point it should be compared to=\r\n\n&gt; &gt; fitness sharing. Would it do better than fitness sharing? I can\nthink =\r\nof\n&gt; &gt; reasons it might (because it truly is not tempted by deceptive\n&gt; tra=\r\nps), but\n&gt; &gt; it would useful to see it compared to fitness sharing controls=\r\n.\n&gt; &gt; \n&gt; &gt; Just my 2 cents. Thanks for putting this out there.\n&gt; &gt; \n&gt; &gt; \n&gt; =\r\n&gt; Cheers,\n&gt; &gt; Jeff Clune\n&gt; &gt; \n&gt; &gt; Digital Evolution Lab, Michigan State Uni=\r\nversity\n&gt; &gt; \n&gt; &gt; jclune@\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; &gt; From: Kenneth Stanley &lt;k=\r\nstanley@&gt;\n&gt; &gt; &gt; Reply-To: &quot;neat@yahoogroups.com&quot; &lt;neat@yahoogroups.com&gt;\n&gt; &gt;=\r\n &gt; Date: Fri, 09 May 2008 02:00:33 -0000\n&gt; &gt; &gt; To: &quot;neat@yahoogroups.com&quot; &lt;=\r\nneat@yahoogroups.com&gt;\n&gt; &gt; &gt; Subject: [neat] Introducing a New Approach to S=\r\nearch: Novelty\n&gt; Search (New\n&gt; &gt; &gt; Paper)\n&gt; &gt; &gt; \n&gt; &gt; &gt; Joel Lehman and I ar=\r\ne excited to announce our new publication to\n&gt; &gt; &gt; appear in the Eleventh I=\r\nnternational Conference on Articifial Life\n&gt; &gt; &gt; (ALIFE XI), called &quot;Exploi=\r\nting Open-Endedness to Solve Problems\n&gt; &gt; &gt; Through the Search for Novelty.=\r\n&quot;\n&gt; &gt; &gt; \n&gt; &gt; &gt; The paper is here:\n&gt; &gt; &gt; \n&gt; &gt; &gt; http://eplex.cs.ucf.edu/publ=\r\nications.html#lehman.alife08\n&gt; &gt; &gt; \n&gt; &gt; &gt; Direct link: http://eplex.cs.ucf.=\r\nedu/papers/lehman_alife08.pdf\n&gt; &gt; &gt; \n&gt; &gt; &gt; This paper is about a new kind o=\r\nf search (which works with NEAT)\nthat\n&gt; &gt; &gt; abandons the longstanding notio=\r\nn in all of machine learning that the\n&gt; &gt; &gt; gradient of search should be me=\r\nasured with respect to the ultimate\n&gt; &gt; &gt; objective.  In other words, it en=\r\ntirely abandons objectives and\n&gt; &gt; &gt; thereby also abandons fitness function=\r\ns as the impetus for search.\n&gt; &gt; &gt; Yet remarkably, we still show that such =\r\nan algorithm can perform\n&gt; &gt; &gt; *better* than one that actually tries to ach=\r\nieve the objective!  I\n&gt; &gt; &gt; believe this strange result has fascinating im=\r\nplications for machine\n&gt; &gt; &gt; learning, artificial life, and even biology.\n&gt;=\r\n &gt; &gt; \n&gt; &gt; &gt; Lately on this forum we have often discussed the nagging proble=\r\nm\nthat\n&gt; &gt; &gt; the fitness function often does not properly recognize or rewa=\r\nrd the\n&gt; &gt; &gt; stepping stones on the way to the solution.  I went as far as\n=\r\n&gt; &gt; &gt; suggesting that the fitness function can become an *obstacle* to\n&gt; &gt; =\r\n&gt; success (e.g. when we discussed creativity in Picbreeder).\n&gt; &gt; &gt; \n&gt; &gt; &gt; W=\r\nhile this discussion was largely philosophical, Joel Lehman and I\n&gt; &gt; &gt; dec=\r\nided to make it concrete and actually introduce an algorithm that\n&gt; &gt; &gt; mak=\r\nes an automated evolutionary process in *any* domain behave like\n&gt; &gt; &gt; huma=\r\nns in Picbreeder, that is, like open-ended evolution.  This\n&gt; &gt; &gt; approach =\r\nis called &quot;novelty search.&quot;  The algorithm simply searches\n&gt; &gt; &gt; for behavi=\r\nor that is novel with respect to what has come before.\n&gt; &gt; &gt; \n&gt; &gt; &gt; The ben=\r\nefit of this approach is that it is immune to deception\nbecause\n&gt; &gt; &gt; it do=\r\nes not even try to achieve the objective.  I know it seems\n&gt; &gt; &gt; strange bu=\r\nt, counterintuitively, we show that in fact it is far more\n&gt; &gt; &gt; effective =\r\nat solving a difficult problem in a deceptive\nlandscape than\n&gt; &gt; &gt; fitness-=\r\nbased search.\n&gt; &gt; &gt; \n&gt; &gt; &gt; In other words, what we are saying is that to ac=\r\nhieve some of\nthe most\n&gt; &gt; &gt; ambitious objectives we might have for evoluti=\r\non, we must abandon\n&gt; &gt; &gt; trying to explicitly achieve them.  To quote from=\r\n the end of our\n&gt; &gt; &gt; Discussion section:\n&gt; &gt; &gt; \n&gt; &gt; &gt; &quot;In summary, almost =\r\nlike a riddle, novelty search suggests\n&gt; &gt; &gt; a surprising new perspective o=\r\nn achievement: To achieve\n&gt; &gt; &gt; your highest goals, you must be willing to =\r\nabandon them.&quot;\n&gt; &gt; &gt; \n&gt; &gt; &gt; I believe this lesson is true in practice and i=\r\ns therefore beyond a\n&gt; &gt; &gt; philosophical curiosity.  In fact, we are instin=\r\nctively familiar\nwith\n&gt; &gt; &gt; it in life in general when people say things li=\r\nke &quot;You are\ntrying too\n&gt; &gt; &gt; hard&quot; or when we focus so much on something so=\r\n far ahead of us\nin life\n&gt; &gt; &gt; that we forget completely to solve the short=\r\n term problems that\nstand\n&gt; &gt; &gt; in our way.  It is no less true in evolutio=\r\nn or search in general.\n&gt; &gt; &gt; \n&gt; &gt; &gt; For NEAT, novelty search should open u=\r\np new opportunities for\n&gt; &gt; &gt; discovery that were previously closed off to =\r\nus.\n&gt; &gt; &gt; \n&gt; &gt; &gt; I look forward to hearing your thoughts on this work.\n&gt; &gt; =\r\n&gt; \n&gt; &gt; &gt; ken\n&gt; &gt; &gt;\n&gt; &gt;\n&gt;\n\n\n\n"}}