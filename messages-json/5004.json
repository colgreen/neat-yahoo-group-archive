{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":256087559,"authorName":"Andrei","from":"&quot;Andrei&quot; &lt;andrei.rusu@...&gt;","profile":"andrei.rusu","replyTo":"LIST","senderId":"anI6ylNyIop91BzpzjqaVW20u2G_qEdpmxq1plTpx7mZLP0_wKDDhZB6LyRVrVAOUDCbFOgfaqqKoCbDdV4r5PByGoL6ykw","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: solution for NEAT on CUDA","postDate":"1260524483","msgId":5004,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGhmdDQ0MythNGY5QGVHcm91cHMuY29tPg==","inReplyToHeader":"PDcyN2E0MDZjMDkxMjEwMTI0OHY1M2YxNjIwOWxlZTljY2M3Y2NlMGZhZGFkQG1haWwuZ21haWwuY29tPg=="},"prevInTopic":5003,"nextInTopic":5005,"prevInTime":5003,"nextInTime":5005,"topicId":4995,"numMessagesInTopic":8,"msgSnippet":"I m afraid that just moving the code to the GPU is not a good approach. The code has to be redesigned with the CUDA architecture in mind to get real speed-ups.","rawEmail":"Return-Path: &lt;andrei.rusu@...&gt;\r\nX-Sender: andrei.rusu@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 12704 invoked from network); 11 Dec 2009 09:41:24 -0000\r\nX-Received: from unknown (98.137.34.44)\n  by m14.grp.re1.yahoo.com with QMQP; 11 Dec 2009 09:41:24 -0000\r\nX-Received: from unknown (HELO n45b.bullet.mail.sp1.yahoo.com) (66.163.168.159)\n  by mta1.grp.sp2.yahoo.com with SMTP; 11 Dec 2009 09:41:24 -0000\r\nX-Received: from [69.147.65.151] by n45.bullet.mail.sp1.yahoo.com with NNFMP; 11 Dec 2009 09:41:24 -0000\r\nX-Received: from [98.137.34.34] by t5.bullet.mail.sp1.yahoo.com with NNFMP; 11 Dec 2009 09:41:24 -0000\r\nDate: Fri, 11 Dec 2009 09:41:23 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;hft443+a4f9@...&gt;\r\nIn-Reply-To: &lt;727a406c0912101248v53f16209lee9ccc7cce0fadad@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;Andrei&quot; &lt;andrei.rusu@...&gt;\r\nSubject: Re: solution for NEAT on CUDA\r\nX-Yahoo-Group-Post: member; u=256087559; y=QzEmO3WwT57qV0XbZh0mCW_-oN5BaC4mVdU6k7WRM4ZkadHq5zc\r\nX-Yahoo-Profile: andrei.rusu\r\n\r\nI&#39;m afraid that just moving the code to the GPU is not a good approach. The=\r\n code has to be redesigned with the CUDA architecture in mind to get real s=\r\npeed-ups. \n\nLet me give you just one example: if you don&#39;t pay attention to=\r\n the memory hierarchy in the GPU and just make random array accesses from m=\r\nain GPU memory, your speed-up will be 1.5x instead of, say, 10x. This is be=\r\ncause individual memory accesses on a GPU take so much time compared to the=\r\n cores&#39; processing speed. On the CPU you seldom have this in mind, since th=\r\ne CPU and compiler put your data in 2 or 3 levels of cache for you. This is=\r\n just one of the things CPU code is blind to, another being the actual orde=\r\nr of kernel execution on a GPU. Kernels on current GPUs are serialised, but=\r\n not for long.\n\nI know that Fermi addresses these problems directly, with b=\r\netter memory management and caches and also multiple kernel executions in p=\r\narallel. I wouldn&#39;t wait for that though, because from the [rumour warning]=\r\n April 2010 launch, it would take at least one year to get it for a decent =\r\nprice. It&#39;s not going to provide straightforward efficient porting anyway.\n=\r\n\nHope this make a case for redesigning the code with the GPU in mind. \n\nChe=\r\ners!\nAndrei.\n\n--- In neat@yahoogroups.com, Colin Green &lt;colin.green1@...&gt; w=\r\nrote:\n&gt;\n&gt; 2009/12/10 openmind767 &lt;openmind767@...&gt;\n&gt; &gt;\n&gt; &gt; The whole NEAT l=\r\nib and experiment are both full of branch. Making the whole things running =\r\non CUDA won&#39;t speed up, and it is\n&gt; &gt; not my target.\n&gt; \n&gt; Hi,\n&gt; \n&gt; So your =\r\nsaying that we:\n&gt; \n&gt; 1) Load all of the networks in a population into the G=\r\nPU\n&gt; 2) Activate them in parallel.\n&gt; 3) Read the outputs.\n&gt; \n&gt; For larger n=\r\networks (e.g. from HyperNEAT) we may only get one or two\n&gt; networks into th=\r\ne GPU, so it would seem we need to put as many\n&gt; networks as we can into th=\r\ne GPU at a time. I think this should be\n&gt; fairly easy to do and this touche=\r\ns on previous comments from Ken Lloyd\n&gt; who I believe has gone into this in=\r\n far more detail - e.g. by also\n&gt; loading problem domain code into the GPU =\r\nand trying to determine how\n&gt; best to divide GPU and CPU resources in the g=\r\neneral case (for a wide\n&gt; range of scenarios).\n&gt; \n&gt; Colin.\n&gt;\n\n\n\n"}}