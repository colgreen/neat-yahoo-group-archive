{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"KOsJod0uSUMDfVsxv2Ngw87PXJtnJPozDUMH_J3SRlIhxMp6kOhc_aqvCC4pD1ooo-vusicoc_2OjaQ1-LWBDKLFLb9MrhbMjoRxt1zlpGYs","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: Wave Generator Experiment / Mackey-Glass Series","postDate":"1131791965","msgId":2413,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGRsNGdvdCsxc2Q3QGVHcm91cHMuY29tPg==","inReplyToHeader":"PDQzNkU3N0Y2LjQwMjAzMDhAZHNsLnBpcGV4LmNvbT4="},"prevInTopic":2392,"nextInTopic":2429,"prevInTime":2412,"nextInTime":2414,"topicId":2391,"numMessagesInTopic":4,"msgSnippet":"Colin, you may want to try leaky integrator neurons in NEAT.  I think they would be great for this task.  These are neurons that have time constants that allow","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 48209 invoked from network); 12 Nov 2005 10:39:37 -0000\r\nReceived: from unknown (66.218.66.218)\n  by m15.grp.scd.yahoo.com with QMQP; 12 Nov 2005 10:39:37 -0000\r\nReceived: from unknown (HELO n7a.bulk.scd.yahoo.com) (66.94.237.41)\n  by mta3.grp.scd.yahoo.com with SMTP; 12 Nov 2005 10:39:37 -0000\r\nComment: DomainKeys? See http://antispam.yahoo.com/domainkeys\r\nReceived: from [66.218.69.4] by n7.bulk.scd.yahoo.com with NNFMP; 12 Nov 2005 10:39:26 -0000\r\nReceived: from [66.218.66.91] by mailer4.bulk.scd.yahoo.com with NNFMP; 12 Nov 2005 10:39:26 -0000\r\nDate: Sat, 12 Nov 2005 10:39:25 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;dl4got+1sd7@...&gt;\r\nIn-Reply-To: &lt;436E77F6.4020308@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: Re: Wave Generator Experiment / Mackey-Glass Series\r\nX-Yahoo-Group-Post: member; u=54567749; y=r2URGb1blJ9cJaBi4xteuuh_6u3toKBss8z4PIMzjXkej6ihefdV\r\nX-Yahoo-Profile: kenstanley01\r\n\r\nColin, you may want to try leaky integrator neurons in NEAT.  I \nthink they=\r\n would be great for this task.  These are neurons that \nhave time constants=\r\n that allow them to process inputs at different \nrates from one another.  O=\r\nf course to do this you&#39;d have to have \nNEAT evolve the time constants in t=\r\nhe neurons, but that has been \ndone before.  Tyler Streeter produced a leak=\r\ny integrator variant of \nmy NEAT code that was able to produce walking patt=\r\nerns for virtual \nbipeds with no inputs.  Basically, it&#39;s just another patt=\r\nern \ngeneration task.  And that&#39;s exactly what leaky integrator neurons \nar=\r\ne used for traditionally: pattern generation. Almost any neural \nmodel of l=\r\nocomotion or repeated ambulatory motion such as swimming \nuses leaky integr=\r\nator neurons.  They are much more powerful at \ngenerating this type of thin=\r\ng than a standard neural model. \n\nLeaky integrator neuron networks are also=\r\n called CTRNNs (continuous \ntime recurrent neural networks).  If you look a=\r\nt section 10.4.2 (in \nthe Discussion chapter) of my dissertation, you can f=\r\nind equations \nfor activating these types of neurons and also references to=\r\n other \npapers on the subject.  \n\nI&#39;m also putting Tyler&#39;s old leaky NEAT c=\r\node into the Files \nsection.  I have a feeling this may not be the easiest =\r\ncode to deal \nwith, and of course it&#39;s not SharpNEAT so you won&#39;t want to w=\r\nork off \nof it, but at least it might be a good reference for how the \nequa=\r\ntions can be coded up.\n\nTyler and I spent weeks writing back and forth to f=\r\nigure out how to \nimplement this type of update since it&#39;s usually only pub=\r\nlished as a \ndifferential equation (instead of as an update rule, which wou=\r\nld be \nway easier to comprehend).  But the final product was a lot of fun. =\r\n \nI spent hours fiddling with time constants in genome files produced \nby T=\r\nyler&#39;s biped program and watching the resultant pattern changes \nin the leg=\r\n motions (maybe I needed a real hobby!).  \n\nken\n\n\n\n--- In neat@yahoogroups.=\r\ncom, Colin Green &lt;cgreen@d...&gt; wrote:\n&gt;\n&gt; My initial function approximation=\r\n experiment tried to approximate \ny=3Df(x) \n&gt; by feeding values of x and co=\r\nmparing an ANN&#39;s output with the \nexpected \n&gt; output. So the ANN is mapping=\r\n x directly to y.\n&gt; \n&gt; I decided to experiment with an alternative techniqu=\r\ne whereby \nthere is \n&gt; no input and I use the ANN as a sort of wave generat=\r\nor. I activate \nthe \n&gt; network and take a reading and repeat this as many t=\r\nimes as I \nwant. I \n&gt; can then match the outputs against an expected output=\r\n and \nhopefully \n&gt; evolve ANNs that can generate a continuous wave form.\n&gt; =\r\n\n&gt; This does seem to work to some extent, e.g. I can evolve sine wave \n&gt; ge=\r\nnerators easily enough, and a square wave approximation has a \nlot of \n&gt; ro=\r\nugh edges and noise but the overall shape is there. So I decided \nto \n&gt; try=\r\n something a bit more challenging, specifically the Mackey-\nGlass \n&gt; series=\r\n, which can be made to generate a chaotic waveform. Here&#39;s \nwere I \n&gt; notic=\r\ned the first problem.\n&gt; \n&gt; If I use a low sampling rate then the output fro=\r\nm the series rises \nand \n&gt; falls within just a few activations of my ANN&#39;s =\r\nand they can do a \n&gt; reasonably good job of tracking the function over shor=\r\nt periods (I \n&gt; haven&#39;t tried anything too long yet). If however I increase=\r\n the \nsampling \n&gt; rate then there may be many activations between rises and=\r\n falls, \nbut the \n&gt; kind of small networks that I start out with when I ini=\r\ntiase a run \nhave \n&gt; a very fast oscilaltion rate (or high natural frequenc=\r\ny) due to \ntheir \n&gt; small size and compact interconnections. I suppose I co=\r\nuld try and \nstart \n&gt; with a population of pre-built networks with a lower =\r\noscillation \nrate, \n&gt; but I don&#39;t think this is a solution for the general =\r\ncase where a \n&gt; function may have many different harmonics.\n&gt; \n&gt; Anyway to =\r\ngive you an idea of the kind of results I&#39;ve been \ngetting \n&gt; here&#39;s a link=\r\n to a graph showing a short section of a mackey-glass \n&gt; waveform in black =\r\nand an approximation in red. The training data \nis \n&gt; shown on the first th=\r\nird of the graph up until the black line \ndrops to \n&gt; zero, the remaining t=\r\nwo thirds show the network&#39;s response beyond \nthe \n&gt; training data.\n&gt; \n&gt; ht=\r\ntp://tinypic.com/fdhssg.png\n&gt; \n&gt; The graph shows the network generating a f=\r\nairly regular output \nthat \n&gt; roughly matches the black line, but there is =\r\nno or very little \nmatching \n&gt; of the subtle variations. The low-res approx=\r\nimations I tried were \n&gt; generally better matching waveforms.\n&gt; \n&gt; Colin.\n&gt;=\r\n\n\n\n\n\n\n"}}