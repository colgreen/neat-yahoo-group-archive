{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":8147458,"authorName":"Christian","from":"&quot;Christian&quot; &lt;Christian.Hofmann@...&gt;","profile":"chhofchhof","replyTo":"LIST","senderId":"8tkwm07_E7HZIgEATX-FDLL9B3KaQhzVN1fOJTdxOy0jEvVqVO_fF0mRpkQzNijx2zO1nO_naAq7fqs5fQWjRQylmPsVmt6jpSquQYY","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Evaluating population with large training data set","postDate":"1218236405","msgId":4258,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGc3aWo1bCtzMTJmQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGE3ODdiNjA2MDgwODA4MTUwOWgzZmNhNzBhNXhhMDFlMzFlN2Q1MTdlNmM3QG1haWwuZ21haWwuY29tPg=="},"prevInTopic":4257,"nextInTopic":4259,"prevInTime":4257,"nextInTime":4259,"topicId":4253,"numMessagesInTopic":11,"msgSnippet":"Thank you Robert, that sounds exactly like what i am looking for! I also understand the theory, but have problems with the mathematics behind you theory. So I","rawEmail":"Return-Path: &lt;Christian.Hofmann@...&gt;\r\nX-Sender: Christian.Hofmann@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 5683 invoked from network); 8 Aug 2008 23:00:06 -0000\r\nX-Received: from unknown (66.218.67.95)\n  by m46.grp.scd.yahoo.com with QMQP; 8 Aug 2008 23:00:06 -0000\r\nX-Received: from unknown (HELO n37a.bullet.mail.sp1.yahoo.com) (66.163.168.131)\n  by mta16.grp.scd.yahoo.com with SMTP; 8 Aug 2008 23:00:06 -0000\r\nX-Received: from [216.252.122.216] by n37.bullet.mail.sp1.yahoo.com with NNFMP; 08 Aug 2008 23:00:06 -0000\r\nX-Received: from [66.218.69.1] by t1.bullet.sp1.yahoo.com with NNFMP; 08 Aug 2008 23:00:06 -0000\r\nX-Received: from [66.218.67.197] by t1.bullet.scd.yahoo.com with NNFMP; 08 Aug 2008 23:00:06 -0000\r\nDate: Fri, 08 Aug 2008 23:00:05 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;g7ij5l+s12f@...&gt;\r\nIn-Reply-To: &lt;a787b6060808081509h3fca70a5xa01e31e7d517e6c7@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;Christian&quot; &lt;Christian.Hofmann@...&gt;\r\nSubject: Re: Evaluating population with large training data set\r\nX-Yahoo-Group-Post: member; u=8147458; y=tyRBcc8wCtOTS_D2okSQ_kCPyL4vzXep83UII8BiCfV4yGugHQ\r\nX-Yahoo-Profile: chhofchhof\r\n\r\nThank you Robert,\n\nthat sounds exactly like what i am looking for!\n\nI also =\r\nunderstand the theory, but have problems with the mathematics\nbehind you th=\r\neory.\n\nSo I need to do the following steps:\n\n1) Choose one random dataset.\n=\r\n2) calculate a coordinate for this dataset in a x dimensional room.\nBut how=\r\n can I do this? Shall I take the whole input and output neurons\nas an n-dim=\r\nensional vector (n =3D # input + # output)?\n3) I choose an other random dat=\r\naset and calculate the coordinates for\nthis one.\n4) calculate the the dista=\r\nnce between both points. But how can i do this? \n...\nThe other things are e=\r\nasy :)\n\nI have looked for &quot;Spherical Exclusion&quot;, but haven&#39;t found anything=\r\n.\nis there an other keyword I can search for?\n\nThanks,\n\nChristian\n--- In ne=\r\nat@yahoogroups.com, &quot;Robert DeLisle&quot; &lt;rkdelisle@...&gt; wrote:\n&gt;\n&gt; Another pos=\r\nsible option would be to use a technique called Spherical\n&gt; Exclusion to pa=\r\nrtition your data.  The premise is that you chose a data\n&gt; point at random =\r\nand remove all those data points that are within a\n&gt; particular distance fr=\r\nom it, in other words, you remove all those\nwithin a\n&gt; sphere around that p=\r\noint.  Pick another point in the remaining set,\nremove\n&gt; the sphere around =\r\nit, etc.  The critical piece is defining the size\nof the\n&gt; sphere.  You can=\r\n easily use Euclidean distance and most likely determine\n&gt; what size sphere=\r\n gives you a certain percentage of the original data for\n&gt; your training se=\r\nt.  This has the appeal that the chosen points roughly\n&gt; represent centroid=\r\n in your data space.\n&gt; \n&gt; \n&gt; \n&gt; \n&gt; On Fri, Aug 8, 2008 at 3:44 PM, Christia=\r\nn &lt;Christian.Hofmann@...&gt; wrote:\n&gt; \n&gt; &gt;   Hello Sandor,\n&gt; &gt;\n&gt; &gt; thank you v=\r\nery much for your detailed post.\n&gt; &gt;\n&gt; &gt; Testing the younger generations wi=\r\nth less training data sounds very\n&gt; &gt; good. But currently I am using (Hyper=\r\n)SharpNeat and there is no way to\n&gt; &gt; get the genome age. I only have the n=\r\network itself and not the genom.\n&gt; &gt; But I think I could implement it.\n&gt; &gt;\n=\r\n&gt; &gt; You write that you don&#39;t want to use completely new random data (out\n&gt; =\r\n&gt; of training set) as you think every genome should have the same\n&gt; &gt; chanc=\r\ne. I don&#39;t think so. When you take one random training data set\n&gt; &gt; and per=\r\n random you have bad luck and some bad samples, every\n&gt; &gt; generation is tra=\r\nined wrong. But when you take a completely new random\n&gt; &gt; training data set=\r\n then bad genomes will get erased soon. Also you have\n&gt; &gt; a much bigger dev=\r\niation.\n&gt; &gt;\n&gt; &gt; So my next question is how big the training part of all sam=\r\nple data\n&gt; &gt; should be so I should have a good average.\n&gt; &gt;\n&gt; &gt; If I have o=\r\nne million training data, mabye 5 percent is enough. When I\n&gt; &gt; only have 1=\r\n000, I will need them all.\n&gt; &gt;\n&gt; &gt; By choosing better samples I could decre=\r\nment the needed value rate\n&gt; &gt; even further.\n&gt; &gt;\n&gt; &gt; Your advice about logg=\r\ning the fitness variance sounds very good. But\n&gt; &gt; what is a high variance?=\r\n Let&#39;s say my fitness range is from 0 to 1.\n&gt; &gt; Is, for example, 5% std. de=\r\nviation a level you raise or lower sample\n&gt; &gt; data amount?\n&gt; &gt;\n&gt; &gt; I am loo=\r\nking for something to give every training data set based on\n&gt; &gt; inputs and =\r\noutputs a special number. So I can choose the samples that\n&gt; &gt; are most cri=\r\ntical for the test.\n&gt; &gt;\n&gt; &gt; What&#39;s about that:\n&gt; &gt;\n&gt; &gt; I just sum up all in=\r\nput and output values. After that I can choose the\n&gt; &gt; sample data with the=\r\n most different sums. Maybe that will work. But\n&gt; &gt; when input 1 has the sa=\r\nme value as input 2 in a different test data\n&gt; &gt; (and vice versa with some =\r\nother inputs) they will get the same sum.\n&gt; &gt;\n&gt; &gt; So they are different but=\r\n would seen as identical. So this apoach is\n&gt; &gt; not good.\n&gt; &gt;\n&gt; &gt; Maybe som=\r\neone has an additional one?\n&gt; &gt;\n&gt; &gt; Kind regards,\n&gt; &gt;\n&gt; &gt; Christian\n&gt; &gt;\n\n\n\n"}}