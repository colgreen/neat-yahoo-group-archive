{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":127853030,"authorName":"Colin Green","from":"Colin Green &lt;cgreen@...&gt;","profile":"alienseedpod","replyTo":"LIST","senderId":"yKExzWKdypOmtfy-7CwYm6pcrEUjPz9C5n4wYKTp2w6FWx7eDZ_tH96L501_cpMjcp5QjaiOMhAj1LEgs0fRk9sEMyJZfyzdgA","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] Performance Sensitivity to float precision","postDate":"1095714239","msgId":1568,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQxNEY0NUJGLjgwMTA0MDNAZHNsLnBpcGV4LmNvbT4=","inReplyToHeader":"PDIwMDQwOTIwMTcwMzA0LjY5MTMyLnFtYWlsQHdlYjYwODA3Lm1haWwueWFob28uY29tPg==","referencesHeader":"PDIwMDQwOTIwMTcwMzA0LjY5MTMyLnFtYWlsQHdlYjYwODA3Lm1haWwueWFob28uY29tPg=="},"prevInTopic":1564,"nextInTopic":1569,"prevInTime":1567,"nextInTime":1569,"topicId":1555,"numMessagesInTopic":16,"msgSnippet":"... I agree with the consensus here that double precision floats are probably excessive for most problems. I have switched the neural network code in SharpNEAT","rawEmail":"Return-Path: &lt;cgreen@...&gt;\r\nX-Sender: cgreen@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 87420 invoked from network); 20 Sep 2004 21:04:08 -0000\r\nReceived: from unknown (66.218.66.217)\n  by m23.grp.scd.yahoo.com with QMQP; 20 Sep 2004 21:04:08 -0000\r\nReceived: from unknown (HELO shockwave.systems.pipex.net) (62.241.160.9)\n  by mta2.grp.scd.yahoo.com with SMTP; 20 Sep 2004 21:04:08 -0000\r\nReceived: from [10.0.0.10] (81-86-175-101.dsl.pipex.com [81.86.175.101])\n\tby shockwave.systems.pipex.net (Postfix) with ESMTP id 468981C0050F\n\tfor &lt;neat@yahoogroups.com&gt;; Mon, 20 Sep 2004 22:04:02 +0100 (BST)\r\nMessage-ID: &lt;414F45BF.8010403@...&gt;\r\nDate: Mon, 20 Sep 2004 22:03:59 +0100\r\nUser-Agent: Mozilla Thunderbird 0.7.1 (Windows/20040626)\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: neat@yahoogroups.com\r\nReferences: &lt;20040920170304.69132.qmail@...&gt;\r\nIn-Reply-To: &lt;20040920170304.69132.qmail@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Remote-IP: 62.241.160.9\r\nFrom: Colin Green &lt;cgreen@...&gt;\r\nSubject: Re: [neat] Performance Sensitivity to float precision\r\nX-Yahoo-Group-Post: member; u=127853030\r\nX-Yahoo-Profile: alienseedpod\r\n\r\nMitchell Timin wrote:\n\n&gt;--- kalmantech &lt;kalmantech@...&gt; wrote:\n&gt;\n&gt;  \n&gt;\n&gt;&gt;Has any research on NEAT performance verse float\n&gt;&gt;precision\n&gt;&gt;been done?  We are looking at a number of embedded\n&gt;&gt;applications\n&gt;&gt;and our processing platform is limited to single\n&gt;&gt;precision floats.\n&gt;&gt;    \n&gt;&gt;\n&gt;\n&gt;ALL of my ANN work has been with 32-bit floats.  I\n&gt;think it would be hard to find an application where\n&gt;more than 32 bits would be helpful.\n&gt;\n&gt;I used 32-bit floats to reduce execution time.\n&gt;\n&gt;  \n&gt;\nI agree with the consensus here that double precision floats are \nprobably excessive for most problems. I have switched the neural network \ncode in SharpNEAT over to floats to improve performance and I have not \nnoticed a detrimental effect in any of my  experiments to date. In fact \nI have considered implementing a neural net class that operates on \nintegers to get yet more performance. So although I know of no research \nas such, my best guess would be that floats are fine.\n\nAre you able to elaborate on the types of problem you&#39;re considering?\n\nColin.\n\n"}}