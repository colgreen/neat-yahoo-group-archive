{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Ken","from":"&quot;Ken&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"QPGYdSlGs_bMtbVfCU-hLJn4dMa7HfewRlBtm9B1RqcRQsnxdVaCeediDvldF-smqtjegABkmKc8vwz8jxp5x2JMufzX","spamInfo":{"isSpam":false,"reason":"3"},"subject":"Re: New paper on why modules evolve, and how to evolve modular artificial neural networks","postDate":"1360549605","msgId":5987,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGtmOWt0NStjdXBjQGVHcm91cHMuY29tPg=="},"prevInTopic":5986,"nextInTopic":5988,"prevInTime":5986,"nextInTime":5988,"topicId":5976,"numMessagesInTopic":30,"msgSnippet":"Hi Oliver, the point you make about bigger heads leading to lower fitness is indeed why I acknowledged that the issue in nature could likely be argued either","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 36498 invoked from network); 11 Feb 2013 02:26:49 -0000\r\nX-Received: from unknown (10.193.84.163)\n  by m12.grp.bf1.yahoo.com with QMQP; 11 Feb 2013 02:26:49 -0000\r\nX-Received: from unknown (HELO ng3-vm5.bullet.mail.gq1.yahoo.com) (98.136.219.31)\n  by mta3.grp.bf1.yahoo.com with SMTP; 11 Feb 2013 02:26:48 -0000\r\nX-Received: from [98.137.0.83] by ng3.bullet.mail.gq1.yahoo.com with NNFMP; 11 Feb 2013 02:26:48 -0000\r\nX-Received: from [10.193.94.45] by tg3.bullet.mail.gq1.yahoo.com with NNFMP; 11 Feb 2013 02:26:47 -0000\r\nDate: Mon, 11 Feb 2013 02:26:45 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;kf9kt5+cupc@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 2:3:4:0:0\r\nFrom: &quot;Ken&quot; &lt;kstanley@...&gt;\r\nSubject: Re: New paper on why modules evolve, and how to evolve modular artificial neural networks\r\nX-Yahoo-Group-Post: member; u=54567749; y=L-wbK7Ka2Uz0WwerHrReUV01XTO8GjtvBPiFMWojF-XYsCp_ExxI\r\nX-Yahoo-Profile: kenstanley01\r\n\r\n\n\nHi Oliver, the point you make about bigger heads leading to lower fitness=\r\n is indeed why I acknowledged that the issue in nature could likely be argu=\r\ned either way.  However, the argument about size does not necessarily have =\r\nto lead to a fitness cost, which is important to point out:  \n\nImagine that=\r\n you can take ten mutations in a row from the DNA that encodes the current =\r\nconnectivity of the brain.  It could simply be the case that, without any s=\r\nelection pressure whatsoever, the *probability* of those ten mutations lead=\r\ning to any substantial change in connection density is vanishingly small.  =\r\nIn other words, the vast majority of mutations simply do not increase conne=\r\nction density, which makes high connection density a priori improbable, reg=\r\nardless of selection or fitness.  Given the complexity of weaving millions =\r\nof long-range connections through a gigantic brain (which is hard because o=\r\nf physics), it is reasonable to believe that such a constraint on the encod=\r\ning indeed exists.  Thus you don&#39;t need fitness or selection to explain why=\r\n it would be held down.\n\nBest,\n\nken\n\n--- In neat@yahoogroups.com, Oliver Co=\r\nleman  wrote:\n&gt;\n&gt; First, congrats on the paper, Jeff et al, it was fascinat=\r\ning and exciting,\n&gt; I really enjoyed reading it.\n&gt; \n&gt; I just had some comme=\r\nnts on Ken&#39;s comments about biasing search via fitness\n&gt; versus encoding. I=\r\n get the impression that in nature it might not be so\n&gt; clear cut, at least=\r\n for the example given of connection count and length.\n&gt; While there may be=\r\n biases provided by encoding (I think at least in part\n&gt; indirectly via neu=\r\nronal spiking and synaptic plasticity dynamics), the\n&gt; example of not being=\r\n able to fit many connections into current head size\n&gt; and maintaining more=\r\n connections in terms of energy consumption seems more\n&gt; like a fitness bia=\r\ns: individuals with bigger heads are more likely to\n&gt; result in birth compl=\r\nications, lowering fitness, and more energy being sunk\n&gt; into more connecti=\r\nons, especially if those connections provide little\n&gt; benefit, would also p=\r\nrovide a fitness hit.\n&gt; \n&gt; Unlike nearly all EAs, in nature the encoding ha=\r\ns also been evolved.\n&gt; I wonder if these kinds of (fitness) pressures resul=\r\nted in the encoding of\n&gt; natural networks being evolved to produce low conn=\r\nection count networks\n&gt; (directly and/or via things like plasticity dynamic=\r\ns). Perhaps mitigating\n&gt; problems like &quot;dead-weight&quot; sub populations.\n&gt; \n&gt; =\r\nCheers,\n&gt; Oliver\n&gt;  On Feb 11, 2013 8:08 AM, &quot;Ken&quot;  wrote:\n&gt; \n&gt; &gt; **\n&gt; &gt;\n&gt; =\r\n&gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; I realized that my thoughts on biasing search were a little m=\r\nisleading in\n&gt; &gt; a couple ways so I wanted to clarify a couple points. Firs=\r\nt, I didn&#39;t do a\n&gt; &gt; good job acknowledging that the multiobjective framewo=\r\nrk does help to\n&gt; &gt; mitigate the danger of manipulating fitness. I shouldn&#39;=\r\nt have used the term\n&gt; &gt; &quot;fitness penalty&quot; because that sounds like it&#39;s no=\r\nt a separate objective.\n&gt; &gt;\n&gt; &gt; The other thing is that I should have elabo=\r\nrated when I said that using\n&gt; &gt; the encoding to bias the search is a good =\r\nidea &quot;if you can figure out a way\n&gt; &gt; to do it.&quot; The problem is that often =\r\nit will not be entirely unclear how\n&gt; &gt; the encoding can be manipulated to =\r\nencourage a certain type of behavior or\n&gt; &gt; property, so it&#39;s not always an=\r\n option. Sometimes fitness may be the only\n&gt; &gt; option, so it&#39;s not somethin=\r\ng that can simply be dismissed, and I think my\n&gt; &gt; prior post made it sound=\r\n like I was being dismissive.\n&gt; &gt;\n&gt; &gt; But hopefully my general point is sti=\r\nll useful, because at least when it\n&gt; &gt; comes to modularity, it is possible=\r\n to encourage it through the encoding\n&gt; &gt; (and arguably that&#39;s how it happe=\r\nned in nature). And also, while\n&gt; &gt; multiobjective optimization does reduce=\r\n deception to some extent, the\n&gt; &gt; problem is still there. For example, if =\r\nyou have a performance objective in\n&gt; &gt; a task and a low connectivity objec=\r\ntive, then if you have someone with\n&gt; &gt; (high performance, low connectivity=\r\n) and someone with (equally high\n&gt; &gt; performance, higher connectivity), the=\r\n one with higher connectivity will be\n&gt; &gt; dominated, even in the multiobjec=\r\ntive case, which means that if it is a\n&gt; &gt; deceptive stepping stone, we wil=\r\nl not be less likely to explore it. You\n&gt; &gt; also still maintain this &quot;dead-=\r\nweight&quot; subpopulation in the multiobjective\n&gt; &gt; case, because a subpopulati=\r\non can survive by having extremely low\n&gt; &gt; connectivity (yet performing ter=\r\nribly) and thereby dominating on the low\n&gt; &gt; connectivity objective. That s=\r\nubpopulation will never go away.\n&gt; &gt;\n&gt; &gt; So the issues are still there, but=\r\n I didn&#39;t do the best job acknowledging\n&gt; &gt; the mitigating factors. At the =\r\nleast, while you could reasonably argue for\n&gt; &gt; using fitness to provide a =\r\nbias in some cases, it&#39;s helpful to think about\n&gt; &gt; whether encoding bias i=\r\ns a viable alternative in any particular case.\n&gt; &gt;\n&gt; &gt; Best,\n&gt; &gt;\n&gt; &gt; ken\n&gt; =\r\n&gt;\n&gt; &gt; --- In neat@yahoogroups.com, &quot;Ken&quot; wrote:\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt; I w=\r\nanted to follow up on Alex&#39;s second question, on whether any approach\n&gt; &gt; b=\r\niased to low connectivity could work. I think that&#39;s a really interesting\n&gt;=\r\n &gt; question with a lot of deep implications and deserves some discussion.\n&gt;=\r\n &gt; &gt;\n&gt; &gt; &gt; First let me echo others&#39; congratulations to Jeff, Jean-Baptiste=\r\n, and\n&gt; &gt; Hod for this important publication, which dispels a lot of myths =\r\nabout the\n&gt; &gt; origins of modularity. Jeff has heard some of what I&#39;m postin=\r\ng here before\n&gt; &gt; in our own private discussions, but it&#39;s something I thin=\r\nk worth sharing on\n&gt; &gt; this group as well given the importance of the issue=\r\n.\n&gt; &gt; &gt;\n&gt; &gt; &gt; In particular, the question of how to bias a search is signif=\r\nicant not\n&gt; &gt; only for biasing towards low connectivity (for the purpose of=\r\n producing\n&gt; &gt; modularity), but potentially for all kinds of different prop=\r\nerties we might\n&gt; &gt; realize are important in the future. And I believe ther=\r\ne are two primary\n&gt; &gt; options: (1) Use the fitness function to bias the sea=\r\nrch, as Jeff et al.\n&gt; &gt; do; or (2) use the encoding to bias the search. Tha=\r\nt is, in the second\n&gt; &gt; option, you would not manipulate the fitness functi=\r\non in any way, but\n&gt; &gt; instead do something to the genetic encoding to make=\r\n it more likely to\n&gt; &gt; produce the phenotypic properties you want to see. I=\r\nn the case of low\n&gt; &gt; connectivity, one example of such a genetic bias is t=\r\nhrough an additional\n&gt; &gt; link expression output (LEO) on CPPNs in HyperNEAT=\r\n, which was introduced by\n&gt; &gt; Philip Verbancsics and myself:\n&gt; &gt; &gt;\n&gt; &gt; &gt; ht=\r\ntp://eplex.cs.ucf.edu/publications/2011/verbancsics-gecco11\n&gt; &gt; &gt;\n&gt; &gt; &gt; To =\r\nbias towards low connectivity with the LEO, you start with a CPPN\n&gt; &gt; that =\r\ntends to express short connections (instead of long ones), which keeps\n&gt; &gt; =\r\nconnectivity down overall, and also thereby seemed to help lead to modular\n=\r\n&gt; &gt; structures.\n&gt; &gt; &gt;\n&gt; &gt; &gt; But like I said there is a larger issue here th=\r\nan just modularity or low\n&gt; &gt; connectivity. The issue is how you should imp=\r\nlement a search bias of any\n&gt; &gt; type. And I believe that doing it by manipu=\r\nlating the fitness function is\n&gt; &gt; generally dangerous in the long run. Lik=\r\ne with many ideas in EC, it is\n&gt; &gt; likely to work well for relatively simpl=\r\ne problems, but the more we aim for\n&gt; &gt; high complexity, the more I think i=\r\nt will burden the search with unintended\n&gt; &gt; consequences that increasingly=\r\n warp the search off the best path.\n&gt; &gt; &gt;\n&gt; &gt; &gt; As a simple example, if you=\r\n directly manipulate fitness to be lower when\n&gt; &gt; connectivity is high, the=\r\nn you create a &quot;niche&quot; for networks of low\n&gt; &gt; connectivity that otherwise =\r\ndo nothing of any substance because they will\n&gt; &gt; be able to survive by hav=\r\ning a higher fitness than networks of high\n&gt; &gt; connectivity that also do no=\r\nthing of any substance. This niche of\n&gt; &gt; non-functional low-connectivity n=\r\networks is essentially a permanent\n&gt; &gt; dead-weight in the population that w=\r\nill last forever. Maybe it&#39;s not enough\n&gt; &gt; to kill you in simple problems,=\r\n but in complex problems it&#39;s something you\n&gt; &gt; probably can&#39;t afford.\n&gt; &gt; =\r\n&gt;\n&gt; &gt; &gt; More generally the issue is the usual problem of deception, which i=\r\ns\n&gt; &gt; compounded by anything you do with fitness. For example, in a complex=\r\n\n&gt; &gt; search space, there is a reasonable chance that the stepping stone to =\r\na\n&gt; &gt; good low-connectivity solution is something with higher connectivity.=\r\n By\n&gt; &gt; manipulating fitness, you are cutting out all chances of encounteri=\r\nng such\n&gt; &gt; a deceptive stepping stone. But even if you don&#39;t believe that =\r\ncould be\n&gt; &gt; true, the single-mindedness of always favoring low-connectivit=\r\ny could\n&gt; &gt; deceive you from many parts of the search space that might be s=\r\ntepping\n&gt; &gt; stones to something worthwhile, relating to connection density =\r\nor not.\n&gt; &gt; &gt;\n&gt; &gt; &gt; On the other hand, manipulating the encoding is differe=\r\nnt because in\n&gt; &gt; effect it actually reorganizes the structure of the searc=\r\nh space itself,\n&gt; &gt; which seems to me a more principled thing to do (if you=\r\n can figure out a\n&gt; &gt; way to do it). Because the thing is, in that case, yo=\r\nu do not need to worry\n&gt; &gt; about a permanent dead weight taking up some pro=\r\nportion of your population\n&gt; &gt; forever. Instead, while the encoding may *te=\r\nnd* to produce e.g.\n&gt; &gt; low-connectivity solutions, it can still escape tha=\r\nt tendency without any\n&gt; &gt; penalty to fitness. Furthermore, in reality the =\r\nbest situation regarding\n&gt; &gt; modularity and connectivity is probably rather=\r\n subtle, with most of the\n&gt; &gt; brain respecting the principle of low connect=\r\nivity, but with a number of\n&gt; &gt; critical exceptions in key areas, such as m=\r\najor inter-module hubs. A\n&gt; &gt; sophisticated encoding can allow its bias to =\r\nbend to make such nuanced\n&gt; &gt; exceptions (e.g. based on locations within a =\r\ngeometry), whereas a fitness\n&gt; &gt; penalty is a heavy hand and blunt instrume=\r\nnt that cannot but help always to\n&gt; &gt; demand global and holistic subservien=\r\nce to dogmatic universals (unless you\n&gt; &gt; are willing to take a hit in fitn=\r\ness).\n&gt; &gt; &gt;\n&gt; &gt; &gt; An interesting question in nature (where our brains evolv=\r\ned modular\n&gt; &gt; structure) is whether its tendency towards low connectivity =\r\nis a result of\n&gt; &gt; an aspect of fitness in the wild, or an aspect of encodi=\r\nng bias. I think\n&gt; &gt; there is a lot of room in this question for arguing ei=\r\nther way, but my\n&gt; &gt; hunch is that the bias is mostly in the encoding. My l=\r\nogic is that I think\n&gt; &gt; the reason that the connectivity of the brain is s=\r\no much lower than what it\n&gt; &gt; could be (e.g. it is a tiny fraction of every=\r\nthing-to-everything\n&gt; &gt; connectivity) is an artifact of physics rather than=\r\n an artifact of fitness.\n&gt; &gt; It is simply physically impossible for a giant=\r\n 100-billion-to-100-billion\n&gt; &gt; connectivity to fit in a head anything clos=\r\ne to our size. And physical\n&gt; &gt; impossibility is in some sense a property o=\r\nf encoding. That is, mutations\n&gt; &gt; that could step from a low-connectivity =\r\nbrain to a high one are few and far\n&gt; &gt; between simply because of physical =\r\nconstraint. So high-connectivity\n&gt; &gt; structures are simply a very small par=\r\nt of the search space of brains in\n&gt; &gt; the physical universe. However, at t=\r\nhe same time, you can still get\n&gt; &gt; long-range connections from time to tim=\r\ne because there is no universal\n&gt; &gt; penalty for doing so, just a lower a pr=\r\niori probability of such mutations\n&gt; &gt; occurring.\n&gt; &gt; &gt;\n&gt; &gt; &gt; In summary, t=\r\nhe key difference between the alternatives is that with\n&gt; &gt; fitness you are=\r\n saying &quot;stay out of this part of the search space&quot; whereas\n&gt; &gt; with encodi=\r\nng you are saying &quot;this part of the search space is much smaller\n&gt; &gt; and he=\r\nnce less likely to encounter.&quot;\n&gt; &gt; &gt;\n&gt; &gt; &gt; So, my speculation is that if yo=\r\nu want to bias the search in highly\n&gt; &gt; complex domains, the best way is th=\r\nrough the encoding. Fitness is a nasty\n&gt; &gt; quagmire that is deceptively tem=\r\npting to manipulate, but never plays by the\n&gt; &gt; rules you wish it would. Of=\r\n course, these are merely my own unproven\n&gt; &gt; intuitions and their veracity=\r\n remains to be demonstrated. But at least it&#39;s\n&gt; &gt; something to think about=\r\n.\n&gt; &gt; &gt;\n&gt; &gt; &gt; Best,\n&gt; &gt; &gt;\n&gt; &gt; &gt; ken\n&gt; &gt; &gt;\n&gt; &gt; &gt; --- In neat@yahoogroups.com=\r\n, Alexandre Devert wrote:\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; Hi,\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; =C2  Simple, =\r\nclean experiment, with sharp results, congrats on that,\n&gt; &gt; definitely\n&gt; &gt; =\r\n&gt; &gt; a step forward ! Of course, it begs for more questions. I would love\n&gt; =\r\n&gt; to hear\n&gt; &gt; &gt; &gt; you on such (fairly open) questions\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; =C2  =\r\n=C2 1) Do you think that selection pressure for low connectivity is\n&gt; &gt; suf=\r\nficient in\n&gt; &gt; &gt; &gt; itself to evolve large coherent networks, or is it just =\r\na piece of the\n&gt; &gt; puzzle ?\n&gt; &gt; &gt; &gt; =C2  =C2 2) Do you see your work as an =\r\nindication that any approach biased\n&gt; &gt; to low\n&gt; &gt; &gt; &gt; connectivity would r=\r\neproduce the result ? Or does the way you guys\n&gt; &gt; enforced\n&gt; &gt; &gt; &gt; this bi=\r\nas matters ?\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; To me=C2\n&gt; &gt; &gt; &gt; 1) =3D&gt; Part of the puzzle. S=\r\nhould see how well it scales for\n&gt; &gt; increasingly\n&gt; &gt; &gt; &gt; complex task, whe=\r\nn the connection graph gets bigger. A randomized=C2\n&gt; &gt; &gt; &gt; search process=\r\n=C2 on large graph sounds not so efficient, need something\n&gt; &gt; to guide it.=\r\n\n&gt; &gt; &gt; &gt; I advocate construction process that have a feedback from what the=\r\n\n&gt; &gt; neuron=C2\n&gt; &gt; &gt; &gt; network is computing. Don&#39;t know how to do it withou=\r\nt creepling\n&gt; &gt; computational\n&gt; &gt; &gt; &gt; cost tho...\n&gt; &gt; &gt; &gt; 2) =3D&gt; I guess t=\r\nhat the bias alone is enough, the way to introduce it\n&gt; &gt; might\n&gt; &gt; &gt; &gt; not=\r\n be such a big deal.=C2\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; Again, great work, very helpful con=\r\ntribution :)\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; Alex\n&gt; &gt; &gt; &gt; =C2\n&gt; &gt; &gt; &gt; Dr. Devert Alexandre\n=\r\n&gt; &gt; &gt; &gt; Researcher at the Nature Inspired Computation and Applications\n&gt; &gt; =\r\nLaboratory (NICAL)\n&gt; &gt; &gt; &gt; Lecturer at School Of Software Engineering of US=\r\nTC\n&gt; &gt; &gt; &gt; ----------------------------------------------------\n&gt; &gt; &gt; &gt; Hom=\r\nepage :=C2 http://www.marmakoide.org\n&gt; &gt; &gt; &gt; ------------------------------=\r\n----------------------\n&gt; &gt; &gt; &gt; 166 Renai Road, Dushu Lake Higher Education =\r\nTown\n&gt; &gt; &gt; &gt; Suzhou Industrial Park,\n&gt; &gt; &gt; &gt; Suzhou, Jiangsu, People&#39;s Repu=\r\nblic of China\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; ________________________________\n&gt; &gt; =\r\n&gt; &gt; From: Jeff Clune\n&gt; &gt; &gt; &gt; To: neat users group group\n&gt; &gt; &gt; &gt; Cc: Jean-Ba=\r\nptiste Mouret ; Hod Lipson\n&gt; &gt; &gt; &gt; Sent: Thursday, February 7, 2013 1:57 AM=\r\n\n&gt; &gt; &gt; &gt; Subject: [neat] New paper on why modules evolve, and how to evolve=\r\n\n&gt; &gt; modular artificial neural networks\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; =C2\n&gt; &gt; &gt; &gt;=\r\n Hello all,\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; I&#39;m extremely pleased to announce a new paper o=\r\nn a subject that\n&gt; &gt; many--including myself--think is critical to making si=\r\ngnificant progress in\n&gt; &gt; our field: the evolution of modularity.=C2\n&gt; &gt; &gt; =\r\n&gt;\n&gt; &gt; &gt; &gt; Jean-Baptiste Mouret, Hod Lipson and I have a new paper that=C2\n&gt;=\r\n &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; 1) sheds light on why modularity may evolve in biological ne=\r\ntworks\n&gt; &gt; (e.g. neural, genetic, metabolic, protein-protein, etc.)\n&gt; &gt; &gt; &gt;=\r\n\n&gt; &gt; &gt; &gt; 2) provides a simple technique for evolving neural networks that a=\r\nre\n&gt; &gt; modular and have increased evolvability, in that they adapt faster t=\r\no new\n&gt; &gt; environments. The modules that formed solved subproblems in the d=\r\nomain.=C2\n&gt; &gt; &gt; &gt; Cite:=C2 Clune J, Mouret J-B, Lipson H (2013) The evoluti=\r\nonary origins\n&gt; &gt; of modularity. Proceedings of the Royal Society B. 280: 2=\r\n0122863.=C2\n&gt; &gt; http://dx.doi.org/10.1098/rspb.2012.2863=C2 (pdf)\n&gt; &gt; &gt; &gt;\n&gt;=\r\n &gt; &gt; &gt; Abstract: A central biological question is how natural organisms are=\r\n\n&gt; &gt; so evolvable (capable of quickly adapting to new environments). A key\n=\r\n&gt; &gt; driver of evolvability is the widespread modularity of biological\n&gt; &gt; n=\r\networks=E2=80&quot;their organization as functional, sparsely connected\n&gt; &gt; subu=\r\nnits=E2=80&quot;but there is no consensus regarding why modularity itself\n&gt; &gt; ev=\r\nolved. Although most hypotheses assume indirect selection for\n&gt; &gt; evolvabil=\r\nity, here we demonstrate that the ubiquitous, direct selection\n&gt; &gt; pressure=\r\n to reduce the cost of connections between network nodes causes the\n&gt; &gt; eme=\r\nrgence of modular networks. Computational evolution experiments with\n&gt; &gt; se=\r\nlection pressures to maximize network performance and minimize connection\n&gt;=\r\n &gt; costs yield networks that are significantly more modular and more evolva=\r\nble\n&gt; &gt; than control experiments that only select for performance. These re=\r\nsults\n&gt; &gt; will catalyse research in numerous disciplines, such as neuroscie=\r\nnce and\n&gt; &gt; genetics, and enhance our ability to harness\n&gt; &gt; &gt; &gt; evolution =\r\nfor engineering purposes.\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; Video:=C2\n&gt; &gt; http://www.youtube.=\r\ncom/watch?feature=3Dplayer_embedded&v=3DSG4_aW8LMng\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; There h=\r\nas been some nice coverage of this work in the popular press,\n&gt; &gt; in case y=\r\nou are interested:\n&gt; &gt; &gt; &gt; National Geographic:=C2\n&gt; &gt; http://phenomena.nat=\r\nionalgeographic.com/2013/01/30/the-parts-of-life/MIT&#39;s\n&gt; &gt; Technology Revie=\r\nw:=C2\n&gt; &gt; http://www.technologyreview.com/view/428504/computer-scientists-r=\r\neproduce-the-evolution-of-evolvability/=C2 Fast\n&gt; &gt; Company:=C2\n&gt; &gt; http://=\r\nwww.fastcompany.com/3005313/evolved-brains-robots-creep-closer-animal-learn=\r\ningCornellChronicle:=C2\n&gt; &gt; http://www.news.cornell.edu/stories/Jan13/modNe=\r\ntwork.htmlScienceDaily:=C2\n&gt; &gt; http://www.sciencedaily.com/releases/2013/01=\r\n/130130082300.htm\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; Please let me know what you think and if =\r\nyou have any questions. I\n&gt; &gt; hope this work will help our field move forwa=\r\nrd!\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; Best regards,\n&gt; &gt; &gt; &gt; Jeff Clun=\r\ne\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; Assistant Professor\n&gt; &gt; &gt; &gt; Computer Science\n&gt; &gt; &gt; &gt; Univ=\r\nersity of Wyoming\n&gt; &gt; &gt; &gt; jclune@\n&gt; &gt; &gt; &gt; jeffclune.com\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt;\n&gt;=\r\n &gt;  \n&gt; &gt;\n&gt;\n\n\n\n"}}