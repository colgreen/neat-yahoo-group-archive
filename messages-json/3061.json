{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":235544548,"authorName":"Alexandre Devert","from":"Alexandre Devert &lt;marmakoide@...&gt;","profile":"marmakoide","replyTo":"LIST","senderId":"hZDZ7kHnhyUhc81dEOkNtV8BOFPVVhGfeK_vAWGEzn1-mDcJY47bkhh_u_c7aGGdrpi95gA6TE7LES5P9-94iisTyWKT_8mHE7kYQ4M","spamInfo":{"isSpam":false,"reason":"0"},"subject":"RE : [neat] Re: HyperNEAT Source Code Release","postDate":"1175104690","msgId":3061,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDUzOTMyMy4yNzQ0OC5xbUB3ZWI1NzAwOS5tYWlsLnJlMy55YWhvby5jb20+","inReplyToHeader":"PGV1ZTM1dStiYjhoQGVHcm91cHMuY29tPg=="},"prevInTopic":3060,"nextInTopic":3062,"prevInTime":3060,"nextInTime":3062,"topicId":3057,"numMessagesInTopic":8,"msgSnippet":"Hi I have my tricks too to make fast neural networks : * All the connection weights on a flat array * The neural network graph is flaten in an array of ","rawEmail":"Return-Path: &lt;marmakoide@...&gt;\r\nX-Sender: marmakoide@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 15721 invoked from network); 28 Mar 2007 17:58:27 -0000\r\nReceived: from unknown (66.218.66.72)\n  by m27.grp.scd.yahoo.com with QMQP; 28 Mar 2007 17:58:27 -0000\r\nReceived: from unknown (HELO web57009.mail.re3.yahoo.com) (66.196.97.113)\n  by mta14.grp.scd.yahoo.com with SMTP; 28 Mar 2007 17:58:27 -0000\r\nReceived: (qmail 31638 invoked by uid 60001); 28 Mar 2007 17:58:10 -0000\r\nX-YMail-OSG: kJ2chTwVM1nH3PWDfxXgeFFBtYlplxdU1Rrr95PkJpBboG963CTJnusC8lvN27tWFaGLsSKxY5N4WzYCcSpP_4ALACAeW.vWqroaZQpV5WvzQSMcWfb.At5v967P6Q--\r\nReceived: from [81.65.254.251] by web57009.mail.re3.yahoo.com via HTTP; Wed, 28 Mar 2007 19:58:10 CEST\r\nDate: Wed, 28 Mar 2007 19:58:10 +0200 (CEST)\r\nTo: neat@yahoogroups.com\r\nIn-Reply-To: &lt;eue35u+bb8h@...&gt;\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=iso-8859-1\r\nContent-Transfer-Encoding: 8bit\r\nMessage-ID: &lt;539323.27448.qm@...&gt;\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Alexandre Devert &lt;marmakoide@...&gt;\r\nSubject: RE : [neat] Re: HyperNEAT Source Code Release\r\nX-Yahoo-Group-Post: member; u=235544548; y=AFZkYKCUQ0O0PQMVCEYFP_eHvyIJqyPGcd0qeUeIrCvgJfH-Ig\r\nX-Yahoo-Profile: marmakoide\r\n\r\nHi\n\nI have my tricks too to make fast neural networks :\n* All the connection weights on a flat array\n* The neural network graph is flaten in an array of\nindexes. \n* To switch from previous to current state neurons, I\nuse an array of size 2N, where N is the number of\nneurons. A simple pointer swap, even faster than a\nmemcpy ;)\n\nThis way, I have no conditional in my update loop, and\nI limit the memory cache breaks to the minimum. Here\nit is my inner loop, to get an idea. \n\n------------------------------------------------------\nvoid\nNet::update(State& inState, IO& inIO) const {\n  swap(inState.mPreviousActivity,\ninState.mCurrentActivity);\n\n  const double* lWeight = mWeightsTab;\n  double* lActivity = inState.mCurrentActivity;\n  const size_t* lIndex = mDriveTab;\n\n  // For each neuron\n  for(size_t i = mNbNeurons; i != 0; --i) {\n    double lSum = 0.0f;\n\n    // Sum of the incoming neurons activations\n    for(size_t j = (*lIndex++); j != 0; --j)\n      lSum += inState.mPreviousActivity[(*lIndex++)] *\n(*lWeight++);\n\n    // Sum of the incoming inputs\n    for(size_t j = (*lIndex++); j != 0; --j)\n      lSum += (*lWeight++) * inIO.mInputs[*lIndex++];\n\n    // Activation of the current neuron\n    (*lActivity++) = 1.0 / (1.0 + exp(-lSum));\n  }\n\n  // Update the outputs\n  for(size_t i = 0; i != mNbOutputs; ++i)\n    inIO.mOutputs[i] =\ninState.mCurrentActivity[(*lIndex++)];\n}\n------------------------------------------------------\n\n\n--- Jason Gauci &lt;jgmath2000@...&gt; a �crit :\n\n&gt; --- In neat@yahoogroups.com, Stephen Waits\n&gt; &lt;steve@...&gt; wrote:\n&gt; &gt; Hi Jason,\n&gt; &gt; \n&gt; &gt; Congrats on the paper, it&#39;s good.\n&gt; \n&gt; Thanks!  I can&#39;t wait to present it =)\n&gt; \n&gt; &gt; \n&gt; &gt; I&#39;m *very* much looking forward to your code.\n&gt; &gt; \n&gt; &gt; That you can update ~90M neurons/second is\n&gt; impressive.\n&gt; &gt; \n&gt; &gt; --Steve\n&gt; &gt;\n&gt; \n&gt; I think the reason why my implementation is so fast\n&gt; is because of \n&gt; memory management.\n&gt; \n&gt; I allocate a block of data big enough to hold the\n&gt; entire network and \n&gt; structure it in this way:\n&gt; \n&gt; double *nodeValues;\n&gt; double *nodeNewValues;\n&gt; ActivationFunction *activationFunctions;\n&gt; NetworkIndexedLink *links;\n&gt; \n&gt; int numConstantNodes;\n&gt; \n&gt; All of the array pointers point to consecutive\n&gt; positions in one \n&gt; massive character array.  Because of this, I don&#39;t\n&gt; run into many \n&gt; paging issues while the network is updating.  Also,\n&gt; because it&#39;s all \n&gt; in one array, I don&#39;t have to go searching through\n&gt; the heap while I&#39;m \n&gt; updating.  Finally, I use core memory operations\n&gt; when possible.  For \n&gt; example, I can update the nodes in the network to\n&gt; their new values by \n&gt; doing this:\n&gt; \n&gt; memcpy(\n&gt; nodeValues+numConstantNodes,\n&gt; nodeNewValues+numConstantNodes,\n&gt; sizeof(double)*(numNodes-numConstantNodes)\n&gt; );\n&gt; \n&gt; This updates all of the non-input nodes without\n&gt; having to make a \n&gt; control structure and assign each node value\n&gt; individually.\n&gt; \n&gt; I think the network could be sped up even more by\n&gt; using 32-bit \n&gt; floating point numbers instead of doubles, but I\n&gt; haven&#39;t tried it.\n&gt; \n&gt; \n\n\n_______________________________\nMarmakoide aka Alexandre Devert \n_______________________________\n\n\n\t\n\n\t\n\t\t\n___________________________________________________________________________ \nD�couvrez une nouvelle fa�on d&#39;obtenir des r�ponses � toutes vos questions ! \nProfitez des connaissances, des opinions et des exp�riences des internautes sur Yahoo! Questions/R�ponses \nhttp://fr.answers.yahoo.com\n\n"}}