{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":344770077,"authorName":"Colin Green","from":"Colin Green &lt;colin.green1@...&gt;","profile":"alienseedpod","replyTo":"LIST","senderId":"tDQ3rXJ1Yf76_xujjbTi_YqYCn2GKPN7Y0otAnuhyicC-lOtvTXaS638jY8Himh2wMo1rrNVfF-nOqp0P48AziFphWfhj8UDvdDz","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Lessons From Data Mining - The Power of Ensembles","postDate":"1345971306","msgId":5844,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PENBRTBNK1lmOEoyWmNEYjJGOXZEUE83d3RqUnlKeUVXZTc9WEZUTjU3SldxbVdYVWY4d0BtYWlsLmdtYWlsLmNvbT4="},"prevInTopic":0,"nextInTopic":5845,"prevInTime":5843,"nextInTime":5845,"topicId":5844,"numMessagesInTopic":4,"msgSnippet":"Hi all, I m guessing that many on here will by now be away of the data mining competition web site - kaggle. If not then I d recommend checking it out as there","rawEmail":"Return-Path: &lt;colin.green1@...&gt;\r\nX-Sender: colin.green1@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 60726 invoked from network); 26 Aug 2012 08:55:07 -0000\r\nX-Received: from unknown (98.137.35.160)\n  by m2.grp.sp2.yahoo.com with QMQP; 26 Aug 2012 08:55:07 -0000\r\nX-Received: from unknown (HELO mail-ey0-f172.google.com) (209.85.215.172)\n  by mta4.grp.sp2.yahoo.com with SMTP; 26 Aug 2012 08:55:07 -0000\r\nX-Received: by eaai11 with SMTP id i11so1005845eaa.17\n        for &lt;neat@yahoogroups.com&gt;; Sun, 26 Aug 2012 01:55:06 -0700 (PDT)\r\nMIME-Version: 1.0\r\nX-Received: by 10.14.206.201 with SMTP id l49mr13624908eeo.3.1345971306530; Sun,\n 26 Aug 2012 01:55:06 -0700 (PDT)\r\nX-Received: by 10.14.136.15 with HTTP; Sun, 26 Aug 2012 01:55:06 -0700 (PDT)\r\nDate: Sun, 26 Aug 2012 09:55:06 +0100\r\nMessage-ID: &lt;CAE0M+Yf8J2ZcDb2F9vDPO7wtjRyJyEWe7=XFTN57JWqmWXUf8w@...&gt;\r\nTo: neat@yahoogroups.com\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Colin Green &lt;colin.green1@...&gt;\r\nSubject: Lessons From Data Mining - The Power of Ensembles\r\nX-Yahoo-Group-Post: member; u=344770077; y=ifhfIUzF6hxliA54GEGePl3AyvNLdfjiAn-dVfMAAgTzXegEJM7j\r\nX-Yahoo-Profile: alienseedpod\r\n\r\nHi all,\n\nI&#39;m guessing that many on here will by now be away of the data mining\ncompetition web site - kaggle. If not then I&#39;d recommend checking it\nout as there are some interesting data sets being put up there now on\na regular basis.\n\nOne of the the main lessons for me from competing in kaggle\ncompetitions has been that no matter which underlying technique I use\nto model the data it generally isn&#39;t competitive. To make that leap\ninto competitive modelling requires ensembles of models. And ensembles\nof many weak models can and do result in strong models. In fact the\nmain method in use on kaggle is Random Forests (ensemble of Decision\nTrees), in which each decision tree is generally a really poor model\nall by itself -  but they are very fast to construct, so you can build\nan ensemble of hundreds or thousands in a reasonable span of time.\n\nSi I&#39;d like to throw out some ideas regarding what the EC community\ncan learn from this, if anything. Does the power of ensembles apply to\nEC in any way?\n\n\nThe general approach is to take an underlying technique - for me this\nis usually a gradient descent approach - and to apply some randomness\nto it, such that each time you run it the model produced will be\ndifferent. One of the most common techniques is to use a random\nsub-set of the training data to learn against (e.g. bootstrap\naggregation) and/or to use a random sub-set of variables to model.\nBasically any sources of randomness you can think of that don&#39;t\ncompletely break the model.\n\nYou can imaging that each model in the ensemble is modelling the data\nin a slightly different way, but a no less correct way overall (they\nall score ok), and therefore when we combine them the errors that each\nof them make will tend to be averaged down - making a better overall\nmodel.\n\nI want to apply this idea NEAT but won&#39;t have time to work on it for a\nwhile so I thought I would throw the idea out there for feedback and\nmaybe it will spark some discussion. My initial thoughts on how this\ncould apply to NEAT are as follows:\n\n\n1) Each genome will now be made up of multiple distinct parts, each\npart representing a distinct ANN, so basically the new genome is a\ncollection of traditional NEAT genomes. Lets call the distinct parts\nchromosomes for now - maybe there is a better word to use there?\n\n2) We evaluate a genome by decoding each chromosome in turn into a\nnetwork and then combing the networks into an ensemble of networks .\nNote that this is functionally different to combining them into one\ngiant network, *if* you allow connections from the output nodes back\ninto the hidden nodes - that would mean that the sub-networks would be\nactually connected up and passing signals between each other.\n\n3) We can put the ensemble behind an interface such as INetwork, such\nthat existing code that handles and evaluates INetworks(s) continues\nas before.\n\n4) When performing crossover we can now choose to do this at the\nsub-network level as an extra option. So swap in a different\nsub-network(chromosome), or add and remove sub-networks, or add a new\nrandomly created sub-network.\n\n\nYou might think that this is not qualitatively different to what we\nhave. The ensemble could be represented with one large ANN and\ntherefore our existing genome structure (barring the issue about\nsharing of output nodes). My suspicion is that there *is* a\nqualitative difference here, that because the sub-nets can be copied\nin their entirety that they will co-evolve and each will focus on\nspecific functionality. It also possibly provides another mechanism\nfor allowing large scale changes to the ensemble ANN that are good and\nsound changes, rather than relying on fitness sharing and speciation\nto allow a chain of small bad mutations that lead to a larger positive\nchange.\n\nI could go into this a bit more but hopefully I&#39;ve said enough to\nconvey the idea.\n\nThoughts?\n\nColin\n\nP.S. I figure this approach is orthogonal to much of the current\nresearch such as novelty search and indirect encodings.\n\n"}}