{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":127853030,"authorName":"Colin Green","from":"Colin Green &lt;cgreen@...&gt;","profile":"alienseedpod","replyTo":"LIST","senderId":"_KV24v7p0n_oMA3ne5i_oBVGHUXmEUHtd-ujozo7no4-GvX1aPU7rWsl3TuswkME3Cory8-dHGABT18MzvTbDHE8kYkg5XuduA","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] Neuron functions","postDate":"1099428492","msgId":1683,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQxODdGMjhDLjUwNTAxMDBAZHNsLnBpcGV4LmNvbT4=","inReplyToHeader":"PDYuMS4yLjAuMC4yMDA0MTEwMjExNTgzMC4wMjUxNDcwOEBwb3AubWFpbC55YWhvby5jby51az4=","referencesHeader":"PDQxODY0NDM3LjEwNTA2MDJAZHNsLnBpcGV4LmNvbT4gPDYuMS4yLjAuMC4yMDA0MTEwMjExNTgzMC4wMjUxNDcwOEBwb3AubWFpbC55YWhvby5jby51az4="},"prevInTopic":1679,"nextInTopic":1686,"prevInTime":1682,"nextInTime":1684,"topicId":1668,"numMessagesInTopic":20,"msgSnippet":"... I m happy to open out the discussion. The objective is slightly vague but I started out thinking about how we could modify NEAT so that it can solve","rawEmail":"Return-Path: &lt;cgreen@...&gt;\r\nX-Sender: cgreen@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 15701 invoked from network); 2 Nov 2004 20:48:15 -0000\r\nReceived: from unknown (66.218.66.167)\n  by m14.grp.scd.yahoo.com with QMQP; 2 Nov 2004 20:48:15 -0000\r\nReceived: from unknown (HELO ranger.systems.pipex.net) (62.241.162.32)\n  by mta6.grp.scd.yahoo.com with SMTP; 2 Nov 2004 20:48:15 -0000\r\nReceived: from [10.0.0.10] (81-86-175-101.dsl.pipex.com [81.86.175.101])\n\tby ranger.systems.pipex.net (Postfix) with ESMTP id 5CF7FE0002E4\n\tfor &lt;neat@yahoogroups.com&gt;; Tue,  2 Nov 2004 20:48:09 +0000 (GMT)\r\nMessage-ID: &lt;4187F28C.5050100@...&gt;\r\nDate: Tue, 02 Nov 2004 20:48:12 +0000\r\nUser-Agent: Mozilla Thunderbird 0.7.1 (Windows/20040626)\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: neat@yahoogroups.com\r\nReferences: &lt;41864437.1050602@...&gt; &lt;6.1.2.0.0.20041102115830.02514708@...&gt;\r\nIn-Reply-To: &lt;6.1.2.0.0.20041102115830.02514708@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Remote-IP: 62.241.162.32\r\nFrom: Colin Green &lt;cgreen@...&gt;\r\nSubject: Re: [neat] Neuron functions\r\nX-Yahoo-Group-Post: member; u=127853030\r\nX-Yahoo-Profile: alienseedpod\r\n\r\nIan Badcoe wrote:\n\n&gt;Hi,\n&gt;         Whilst I&#39;m glad to see this idea getting explored.  I wonder \n&gt;whether you need to think through what the objective is.\n&gt;  \n&gt;\nI&#39;m happy to open out the discussion. The objective is slightly vague \nbut I started out thinking about how we could modify NEAT so that it can \nsolve problems such as multiplication and 6-multiplexer. While I&#39;m sure \nthat plain old sigmoid neurons (POSN&#39;s) can represent solutions to these \nproblems, the structures required are non-trivial and therefore are hard \nto discover, especially if the solution requires multiple instances of \nsay, a multiplier structure, as required by the vector cross product \nproblem. So in one respect, adding extra neuron functions could be seen \nas a stopgap measure in place of a modular encoding.\n\nOn the other hand you could take the view that some functions are so \nfundamental, that incorporating them into the neuron makes sense for all \nNEAT variations, modular or otherwise. As I say, I think there&#39;s a \nbalance between providing functions and increasing the parameter/search \nspace, and not having so many and reducing the search space. Biological \nneural nets suggest the right balance might be towards adding functions.\n\n&gt;         e.g. what I mean is, when we have a statement like:\n&gt;\n&gt;  \n&gt;\n&gt;&gt;Multiply (can also perform division by using input&lt;1.0)\n&gt;&gt;    \n&gt;&gt;\n&gt;\n&gt;         (which I guess was originally me?) then I realized that the part \n&gt;in braces is only true if we also have a means to find the reciprocal of \n&gt;the input.  _BUT_ reciprocal is a very unnatural operator to include, e.g. \n&gt;because natural neurones have a fixed range of activation (call it 0 - 1) \n&gt;and (i) reciprocal would need to output infinity sometimes, and (ii) even \n&gt;if we regard it as more like an inverse, outputting 1 continually just \n&gt;because all your inputs are off doesn&#39;t sound plausible for a biological \n&gt;system (e.g. burning energy to do nothing).\n&gt;  \n&gt;\nActually I added the comment about division and removed division from \nthe list. Mainly because division does cause some problems, like how do \nyou &#39;divide&#39; a set of numbers at the collection function stage? The \noutput range problem can be addressed by using log scales or some other \nmapping system. Normally such systems are only applied to map a \nnetwork&#39;s output signals to a meaningful range, but internally we let \nthe network work this sort of thing out for itself, so that it might \nwell use a combination of multiplication and inversion to calculate the \nrecipricol - but using it&#39;s own representation system internally.\n\nAnyway this is my thinking for dropping division, and I *think* it makes \nsense.\n\n&gt;         So, you might want to think about whether your overall objective \n&gt;is &quot;nerve-like&quot; or &quot;maths-like&quot; and select your function set \n&gt;accordingly.  Personally I would go with nerve-like, because maths-like \n&gt;would be very like GP and we know GP is very fragile w.r.t mutation.  Now \n&gt;once again I have to say I&#39;m not just backing natural approaches because \n&gt;they are natural.  I just come to this with an intuition and end up[ using \n&gt;nature to explain it....\n&gt;  \n&gt;\nI agree the fragility of maths based approaches is a concern, and I \ntried to address this with my weighted functions idea, but thinking it \nthrough that approach has problems. (i) Expands the search space even \nfurther. (ii) Allows neurons with the same innovation ID to describe \ndistinct behaviours, thus reducing the efficacy of crossover.\n\n&gt;         w.r.t your plan for collector functions and activation functions, \n&gt;I like it.  I&#39;m not sure I like so much adding a whole new layer just to \n&gt;support &quot;leaky integrators&quot; however.\n&gt;  \n&gt;\nMe neither. I might drop the leaky-integrator function altogether. If we \nselect neuron functions carefully then we should be able to represent \nthis type of circuit using a simple arrangement of neurons.\n\n&gt;         Did you consider making it an homologous pool of functions.  e.g. \n&gt;not distinguishing collectors from activators but allowing them to be \n&gt;plugged in any order.  That way if the system needs linearity through some \n&gt;sub-net, it does not need to select a whole load of &quot;linear&quot; activators, it \n&gt;just omits the activation functions altogether.  That would cover the leaky \n&gt;integrator as well...\n&gt;  \n&gt;\nI don&#39;t follow. One of the proposed activation functions is a \nstraight-through/linear function, surely selecting that function has the \nsame effect?\n\n&gt;         Alternatively, you could try some sort of single neurone which \n&gt;could cover the whole (or a lot) of what you intend.  But I would only do \n&gt;that if you can come up with a single neurone design which forms a coherent \n&gt;whole and doesn&#39;t look like several unrelated ideas bolded together...\n&gt;\n&gt;How about:\n&gt;         one type of neurone with several &quot;input channels&quot;, allow multiple \n&gt;connections to each channel and sum them.\n&gt;\n&gt;         Sum_Channel - like in a standard ANN\n&gt;         Act_Channel - makes the neurone more activatable by tightening the \n&gt;sigmoid\n&gt;         Scale_Channel - multiplies the input\n&gt;\n&gt;  \n&gt;\nThis is similar to an idea that I came across some years ago, whereby \nthe output of a neuron could become a connection weight between two \nother neurons. Thus the dynamic connection weight acts like a switch, \nkind of like the base pin on a transistor. Unfortunately that project \nnever got off the ground.\n\nI like the idea though, and the model you descibe covers a lot of what \nI&#39;m trying to achieve. e.g. neurons that can act as a switch should be \nuseful in solving the 6-multiplexer problem, since that is a problem \nbased around switching. Multiplication should also do well because by \nincreasing the input to Act_Channel we increase the neurons sensitivity, \nthus performing a form of multiplication. This model also fits in nicely \nwith the current genome encoding - all we need is an extra &#39;target \nchannel&#39; field on each connection.\n\n\n\n&gt;         (scale and act are different in that when act is very low, the \n&gt;sigmoid becomes linear, but when scale is very low, the sigmoid just \n&gt;becomes very shallow)\n&gt;\n&gt;         Wiring Act and Scale just to the bias would be the same as having \n&gt;fixed, mutatable &quot;curve shape&quot; parameters on the node.  Wiring them to \n&gt;other inputs could give you potentiation/depotentiation (those are where X \n&gt;does not trigger Y but does increase the ability of Z to trigger Y).\n&gt;  \n&gt;\nI&#39;m not sure I fully understand what you are describing. We can \nparameterise activation using something like:\n\ny = 1 /  (1+exp(-x * activation))\n\nBut if you then scale the whole thing:\n\ny = scale* (1/(1+exp(-x * activation)))\n\nthen the function now outputs over the range 0.0 to scale, instead of \n0.0 to 1.0. Is this what you meant? And would this be beneficial?\n\nAlso note that if either activation or scale are -ve then the sigmoid is \nflipped, which could well be useful. Another option is to translate the \nsigmoid, but I think values can be translated (on the x-axis) by passing \nthrough a neuron with a linear activation function and a bias input (the \ntranslation amount).\n\nSo the main question that arises is - should we maintain an activation \noutput range of 0.0 to 1.0 at all times, or is it beneficial to output \nover other ranges? I would resist this on the basis that keeping signals \nwithin the same range provides us with a sort of standardisation that \naids compatibility of neurons, and thus aids evolution.\n\n&gt;p.s. OTOH, mean, median, min and max collectors feel &quot;right&quot; to me, &quot;mode&quot; \n&gt;is less intuitive...\n&gt;  \n&gt;\nBecause the number of inputs is fixed, mean is already calculated (sort \nof) by adding input signals and applying some scaling factor (connection \nweight). Just as with reciprical it&#39;s not the precise answer we are \nafter, but the functionality or the shape of the function.\n\nThe system you described might still have difficulty with certain types \nof problem, but it does potentially expand NEATs ability to find \nsolutions and does so without significant disruption to the existing \ncode base. I think it would be an interesting exercise to speculate on \nwhat sorts of problems this model couldn&#39;t handle well,  e.g. or we \ncould just implement a function regression problem domain which allows \nus to throw all sorts of difficult functions at NEAT.\n\nA possible extension to the model would be to define a more generic \nparameterised activation function that can represent a wider range of \nfunctionality.\n\nColin.\n\n\n"}}