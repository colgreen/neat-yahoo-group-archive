{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":7192225,"authorName":"Ian Badcoe","from":"Ian Badcoe &lt;ian_badcoe@...&gt;","profile":"ian_badcoe","replyTo":"LIST","senderId":"5LzT8yWW-snripAA99-yhzydp_CiHHt0Cx2oiuSJVXKHaqxi9g7h5EzZzLEi_C4JxgTKKV04TQ3j17rnPShndNtB91D4Wme1J9k","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] Symmetry, concepts and data buses in the brain","postDate":"1107121191","msgId":1842,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDUuMS4wLjE0LjAuMjAwNTAxMjcwOTIyMTMuMDBhNDFjZjBAcG9wLm1haWwueWFob28uY28udWs+","inReplyToHeader":"PDQxRjJBRjYzLjcwNzA4MDJAZHNsLnBpcGV4LmNvbT4=","referencesHeader":"PDYuMi4wLjE0LjAuMjAwNDEyMjExNzIxNTYuMDNiYTYyZThAcG9wLm1haWwueWFob28uY28udWs+IDw0MUM3NDQwQi41MDEwMzA1QGRzbC5waXBleC5jb20+IDxjcTdzZnQraHNvZ0BlR3JvdXBzLmNvbT4gPDYuMi4wLjE0LjAuMjAwNDEyMjExNzIxNTYuMDNiYTYyZThAcG9wLm1haWwueWFob28uY28udWs+"},"prevInTopic":1841,"nextInTopic":1844,"prevInTime":1841,"nextInTime":1843,"topicId":1698,"numMessagesInTopic":40,"msgSnippet":"Hi Colin, This is broadly the point I was trying to make. Except that I wasn t going to mention the C word (consciousness, that is) because it is just too","rawEmail":"Return-Path: &lt;ian_badcoe@...&gt;\r\nX-Sender: ian_badcoe@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 26714 invoked from network); 30 Jan 2005 21:39:46 -0000\r\nReceived: from unknown (66.218.66.172)\n  by m22.grp.scd.yahoo.com with QMQP; 30 Jan 2005 21:39:46 -0000\r\nReceived: from unknown (HELO cmailg2.svr.pol.co.uk) (195.92.195.172)\n  by mta4.grp.scd.yahoo.com with SMTP; 30 Jan 2005 21:39:46 -0000\r\nReceived: from modem-2519.porcupine.dialup.pol.co.uk ([217.134.201.215] helo=giles.yahoo.co.uk)\n\tby cmailg2.svr.pol.co.uk with esmtp (Exim 4.14)\n\tid 1CvMnD-0006qA-On\n\tfor neat@yahoogroups.com; Sun, 30 Jan 2005 21:39:40 +0000\r\nMessage-Id: &lt;5.1.0.14.0.20050127092213.00a41cf0@...&gt;\r\nX-Sender: ian_badcoe@...\r\nX-Mailer: QUALCOMM Windows Eudora Version 5.1\r\nDate: Sun, 30 Jan 2005 21:39:51 +0000\r\nTo: neat@yahoogroups.com\r\nIn-Reply-To: &lt;41F2AF63.7070802@...&gt;\r\nReferences: &lt;6.2.0.14.0.20041221172156.03ba62e8@...&gt;\n &lt;41C7440B.5010305@...&gt;\n &lt;cq7sft+hsog@...&gt;\n &lt;6.2.0.14.0.20041221172156.03ba62e8@...&gt;\r\nMime-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;us-ascii&quot;; format=flowed\r\nX-eGroups-Remote-IP: 195.92.195.172\r\nFrom: Ian Badcoe &lt;ian_badcoe@...&gt;\r\nSubject: Re: [neat] Symmetry, concepts and data buses in the brain\r\nX-Yahoo-Group-Post: member; u=7192225\r\nX-Yahoo-Profile: ian_badcoe\r\n\r\nHi Colin,\n\n         This is broadly the point I was trying to make.\n\n         Except that I wasn&#39;t going to mention &quot;the C word&quot; (consciousness, \nthat is) because it is just too problematic by far, being mired in \nphilosophical complexities.  Also, it is is far above out current level \nthat it is impractical to try and draw examples from it.  e.g. because we \ndo not even know how many levels of complexity lie between the neurone and \nthe mind, let alone what they are and how they work.\n\n         OTOH, consciousness is the only tool I have to examine my own \nabilities (since the wife refuses to let me put a 3 ton brain-scanner in \nthe living room)....\n\n         So, I&#39;m trying to look at those big things that brains do \n(consciousness, memory, ...), and then extrapolate back to features that \nmust (or might) be present at the low level.  e.g. The idea of the \nartificial neurone was just such an abstraction, real neurones are not so \nlike the mathematical models we all use, but I think we all feel they \ncapture some critical properties of the real systems.\n\n--\n\n         Neurology literature is something I do not really go directly \ninto, but fortunately the area gets reported fairly frequently in some of \nthe deeper &quot;popular science&quot; books.  I&#39;m just reading &quot;Consciousness \nExplained&quot; by Daniel C Dennett and I expect him to come around with quite a \nfew examples of the current neurology structure vs. function ideas before \nhe&#39;s done.  Then I&#39;ve seen several people recommend &quot;Society of Mind&quot; for \nits neurological back ground (whether you believe the central tenet of it \nor not).\n\n--\n\n&gt;The first thing to note is that I home in on small groups of pieces and\n&gt;consider them in isolation of the rest of the board. When detecting\n&gt;patterns I won&#39;t always mentally rotate the pieces, usually I need to do\n&gt;this at first but then learn to recognise the various patterns in their\n&gt;different rotations directly. Secondly, the high level process of\n&gt;identifying interesting groups, rotating them and deciding if they are\n&gt;interesting is a concious process (state the obvious alert!),\n&gt;undoubtedly there is a heck of a lot of sub-concious activity (e.g.\n&gt;visual cortex) but  I see that as mostly being much lower level\n&gt;processes such as building a mental model of the board from the visual\n&gt;input - which is not what we&#39;re discussing.\n\nHoming in on groups of pieces is, I believe, the way we mere mortals play \nchess.  Grand masters, OTOH, are reputed to see the whole board more as a \nsingle strategic situation.  I interpret this as possible because the lower \nlevel &quot;groups of pieces&quot; stuff that we have to think about; they have \npractised so much that it is no longer conscious.  So what I&#39;m saying is \nthat when we use consciousness for this, we are actually using a &quot;bigger \ngun&quot; than is strictly required, and unconscious systems can do it, and \nmaybe they are far simpler solution, but one that needs creating and is not \nbuilt in...\n\nHaving to rotate groups consciously at first, but later recognising them \nimmediately is perhaps a case where a general purpose &quot;module&quot; is used in \nthe early stages, but later a custom system is set up to do the same thing \nmore efficiently (faster) for a limited domain?\n\nUnless what&#39;s happening is that the general module is always doing it, and \nalways doing it fast, but at first it is competing with 2 dozen other \ngeneral modules (rotation, skew, mirroring, parity switch (black for \nwhite), occlusion, sphere of influence, framblicity, pleth, and \ncristophalency -- we will not necessarily find familiar concepts here...) \nso the master processor (be it conscious or not) has a lot to choose from, \nwhich slows it down.  Later the master &quot;knows&quot; from experience which \npre-process modules to pay attention to, and the whole thing becomes instant...\n\nIn either case it could be multiple pattern-recognisers (one for each \norientation) or a general purpose module.  But I feel the general module \nhas to be there if we are going to recognise the relationship in the first \ninstance.  I may be fooling myself...\n\n--\n\nThe visual-cortex stuff is an interesting point.  e.g. we might actually \n_not_ have any ability to learn symmetric relationships.  It might be that \nit is something built in to us during evolution.  Presented by an equally \nsimply but non built-in relationship (err... well I cannot name one, \nobviously, because I would be unaware of it) we may have far more trouble \nrecognising any relationship at all.  Rather like (say) seeing the \nrelationship between &quot;be&quot;, &quot;is&quot; and &quot;am&quot;, but not &quot;etre&quot;, &quot;es&quot; and &quot;suis&quot;.\n\nIn that case, the presence of symmetrical understanding in the visual \ncortex (VC) is leveraged by conscious processes (either (i) by routing \nprocessing through part of the VC, or (ii) because the information \nhistorically fed from the VC has long ago required the higher systems to \ngrasp symmetry, or (iii) because the VC symmetry module has been \ngenetically copied into the higher systems) and there may be no general \n&quot;symmetries&quot; learning system at all.  In any case, all of these systems \nwould still need some sort of &quot;formats&quot; to talk to one another...\n\n--\n\nOn the subject of lower and higher level processes.  I&#39;m not sure it is \nclear-cut.  OK, so clearly there are processes which are &quot;below&quot; what we \nare talking about.  But one of the messages I have taken from various \nreadings in AI/neurology over the years, is that things are not done in a \ncleanly hierarchic manner.  Or rather they maybe, but feedback from higher \nlevels of organization is required before lower levels can completely \nclassify their input.\n\nExamples:\n\n1) &quot;He took the plate with the bun and ate it.&quot; vs. &quot;He took the plate with \nthe bun and smashed it.&quot;\n\n         -- no amount of context-based parsing will assign the &quot;it&quot; \ncorrectly to either the plate or the bun.  In fact, the only thing that \nenables us to assign that it at all is high-level semantic knowledge about \nthe natures of plates and buns.  To prove it:\n\n         &quot;He took the plate with the vase and smashed it.&quot;\n\n         -- which was smashed?  You don&#39;t know, because both plates and \nvases are good candidates for smashing...\n\n2) Looking at a noisy picture of some 3D geometry, the levels in the brain \ndo something like:\n\n         &quot;Pixel&quot; level -&gt; Edge detection level -&gt; Edge grouping level -&gt; \nGeneral 3D metry level -&gt; &quot;things we expect in this scene&quot; level\n\nor:\n\n         &quot;Pixie&quot; -&gt; &quot;Edgy&quot; -&gt; &quot;Groupy&quot; -&gt; &quot;General Threed&quot; -&gt; &quot;Thingy&quot;\n\n         But each of those arrows runs both ways as well, thus we get a \n&quot;conversation&quot; something like this:\n\nPixie: Here you go!\nEdgy: OK, three clear edges and..., wait a minute is this another edge here.\nPixels: Hang on, I&#39;ll look more carefully.\nEdgy: That&#39;s better, I see it isn&#39;t an edge now.\nGroupy: Hang on, I&#39;ve got an edge here that doesn&#39;t connect to anything...\nEdgy: Just a second.... OK, there might be another edge *here*\nGroupy: Yes I can believe that.\nGeneral Threed: That&#39;s very nice but we have what looks like a cube with no \ntop face\nGroupy: Really?  Let me look again...\n\n... and so on until ...\n\nThingy: Good work guys, but I was expecting concave objects.  Maybe you&#39;ve \ngot the light on the wrong side?\nAll: Bugger!\n\n--\n\nYou could view this as each higher level module has the option to &quot;tweak&quot; \nsettings on the previous module (&quot;I can&#39;t see anything on the left, turn up \nthe gain a bit&quot;) in order to get the information it needs.\n\nThis is, of course, the source of optical illusions, where we deliberately \npresent one of the levels with a picture that will mislead it, and it gets \ntaken in by the trick and &quot;persuades&quot; both higher and lower level systems \nto go along.  The &quot;two faces and a vase&quot; or &quot;old crone vs. young woman&quot; \nprove that this goes on right up the level of &quot;Thingy&quot; and Necker Cubes \ndemonstrate it happening to &quot;General Threed&quot;.\n\n--\n\nNow, all of those are built in, I believe (or learned in infancy which, \nsince it is almost always over before one starts a degree by research, is \njust as much a fait accompli) but the feedback does not stop there, and a \nhigher level still &quot;learned&quot; module can just as easily go back to the lower \nlevels and ask them to change their minds.\n\nIn fact, the idea of asking for a change of mind after the event is \nmisleading, because the lower levels actually need to have these settings \nset _before_ they begin processing at all.  As in illustration, another \nclass of illusion, the &quot;Dalmatian on a spotty backgroud&quot; illusion shows \nthat often you cannot see the dog _at_all_ until after a higher level \nsystem has got the idea and started suggest to lower levels that &quot;a dog \nmight be involved somewhere&quot;.\n\n--\n\nBut all of the previous two sections are about processes which are way more \ncomplex than we can really hope to imitate.\n\nThey do give us a nice take home message however, see at the bottom.\n\nI think I _am_ discussing those &quot;lower level&quot; processes -- if only because \nthose are the only ones we have any hope of getting near in an programming \nexperiment -- but that I have to use the language of the higher lever \nprocesses (mixed liberally with terms from computer science) because that \nis the only tool I have to explore it with.  Dennet in particular issues a \nlot of warnings about the fallibility of trying to fathom ones own mind -- \nconsider me warned but still pressing on.\n\n&gt;Some perhaps obvious notes to make at this point:\n&gt;\n&gt;1) My concious thought can generally only focus on one thing at a time.\n&gt;I can quickly switch back to very recent thoughts (short term memory),\n&gt;allowing me to consider, say,  a number of different moves and how well\n&gt;they relate to each other. But I cannot consider them concurrently. As I\n&gt;consider each one I am assigning a sort of rating to it, while also\n&gt;trying to keep in mind the best rated move so far.\n\nIs your consciousness performing this procedure, or merely monitoring \nit.  e.g. when you return to one of your recent memories, is it really the \nsame as it was, or has it actually been processed in the mean-time and you \nare not so much returning to it as reviewing its current status.  You would \nnot necessarily know if a memory had been updated because, by definition, \nwhat has changed is your *memory* of it and the only record of its previous \nstate.\n\nI mention this just for perspective, not to pick holes in your analysis.\n\n&gt;So it seems to me that the concious part of the brain is loading in\n&gt;single thoughts from the short and long term memory and acting a little\n&gt;like a CPU loading and executing single instructions at a time. Only\n&gt;instead of a simple instruction, the conciousness loads a chunck or unit\n&gt;of thought. These units I would expect to have a consistent structure to\n&gt;them so that they can be read/written to memory and passed around the\n&gt;brain using the same communication channels. This includes all thoughts,\n&gt;from simple sensory based information such as images, sounds, taste and\n&gt;sensation, but also including all other types of thoughts such as facts,\n&gt;language elements, concepts such as magnitude, orientation, momentum,\n&gt;hardness, the list goes on.\n\nI think there is a flaw in this view (and since it is taking me days to \nwrite this reply :)  I have now read enough of &quot;consciousness explained&quot; to \nknow that Dennett doesn&#39;t like it either; this argument I present is mine, \nhowever, not his).\n\nThe problem is right there in the first line with the word &quot;loading&quot;.\n\nIn order for it to be possible for any thought what-so-ever to be loaded \ninto the &quot;consciousness module&quot;, then the consciousness module would have \nto speak in a data format so general and all-encompassing that it was \ncapable of representing _anything_ in human experience: pain, geometry, \nlove, minor thirds, social obligation, unease, the smell of wet clay, \nplans, the technique of juggling...\n\nCall this the &quot;omni-format&quot; (OF)\n\nSuch a general format has many problems with it (bearing in mind that my \nuse of &quot;format&quot; is only a metaphor for whatever really exists, but that the \nproblems with formats will apply equally to whatever that is).  I was \nwriting these problems out, but I just remembered my purpose here is not to \ndiscuss consciousness, so I will leave it at this.  We can get into the \nproblems in a more specific thread, if anybody wants to.\n\nI do recommend reading Dennett, however.  He&#39;s very interesting....\n\n&gt;Actually language is quite an interesting thing to consider. If I hear\n&gt;some phrase and remember it I would assume that I don&#39;t remember it as a\n&gt;chunk of audio like some mental equivalent of a WAV file, instead I am\n&gt;remembering the sequence of (recognised) words. Which leads to the\n&gt;question - am I remebering a copy of each word&#39;s unit representation as\n&gt;recalled from the language part of the brain, or am I remembering a sort\n&gt;of pointer to each word?\n\nThis is a question that I have also pondered.  Do not completely dismiss \nthe wav-file aspect, however, because you can also remember songs and tone \nof voice...\n\nI think that the &quot;recalling a sequence of pointers&quot;, although a very \nstrained metaphor, is a good one to think though.  e.g. it works really \nwell if we suppose that words are handled (at some level) as labels for \nconcepts, and each concept is stored in some sort of &quot;module&quot; (&quot;module&quot; \nhere possibly not with exactly the same implications as the purely \nfunctional ones we were discussing earlier) because we can suppose that the \n&quot;pointers&quot; we stored are actual connections to those remote modules (or \nrouting information for reaching them, if we feel that a &quot;connect \neverything to everything&quot; approach is unworkable for the long-lived brain) \nand that playing back the memory simply triggers the connections in the \nright order.\n\nThere is even some (very very crude) brain scan evidence for this...\n\n&gt;Also consider how different facts, ideas and concepts (thought units)\n&gt;become related in the brain. Most of the time these thought units are\n&gt;just sitting in long term memory and not taking part in concious\n&gt;thought. e.g. If I say to you &quot;think of a yellow submarine&quot; you&#39;ve just\n&gt;recalled what a submarine is, and probably some related thoughts about\n&gt;The Beatles. Previously those thoughts were sitting dormant in your\n&gt;memory. So is there a direct physical connection between the phrase\n&gt;yellow-submarine, the concept of a submarine that is yellow and the\n&gt;beatles memory? or do these memories respond to the same stimulus, e.g.\n&gt;they had all been &#39;indexed&#39; under yellow-sub but not physically\n&gt;connected.  I suppose something like the later approach makes sense,\n&gt;otherwise you might have two unrelated memories in physically seperate\n&gt;parts of the hippocampus (or wherever memory is), which would make\n&gt;physically connecting at some later time difficult if say some new fact\n&gt;was introduced that linked the two.\n\nI think this last idea is crucial.  e.g. the question of whether memories \nget associated together by:\n\n(i) being stored together\n(ii) being connected (however remote) when the association occurs, or,\n(iii) being somehow associated by not modifying the memories, but by \nwritting in the index that they are associated\n\nI suspect that our current data/understanding is completely inadequate to \nusefully theorise.  One can easily pick holes in all three ideas.\n\nOne further thought, however.  Who says we have only one &quot;submarine&quot; \nconcept stored.  e.g. we might have a custom store for &quot;submarines&quot; and \nanother for &quot;atomic submarines&quot; and another for &quot;that submarine I went on a \ntour of in Portsmouth&quot; and another for &quot;Beatles Yellow \nSubmarine&quot;.  Obviously direct wiring between all of these is impractical \n(because of the quantity of required wire for an N-squared interconnection \nof all our memories) but maybe memories could each have a sort of \n&quot;fingerprint&quot; (or hash) that they write to a central location when \nactivated, and other memories would read the current fingerprints, say &quot;oh \nyes, I&#39;m also about that!&quot;, and activate themselves...\n\n&gt;So I would hazard a guess that thoughts are packaged up and passed\n&gt;around the brain via something analogous to a data bus.\n\nExcept that I would query the existence of an omni format that would be \nsuitable for any databus.  Or at least, a common data bus might transmit \nomni format but any pair of modules using the bus would actually speak \nquite small subsets of it.  And even then the common data bus might not \nexist but really consist of a lot of specialised busses, each with their \nown format.\n\n&gt;  As for modules\n&gt;that perform different functions - well perhaps these are analogous to\n&gt;the various execution circuits in a CPU and as such they would be fixed\n&gt;structures that operate on whatever data that is passed to them. This\n&gt;might explain how we can apply these funtions in different contexts.\n&gt;Although I strongly suspect that the &#39;execution units&#39; are highly\n&gt;abstract and that new skills that are learned are actually just new\n&gt;memories that cause a given chain of thought units to get recalled,\n&gt;perhaps some thought units specify how to combine other thoughts with\n&gt;the execution units to achieve a result.\n\nPossibly.  If we suppose that all the basic units are built into the brain \nby evolution and that &quot;skills&quot; can only draw on those units we already have.\n\nHowever, I would like to believe we also create new units as we go along, \nc.f. the grand master no longer needing any time to look at the small \ngroups of pieces... It is this case which leads me to speculate that the \ncreating of new units is at least as much about creating formats for \nbetween units as creating the units themselves.  You could if you wish, \nregard the creation of a new format as a means of extending the &quot;sum of all \nformats&quot; which gives you the effect of an omni format without actually \nhaving one...\n\n&gt;The area of the brain where thought units are retrieved to and processed\n&gt;could then be said to be where conciousness lies, but that would\n&gt;probably be a gross oversimplification.\n\nOnce again, Dennet has a lot to say on this...\n\n---\n\n         Ian (with apologies for taking so long and being so verbose)\n\n\n\n\nhttp://livingathome.sourceforge.net/ - evolution for the desktop\n\n\n\n"}}