{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":283334584,"authorName":"petar_chervenski","from":"&quot;petar_chervenski&quot; &lt;petar_chervenski@...&gt;","profile":"petar_chervenski","replyTo":"LIST","senderId":"ZjBp8aGcTWbsQR9i-9AufXO7ypktBWomgZrj72JGSYwSKVaAPrH9zFY4gFLmGU-LBa6A1x4yfJiMjWus8uJykmWGYstNh2pIKqMQouUfBNxg9AD-kQ0","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: rtNEAT: max_depth() in an endless loop","postDate":"1225758827","msgId":4414,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGdlbzU5YitmYzI4QGVHcm91cHMuY29tPg==","inReplyToHeader":"PDQ5MEY0QjgyLjYwMzA4MDZAeWFob28uY29tPg=="},"prevInTopic":4413,"nextInTopic":4420,"prevInTime":4413,"nextInTime":4415,"topicId":4396,"numMessagesInTopic":16,"msgSnippet":"Jim, Computing the depth of a network does not have anything to do with the way you activate the network. By activating a network I mean just what you say, the","rawEmail":"Return-Path: &lt;petar_chervenski@...&gt;\r\nX-Sender: petar_chervenski@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 9988 invoked from network); 4 Nov 2008 00:33:48 -0000\r\nX-Received: from unknown (66.218.67.96)\n  by m35.grp.scd.yahoo.com with QMQP; 4 Nov 2008 00:33:48 -0000\r\nX-Received: from unknown (HELO n50a.bullet.mail.sp1.yahoo.com) (66.163.168.144)\n  by mta17.grp.scd.yahoo.com with SMTP; 4 Nov 2008 00:33:48 -0000\r\nX-Received: from [69.147.65.151] by n50.bullet.mail.sp1.yahoo.com with NNFMP; 04 Nov 2008 00:33:48 -0000\r\nX-Received: from [66.218.69.1] by t5.bullet.mail.sp1.yahoo.com with NNFMP; 04 Nov 2008 00:33:48 -0000\r\nX-Received: from [69.147.65.171] by t1.bullet.scd.yahoo.com with NNFMP; 04 Nov 2008 00:33:47 -0000\r\nX-Received: from [66.218.66.90] by t13.bullet.mail.sp1.yahoo.com with NNFMP; 04 Nov 2008 00:33:47 -0000\r\nDate: Tue, 04 Nov 2008 00:33:47 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;geo59b+fc28@...&gt;\r\nIn-Reply-To: &lt;490F4B82.6030806@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;petar_chervenski&quot; &lt;petar_chervenski@...&gt;\r\nSubject: Re: rtNEAT: max_depth() in an endless loop\r\nX-Yahoo-Group-Post: member; u=283334584; y=9YIC4yYKeQ-ayTSavcCNm_PCpEb5O7KKFk_43lfjunsPsNvK2Hgy9Qaykw\r\nX-Yahoo-Profile: petar_chervenski\r\n\r\nJim, \n\nComputing the depth of a network does not have anything to do with t=\r\nhe \nway you activate the network. By activating a network I mean just what =\r\n\nyou say, the information flow across the whole network from one neuron \nto=\r\n the next in a single time step. But in classification tasks, you \nwould pr=\r\nefer a kind of &quot;strong&quot; output, that does not change if you continue inputt=\r\ning the same pattern and activating. Like in XOR, or an \nimage recognition =\r\ntask, you take the output as a &quot;snapshot&quot;, all \ninformation flowing from th=\r\ne inputs to outputs all the way. This can \nbe achieved by activating the ne=\r\ntwork a number of times (depending of \nits depth), before taking the output=\r\n. \nIn order to allow that information to travel the whole way to the \noutpu=\r\nts without any loss, you need the maximum number of links from an \ninput to=\r\n an output, because the more links along the way, the more \ntimes you need =\r\nto activate the network to allow the information to \ntravel to the outputs.=\r\n \nBut looped or recurrent networks can produce different outputs every \ntim=\r\ne even if you do input the same pattern at each timestep.\nAnd more, because=\r\n of speed issues, you need the real depth of the \nnetwork, not some excessi=\r\nvely high value, so here is the need to \ncompute the depth of the network. =\r\nLooped networks have infinite depth. \nAnd this is the problem that Cesar is=\r\n encountering. \n\nPeter\n\n--- In neat@yahoogroups.com, Jim O&#39;Flaherty &lt;jim_of=\r\nlaherty_jr@...&gt; \nwrote:\n&gt;\n&gt; Peter,\n&gt; \n&gt; That doesn&#39;t make sense to me. Cesa=\r\nr&#39;s comments indicate he has a \n&gt; similar implementation to mine. Basically=\r\n, every node&#39;s value in a \n&gt; single instance of an ANN is calculated just o=\r\nnce on an activation \npass. \n&gt; When the node is calculated, each of the nod=\r\nes attached to it as \n&quot;input&quot; \n&gt; are used (whether they have been activated=\r\n in this pass, or hold a \n&gt; residual value from the previous pass) to calcu=\r\nlate it&#39;s current \nvalue. \n&gt; And assuming the ordering of the activations i=\r\ns lined up by \ndependency \n&gt; and they by node age, the network ought to act=\r\nivate \ndeterministically \n&gt; meaning that starting with an empty network (al=\r\nl node activation \nvalues \n&gt; are set to 0.0), providing the same input and =\r\nmaking several \nactivation \n&gt; passes ought to be able to return reliably re=\r\npeatable results given \nthe \n&gt; node activation values are reset to 0.0 and =\r\nprecisely the same input \nis \n&gt; submitted for the same number of activation=\r\n passes.\n&gt; \n&gt; I get that the network graph can have cycles, as in recurrent=\r\n \n&gt; connections. However, those have no relevance to activation order or \n&gt;=\r\n cause any sort of &quot;loop&quot; in a straightforward ANN implementation. \nSo, I \n=\r\n&gt; am getting confused about where and how an &quot;endless loop&quot; could \nform. \n&gt;=\r\n Perhaps they are talking about the network output never stabilizing \nto \n&gt;=\r\n within some stable value ranges when activating it repeatedly with \nthe \n&gt;=\r\n same input. That&#39;s to be expected in a network with enough recurrent \n&gt; co=\r\nnnections - non-uniformity in the output following activations \nwith \n&gt; the=\r\n same input. Each additional recurrent connection increases the \n&gt; &quot;echos f=\r\nrom the past&quot; causing a stable output cycle to become less \nand \n&gt; less pro=\r\nbable.\n&gt; \n&gt; Said another way, a single activation of an ANN, with or withou=\r\nt \n&gt; recurrent connections, will results in exactly the same number of \nnod=\r\ne \n&gt; activation calculations each time it is told to process input. There \n=\r\n&gt; should not be any variability to the number of node activations AT \nALL. =\r\n\n&gt; If so, then some sort of different activation strategy is being \n&gt; emplo=\r\nyed. And if so, what is it? And why?\n&gt; \n&gt; \n&gt; Jim\n&gt; \n&gt; \n&gt; petar_chervenski w=\r\nrote:\n&gt; &gt;\n&gt; &gt; It is about the loop in which you calculate the longest path =\r\nfrom \nan\n&gt; &gt; input node to an output node. Not the activation loop in which=\r\n you\n&gt; &gt; activate the network.\n&gt; &gt;\n&gt; &gt; Peter\n&gt; &gt;\n&gt; &gt; --- In neat@yahoogroup=\r\ns.com &lt;mailto:neat%40yahoogroups.com&gt;, Jim \n&gt; &gt; O&#39;Flaherty &lt;jim_oflaherty_j=\r\nr@&gt;\n&gt; &gt; wrote:\n&gt; &gt; &gt;\n&gt; &gt; &gt; Ken,\n&gt; &gt; &gt;\n&gt; &gt; &gt; I am confused how you could end=\r\n up in an endless activation \nloop. If\n&gt; &gt; you\n&gt; &gt; &gt; are moving from the in=\r\nput nodes forward through the hidden nodes \nto\n&gt; &gt; the\n&gt; &gt; &gt; output nodes a=\r\ns a state calculation progression, there would be \nno\n&gt; &gt; need\n&gt; &gt; &gt; to wor=\r\nry about activation loops - you would only calculate the \nvalue\n&gt; &gt; of\n&gt; &gt; =\r\n&gt; each node once in a single pass. The point of a recurrent \nconnection\n&gt; &gt;=\r\n is\n&gt; &gt; &gt; to carry state between full network activations. So, the value\n&gt; =\r\n&gt; provided\n&gt; &gt; &gt; by a recurrent connection would not be used until the next=\r\n\n&gt; &gt; activation\n&gt; &gt; &gt; pass (assuming all the nodes from the previous pass h=\r\nave not be\n&gt; &gt; &quot;zeroed\n&gt; &gt; &gt; out&quot;).\n&gt; &gt; &gt;\n&gt; &gt; &gt; How is it there is an endle=\r\nss loop?\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt; Jim\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt; Kenneth Stanley wrote:\n&gt; &gt;=\r\n &gt; &gt;\n&gt; &gt; &gt; &gt; Yes I have heard about this problem coming up in the XOR\n&gt; &gt; e=\r\nxperiment.\n&gt; &gt; &gt; &gt; Most neuroevolution experiments are not classification \n=\r\nexperiments\n&gt; &gt; &gt; &gt; (i.e. they don&#39;t have a &quot;final&quot; output), or they allow =\r\n\nrecurrent\n&gt; &gt; &gt; &gt; connections, and therefore do not require depth to be \nc=\r\nomputed.\n&gt; &gt; &gt; &gt; Therefore, this problem will not come up in most expeirmen=\r\nts.\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; However, XOR is a benchmark classification problem that=\r\n is \nonly\n&gt; &gt; meant\n&gt; &gt; &gt; &gt; to be attempted by feedforward networks so it n=\r\needs to have \ndepth\n&gt; &gt; &gt; &gt; computed. It appears that my attempts to keep t=\r\nhe network\n&gt; &gt; feedforward\n&gt; &gt; &gt; &gt; in all cases is not perfect, so sometime=\r\ns when a loop arises, \nit\n&gt; &gt; sends\n&gt; &gt; &gt; &gt; the depth computation into an i=\r\nnfinite loop. I have not had \ntime\n&gt; &gt; to\n&gt; &gt; &gt; &gt; think about the most eleg=\r\nant solution to this problem: Maybe \nit\n&gt; &gt; should\n&gt; &gt; &gt; &gt; be a stronger ch=\r\neck on recurrence, perhaps entirely different \nfrom\n&gt; &gt; how\n&gt; &gt; &gt; &gt; it work=\r\ns now. Or perhaps it should be a fixed abort-iteration \nfor\n&gt; &gt; the\n&gt; &gt; &gt; &gt;=\r\n depth routine.\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; If someone does feel they have an elegant b=\r\nit of code to \naddress\n&gt; &gt; the\n&gt; &gt; &gt; &gt; issue, I will be happy to take a loo=\r\nk.\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; In any case, it should not cause serious problems in gen=\r\neral. \nI\n&gt; &gt; &gt; &gt; apologize for any inconvenience.\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; ken\n&gt; &gt; &gt;=\r\n &gt;\n&gt; &gt; &gt; &gt; --- In neat@yahoogroups.com &lt;mailto:neat%40yahoogroups.com&gt; \n&gt; &gt;=\r\n &lt;mailto:neat%40yahoogroups.com&gt;,\n&gt; &gt; &gt; &gt; &quot;petar_chervenski&quot; &lt;petar_cherven=\r\nski@&gt;\n&gt; &gt; &gt; &gt; wrote:\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; Hi Cesar,\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; I kn=\r\now of this problem. It is obvious that depth cannot be\n&gt; &gt; determined\n&gt; &gt; &gt;=\r\n &gt; &gt; in a recurrent network, but in general it depends, what is \nyour\n&gt; &gt; w=\r\nay to\n&gt; &gt; &gt; &gt; &gt; handle the situation. Try improving the add_link() code so =\r\n\nthat\n&gt; &gt; the\n&gt; &gt; &gt; &gt; &gt; right nodes are picked up when trying to add a forw=\r\nard or a\n&gt; &gt; recurrent\n&gt; &gt; &gt; &gt; &gt; connection. This is a good solution but in=\r\n general the \nproblem\n&gt; &gt; with\n&gt; &gt; &gt; &gt; &gt; looped networks cannot be avoided.=\r\n Suppose you have 3 hidden\n&gt; &gt; nodes, A,\n&gt; &gt; &gt; &gt; &gt; B, and C. If you link th=\r\nese like A-&gt;B, B-&gt;C, C-&gt;A, it is a \nloop\n&gt; &gt; in the\n&gt; &gt; &gt; &gt; &gt; network, even=\r\n though all connections are meant to be \nforward.\n&gt; &gt; &gt; &gt; &gt; The best soluti=\r\non in my opinion is to put a limit on the\n&gt; &gt; possible\n&gt; &gt; &gt; &gt; &gt; depth, say=\r\n 32, if the depth exceeds 32, quit the recursion \nand\n&gt; &gt; &gt; &gt; &gt; activate th=\r\ne network 32 times. It slows things down but at \nleast\n&gt; &gt; it\n&gt; &gt; &gt; &gt; &gt; wil=\r\nl not hurt evolution as if you penalize looped networks.\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; =\r\n&gt; Peter\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; --- In neat@yahoogroups.com &lt;mailto:neat%40yaho=\r\nogroups.com&gt; \n&gt; &gt; &lt;mailto:neat%40yahoogroups.com&gt;,\n&gt; &gt; &quot;Cesar\n&gt; &gt; &gt; &gt; G. Mi=\r\nguel&quot; &lt;cesar.gomes@&gt;\n&gt; &gt; &gt; &gt; &gt; wrote:\n&gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; Hi all,\n&gt; &gt; &gt;=\r\n &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; I&#39;m not sure if someone else has notice this &quot;bug&quot; in \nr=\r\ntNEAT,\n&gt; &gt; but\n&gt; &gt; &gt; &gt; &gt; the\n&gt; &gt; &gt; &gt; &gt; &gt; max_depth() method in network.cpp =\r\nhas the potential to be\n&gt; &gt; forever in\n&gt; &gt; &gt; &gt; &gt; &gt; loop if a recurrent link=\r\n is added in a feedforward \ntopology,\n&gt; &gt; e.g.,\n&gt; &gt; &gt; &gt; &gt; the\n&gt; &gt; &gt; &gt; &gt; &gt; X=\r\nOR experiment (and that can happen even when the\n&gt; &gt; recur_only_prob\n&gt; &gt; &gt; =\r\n&gt; &gt; &gt; parameter is set to zero). A real example is attached.\n&gt; &gt; &gt; &gt; &gt; &gt;\n&gt; =\r\n&gt; &gt; &gt; &gt; &gt; It seems to happen in 1 out of 10 runs. The max_depth() \nmethod\n&gt;=\r\n &gt; calls\n&gt; &gt; &gt; &gt; &gt; &gt; depth() in nnode.cpp, which should return the max dept=\r\nh of\n&gt; &gt; that\n&gt; &gt; &gt; &gt; &gt; node.\n&gt; &gt; &gt; &gt; &gt; &gt; But it can get trapped in a loop =\r\nif any recurrent link is\n&gt; &gt; present\n&gt; &gt; &gt; &gt; &gt; (as\n&gt; &gt; &gt; &gt; &gt; &gt; it is commen=\r\nted out in the source: DEPTH NOT DETERMINED \nFOR\n&gt; &gt; NETWORK\n&gt; &gt; &gt; &gt; &gt; &gt; WI=\r\nTH LOOP).\n&gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; For those cases I have to force a return =\r\nif an endless \nloop is\n&gt; &gt; &gt; &gt; &gt; &gt; detected and then set the chromosome&#39;s f=\r\nitness to zero in\n&gt; &gt; order to\n&gt; &gt; &gt; &gt; &gt; &gt; continue.\n&gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; =\r\n&gt; Has anyone dealed with that before?\n&gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; []&#39;s\n&gt; &gt; &gt; &gt; =\r\n&gt; &gt; Cesar\n&gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt;\n\n\n\n\n"}}