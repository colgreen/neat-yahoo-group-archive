{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":127853030,"authorName":"Colin Green","from":"Colin Green &lt;cgreen@...&gt;","profile":"alienseedpod","replyTo":"LIST","senderId":"9eHK65vCsC_PNAM4JwPq9bNaZYQ3AjyAVOQOQDudnx93XjxEoFn_xdYcncBRmWi3tO-nHzyTwwPRWLAK_UeW8VYHznhjv0tFjQ","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] Re: NEAT enhancements","postDate":"1157142962","msgId":2722,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ0Rjg5OUIyLjcwODAxMDlAZHNsLnBpcGV4LmNvbT4=","inReplyToHeader":"PGVkNnAwZyt0cjFzQGVHcm91cHMuY29tPg==","referencesHeader":"PGVkNnAwZyt0cjFzQGVHcm91cHMuY29tPg=="},"prevInTopic":2721,"nextInTopic":0,"prevInTime":2721,"nextInTime":2723,"topicId":2684,"numMessagesInTopic":17,"msgSnippet":"... I think I m going to use a moving average and moving std. deviation. So there ll be an initial set of data used to sample the mean and standard deviation","rawEmail":"Return-Path: &lt;cgreen@...&gt;\r\nX-Sender: cgreen@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 72444 invoked from network); 1 Sep 2006 21:24:57 -0000\r\nReceived: from unknown (66.218.66.217)\n  by m27.grp.scd.yahoo.com with QMQP; 1 Sep 2006 21:24:57 -0000\r\nReceived: from unknown (HELO ranger.systems.pipex.net) (62.241.162.32)\n  by mta2.grp.scd.yahoo.com with SMTP; 1 Sep 2006 21:24:57 -0000\r\nReceived: from [10.0.0.11] (81-86-161-87.dsl.pipex.com [81.86.161.87])\n\tby ranger.systems.pipex.net (Postfix) with ESMTP id 146D9E00033B\n\tfor &lt;neat@yahoogroups.com&gt;; Fri,  1 Sep 2006 21:36:00 +0100 (BST)\r\nMessage-ID: &lt;44F899B2.7080109@...&gt;\r\nDate: Fri, 01 Sep 2006 21:36:02 +0100\r\nUser-Agent: Mozilla Thunderbird 1.0.7 (Windows/20050923)\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: neat@yahoogroups.com\r\nReferences: &lt;ed6p0g+tr1s@...&gt;\r\nIn-Reply-To: &lt;ed6p0g+tr1s@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Colin Green &lt;cgreen@...&gt;\r\nSubject: Re: [neat] Re: NEAT enhancements\r\nX-Yahoo-Group-Post: member; u=127853030; y=E6vz3L7pof1c8S-xrxgCTZoZl8a1R06CB-5B_uAMqu0IXKA_6w_Q\r\nX-Yahoo-Profile: alienseedpod\r\n\r\nMike Woodhouse wrote:\n\n&gt;--- In neat@yahoogroups.com, Colin Green &lt;cgreen@...&gt; wrote:\n&gt;  \n&gt;\n&gt;&gt;Hi Mike, Emyr, All\n&gt;&gt;\n&gt;&gt;This definitely seems to be the most sound approach to take to me. It \n&gt;&gt;does introduce the problem of having to calculate (sample) the mean and \n&gt;&gt;standard deviation from a sample set of data which, as I was saying \n&gt;&gt;before, you should then avoid for training and testing purposes - which \n&gt;&gt;is a problem if, like me, you already have limited data.\n&gt;&gt;    \n&gt;&gt;\n&gt;\n&gt;Good point.\n&gt;  \n&gt;\n\nI think I&#39;m going to use a moving average and moving std. deviation. So \nthere&#39;ll be an initial set of data used to sample the mean and standard \ndeviation that I won&#39;t use to input into the ANN, but as I progress into \nthe data for applying to the ANN I will update the moving variables as \nmore data becomes available. Think of this as the sample size increasing \nand therefor the mean and std. deviation gradually getting closer to \ntheir &#39;true&#39; values.\n\n\n&gt;&gt;Currently I&#39;m using the daily percentage change in price as an input \n&gt;&gt;signal as you suggested, but I use the raw figure as it&#39;s typically  \n&gt;&gt;within the range -1 to 1 anyway (in fact more like +-0.1 of course), \n&gt;&gt;    \n&gt;&gt;\n&gt;\n&gt;I can&#39;t see anything wrong with that. The main thing is to get away\n&gt;from input values that are scaled differently for different shares,\n&gt;which is likely to confuse the dickens out of your networks.\n&gt;  \n&gt;\n\nIndeed. Actually what do you think of inputiing the difference (in \npercentage terms) between the current price and a moving average, or set \nof moving averages each defined over a differen period. Potentially the \nANN could recreate this sort of system given the price change each day, \nbut if it is useful then why not supply it directly? I suppose the \nquestion arises of what moving average period to use. On the whole \nthough I reckon it&#39;s a sound idea given that reversion to mean is a very \nreal phenomenon in financial markets.\n\n\n&gt;\n&gt;I have always felt that having some idea of the distribution of data\n&gt;points historically ought to lead to identifying the most appropriate\n&gt;transformations. When you have a strange distribution, say one where\n&gt;there are frequent small positive values and occasional large negative\n&gt;ones, most &quot;usual&quot; transformations will compress the normal case into\n&gt;a very small range which may in fact be removing information. I&#39;ve\n&gt;tried in the past applying a preprocessor that tranforms inputs into\n&gt;percentiles from a given historic range, so that the small normal\n&gt;range is expanded. It might be better to apply some sort of continuous\n&gt;function; this is an area where my knowledge needs some significant\n&gt;increase, I&#39;m afraid.\n&gt;  \n&gt;\n\nPerhaps you could a moving sample set as metioned above. I was actually \nthinking in that case (above) that the sample set would continue \nincreasing forever, but actually traditional moving averages are \ncalculated using a &#39;sliding window&#39; sample set. Using such a technique \nlong period of small movements would result in a &#39;moving&#39; mean and std. \ndev. appropriate to the local (recent) pattern of price movements, but \nthen the large spikes would be of the scale. Perhaps the answer is to \nhave long and short term moving sample sets and to then have inputs for \neach of these sample sets? So you have todays price inputted in units of \nthe short term std dev. and the long term.\n\n&gt;I have played around with the idea of extending the power of the input\n&gt;node to include a genetically-coded input transformation. I haven&#39;t\n&gt;managed to get beyond playing yet - there really do need to be at\n&gt;least three or four hours in a day.\n&gt;  \n&gt;\n\nGolden Rule: keep it simple!\n\nRegards,\n\nColin\n\n"}}