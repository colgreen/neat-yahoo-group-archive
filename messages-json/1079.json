{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":115403844,"authorName":"John Arrowwood","from":"&quot;John Arrowwood&quot; &lt;jarrowwx@...&gt;","profile":"jarrowwx","replyTo":"LIST","senderId":"H7QBHvChvj8dgIdNAVT-Y5-t9Y88SCSyZCbqtRiM4qeU4HRqNfuXvURhaEfCgsyjedGsx_-JLI9BX89Y6fx7CHsDamXjPjFP1W3k2GQd","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Computation Time","postDate":"1087396254","msgId":1079,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PEJBWTItRjg4eFE4U1RJMXJnUnAwMDA2OTFlMUBob3RtYWlsLmNvbT4="},"prevInTopic":1076,"nextInTopic":1080,"prevInTime":1078,"nextInTime":1080,"topicId":845,"numMessagesInTopic":99,"msgSnippet":"I did some tests. I created a simulated (i.e. meaningless) network activation function.  The simulated network had 10 input nodes, 10 hidden nodes, and one","rawEmail":"Return-Path: &lt;jarrowwx@...&gt;\r\nX-Sender: jarrowwx@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 92216 invoked from network); 16 Jun 2004 14:33:13 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m16.grp.scd.yahoo.com with QMQP; 16 Jun 2004 14:33:13 -0000\r\nReceived: from unknown (HELO hotmail.com) (65.54.247.88)\n  by mta5.grp.scd.yahoo.com with SMTP; 16 Jun 2004 14:33:13 -0000\r\nReceived: from mail pickup service by hotmail.com with Microsoft SMTPSVC;\n\t Wed, 16 Jun 2004 07:30:55 -0700\r\nReceived: from 64.122.44.102 by by2fd.bay2.hotmail.msn.com with HTTP;\n\tWed, 16 Jun 2004 14:30:54 GMT\r\nX-Originating-Email: [jarrowwx@...]\r\nX-Sender: jarrowwx@...\r\nTo: neat@yahoogroups.com\r\nBcc: \r\nDate: Wed, 16 Jun 2004 07:30:54 -0700\r\nMime-Version: 1.0\r\nContent-Type: text/plain; format=flowed\r\nMessage-ID: &lt;BAY2-F88xQ8STI1rgRp000691e1@...&gt;\r\nX-OriginalArrivalTime: 16 Jun 2004 14:30:55.0035 (UTC) FILETIME=[885BD0B0:01C453AE]\r\nX-eGroups-Remote-IP: 65.54.247.88\r\nFrom: &quot;John Arrowwood&quot; &lt;jarrowwx@...&gt;\r\nReply-To: john@...\r\nSubject: Computation Time\r\nX-Yahoo-Group-Post: member; u=115403844\r\nX-Yahoo-Profile: jarrowwx\r\n\r\nI did some tests.\n\nI created a simulated (i.e. meaningless) network activation function.  The \nsimulated network had 10 input nodes, 10 hidden nodes, and one output node.  \nThe network was fully connected, with random weights.\n\nI then tried four variations of the code, to see the effect on the speed:\n\n1. declare an array, and store the intermediate values (node * weight) in \nthe array.  Then perform all the additions, then all of the sigmoids.  Idea \nwas to allow as much parallelism as the chip could muster during the \nmultiplication phase...\n\n2. alter the order of processing so that a single node value was being \nmultiplied by each of its different weights, such that memory access was \nstreamlined\n\n3. intersperse the additions and sigmoids among the multiplications.\n\n4. compose single expressions that perform all the multiplications, \nadditions, and sigmoid all at once\n\nThe results were:\n\n1. 377k/sec\n2. 380k/sec  (so the memory order did make a difference here)\n3. 430k/sec  (made a big difference to give the CPU something to do while it \nwaited for the sigmoid to finish)\n4. 453k/sec  (letting the compiler do the optimization works well)\n\nThe one configuration that I did NOT try was the traditional &#39;loop&#39; \napproach, where node and weights are fetched from memory.  I can not see how \nthat approach could ever compare to the unrolled version.  Since there are \nno conditionals, there is no chance of a branch-prediction miss.  So the \nvalues of weights will always be available as soon as they are needed.   And \nsince the only thing stored outside the code stream are node weights, the \nwhole thing should have no trouble fitting in the cache, unless you have a \nreally, REALLY large network.\n\nThoughts?\n\n-- John\n\n\n&gt;From: Colin Green &lt;cgreen@...&gt;\n&gt;Reply-To: neat@yahoogroups.com\n&gt;To: neat@yahoogroups.com\n&gt;Subject: Re: [neat] Re: Computation Time\n&gt;Date: Tue, 15 Jun 2004 23:52:06 +0100\n&gt;\n&gt;Ian Badcoe wrote:\n&gt;\n&gt; &gt;&gt;Hi Philip,\n&gt; &gt;&gt;\n&gt; &gt;&gt;My curiosity got the better of me :) I tried the above functions using\n&gt; &gt;&gt;optimized C# on an AMD Athlon 2400+ (actually 2.17Ghz). The results are\n&gt; &gt;&gt;slightly bizarre,\n&gt; &gt;&gt; oh BTW I think you quoted the tanh function wrong, so I used y =\n&gt; &gt;&gt;tanh(0.9*x) which gives a nice sigmoid. Firstly I had to use 100 million\n&gt; &gt;&gt;(10^8) loops to get readable results, the approx. 50x difference is\n&gt; &gt;&gt;partly due to the CPU (obviously!) but maybe the rest is due to my\n&gt; &gt;&gt;oversimplistic implementation whereby I used the same value for x every\n&gt; &gt;&gt;time - did you generate random numbers perhaps? Also I know that Java\n&gt; &gt;&gt;has JIT compilers but sometime only optimize in code hot-spots during\n&gt; &gt;&gt;code execution, they can also run in interpreter mode - my run was with\n&gt; &gt;&gt;JITed code.\n&gt; &gt;&gt;\n&gt; &gt;&gt;Here are the figures:\n&gt; &gt;&gt;\n&gt; &gt;&gt;sigmoid:  3625ms\n&gt; &gt;&gt;evsail:    2359ms\n&gt; &gt;&gt;inv-abs:  188ms\n&gt; &gt;&gt;\n&gt; &gt;&gt;\n&gt; &gt;\n&gt; &gt;By my calculations, this makes just over 3 cycles per complete\n&gt; &gt;calculation.  That&#39;s not impossible.  e.g. ISRT on the K7 (Athlon\n&gt; &gt;predecessor) a floating-divide took 3 cycles but that the chip was able \n&gt;to\n&gt; &gt;have 2 fdivs and 2fadds and some integer instructions running \n&gt;simulatneously.\n&gt; &gt;\n&gt; &gt;\n&gt;Yep, via the various  instruction pipelines. Assuming the a 2.17Ghz\n&gt;clock I translate the above figures as follows:\n&gt;\n&gt;sigmoid: 78.66 cycles\n&gt;evsail:     51.00\n&gt;inv-abs:    4.08\n&gt;tanh:      269.00\n&gt;\n&gt;certainly interesting.\n&gt;\n&gt; &gt;It does sound suspiciously good, however.  I don&#39;t know much about C# but\n&gt; &gt;presumably it&#39;s inlining the function, and maybe unrolling the loop a\n&gt; &gt;little.  OTOH, if it did all that, then it should be able to see that you\n&gt; &gt;are making the same call every time and that the function has no side\n&gt; &gt;effects, so did it need to run the function at all?\n&gt; &gt;\n&gt; &gt;\n&gt;That particular test was a loop, no methods calls involved. But yes the\n&gt;.Net compiler does do inlining, although there is no inline hint keyword\n&gt;as in some C++ compilers - as I understood it the keyword was largely\n&gt;ignored in later compilers anyway - based on the idea that the compiler\n&gt;knows best.\n&gt;\n&gt; &gt;Compilers can be blind to that sort of thing, however, like I mentioned \n&gt;before.\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;&gt;tanh:     12,400ms\n&gt; &gt;&gt;\n&gt; &gt;&gt;weird huh.  The tanh loop took 66x longer then the ins-abs one!\n&gt; &gt;&gt;\n&gt; &gt;&gt;\n&gt; &gt;\n&gt; &gt;All trig, exp and log are very expensive.\n&gt; &gt;Sqrt is expensive but maybe not so bad.\n&gt; &gt;Divide is releatively cheap nowadays.\n&gt; &gt;\n&gt; &gt;The thing about the more exotic instructions, like tan, is that not only \n&gt;do\n&gt; &gt;they take a lot of cycles, but the chip only has one processor for\n&gt; &gt;them.  Also slow instructions have a disproportionate effect on \n&gt;throughput\n&gt; &gt;because all the shorter instructions, which could run in parallel, can \n&gt;only\n&gt; &gt;go so far before they hit a dependency on the result of the long\n&gt; &gt;instruction and have to stop.  Thus effectively the whole chip hangs on \n&gt;the\n&gt; &gt;result of the tan.\n&gt; &gt;\n&gt; &gt;\n&gt;I think modern cpu&#39;s have more than one fpu pipeline - but yes, the\n&gt;principle still holds.\n&gt;\n&gt; &gt;&gt;  I wonder though if the technqiue of trying to do\n&gt; &gt;&gt;many sequentail ops in order wll only become beneficial when the\n&gt; &gt;&gt;networks get *really* big, simply because the memory caches in modern\n&gt; &gt;&gt;CPU&#39;s are so large. So there may be some network size at which we would\n&gt; &gt;&gt;see a dramatic slow down of our code if it&#39;s not optimized in such a \n&gt;way.\n&gt; &gt;&gt;\n&gt; &gt;&gt;\n&gt; &gt;\n&gt; &gt;That&#39;s what I would expect, not just the network size, however, also the\n&gt; &gt;total size of the data you want to pass through.  If you run many copies \n&gt;of\n&gt; &gt;the same small network on different data then memory-access may be your\n&gt; &gt;bottleneck.\n&gt; &gt;\n&gt; &gt;\n&gt;\n&gt;Ok but the input data is copied into the input nodes and then the bulk\n&gt;of the network CPU time is in activating the whole network several times\n&gt;over. So this really depends on how many network epochs you run on each\n&gt;set of input data.\n&gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;&gt;Another way of estimating how efficient my code is is to caclulate the\n&gt; &gt;&gt;average number of clock cycles that it requires per neuron and\n&gt; &gt;&gt;connection. So e.g. My 53 neuron / 413 connection network performs 413\n&gt; &gt;&gt;additions and 53 activations per epoch. So that&#39;s 466 necessary\n&gt; &gt;&gt;operations in all, this is an absolute minimum. ok, plus a couple\n&gt; &gt;&gt;because the activation fn is several operations (but this is just a\n&gt; &gt;&gt;rough bit of maths). Using a simple bit of maths I can then determine:\n&gt; &gt;&gt;\n&gt; &gt;&gt;ops per epoch = 466\n&gt; &gt;&gt;ops per test run = 466 * 100,000 (loops) = 46,600,000\n&gt; &gt;&gt;ops per second = 46,600,000 / 5000ms(approx) = 9,320,000\n&gt; &gt;&gt;CPU clock cycles per op = 2.17Ghz / 9,320,000 = 232.\n&gt; &gt;&gt;\n&gt; &gt;&gt;\n&gt; &gt;&gt;Now 232 isn&#39;t all that bad when you consider this doesn&#39;t take into\n&gt; &gt;&gt;account the extra code that is required to do the looping/indexing\n&gt; &gt;&gt;through all of the neurons and connections. So perhaps hand optimized\n&gt; &gt;&gt;assembler could get this down to 100 cylcles or maybe 50, but this is in\n&gt; &gt;&gt;the same ball park as optimum - and therefore I wouldn&#39;t expect any\n&gt; &gt;&gt;massive improvements. Well, not unless you start using SIMD\n&gt; &gt;&gt;instructions, which I&#39;m definitely NOT! :)\n&gt; &gt;&gt;\n&gt; &gt;&gt;\n&gt; &gt;\n&gt; &gt;You easily can do a better analysis than that.  Run the timing a few \n&gt;times\n&gt; &gt;with different sizes of network (number of Ops) then plot the line of\n&gt; &gt;number of ops (x) vs time (y).  You should get an +ve intercept on the\n&gt; &gt;y-axis which is the constant cost of your program and a +ve sloping line,\n&gt; &gt;which is the cost per op...\n&gt; &gt;\n&gt; &gt;\n&gt;OK I&#39;ve made 3 measurements, the same network as before but with 104,\n&gt;207, and 413 connections. The times are:\n&gt;\n&gt;104: 2481ms\n&gt;207: 3343ms\n&gt;413:  4678ms\n&gt;\n&gt;If you plot these on a graph it is slightly non-linear, the line is\n&gt;curving upwards - which is what you might expect if, say, the cache is\n&gt;becoming less efficient with the accessing of more data. Assuming a\n&gt;straight line between the first and last reading, this then gives:\n&gt;\n&gt;secs per connection: 7.11 * 10^-8\n&gt;connections/sec : 14,064,633\n&gt;clock cycles/connection: 154\n&gt;\n&gt;\n&gt;These times don&#39;t include calculating the activation fn, this is all\n&gt;time spent executing loops to fetch a neuron output value, multiply the\n&gt;value by a weight and then add that to a total  ready to be put through\n&gt;the activation fn. So perhaps there is room for improvment there, a\n&gt;multiply an add and a couple of memory accesses taking 154 cycles is a\n&gt;little bit sloppy, but then this is .NET remember - and as such there is\n&gt;also a single type cast in there because .NET does not yet support\n&gt;templates (to be called Generics I believe), this could well be the bulk\n&gt;of the 154 cycles!\n&gt;\n&gt;Colin\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;Yahoo! Groups Links\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n\n\n\n"}}