{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"7tT_K1RO-4uyWhVlcmwXT52zwv1pqRprkkdJXmTaCu5WcUqt8EG1mmWD9BwikO6xR6JlCdyAfpYir2-EQt7qxcvHGAdfgMoR_9oOlfR-z4xJ","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Machine Learning and the Long View of AI","postDate":"1209436213","msgId":4006,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGZ2NjE3bCtna2hkQGVHcm91cHMuY29tPg==","inReplyToHeader":"PEM0M0JDNTM2LjIyN0YyJWpjbHVuZUBtc3UuZWR1Pg=="},"prevInTopic":4005,"nextInTopic":4007,"prevInTime":4005,"nextInTime":4007,"topicId":3955,"numMessagesInTopic":49,"msgSnippet":"Jeff, I think it would be interesting to step back and look at the assumptions that underly your straw man argument.  In particular, why are we using evolution","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 18558 invoked from network); 29 Apr 2008 02:30:15 -0000\r\nX-Received: from unknown (66.218.67.96)\n  by m50.grp.scd.yahoo.com with QMQP; 29 Apr 2008 02:30:15 -0000\r\nX-Received: from unknown (HELO n36a.bullet.mail.sp1.yahoo.com) (66.163.168.130)\n  by mta17.grp.scd.yahoo.com with SMTP; 29 Apr 2008 02:30:15 -0000\r\nX-Received: from [216.252.122.217] by n36.bullet.mail.sp1.yahoo.com with NNFMP; 29 Apr 2008 02:30:14 -0000\r\nX-Received: from [66.218.69.2] by t2.bullet.sp1.yahoo.com with NNFMP; 29 Apr 2008 02:30:14 -0000\r\nX-Received: from [66.218.66.91] by t2.bullet.scd.yahoo.com with NNFMP; 29 Apr 2008 02:30:14 -0000\r\nDate: Tue, 29 Apr 2008 02:30:13 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;fv617l+gkhd@...&gt;\r\nIn-Reply-To: &lt;C43BC536.227F2%jclune@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: Re: Machine Learning and the Long View of AI\r\nX-Yahoo-Group-Post: member; u=54567749; y=ZP6KzTtotqTGL5i2hCZjH5zwlvvtOp7JEzq5__8WRFBl1wY46PaV\r\nX-Yahoo-Profile: kenstanley01\r\n\r\nJeff,\n\nI think it would be interesting to step back and look at the\nassumpt=\r\nions that underly your straw man argument.  In particular, why\nare we using=\r\n evolution in the first place?\n\nYour straw man implies a world view wherein=\r\n we are using evolution\nbecause we like evolution and want it to succeed.  =\r\nUnder that\nphilosophy, then indeed, creating a super-powerful neural networ=\r\nk\n(DeepNet) by hand and then creating a faux-mutation operator that\nsimply =\r\nmakes a neural network turn into DeepNet would be a\ndisappointment, because=\r\n it would mean that evolution didn&#39;t really do\nwhat we wanted, and we would=\r\n be being disingenuous (and unimpressed).\n\nHowever, my world view is differ=\r\nent.  I am not using evolution because\nI like evolution and want to prove t=\r\nhat it is impressive. (Note that I\n*do* like evolution, but that&#39;s not the =\r\nreason I use it.)  Rather, I\nam using evolution because I genuinely believe=\r\n that full-fledged AI\nlikely *cannot* be constructed by hand, and that evol=\r\nution is the best\nalternative.  If someone went ahead and built a general-A=\r\nI neural\nnetwork by hand, it would simply prove me wrong.  But it would mea=\r\nn\nnothing with respect to how we should go about doing the same with\nevolut=\r\nion (which would however be a moot point at that point anyway\nbecause why b=\r\nother when someone figured out a better way?).  So as\nsoon as someone did w=\r\nhat you said, evolution would be thrown out\nanyway, at least in terms of be=\r\ning the best path to general AI.\n\nSo I am looking at things in a kind of re=\r\nverse perspective from you. \nTo me, the point is not to bolster up evolutio=\r\nn and show how powerful\nit is.  Rather, the point is that I believe it *is*=\r\n powerful and\ntherefore I am using it.  If I can do something to boost it f=\r\nurther,\ngood.  What else is there to prove?\n\nHowever, I am a realist and I =\r\ndoubt that evolution alone will get the\njob done in the long view.  It&#39;s ju=\r\nst too gigantic a search space and\nthe problem is too poorly specified.  Th=\r\nerefore, I think there will be\na lot of biases and manipulations along the =\r\nway.\n\nI&#39;m way off speculating about the far off future here, but my guess i=\r\ns\nthat those manipulations will come mostly at the genetic level rather\ntha=\r\nn the ANN level.  In other words, the kinds of hacks that you are\ntalking a=\r\nbout (&quot;building blocks&quot; provided a priori) generally seem to\nbe kind of neu=\r\nral &quot;modules&quot; that are built a priori and just dumped\ninto the network en m=\r\nasse.  Those are indeed a bit cringe-inducing.\nHowever, my problem with the=\r\nm is not that they are cringe-inducing. \nRather, again, I doubt they will r=\r\neally be a big help in the long run.\n The reason I doubt their utility is b=\r\necause I believe that a massive\nbrain needs to be also massively interwoven=\r\n, such that each internal\narea of each part is entirely accessible and &quot;spe=\r\naks the language&quot; of\nany other part.  Some ad hoc module thrown in the mix,=\r\n while perhaps\nhelpful in the short run, will never be able to fill that ro=\r\nle because\nit was not built along with the rest of the infrastructure.  So =\r\nthat&#39;s\nwhy I&#39;m against it: Not because it&#39;s cheating, but because it won&#39;t =\r\nwork.\n\nSo I think you have to distinguish between that type of hack and the=\r\n\nkind of thing where we provide sort of &quot;genetically-engineered&quot;\ninformatio=\r\nn, i.e. at the genetic level.  That I do believe will be\nuseful, and should=\r\n be exploited, because those are knobs and\ncoordinate frames upon which a c=\r\nastle can be built.  So providing\ncoordinate systems that are useful seems =\r\nto me likely *long-run*\nuseful.  It is not the same as telling it how to co=\r\nnnect up, and the\nsubstrate that pops out in the end is going to be as pure=\r\n as any:\ncompletely ANN through and through and totally a product of the\nin=\r\ndirect encoding.\n\nFinally, I think you are inferring too much about how muc=\r\nh prior\ninformation I am advocating based on just Multiagent HyperNEAT.  Th=\r\nere\nis no animal on earth that has to develop five disconnected brains on\na=\r\n single sheet with five compartments.  Expecting evolution to just\nfigure o=\r\nut where one brain begins and the other ends seems to me very\nunnatural and=\r\n bizarre, and also uninteresting.  Statistically\nspeaking, it is evident th=\r\nat *any* intelligence would take longer to\nfigure that out and solve the pr=\r\noblem on average than one that was\nprovided such information a priori.  So =\r\nthere&#39;s no surprise in that.  \n\nSo of course HyperNEAT performs worse witho=\r\nut knowing the divisions\nbetween separate brains on a substrate than when i=\r\nt knows them up\nfront.  That doesn&#39;t imply  that HyperNEAT cannot figure it=\r\n out on its\nown, or that I think it doesn&#39;t matter if HyperNEAT can find\nre=\r\ngularities on its own.  It&#39;s just, it would take a while longer and\nwould b=\r\ne less reliable, so why bother waiting?  The spatial divisions\namong the br=\r\nains is ad hoc (something we simply decided a priori by\nfiat) and thus is n=\r\not the interesting issue.\n\nI don&#39;t think it will be the same in a lot of no=\r\nn-multiagent tasks\nbecause this unnatural issue of multiple brains and thei=\r\nr positions\ndoes not come up, and I do believe that HyperNEAT often does di=\r\nscover\nregularities on its own, and that&#39;s a good thing.\n\nAnyway, the broad=\r\ner point is that I will stick to my strong position:\nI do not believe that =\r\nfinding  middle ground or a sweet spot in terms\nof biases and constraints i=\r\ns the important issue in the long view of\nachieving general AI through evol=\r\nution *unless* you are only doing it\nto prove how cool evolution is.  In co=\r\nntrast, I&#39;m using evolution\nbecause I think it is the best hope. The funny =\r\nthing is that we will\nregardless end up agreeing on a lot, because I too do=\r\nn&#39;t like to\nprovide big building blocks.  But my reason is that they will e=\r\nnd up\nbeing incapable of building a general AI.  So I think a lot of things=\r\n\nthat look bad also won&#39;t work, so someone who is trying to make\nevolution =\r\nlook good will often see me as sharing their assumptions.\n\nken\n\n\n--- In nea=\r\nt@yahoogroups.com, Jeff Clune &lt;jclune@...&gt; wrote:\n&gt;\n&gt; Ken-\n&gt; \n&gt; Thank you f=\r\nor explaining these arguments at length. I always learn a lot\n&gt; when we hav=\r\ne these sorts of discussions, and there is no exception\nin this\n&gt; case. I a=\r\ngree with most of what you write below, especially that\n&gt; constraining/bias=\r\ning evolution is very important. I guess the only\nplace we\n&gt; disagree is th=\r\nat I believe there is a middle ground of constraint\nthat we\n&gt; should shoot =\r\nfor, whereas you seem to feel &#39;the more the better&#39;.\n&gt; \n&gt; You write:\n&gt; \n&gt; &gt;=\r\n  Therefore, progress is NE should in part be measured with respect to\n&gt; &gt; =\r\nprogress in constraining the problem to make such a discovery more\n&gt; &gt; like=\r\nly.  When an NE algorithm is improved to allow us to tell it more\n&gt; &gt; about=\r\n the world in which its output will be situated, that is good\n&gt; &gt; news for =\r\nthe long view.  In short, we don&#39;t care at all how NE\n&gt; &gt; produced a brain =\r\nas long as it really does.\n&gt; \n&gt; This reminds me of something that Hod Lipso=\r\nn says repeatedly. Whenever\n&gt; someone evolves something impressive the firs=\r\nt question to ask is,\n&quot;How big\n&gt; are your building blocks?&quot; I am going to p=\r\nrovide a straw man of your\n&gt; argument. Hopefully the fact that I admit that=\r\n up front will make it\nless\n&gt; objectionable. Imagine that Kasparov and a ne=\r\nural net engineer\nteamed up and\n&gt; hand-created a neural  net (call it &#39;Deep=\r\nNet&#39;)  that played chess at a\n&gt; grandmaster level. Now imagine that we crea=\r\nte an NE algorithm for\nlearning\n&gt; chess playing that was otherwise identica=\r\nl to NEAT, but had one extra\n&gt; mutation operator, which was &#39;clear out the =\r\ncurrent phenotype and\nreplace it\n&gt; with DeepNet&#39;. In this case we would hav=\r\ne highly constrained the\nproblem to\n&gt; find a good chess playing solution. W=\r\ne would have also successfully\ninjected\n&gt; our a priori knowledge of the pro=\r\nblem. However, it would be very\n&gt; unimpressive as an accomplishment in the =\r\nfield of evolutionary\ncomputation.\n&gt; The credit goes to the humans that des=\r\nigned DeepNet, not for the\n&gt; evolutionary algorithm that recreated it.\n&gt; \n&gt;=\r\n As I said, this is an unfair caricature of your view. However, I\nthink it\n=\r\n&gt; might reveal what I have been trying to say. In my mind, the goal is to\n&gt;=\r\n provide smaller and smaller building blocks because then we know it is\n&gt; e=\r\nvolution that is doing the work, and not us. There is a sweet spot\nin the\n&gt;=\r\n middle. If we humans don&#39;t do any work in biasing the search, then\nevoluti=\r\non\n&gt; will perform terribly. But if we provide building blocks that are\ntoo =\r\nlarge,\n&gt; then evolution did not really do the heavy lifting. So, as opposed=\r\n\nto saying\n&gt; &#39;the more constraint the better,&#39; I think it is interesting to=\r\n try to\n&gt; provide smaller building blocks while still gaining high levels o=\r\nf\n&gt; performance. As I have said before, I also think that if we make\nprogre=\r\nss on\n&gt; this front, the evolutionary algorithm (not its product) will be\nmo=\r\nre likely\n&gt; to generalize to solving other problems. The long term goal, of=\r\n\ncourse, is\n&gt; to have our algorithms solve problems and create things where=\r\n we either\n&gt; don&#39;t know how to solve the problems, or can&#39;t be bothered to =\r\ndo so. For\n&gt; example, the NE that produced DeepNet would not do very well a=\r\nt race car\n&gt; driving. But an algorithm that was constrained in a more abstr=\r\nact way to\n&gt; exploit regularities in its environment might do better on bot=\r\nh car\nracing\n&gt; and chess. \n&gt; \n&gt; I guess I start from the recognition that e=\r\nvolution produced humans\nwithout\n&gt; any bias from a conscious entity. How it=\r\n did that is one of the most\n&gt; fascinating and open questions both in our f=\r\nield and in biology. We\nagree\n&gt; that trying to emulate ways in which natura=\r\nl evolution did things\nlike bias\n&gt; itself, and thus allow the evolution of =\r\nmodularity, is the way\nforward for\n&gt; our field. HyperNEAT represents such a=\r\nmazing progress because it\nemployed\n&gt; this strategy. But it strikes me that=\r\n nature was not told a priori\nhow many\n&gt; leg modules it should make or lear=\r\nn to control. Nor was it told how many\n&gt; neural modules it should create in=\r\n the brain. It figured that stuff\nout on\n&gt; its own, and probably performed =\r\nbetter as a result because it could\nlearn to\n&gt; tailor the number of modules=\r\n it needed to the regularity of the\nproblems it\n&gt; faced. I guess I don&#39;t th=\r\nink we will make it very far towards evolving\n&gt; brains that are generally i=\r\nntelligent if our evolutionary algorithms\ncannot\n&gt; do likewise. It seems th=\r\nat something is majorly lacking if we have\nto tell\n&gt; it each time what the =\r\nregularities are in the environment, and how to go\n&gt; about exploiting them.=\r\n\n&gt; \n&gt; Apologies for the straw man argument. I do think there is a lot of\nme=\r\nrit to\n&gt; the general thrust of what you say. I may be overreacting in\nfocus=\r\ning on the\n&gt; extremes\n&gt; \n&gt; \n&gt; \n&gt; Cheers,\n&gt; Jeff Clune\n&gt; \n&gt; Digital Evolutio=\r\nn Lab, Michigan State University\n&gt; \n&gt; jclune@...\n&gt; \n&gt; \n&gt; \n&gt; \n&gt; &gt; From: Kenn=\r\neth Stanley &lt;kstanley@...&gt;\n&gt; &gt; Reply-To: &quot;neat@yahoogroups.com&quot; &lt;neat@yahoo=\r\ngroups.com&gt;\n&gt; &gt; Date: Sun, 27 Apr 2008 21:36:33 -0000\n&gt; &gt; To: &quot;neat@yahoogr=\r\noups.com&quot; &lt;neat@yahoogroups.com&gt;\n&gt; &gt; Subject: [neat] Re: Machine Learning a=\r\nnd the Long View of AI\n&gt; &gt; \n&gt; &gt; --- In neat@yahoogroups.com, &quot;Derek James&quot; =\r\n&lt;djames@&gt; wrote:\n&gt; &gt; \n&gt; &gt;&gt; \n&gt; &gt;&gt;&gt;  In RL, in contrast, the long view is alm=\r\nost the opposite: They\n&gt; &gt; want to\n&gt; &gt;&gt;&gt;  remove all constraints and still =\r\nlearn nevertheless.\n&gt; &gt;&gt; \n&gt; &gt;&gt; I&#39;m not sure what you mean by this, Ken. Cou=\r\nld you elaborate a\nlittle?\n&gt; &gt;&gt; \n&gt; &gt; \n&gt; &gt; Sure.  I think the problem is tha=\r\nt I can&#39;t find a way to explain my\n&gt; &gt; point concisely.  As I try to explai=\r\nn it, it starts taking up too much\n&gt; &gt; text so I shorten it and then it los=\r\nes its meaning.  Let me give it a\n&gt; &gt; try again...\n&gt; &gt; \n&gt; &gt; I think the dif=\r\nference between the goals of RL and NE is an\n&gt; &gt; interesting topic because =\r\nthey are almost always conflated, as if they\n&gt; &gt; are trying to solve the sa=\r\nme problem.\n&gt; &gt; \n&gt; &gt; The RL community (e.g. value-function approaches) is t=\r\nrying to build\n&gt; &gt; something that learns like a natural brain.  They are sa=\r\nying, through\n&gt; &gt; analytic means we can deduce how a brain can learn from s=\r\nparse\n&gt; &gt; reinforcement and formalize that process in an algorithm.  The ho=\r\npe, I\n&gt; &gt; would think, is to eventually build the &quot;general intelligence&quot; th=\r\nat\n&gt; &gt; aligns with the holy grail of AI.  So each step along the way is an\n=\r\n&gt; &gt; improvement in that general ability.\n&gt; &gt; \n&gt; &gt; So if that is your goal, =\r\nthen the benchmarks you choose have to be\n&gt; &gt; designed to measure progress =\r\nto that goal.  So what they need to do is\n&gt; &gt; show that their designed inte=\r\nlligence can work largely independently\n&gt; &gt; of a priori &quot;cheats&quot; that provi=\r\nde the meat of the solution.  Because,\n&gt; &gt; after all, how can it be a gener=\r\nal intelligence if it needs you to\n&gt; &gt; tell it something that it is suppose=\r\nd to be able to figure out?  This\n&gt; &gt; perspective, I believe, is aligned wi=\r\nth Jeff&#39;s view.\n&gt; &gt; \n&gt; &gt; However, NE as a long-term pursuit is involved in =\r\nsomething different,\n&gt; &gt; even though it can be applied to the same problems=\r\n.  NE is not an\n&gt; &gt; attempt to formalize how people learn with sparse reinf=\r\norcement.\n&gt; &gt; Rather, it is an attempt to formalize how evolution can build=\r\n a brain.\n&gt; &gt;  So RL is formalizing the brain itself and NE is formalizing =\r\nhow\n&gt; &gt; evolution succeeds in creating a brain.  NE is therefore one step\nr=\r\nemoved.\n&gt; &gt; \n&gt; &gt; This difference is ultimately a philosophical difference o=\r\nn the best\n&gt; &gt; approach to creating a full-blown AI.  The instrumental issu=\r\ne is\n&gt; &gt; whether you think it&#39;s easier to build it yourself or to design an=\r\n\n&gt; &gt; algorithm that can build it.  The confusion and hence conflation of\n&gt; =\r\n&gt; the two approaches arises in part because they do indeed both aim at\n&gt; &gt; =\r\nthe same long view goal: a general AI.  But they are coming at it from\n&gt; &gt; =\r\nvery different angles.\n&gt; &gt; \n&gt; &gt; And because of this stark difference, the *=\r\nmetric* of progress should\n&gt; &gt; be quite different.  We cannot measure our p=\r\nrogress in building a\n&gt; &gt; general intelligence directly in the same way tha=\r\nt we measure our\n&gt; &gt; progress in creating an evolutionary algorithm that it=\r\nself will\n&gt; &gt; someday output one.\n&gt; &gt; \n&gt; &gt; This distinction is potentially =\r\nsubtle and confusing so let me try to\n&gt; &gt; make it clearer:  Human brains ar=\r\nen&#39;t designed to build yet more human\n&gt; &gt; brains.  We are good at a lot of =\r\nthings, and we learn generally, but\n&gt; &gt; we do not build 100-trillion part d=\r\nevices that are more complex than\n&gt; &gt; any known object in the universe.  I&#39;=\r\nm not saying we won&#39;t ever be\n&gt; &gt; able to do it, but if you want to simulat=\r\ne a human brain, your first\n&gt; &gt; thought would not be that it needs to be ca=\r\npable of designing yet\n&gt; &gt; another brain by itself.  Your first thought is =\r\nabout things like\n&gt; &gt; object recognition or pursuit and evasion.\n&gt; &gt; \n&gt; &gt; I=\r\nn contrast, building brains is exactly what natural evolution did,\n&gt; &gt; and =\r\nit did it quite well.  Natural evolution does not perform object\n&gt; &gt; recogn=\r\nition; it does not communicate with language; it does not run\n&gt; &gt; away from=\r\n predators or hunt for prey.  Yet it does build brains that\n&gt; &gt; themselves =\r\ndo those things.  And that is the aspect of it we wish to\n&gt; &gt; harness- a ve=\r\nry specific niche kind of skill (though radically\n&gt; &gt; impressive)- not a ge=\r\nneral skill.\n&gt; &gt; \n&gt; &gt; So the two pursuits are really quite different.  And =\r\ntherefore they\n&gt; &gt; deserve different metrics to judge their progress with r=\r\nespect to the\n&gt; &gt; long term goal.  That is, unless we conflate them to be t=\r\nhe same\n&gt; &gt; thing, which we often do without thinking about it.\n&gt; &gt; \n&gt; &gt; Fo=\r\nr example, we could just say, well, both NE and RL are learning\n&gt; &gt; techniq=\r\nues, and after all, we can apply them to the same problems, so\n&gt; &gt; why make=\r\n a big distinction in how we judge them?  Let&#39;s just compare\n&gt; &gt; them direc=\r\ntly on the same benchmarks and get on with it.\n&gt; &gt; \n&gt; &gt; That&#39;s fine for the=\r\n short-term view, i.e. let&#39;s just improve our\n&gt; &gt; ability to tackle practic=\r\nal problems, but for the long view, they\n&gt; &gt; cannot be judged in the same w=\r\nay.  If I improve at my ability to\n&gt; &gt; balance on one foot is that a sign t=\r\nhat I will be able to build a\n&gt; &gt; brain someday?  If evolution evolves a br=\r\nain that plays checkers, is\n&gt; &gt; that a sign that evolution *itself* is on t=\r\nhe road to performing\n&gt; &gt; object recognition?  These are totally different =\r\npursuits.\n&gt; &gt; \n&gt; &gt; So in that context, how should they be judged with respe=\r\nct to long\n&gt; &gt; term goals?  Well, I think RL deserves to be judged based on=\r\n its\n&gt; &gt; increasing ability to learn more generally.  And in that sense,\n&gt; =\r\n&gt; exactly Jeff&#39;s criteria should apply to it: We should be interested in\n&gt; =\r\n&gt; whether it &quot;needs&quot; a priori information to learn.  In other words, the\n&gt; =\r\n&gt; less we need to constrain the problem for the learner, the more\n&gt; &gt; impre=\r\nssed we deserve to be.  That shows progress towards more and more\n&gt; &gt; gener=\r\nal AI and ML.\n&gt; &gt; \n&gt; &gt; But if evolution is not *itself* supposed to be a ge=\r\nneral learner\n&gt; &gt; (rather, we just want it to concentrate on one very speci=\r\nfic skill:\n&gt; &gt; brain building), then those considerations are orthogonal to=\r\n its\n&gt; &gt; greatest promise.  Its promise is to evolve a brain itself, and as=\r\n\n&gt; &gt; such, neuroevolutionary algorithms deserve to be judged on our ability=\r\n\n&gt; &gt; to *constrain* the problem so that they can accomplish exactly that.\n&gt;=\r\n &gt; In other words, the problem NE *algorithms* face is leaps and bounds\n&gt; &gt;=\r\n beyond what RL algorithms face.  RL algorithms just need to be able to\n&gt; &gt;=\r\n do as well as brains; NE has to be able to discover brains themselves.\n&gt; &gt;=\r\n  Therefore, progress is NE should in part be measured with respect to\n&gt; &gt; =\r\nprogress in constraining the problem to make such a discovery more\n&gt; &gt; like=\r\nly.  When an NE algorithm is improved to allow us to tell it more\n&gt; &gt; about=\r\n the world in which its output will be situated, that is good\n&gt; &gt; news for =\r\nthe long view.  In short, we don&#39;t care at all how NE\n&gt; &gt; produced a brain =\r\nas long as it really does.  Will anyone complain if a\n&gt; &gt; human brain pops =\r\nout of a system that was a priori given the concept\n&gt; &gt; of symmetry?  Rathe=\r\nr, we should be glad that such a priori context was\n&gt; &gt; possible to provide=\r\n in the first place, because it may have saved us a\n&gt; &gt; year of wasted comp=\r\nutation in figuring it out needlessly.\n&gt; &gt; \n&gt; &gt; This distinction is almost =\r\ncompletely ignored when NE and RL are\n&gt; &gt; compared directly.  Therefore, th=\r\ne implications of any such comparison\n&gt; &gt; are fuzzy and lacking context wit=\r\nh respect to the long view.  I am not\n&gt; &gt; sure if I should care or not if R=\r\nL solves something better than NE, or\n&gt; &gt; vice versa, because the author do=\r\nesn&#39;t explain how the result aligns\n&gt; &gt; with the long-term goals of the fie=\r\nlds.  Long term goals seem like\n&gt; &gt; unwelcome guests these days in AI, whic=\r\nh is why I probably won&#39;t be\n&gt; &gt; writing about any of this in a publication=\r\n any time soon.\n&gt; &gt; \n&gt; &gt; ...\n&gt; &gt; \n&gt; &gt; So Derek what you are saying about NE=\r\n being good at &quot;hard-wired&quot;\n&gt; &gt; solutions and RL being appropriate for onto=\r\ngenetic lifetime learning,\n&gt; &gt; while true, is not what I think of as the pr=\r\nimary long-view issue.\n&gt; &gt; \n&gt; &gt; In the long view, NE will be used to evolve=\r\n structures that do learn\n&gt; &gt; over their lifetime, i.e. not hardwired at al=\r\nl.  The only reason that\n&gt; &gt; it tends to be used to evolve hardwired soluti=\r\nons today is because we\n&gt; &gt; are trying to get a foothold on how to evolve c=\r\nertain types of complex\n&gt; &gt; structures.   Once we get very good at it, focu=\r\ns will naturally shift\n&gt; &gt; to evolving dynamic brains (and of course there =\r\nis already work along\n&gt; &gt; these lines today, much from Floreano).  I do not=\r\n even think that we\n&gt; &gt; will need to include stock learning algorithms like=\r\n Hebbian learning.\n&gt; &gt;  When we achieve our long-term goals, those *themsel=\r\nves* will be left\n&gt; &gt; up to evolution because after all there may be someth=\r\ning even better.\n&gt; &gt;  \n&gt; &gt;&gt;&gt; My aim is to design an\n&gt; &gt;&gt;&gt;  algorithm that w=\r\nill output a brain, not to design the brain itself.\n&gt; &gt;&gt; \n&gt; &gt;&gt; But what kin=\r\nd of brain are you wanting to output?\n&gt; &gt;&gt; \n&gt; &gt; \n&gt; &gt; Note that I&#39;m speaking=\r\n purely about the long view for these different\n&gt; &gt; fields here.  Of course=\r\n on a day-to-day basis I am not solely focused\n&gt; &gt; on what will happen 100 =\r\nyears from now.  On a practical day-to-day\n&gt; &gt; basis, of course I want to m=\r\nake NE better capable to tackle problems\n&gt; &gt; that e.g. RL tackles.  So in t=\r\nhe short-term context, I just want to\n&gt; &gt; output something that works for t=\r\nhe problem at hand.\n&gt; &gt; \n&gt; &gt; But in the long view, which we were talking ab=\r\nout, I think the\n&gt; &gt; ultimate goal would be to output a full-fledged adapti=\r\nve system with\n&gt; &gt; astronomical complexity and the power and subtlety of hu=\r\nman reasoning.\n&gt; &gt;  On that path, constraint is the only hope, unless you w=\r\nant to wait\n&gt; &gt; three billion years and just hope in the meantime that the =\r\ninitial\n&gt; &gt; conditions were set up correctly.  Therefore, demonstrations of=\r\n the\n&gt; &gt; power of constraint deserve to be judged as evidence of the promis=\r\ne of\n&gt; &gt; and progress towards the long term goal in NE.\n&gt; &gt; \n&gt; &gt; ken\n&gt; &gt; \n&gt;=\r\n &gt;\n&gt;\n\n\n\n"}}