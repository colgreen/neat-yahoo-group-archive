{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"2t8xMBL3c6vzVIg82LSaFmhCOos6qLtIYyY_uG9n_lA028GhY-sP2qZG4r04rHuwAYZ3pEhl898GnsZt81rkDfbwChr0fvCTy55coTMY-s7x","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Tile Coding and HyperNEAT","postDate":"1179460203","msgId":3319,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGYyajdwYitnajViQGVHcm91cHMuY29tPg==","inReplyToHeader":"PEYyNzM1NENGLTY0OEItNDQzQS04Nzc2LTA2NjYwRTA0RUY0M0Bjcy51dGV4YXMuZWR1Pg=="},"prevInTopic":3305,"nextInTopic":0,"prevInTime":3318,"nextInTime":3320,"topicId":3214,"numMessagesInTopic":27,"msgSnippet":"Joe, maybe a little more big picture perspective on why I ve come out so forcefully against tile coding will be useful. Until a couple months ago, I had no","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 30070 invoked from network); 18 May 2007 03:50:14 -0000\r\nReceived: from unknown (66.218.67.33)\n  by m45.grp.scd.yahoo.com with QMQP; 18 May 2007 03:50:13 -0000\r\nReceived: from unknown (HELO n14c.bullet.sp1.yahoo.com) (69.147.64.117)\n  by mta7.grp.scd.yahoo.com with SMTP; 18 May 2007 03:50:09 -0000\r\nReceived: from [216.252.122.219] by n14.bullet.sp1.yahoo.com with NNFMP; 18 May 2007 03:50:05 -0000\r\nReceived: from [66.218.69.2] by t4.bullet.sp1.yahoo.com with NNFMP; 18 May 2007 03:50:04 -0000\r\nReceived: from [66.218.66.80] by t2.bullet.scd.yahoo.com with NNFMP; 18 May 2007 03:50:04 -0000\r\nDate: Fri, 18 May 2007 03:50:03 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;f2j7pb+gj5b@...&gt;\r\nIn-Reply-To: &lt;F27354CF-648B-443A-8776-06660E04EF43@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: Re: Tile Coding and HyperNEAT\r\nX-Yahoo-Group-Post: member; u=54567749; y=UxKOYAcHvhNXSvco3bGq7nOypjKyR0D746hYIDqQAM3mzxH_DGnt\r\nX-Yahoo-Profile: kenstanley01\r\n\r\nJoe, maybe a little more big picture perspective on why I&#39;ve come out \nso f=\r\norcefully against tile coding will be useful.\n\nUntil a couple months ago, I=\r\n had no issue with tile coding \nwhatsoever.  Like most people who study RL,=\r\n I thought of it as a \nuseful method for enhancing performance.  I thought =\r\nof it, basically, \nas a generally good idea that may not be right in all si=\r\ntuations but \nis likely appropriate for many.  I even asked a student to st=\r\nudy tile \ncoding so we could better articulate how it relates to HyperNEAT.=\r\n\n\nBut then when I started to think about how it relates to HyperNEAT \nmysel=\r\nf, it dawned on me how bad tile coding is.  It was a kind of \nminor revelat=\r\nion.  The ideas behind HyperNEAT *caused* me to see that \ntile coding is fa=\r\ntally flawed.  \n\nThat kind of experience is rare and rewarding; it happens =\r\nwhen new \nideas cause you to see the current way of thinking in a new light=\r\n.  \nSo it wasn&#39;t an innate bias against RL or a particular grudge against \n=\r\ntile coding that drove me to my conclusions.  Rather, it is a new \ncontext =\r\nfor thinking about RL problems, brought on by HyperNEAT.  \nWhat HyperNEAT c=\r\naused me to realize is that, as a practical matter, \nthe geometry of a prob=\r\nlem is essential to its solution.  In \nretrospect it looks obvious to say t=\r\nhat, but that&#39;s how most \nrevelations are.\n\nAnd the corollary to that reali=\r\nzation is that tile coding is exactly \nthe wrong direction, because it is a=\r\nll about breaking up geometry.  \nSo that too was a minor revelation.  That =\r\nalso now looks painfully \nobvious to me, but of course that is in retrospec=\r\nt as well. Finally, \nit now looks obvious to me that the way we&#39;ve been inp=\r\nutting things \nlike game boards and visual fields is flawed.  I&#39;ve \nrealize=\r\nd all this because of HyperNEAT.\n\nTo my mind, those kinds of realizations, =\r\nwhere one idea changes how \nyou see familiar things, is a sign of progress =\r\nin thinking, so I&#39;m \nexcited about it, because I can now see old things in =\r\na new light.  \nTile coding is just an unintentional casualty the new perspe=\r\nctive.\n\nSo anyway, that&#39;s just my explanation of where I&#39;m coming from.  No=\r\nw \nlet me respond specifically...\n\n--- In neat@yahoogroups.com, Joseph Reis=\r\ninger &lt;joeraii@...&gt; wrote:\n&gt; \n&gt; The thing is, though, tile-coding can be us=\r\ned in conjunction with \nRAM  \n&gt; and other geometry learning methods. So til=\r\ne-coding itself is not &gt; \na  \n&gt; misdirection. It might be the case that is =\r\nhas been narrowly \napplied  \n&gt; in the past (causing you to label it a disea=\r\nse) but it most \ncertainly  \n&gt; can be used in ways that ultimately respect =\r\ngeometry. So in a \nsense  \n&gt; tile-coding is similar to NEAT without the Hyp=\r\ner; its a more \ngeneral  \n&gt; tool that can be easily be misapplied (e.g. if =\r\nyou have a bad \ninput  \n&gt; encoding), but also can be used to respect geomet=\r\nry (e.g. by \nadding  \n&gt; in Hyper).\n&gt; \n\nJoe, first I want to thank you for p=\r\nointing to Littman&#39;s work \nwith RAM and Stone&#39; work.  I really appreciate y=\r\nou drawing \nconnections like that of which I may not be aware. I&#39;ve read th=\r\nrough \nit and now have a pretty good grasp of the idea.\n\nHaving now got the=\r\n Relocatable Action Model idea, my read seems to be \ndifferent from yours. =\r\n First, it is not a &quot;geometry learning \nmethod.&quot;  Rather, it is a method fo=\r\nr a priori identification of \ngeometric relationships in the state space.  =\r\nNote that HyperNEAT \nintroduces a priori relationships in the state *repres=\r\nentation*, \nwhich then allow it to *learn* relationships among states.  The=\r\n RAM \napproach (so far) just has the user enumerate all the relationships \n=\r\namong states from the get go, not learning anything at all.  It \nfurthermor=\r\ne does not have any facility whatsoever for leveraging the \ngeometry of the=\r\n state representation (i.e. the input vector).\n\nThe way the user specifies =\r\nthe relatedness of states is by assigning \nthem to &quot;types,&quot; which are class=\r\n partitions of the state space.\n\nThen it just uses those to generalize its =\r\npolicy.  The paper by \nLittman explicitly says that in the future they woul=\r\nd like to work on \nlearning the *types*.  Now that would be a geometry lear=\r\nning method.  \nBut that&#39;s going to be nontrivial:  What they are about to d=\r\no is run \ninto their biggest dilemma- how can they deal with learning \nrepr=\r\nesentation when they can&#39;t optimize the model?  I&#39;m not saying it \ncan&#39;t be=\r\n done, but they are a few steps behind us, because we&#39;ve \nalready faced up =\r\nto it and started to offer solutions.  So it&#39;s not \naccurate to hold RAM up=\r\n as an example of analogous methods in RL to \nwhat we have now in EC.\n\nWith=\r\n that said, even given what RAM can do, the idea that you can \napply tile c=\r\noding &quot;in conjunction&quot; with it seems ill-defined.  I&#39;m \nnot sure why you as=\r\nsert this capability?  Or are you just \nspeculating?  If you applied tile c=\r\noding to the state representation, \nthen the RAM would simply supersede the=\r\n tile coding because the RAM \nwould map states (whether broken apart or not=\r\n) to types, that is, \ntile coding would contribute nothing whatsoever.  Sin=\r\nce the user is \nthe one who defines that mapping, the user would simply ign=\r\nore the \ntile coding.\n\nOr, you could perhaps apply tile coding to the type =\r\nrepresentation, \nin the event a &quot;type&quot; can vary along a dimension.  If that=\r\n was \npossible, then tile coding would (as usual) be a bad idea because it =\r\n\nwould break up inherent relationships among types that could have \nbeen ex=\r\nploited by the learning algorithm.  \n\nSo when you get into the details, I d=\r\non&#39;t see it.  Tile coding at \nbest does nothing to enhance such an approach=\r\n and at worst screws it \nup as usual.  Tile coding is simply bad news when =\r\nit comes to \nprogress in the field.\n\n&gt; \n&gt; Not complete generality, obviousl=\r\ny that&#39;s stupid and its what gets \nus  \n&gt; in trouble in the first place. Bu=\r\nt I do want the maximum amount of  \n&gt; generality possible. Remember no one =\r\nknows what happens to NFL in  \n&gt; classes of &quot;real world&quot; problems. Certainl=\r\ny in classes of  \n&gt; compressible problems it no longer holds. So what&#39;s the=\r\n best we \ncan  \n&gt; do, in general, on real-world problems? I think this is a=\r\n \nperfectly  \n&gt; valid line of inquiry, and possibly very fruitful.\n&gt; \n\nYes =\r\nit&#39;s valid, but the problem is that setting broad theoretical \ngoals like t=\r\nhat has a way of obfuscating common sense.  Common sense \nsays that you don=\r\n&#39;t throw out a gold mine of information unless you \nwant to experience pove=\r\nrty.  I&#39;ll take the geometry of the world with \nme when I attack a problem,=\r\n and you can feel free to try to learn \nboth the problem and its geometry a=\r\nt the same time, and we will see \nwho gets farther at solving the problem.\n=\r\n\n&gt; \n&gt; But why shouldn&#39;t we want learning algorithms that can figure out  \n&gt;=\r\n geometry? Then they would be really generally useful in arbitrary  \n&gt; lear=\r\nning problems. What&#39;s the geometry of arbitrary classification  \n&gt; problems=\r\n? What&#39;s the geometry of learning natural language? What&#39;s  \n&gt; the geometry=\r\n of learning to diagnose heart-disease? What&#39;s the  \n&gt; geometry of poker? F=\r\nor control problems and board games I can \nimagine  \n&gt; how geometry might h=\r\nelp, but there are so many more ML problems \nthat  \n&gt; don&#39;t seem to have na=\r\ntural geometries (or at least geometries that  \n&gt; occur to me offhand). So =\r\nwhat can we do? Well if we want to \nleverage  \n&gt; geometry, we&#39;ll have to ha=\r\nve the learning algorithm figure out the  \n&gt; geometry by itself.\n&gt; \n\nYou&#39;re=\r\n right: if we can learn geometry to the extent we don&#39;t know it \nalready, w=\r\ne should do that.  So in some domains, like those you cite, \nit might be gr=\r\neat.  Of course, in reality, many of those domains do \nhave at least some u=\r\nseful a priori geometry (e.g. classification \nproblem inputs often have geo=\r\nmetric relationships and words in \nnatural language can be clustered in spa=\r\nce by similarity- which is \nactually pretty interesting because HyperNEAT c=\r\nould exploit that).  \nBut ok, in some cases, there is geometry yet to be kn=\r\nown.  \n\nBut nothing prevents any algorithm including HyperNEAT from learnin=\r\ng \nthe relationships of arbitrarily-ordered inputs.  It&#39;s not \nimpossible, =\r\nit&#39;s just harder than having them at the get-go.  \nHyperNEAT would eventual=\r\nly impose a meaningful geometric transform \nonto an ad hoc geometric organi=\r\nzation at a low level in the \nconnective CPPN.  \n\nThe point is just that wh=\r\nen it&#39;s known to begin with, it should be \npart of the problem definition, =\r\nnot thrown away by any means (like \ntile coding does on purpose!).\n\n&gt; \n&gt; Ok=\r\n now you&#39;re being disingenuous again. &quot;Of course no one would do  \n&gt; that.&quot;=\r\n Even I agree. But please don&#39;t characterize my argument so  \n&gt; myopically.=\r\n The correct approach would be to first learn the \ngeometry  \n&gt; and then tr=\r\ny to leverage that to do higher level learning. \nPreferably  \n&gt; you&#39;d have =\r\na learning algorithm capable of doing both in tandem. \nThat  \n&gt; would be re=\r\nally cool, and really generally applicable. Thus I \nthink  \n&gt; there is a lo=\r\nt of room for algorithm that learn &quot;generally&quot; but \nstill  \n&gt; are clever. Y=\r\nou seem to be neglecting that point.\n&gt; \n\nSorry you feel I&#39;m being disingenu=\r\nous; my intention is not to pardody \nyour view and I apologize if I&#39;ve misr=\r\neprented it.  But what you&#39;re \nadvocating by asking for an algorithm that d=\r\noes both would not be \nanything new.  Any old RL or ML algorithm starts out=\r\n knowing nothing \nabout the geometry of the input representation.  In effec=\r\nt, then, \nthey have to learn that geometry themselves as part of solving th=\r\ne \nproblem.  What&#39;s new is to actually help them see it from the start.\n&gt; \n=\r\n&gt; I agree tile-coding is useful only after we&#39;ve learned the \ngeometry  \n&gt; =\r\nof the problem. I think that much is clear. We shouldn&#39;t try \nfitting  \n&gt; t=\r\no a curve until we well understand the geometry of the manifold \nthat  \n&gt; i=\r\nts embedded on.\n&gt; \n\nI still don&#39;t see how you&#39;re reaching this conclusion. =\r\n When exactly \nwould be &quot;after we&#39;ve learned the geometry?&quot;  Are you talkin=\r\ng about \nRAM?  RAM doesn&#39;t learn the geometry; it just has the user input i=\r\nt \ndirectly.  And after that, tile coding would duly screw it up as much \na=\r\ns it does anything else.\n\n&gt; It strikes me than at interesting turn that thi=\r\ns discussion could  \n&gt; take is towards Kernel methods in ML. Specifying a k=\r\nernel in a  \n&gt; support-vector machine (SVM) determines the geometry, since =\r\nthe  \n&gt; kernel basically defines a notion of distance between points (a  \n&gt;=\r\n metric, to be mathematically precise). So how do you view your \nideas  \n&gt; =\r\nfor HyperNEAT as separate from what the kernel people have been  \n&gt; doing? =\r\nOr maybe HyperNEAT is a way to bridge computational  \n&gt; intelligence method=\r\ns and machine learning?\n&gt; \n\nThat is indeed an interesting parallel.  I will=\r\n have to think about \nit more.\n\nken\n\n\n\n"}}