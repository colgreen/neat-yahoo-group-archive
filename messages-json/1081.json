{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":115403844,"authorName":"John Arrowwood","from":"&quot;John Arrowwood&quot; &lt;jarrowwx@...&gt;","profile":"jarrowwx","replyTo":"LIST","senderId":"RirEfedXU619TjW4LD8Aw2wJ45O8zXjwXtkaRJjn93dZufSNsXamfld5m9SJXlh89XVxb2E-QlaYNr4F_wddsfrEocCoqtbP20LiusKT","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] Computation Time","postDate":"1087397736","msgId":1081,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PEJBWTItRjExNlVpQ2R6T0Vnd3MwMDA2OTQwM0Bob3RtYWlsLmNvbT4="},"prevInTopic":1080,"nextInTopic":1086,"prevInTime":1080,"nextInTime":1082,"topicId":845,"numMessagesInTopic":99,"msgSnippet":"The language was C, compiled using GCC -O3 I tried it both on my laptop, a P2 400mhz with 256 megs of ram and my dual P4 xeon 2.4 ghz with 2 gigs of ram.","rawEmail":"Return-Path: &lt;jarrowwx@...&gt;\r\nX-Sender: jarrowwx@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 30149 invoked from network); 16 Jun 2004 14:55:59 -0000\r\nReceived: from unknown (66.218.66.217)\n  by m22.grp.scd.yahoo.com with QMQP; 16 Jun 2004 14:55:59 -0000\r\nReceived: from unknown (HELO hotmail.com) (65.54.247.116)\n  by mta2.grp.scd.yahoo.com with SMTP; 16 Jun 2004 14:55:59 -0000\r\nReceived: from mail pickup service by hotmail.com with Microsoft SMTPSVC;\n\t Wed, 16 Jun 2004 07:55:36 -0700\r\nReceived: from 64.122.44.102 by by2fd.bay2.hotmail.msn.com with HTTP;\n\tWed, 16 Jun 2004 14:55:36 GMT\r\nX-Originating-Email: [jarrowwx@...]\r\nX-Sender: jarrowwx@...\r\nTo: neat@yahoogroups.com\r\nBcc: \r\nDate: Wed, 16 Jun 2004 07:55:36 -0700\r\nMime-Version: 1.0\r\nContent-Type: text/plain; format=flowed\r\nMessage-ID: &lt;BAY2-F116UiCdzOEgws00069403@...&gt;\r\nX-OriginalArrivalTime: 16 Jun 2004 14:55:36.0528 (UTC) FILETIME=[FB65B500:01C453B1]\r\nX-eGroups-Remote-IP: 65.54.247.116\r\nFrom: &quot;John Arrowwood&quot; &lt;jarrowwx@...&gt;\r\nReply-To: john@...\r\nSubject: Re: [neat] Computation Time\r\nX-Yahoo-Group-Post: member; u=115403844\r\nX-Yahoo-Profile: jarrowwx\r\n\r\nThe language was C, compiled using GCC -O3\n\nI tried it both on my laptop, a P2 400mhz with 256 megs of ram and my dual \nP4 xeon 2.4 ghz with 2 gigs of ram.  While the laptop was obviously slower, \nthe same proportional improvements were seen.\n\nI&#39;ll upload the test code to the files section so you can see how I did it.  \nAnd you can try it on your machine so you can compare my results.\n\n\n&gt;From: &quot;Jim O&#39;Flaherty, Jr.&quot; &lt;jim_oflaherty_jr@...&gt;\n&gt;Reply-To: neat@yahoogroups.com\n&gt;To: neat@yahoogroups.com\n&gt;Subject: Re: [neat] Computation Time\n&gt;Date: Wed, 16 Jun 2004 07:44:36 -0700 (PDT)\n&gt;\n&gt;John,\n&gt;\n&gt;What language, compiler, memory and CPU were you using?  All of these have \n&gt;impacts on timing and\n&gt;performance?\n&gt;\n&gt;\n&gt;Jim O&#39;Flaherty\n&gt;\n&gt;\n&gt;--- John Arrowwood &lt;jarrowwx@...&gt; wrote:\n&gt; &gt; I did some tests.\n&gt; &gt;\n&gt; &gt; I created a simulated (i.e. meaningless) network activation function.  \n&gt;The\n&gt; &gt; simulated network had 10 input nodes, 10 hidden nodes, and one output \n&gt;node.\n&gt; &gt; The network was fully connected, with random weights.\n&gt; &gt;\n&gt; &gt; I then tried four variations of the code, to see the effect on the \n&gt;speed:\n&gt; &gt;\n&gt; &gt; 1. declare an array, and store the intermediate values (node * weight) \n&gt;in\n&gt; &gt; the array.  Then perform all the additions, then all of the sigmoids.  \n&gt;Idea\n&gt; &gt; was to allow as much parallelism as the chip could muster during the\n&gt; &gt; multiplication phase...\n&gt; &gt;\n&gt; &gt; 2. alter the order of processing so that a single node value was being\n&gt; &gt; multiplied by each of its different weights, such that memory access was\n&gt; &gt; streamlined\n&gt; &gt;\n&gt; &gt; 3. intersperse the additions and sigmoids among the multiplications.\n&gt; &gt;\n&gt; &gt; 4. compose single expressions that perform all the multiplications,\n&gt; &gt; additions, and sigmoid all at once\n&gt; &gt;\n&gt; &gt; The results were:\n&gt; &gt;\n&gt; &gt; 1. 377k/sec\n&gt; &gt; 2. 380k/sec  (so the memory order did make a difference here)\n&gt; &gt; 3. 430k/sec  (made a big difference to give the CPU something to do \n&gt;while it\n&gt; &gt; waited for the sigmoid to finish)\n&gt; &gt; 4. 453k/sec  (letting the compiler do the optimization works well)\n&gt; &gt;\n&gt; &gt; The one configuration that I did NOT try was the traditional &#39;loop&#39;\n&gt; &gt; approach, where node and weights are fetched from memory.  I can not see \n&gt;how\n&gt; &gt; that approach could ever compare to the unrolled version.  Since there \n&gt;are\n&gt; &gt; no conditionals, there is no chance of a branch-prediction miss.  So the\n&gt; &gt; values of weights will always be available as soon as they are needed.   \n&gt;And\n&gt; &gt; since the only thing stored outside the code stream are node weights, \n&gt;the\n&gt; &gt; whole thing should have no trouble fitting in the cache, unless you have \n&gt;a\n&gt; &gt; really, REALLY large network.\n&gt; &gt;\n&gt; &gt; Thoughts?\n&gt; &gt;\n&gt; &gt; -- John\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; &gt;From: Colin Green &lt;cgreen@...&gt;\n&gt; &gt; &gt;Reply-To: neat@yahoogroups.com\n&gt; &gt; &gt;To: neat@yahoogroups.com\n&gt; &gt; &gt;Subject: Re: [neat] Re: Computation Time\n&gt; &gt; &gt;Date: Tue, 15 Jun 2004 23:52:06 +0100\n&gt; &gt; &gt;\n&gt; &gt; &gt;Ian Badcoe wrote:\n&gt; &gt; &gt;\n&gt; &gt; &gt; &gt;&gt;Hi Philip,\n&gt; &gt; &gt; &gt;&gt;\n&gt; &gt; &gt; &gt;&gt;My curiosity got the better of me :) I tried the above functions \n&gt;using\n&gt; &gt; &gt; &gt;&gt;optimized C# on an AMD Athlon 2400+ (actually 2.17Ghz). The results \n&gt;are\n&gt; &gt; &gt; &gt;&gt;slightly bizarre,\n&gt; &gt; &gt; &gt;&gt; oh BTW I think you quoted the tanh function wrong, so I used y =\n&gt; &gt; &gt; &gt;&gt;tanh(0.9*x) which gives a nice sigmoid. Firstly I had to use 100 \n&gt;million\n&gt; &gt; &gt; &gt;&gt;(10^8) loops to get readable results, the approx. 50x difference is\n&gt; &gt; &gt; &gt;&gt;partly due to the CPU (obviously!) but maybe the rest is due to my\n&gt; &gt; &gt; &gt;&gt;oversimplistic implementation whereby I used the same value for x \n&gt;every\n&gt; &gt; &gt; &gt;&gt;time - did you generate random numbers perhaps? Also I know that \n&gt;Java\n&gt; &gt; &gt; &gt;&gt;has JIT compilers but sometime only optimize in code hot-spots \n&gt;during\n&gt; &gt; &gt; &gt;&gt;code execution, they can also run in interpreter mode - my run was \n&gt;with\n&gt; &gt; &gt; &gt;&gt;JITed code.\n&gt; &gt; &gt; &gt;&gt;\n&gt; &gt; &gt; &gt;&gt;Here are the figures:\n&gt; &gt; &gt; &gt;&gt;\n&gt; &gt; &gt; &gt;&gt;sigmoid:  3625ms\n&gt; &gt; &gt; &gt;&gt;evsail:    2359ms\n&gt; &gt; &gt; &gt;&gt;inv-abs:  188ms\n&gt; &gt; &gt; &gt;&gt;\n&gt; &gt; &gt; &gt;&gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;By my calculations, this makes just over 3 cycles per complete\n&gt; &gt; &gt; &gt;calculation.  That&#39;s not impossible.  e.g. ISRT on the K7 (Athlon\n&gt; &gt; &gt; &gt;predecessor) a floating-divide took 3 cycles but that the chip was \n&gt;able\n&gt; &gt; &gt;to\n&gt; &gt; &gt; &gt;have 2 fdivs and 2fadds and some integer instructions running\n&gt; &gt; &gt;simulatneously.\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt;Yep, via the various  instruction pipelines. Assuming the a 2.17Ghz\n&gt; &gt; &gt;clock I translate the above figures as follows:\n&gt; &gt; &gt;\n&gt; &gt; &gt;sigmoid: 78.66 cycles\n&gt; &gt; &gt;evsail:     51.00\n&gt; &gt; &gt;inv-abs:    4.08\n&gt; &gt; &gt;tanh:      269.00\n&gt; &gt; &gt;\n&gt; &gt; &gt;certainly interesting.\n&gt; &gt; &gt;\n&gt; &gt; &gt; &gt;It does sound suspiciously good, however.  I don&#39;t know much about C# \n&gt;but\n&gt; &gt; &gt; &gt;presumably it&#39;s inlining the function, and maybe unrolling the loop a\n&gt; &gt; &gt; &gt;little.  OTOH, if it did all that, then it should be able to see that \n&gt;you\n&gt; &gt; &gt; &gt;are making the same call every time and that the function has no side\n&gt; &gt; &gt; &gt;effects, so did it need to run the function at all?\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt;That particular test was a loop, no methods calls involved. But yes the\n&gt; &gt; &gt;.Net compiler does do inlining, although there is no inline hint \n&gt;keyword\n&gt; &gt; &gt;as in some C++ compilers - as I understood it the keyword was largely\n&gt; &gt; &gt;ignored in later compilers anyway - based on the idea that the compiler\n&gt; &gt; &gt;knows best.\n&gt; &gt; &gt;\n&gt; &gt; &gt; &gt;Compilers can be blind to that sort of thing, however, like I \n&gt;mentioned\n&gt; &gt; &gt;before.\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;&gt;tanh:     12,400ms\n&gt; &gt; &gt; &gt;&gt;\n&gt; &gt; &gt; &gt;&gt;weird huh.  The tanh loop took 66x longer then the ins-abs one!\n&gt; &gt; &gt; &gt;&gt;\n&gt; &gt; &gt; &gt;&gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;All trig, exp and log are very expensive.\n&gt; &gt; &gt; &gt;Sqrt is expensive but maybe not so bad.\n&gt; &gt; &gt; &gt;Divide is releatively cheap nowadays.\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;The thing about the more exotic instructions, like tan, is that not \n&gt;only\n&gt; &gt; &gt;do\n&gt; &gt; &gt; &gt;they take a lot of cycles, but the chip only has one processor for\n&gt; &gt; &gt; &gt;them.  Also slow instructions have a disproportionate effect on\n&gt; &gt; &gt;throughput\n&gt; &gt; &gt; &gt;because all the shorter instructions, which could run in parallel, \n&gt;can\n&gt; &gt; &gt;only\n&gt; &gt; &gt; &gt;go so far before they hit a dependency on the result of the long\n&gt; &gt; &gt; &gt;instruction and have to stop.  Thus effectively the whole chip hangs \n&gt;on\n&gt; &gt; &gt;the\n&gt; &gt; &gt; &gt;result of the tan.\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt;I think modern cpu&#39;s have more than one fpu pipeline - but yes, the\n&gt; &gt; &gt;principle still holds.\n&gt; &gt; &gt;\n&gt; &gt; &gt; &gt;&gt;  I wonder though if the technqiue of trying to do\n&gt; &gt; &gt; &gt;&gt;many sequentail ops in order wll only become beneficial when the\n&gt; &gt; &gt; &gt;&gt;networks get *really* big, simply because the memory caches in \n&gt;modern\n&gt; &gt; &gt; &gt;&gt;CPU&#39;s are so large. So there may be some network size at which we \n&gt;would\n&gt; &gt; &gt; &gt;&gt;see a dramatic slow down of our code if it&#39;s not optimized in such a\n&gt; &gt; &gt;way.\n&gt; &gt; &gt; &gt;&gt;\n&gt; &gt; &gt; &gt;&gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;That&#39;s what I would expect, not just the network size, however, also \n&gt;the\n&gt; &gt; &gt; &gt;total size of the data you want to pass through.  If you run many \n&gt;copies\n&gt; &gt; &gt;of\n&gt; &gt; &gt; &gt;the same small network on different data then memory-access may be \n&gt;your\n&gt; &gt; &gt; &gt;bottleneck.\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;Ok but the input data is copied into the input nodes and then the bulk\n&gt; &gt; &gt;of the network CPU time is in activating the whole network several \n&gt;times\n&gt; &gt; &gt;over. So this really depends on how many network epochs you run on each\n&gt; &gt; &gt;set of input data.\n&gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;&gt;Another way of estimating how efficient my code is is to caclulate \n&gt;the\n&gt; &gt; &gt; &gt;&gt;average number of clock cycles that it requires per neuron and\n&gt; &gt; &gt; &gt;&gt;connection. So e.g. My 53 neuron / 413 connection network performs \n&gt;413\n&gt; &gt; &gt; &gt;&gt;additions and 53 activations per epoch. So that&#39;s 466 necessary\n&gt; &gt; &gt; &gt;&gt;operations in all, this is an absolute minimum. ok, plus a couple\n&gt; &gt; &gt; &gt;&gt;because the activation fn is several operations (but this is just a\n&gt; &gt; &gt; &gt;&gt;rough bit of maths). Using a simple bit of maths I can then \n&gt;determine:\n&gt; &gt; &gt; &gt;&gt;\n&gt; &gt; &gt; &gt;&gt;ops per epoch = 466\n&gt; &gt; &gt; &gt;&gt;ops per test run = 466 * 100,000 (loops) = 46,600,000\n&gt; &gt; &gt; &gt;&gt;ops per second = 46,600,000 / 5000ms(approx) = 9,320,000\n&gt; &gt; &gt; &gt;&gt;CPU clock cycles per op = 2.17Ghz / 9,320,000 = 232.\n&gt; &gt; &gt; &gt;&gt;\n&gt; &gt; &gt; &gt;&gt;\n&gt; &gt; &gt; &gt;&gt;Now 232 isn&#39;t all that bad when you consider this doesn&#39;t take into\n&gt; &gt; &gt; &gt;&gt;account the extra code that is required to do the looping/indexing\n&gt; &gt; &gt; &gt;&gt;through all of the neurons and connections. So perhaps hand \n&gt;optimized\n&gt; &gt; &gt; &gt;&gt;assembler could get this down to 100 cylcles or maybe 50, but this \n&gt;is in\n&gt; &gt; &gt; &gt;&gt;the same ball park as optimum - and therefore I wouldn&#39;t expect any\n&gt; &gt; &gt; &gt;&gt;massive improvements. Well, not unless you start using SIMD\n&gt; &gt; &gt; &gt;&gt;instructions, which I&#39;m definitely NOT! :)\n&gt; &gt; &gt; &gt;&gt;\n&gt; &gt; &gt; &gt;&gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;You easily can do a better analysis than that.  Run the timing a few\n&gt; &gt; &gt;times\n&gt; &gt; &gt; &gt;with different sizes of network (number of Ops) then plot the line of\n&gt; &gt; &gt; &gt;number of ops (x) vs time (y).  You should get an +ve intercept on \n&gt;the\n&gt; &gt; &gt; &gt;y-axis which is the constant cost of your program and a +ve sloping \n&gt;line,\n&gt; &gt; &gt; &gt;which is the cost per op...\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt;OK I&#39;ve made 3 measurements, the same network as before but with 104,\n&gt; &gt; &gt;207, and 413 connections. The times are:\n&gt; &gt; &gt;\n&gt; &gt; &gt;104: 2481ms\n&gt; &gt; &gt;207: 3343ms\n&gt; &gt; &gt;413:  4678ms\n&gt; &gt; &gt;\n&gt; &gt; &gt;If you plot these on a graph it is slightly non-linear, the line is\n&gt; &gt; &gt;curving upwards - which is what you might expect if, say, the cache is\n&gt; &gt; &gt;becoming less efficient with the accessing of more data. Assuming a\n&gt; &gt; &gt;straight line between the first and last reading, this then gives:\n&gt; &gt; &gt;\n&gt; &gt; &gt;secs per connection: 7.11 * 10^-8\n&gt; &gt; &gt;connections/sec : 14,064,633\n&gt; &gt; &gt;clock cycles/connection: 154\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;These times don&#39;t include calculating the activation fn, this is all\n&gt; &gt; &gt;time spent executing loops to fetch a neuron output value, multiply the\n&gt; &gt; &gt;value by a weight and then add that to a total  ready to be put through\n&gt; &gt; &gt;the activation fn. So perhaps there is room for improvment there, a\n&gt; &gt; &gt;multiply an add and a couple of memory accesses taking 154 cycles is a\n&gt; &gt; &gt;little bit sloppy, but then this is .NET remember - and as such there \n&gt;is\n&gt; &gt; &gt;also a single type cast in there because .NET does not yet support\n&gt; &gt; &gt;templates (to be called Generics I believe), this could well be the \n&gt;bulk\n&gt; &gt; &gt;of the 154 cycles!\n&gt; &gt; &gt;\n&gt; &gt; &gt;Colin\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;Yahoo! Groups Links\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;Yahoo! Groups Links\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n\n\n\n"}}