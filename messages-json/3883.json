{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":203001720,"authorName":"Wesley Tansey","from":"Wesley Tansey &lt;tansey@...&gt;","profile":"tansey4","replyTo":"LIST","senderId":"OKHdzRNNFe98D7cnOIzDDxM6CWdOPrRrQ7gEWqaYuovJMK7bnKmRK8CYC0hbSNBCssJjnIZq26HoSIIiH9yeAREP_xs","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [neat] Re: Backpropagation and NEAT","postDate":"1205775688","msgId":3883,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ3REVBRDQ4LjMwNTA5MDFAdnQuZWR1Pg==","inReplyToHeader":"PGZybTI2cyt1MjBrQGVHcm91cHMuY29tPg==","referencesHeader":"PGZybTI2cyt1MjBrQGVHcm91cHMuY29tPg=="},"prevInTopic":3876,"nextInTopic":3884,"prevInTime":3882,"nextInTime":3884,"topicId":3846,"numMessagesInTopic":41,"msgSnippet":"I just want to add to two of the issues raised here. ... This is really starting to be recognized by a lot of the machine learning crowd. At ILP 2007, Pedro","rawEmail":"Return-Path: &lt;tansey@...&gt;\r\nX-Sender: tansey@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 86553 invoked from network); 17 Mar 2008 17:41:37 -0000\r\nX-Received: from unknown (66.218.67.96)\n  by m55.grp.scd.yahoo.com with QMQP; 17 Mar 2008 17:41:37 -0000\r\nX-Received: from unknown (HELO lennier.cc.vt.edu) (198.82.162.213)\n  by mta17.grp.scd.yahoo.com with SMTP; 17 Mar 2008 17:41:37 -0000\r\nX-Received: from dagger.cc.vt.edu (evil-dagger.cc.vt.edu [10.1.1.11])\n\tby lennier.cc.vt.edu (8.12.11.20060308/8.12.11) with ESMTP id m2HHfarI014029\n\tfor &lt;neat@yahoogroups.com&gt;; Mon, 17 Mar 2008 13:41:36 -0400\r\nX-Received: from [10.0.2.5] (chaos.cs.vt.edu [128.173.236.168])\n\tby dagger.cc.vt.edu (MOS 3.8.6-GA)\n\twith ESMTP id JFJ93641;\n\tMon, 17 Mar 2008 13:41:36 -0400 (EDT)\r\nMessage-ID: &lt;47DEAD48.3050901@...&gt;\r\nDate: Mon, 17 Mar 2008 13:41:28 -0400\r\nUser-Agent: Thunderbird 2.0.0.12 (Windows/20080213)\r\nMIME-Version: 1.0\r\nTo: neat@yahoogroups.com\r\nReferences: &lt;frm26s+u20k@...&gt;\r\nIn-Reply-To: &lt;frm26s+u20k@...&gt;\r\nContent-Type: text/plain; charset=windows-1252; format=flowed\r\nContent-Transfer-Encoding: 8bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Wesley Tansey &lt;tansey@...&gt;\r\nSubject: Re: [neat] Re: Backpropagation and NEAT\r\nX-Yahoo-Group-Post: member; u=203001720; y=dyYQR4PqTf30hvWcw_DO_vj5S76Dw6jKukzl-YVG6vvhrw\r\nX-Yahoo-Profile: tansey4\r\n\r\nI just want to add to two of the issues raised here.\n\n&gt;  So you see, in\n&gt; building a new algorithm or a new theory with practical implications,\n&gt; often traditional problems are exactly the wrong vehicle to discovery\n&gt; because they perpetuate the same dogmatic perspectives that already\n&gt; permeate the field to begin with and cause it to be staying in one\n&gt; place.   Thus all of these applications are chosen with careful\n&gt; scrutiny with a sincere belief in their practical ramifications.\n&gt;   \nThis is really starting to be recognized by a lot of the machine \nlearning crowd. At ILP 2007, Pedro Domingos gave an invited presentation \nentitled &quot;Structured Machine Learning: Ten Problems for the Next Ten \nYears.&quot; (http://www.cs.washington.edu/homes/pedrod/papers/ilp07.pdf) \nWhile it&#39;s a different subdomain of machine learning, he had the same \nthoughts on &quot;traditional&quot; problems as targets for new research. From the \narticle:\n\n&gt; We need to avoid falling into local optima in our research: once a \n&gt; problem is solved\n&gt; �80/20,� we should move on to the next larger one that includes it, \n&gt; not continue to refine our\n&gt; solution with diminishing returns. Our natural tendency to do the \n&gt; latter greatly slows down the\n&gt; progress of research. Moreover, the best solutions to subproblems \n&gt; taken in isolation are often not\n&gt; the best ones in combination. Because of this, refining solutions to \n&gt; subproblems can in fact be\n&gt; counterproductive�digging deeper into the local optimum instead of \n&gt; escaping it.\n\nThe second point I want to address is on the use of NEAT, or more \ngenerally EAs, in &quot;Fortune 100 defense firms.&quot; I recently interviewed at \nBBN Technologies, and they told me about one project their Cambridge \nteam worked on and is now deployed in Iraq. When a convoy of troops is \ntraveling and a sniper fires a shot, an array of microphones mounted on \ntop of one of the vehicles records the sound, then runs a \nmulti-objective evolutionary algorithm to determine the distance, \nelevation, etc., that the shot originated from. The result is that when \nan insurgent sniper fires a shot, within 3 seconds the entire convoy \nturns around and fires everything at that spot. Apparently the mic array \nsystem works well enough that it has been nicknamed the &quot;Octopus of Death.&quot;\n\nWarning: the rest of this message is extremely off topic.\n\nThat&#39;s just my thoughts/experiences though. For what it&#39;s worth, I think \nit&#39;s pretty hard to deny that EAs have fallen out of favor with the \nmainstream machine learning crowd. To a large degree, I believe this is \nthe result of an easily satisfied community. Groucho Marx said, &quot;I \nwouldn&#39;t want to belong to any club that would have me as a member&quot; and \nthe same is true of research communities. Conference quality is \ndetermined almost entirely by acceptance rates. The &quot;top tier&quot; \nconferences in AI like AAAI, IJCAI, ICML, etc. all have acceptance rates \naround or below 25%. The EA community decided to break off and form lots \nof specialized conferences, which would be fine, except that these \nconferences have ridiculous acceptance rates of around ~45% or higher. \nWith such a low barrier to entry, the amount of rigor applied to EA \npapers is routinely unacceptable and occasionally shameful. The result \nis that work by Ken&#39;s group at UCF or Risto&#39;s group at UT Austin gets \nbest paper at GECCO or CEC, but among a lot of machine learning faculty \nit still isn&#39;t considered as prestigious as a paper accepted to one of \nthe top tier conferences mentioned above, despite the fact that the work \nis more likely to have a significant impact on practical machine learning.\n\nAs a Software Engineering student, I know what it&#39;s like to be on the \nother extreme end of the spectrum. SE conference acceptance rates at the \ntop tier are dipping into the low teens or even single digits. This \ncertainly has a lot of negative implications for researchers. Many \ntimes, reviewers simply look for any little way to reject a paper, and \nyou typically have to resubmit 2 or 3 times, with major revisions and \nadditions each time, before a paper is accepted. However, when a \ngraduating PhD. student has several publications at ICSE, OOPSLA, FSE, \netc., their resume automatically gets them considered for tenure-track \npositions at top 10 schools. There are just no such conferences in the \nEA domain where people would look at a set of publications and say &quot;Wow, \nhe got in there--TWICE!&quot;\n\nSo I guess the question that I&#39;m throwing in here is: why isn&#39;t there a \ntop tier conference for EAs? How about Foundations of Evolutionary \nAlgorithms (FEA)?\n\nWesley\n\n\n"}}