{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":253727962,"authorName":"Nikolai","from":"&quot;Nikolai&quot; &lt;ker_31_toluca@...&gt;","profile":"ker_31_toluca","replyTo":"LIST","senderId":"M3aKJDxOUuhs4ByImyLjNv9knp8A4frey95zr_riBNpvxex4oZ3Y98QFr81TnmSxZpAj5DArw7hcpIm2ETxF4YKaKloq9OoNbg8-_fA","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: hyperneat questions","postDate":"1250123837","msgId":4827,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGg1dm43dCt0NWgwQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDExNzBFOTAxMjM4QjRDNzM5RjMwMTJDM0M2ODc1OThGQHdhdHRwND4="},"prevInTopic":4826,"nextInTopic":0,"prevInTime":4826,"nextInTime":4828,"topicId":4808,"numMessagesInTopic":10,"msgSnippet":"Hi Ken ... well i think this depends on how much can you optimize your HOST(CPU) code. my fitness function on GPU takes a lot of time, so i don t think 4 dual","rawEmail":"Return-Path: &lt;ker_31_toluca@...&gt;\r\nX-Sender: ker_31_toluca@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 4593 invoked from network); 13 Aug 2009 00:37:56 -0000\r\nX-Received: from unknown (69.147.108.200)\n  by m4.grp.re1.yahoo.com with QMQP; 13 Aug 2009 00:37:56 -0000\r\nX-Received: from unknown (HELO n40b.bullet.mail.sp1.yahoo.com) (66.163.168.154)\n  by mta1.grp.re1.yahoo.com with SMTP; 13 Aug 2009 00:37:56 -0000\r\nX-Received: from [69.147.65.147] by n40.bullet.mail.sp1.yahoo.com with NNFMP; 13 Aug 2009 00:37:18 -0000\r\nX-Received: from [98.137.34.184] by t10.bullet.mail.sp1.yahoo.com with NNFMP; 13 Aug 2009 00:37:17 -0000\r\nDate: Thu, 13 Aug 2009 00:37:17 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;h5vn7t+t5h0@...&gt;\r\nIn-Reply-To: &lt;1170E901238B4C739F3012C3C687598F@wattp4&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;Nikolai&quot; &lt;ker_31_toluca@...&gt;\r\nSubject: Re: hyperneat questions\r\nX-Yahoo-Group-Post: member; u=253727962; y=Z0RxraWoVHovJ-NjI-qJBclf3oYfjzJlDBPV30NOQWxkm0JJqQ0cCA\r\nX-Yahoo-Profile: ker_31_toluca\r\n\r\nHi Ken\n--- In neat@yahoogroups.com, &quot;Ken Lloyd&quot; &lt;kalloyd@...&gt; wrote:\n&gt;\n&gt; Ni=\r\nkolai,\n&gt;  \n&gt; After doing NEAT w/ CUDA for a while, I still consider myself =\r\nvery much a\n&gt; newbie.\n&gt;  \n&gt; There is a practical limit to how many GPUs you=\r\n can run on a motherboard.\n&gt;\nwell i think this depends on how much can you =\r\noptimize your HOST(CPU) code. my fitness function on GPU takes a lot of tim=\r\ne, so i don&#39;t think 4 dual GPUs will be a problem.\n\n This is constrained by=\r\n the number of lanes on the PCIe 2.0 bus and the # of\n&gt; memcpys between hos=\r\nt and devices. The most I have been able to run are 3\n&gt; GTX-285 (or similar=\r\n) devices, or 2 GTX-295&#39;s.  I haven&#39;t generated figures\n&gt; on dual Nehalem b=\r\noards with dual 5520 chips but the only such mobo I know of\n&gt; has only 3 PC=\r\nIe x16 slots.  It is interesting how closely QDR \n\nusually when you insert =\r\nmore than 2 GPUs in a 16x slot, the transfer speed drops by half, but there=\r\n is a new ASUS mobo, which can transfer at full 16x to each of 4 GPUs, http=\r\n://www.tomshardware.com/news/Asus-supercomputer-motherboard-P6T7,7720.html =\r\nand if you use PCI risers (i didn&#39;t try that) , you could theoricaly have 7=\r\n GPUs\n\n\nInfiniBand\n&gt; HCAs matches the new PCIe 2.0 in throughput, and what =\r\nthis holds for my\n&gt; Rocks/CUDA implementation is still underdetermined.\nver=\r\ny expensive stuff, i think island style evolution on each node, and then mi=\r\ngration over gigabit ethernet would be more practical. \n\n&gt;  \n&gt; There are se=\r\nveral (understatement) ways to slice and dice the computations,\n&gt; and where=\r\n in the various kinds of GPU memory (shared, texture) to put what\n&gt; data ma=\r\ntrices.  What seems to work best for me (so far) for MPP on the GPU\n&gt; kerne=\r\nl is alternating between cycles of evolution, _syncthread, evaluation,\n&gt; _s=\r\nyncthread, then repeating that cycle - in parallel across the block(s).  I\n=\r\n&gt; have other stuff beside the kernel launch and memcpy ops from host to dev=\r\nice\n&gt; going on in the other CPUs during this time. Generally I try to stay =\r\nwith 1\n&gt; CPU core for 1 GPU device. Using Intel&#39;s TBB and some early VS-201=\r\n0 C++ OX\n&gt; using the Parallel Pattern Library were encouraging.\n&gt;  \n&gt; I hav=\r\nen&#39;t made up my mind yet on whether texture memory is really the proper\n&gt; p=\r\nlace for just storing NEAT parameters or forward evolution operators\n&gt; (wit=\r\nhout parameters).  The overhead for changing your mind here is steep,\n&gt; but=\r\n storing NEAT parameters in texture has the current lead.  Stay tuned.\n&gt; As=\r\n always, any suggestions from others are always welcome.\n&gt;  \n&gt; Isn&#39;t this s=\r\ntuff wickedly complicated?  Too many variables.\nyes parallel programming is=\r\n very hard. and programming is very costly. here is a good guide for high-p=\r\nerformance programing: http://www.agner.org/optimize/  , and here are one g=\r\nood for cuda optimizing: www.icos.ethz.ch/cse/software/cuda/cuda8\n\nIn my ca=\r\nse i will have to reprogram all the NEAT code to be highly optimized, and i=\r\n have made some test with MMX instructions + hiding DRAM latency, i got spe=\r\ned up of about 1000x against single for() loop.  I got many ideas for optim=\r\nization, for example, for random numbers, i do not generate them when neede=\r\nd, but in bulk (kind of chinese production model, you got the resources fir=\r\nst, and then you start production), I put 4 GPUs to do it in parallel and t=\r\nhen transfer data to CPU. so, in a few millisecs i get around 2GB of random=\r\n numbers (and btw, i am planing to reuse them a few times if it works). I d=\r\no not use dynamic memory malloc() for one network, all the networks are loc=\r\nated in blocks of same size of 32MB (to avoid memory fragmentation). One bl=\r\nock only contains networks of the same size (again, because of fragmentatio=\r\nn reasons). Another change is that in my neat implementation i will be not =\r\ndoing mutations in random manner, but by phases. During one phase, all the =\r\nnetworks will be mutating weights, in another phase, all will be mating and=\r\n so on. When mutating this way , for example for adding links i do not have=\r\n to add the space for the genome dynamically, the space is already allocate=\r\nd between phases for all possible genome increases and so, all the memory l=\r\natency is hidden. When mutating weights, because all the weights are locate=\r\nd in an separate array, and random numbers in another it will be much faste=\r\nr to run one function on all networks to match one weight with another rand=\r\nom number (in bulk), and do the multiplication in a XMM register + again, h=\r\nide memory latency. This should give me 4x speed up in single precision as =\r\nminimum. These days all the MMX instructions are mapped to a C function, so=\r\n little assembly language knowledge is needed. As you can see, there is a l=\r\not of this kind of engineering optimizations that can be done on CPU, so it=\r\n can keep up with GPUs. \n\n\nRegards\nNikolai\n\n\n"}}