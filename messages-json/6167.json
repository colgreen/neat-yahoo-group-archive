{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":413744767,"authorName":"spoonsx21","from":"&quot;spoonsx21&quot; &lt;spoonsx21@...&gt;","profile":"spoonsx21","replyTo":"LIST","senderId":"I_lyerQK1MDnrc-PGP9MZ1RYrh3RE_3NsmnytPb8yJzQLubkuHehHmSdIQctACW7mWbwj5yRqTjTVgh4gbg7wYUp835Ecp2i","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: A fresh look at GPUs and OpenCL","postDate":"1372980994","msgId":6167,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGtyNTB1MisxNDNyQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGtyMXZyMytpOGhuQGVHcm91cHMuY29tPg=="},"prevInTopic":6166,"nextInTopic":0,"prevInTime":6166,"nextInTime":6168,"topicId":6161,"numMessagesInTopic":7,"msgSnippet":"I had written some code to  convert a feed forward neural network into a vertex shader (in GLSL) that calculates the CPPN queries in a massively parallel","rawEmail":"Return-Path: &lt;spoonsx21@...&gt;\r\nX-Sender: spoonsx21@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 36054 invoked by uid 102); 4 Jul 2013 23:36:35 -0000\r\nX-Received: from unknown (HELO mtaq2.grp.bf1.yahoo.com) (10.193.84.33)\n  by m3.grp.bf1.yahoo.com with SMTP; 4 Jul 2013 23:36:35 -0000\r\nX-Received: (qmail 22483 invoked from network); 4 Jul 2013 23:36:35 -0000\r\nX-Received: from unknown (HELO ng11.bullet.mail.bf1.yahoo.com) (98.139.164.106)\n  by mtaq2.grp.bf1.yahoo.com with SMTP; 4 Jul 2013 23:36:35 -0000\r\nX-Received: from [98.139.164.122] by ng11.bullet.mail.bf1.yahoo.com with NNFMP; 04 Jul 2013 23:36:35 -0000\r\nX-Received: from [10.193.94.106] by tg3.bullet.mail.bf1.yahoo.com with NNFMP; 04 Jul 2013 23:36:35 -0000\r\nDate: Thu, 04 Jul 2013 23:36:34 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;kr50u2+143r@...&gt;\r\nIn-Reply-To: &lt;kr1vr3+i8hn@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: &quot;spoonsx21&quot; &lt;spoonsx21@...&gt;\r\nSubject: Re: A fresh look at GPUs and OpenCL\r\nX-Yahoo-Group-Post: member; u=413744767; y=QEMPzOgpjRDxTBOe99lN7-Wir1gPg51GSqrXO8W7gy4M6R9h\r\nX-Yahoo-Profile: spoonsx21\r\n\r\nI had written some code to  convert a feed forward neural network into a ve=\r\nrtex shader (in GLSL) that calculates the CPPN queries in a massively paral=\r\nlel fashion.  One of the benefits being that you could run such a process i=\r\nnside of a browser using WebGL. Not to mention almost every platform has ac=\r\ncess to OpenGL and can run vertex shaders (anything over OpenGL 2.x). \n\nAll=\r\n you really need to do is compile a neural network down to the math functio=\r\nns the network represents. From there, you are almost as optimized as possi=\r\nble in software. \n\nI found that to boost query speed 10x inside of JavaScri=\r\npt on a MacBook Pro. I also think its probably one of the best optimization=\r\ns possible on the software side. The next level of optimization would be to=\r\n split up massive functions by dependencies and &quot;layers&quot; and run them acros=\r\ns multiple servers each with multiple GPUs with some architecture to pool t=\r\nhose queries together (similar to a map reduce). You would need a huge netw=\r\nork for that to be worth it. \n\nHowever, for small CPPNs, compiling to pure =\r\nmath functions and running a shader is one of the least complicated speed u=\r\nps, in my opinion. You don&#39;t have to deal with hardware, or CUDA compiling.=\r\n You need code to convert a network to GLSL string, and then a program that=\r\n loads in the shader sting, and can pull out an array of outputs after the =\r\ncalculations. \n\n-Paul\n\n\n--- In neat@yahoogroups.com, &quot;Ken&quot; &lt;kstanley@...&gt; w=\r\nrote:\n&gt;\n&gt; \n&gt; \n&gt; Hi Colin, one other interesting thing I&#39;d throw in here is =\r\nthe looming bottleneck in neuroevolution research of extremely large networ=\r\nks.  We&#39;re not really there yet today, but at some point making progress wi=\r\nll sometimes require investigating networks with millions or more connectio=\r\nns (maybe evolved by HyperNEAT or something HyperNEAT-like).  Simply queryi=\r\nng all the weights with the CPPN (which means activating the CPPN millions =\r\nof times) could then become a big obstacle to running experiments.  Even if=\r\n there was some way to streamline just that one aspect of the algorithm it =\r\ncould make a big difference in the future.\n&gt; \n&gt; Best,\n&gt; \n&gt; ken\n&gt; \n&gt; --- In =\r\nneat@yahoogroups.com, Colin Green &lt;colin.green1@&gt; wrote:\n&gt; &gt;\n&gt; &gt; Hi all,\n&gt; =\r\n&gt; \n&gt; &gt; I know the topic of GPU use has come up before but there have been a=\r\n\n&gt; &gt; few recent developments in the GPU world so I thought it would be\n&gt; &gt; =\r\ninteresting to review the current situation.\n&gt; &gt; \n&gt; &gt; [CUDA]\n&gt; &gt; CUDA has b=\r\neen mentioned previously and I think Ken Lloyd did some work\n&gt; &gt; using CUDA=\r\n in NEAT, but AFAIK none of the NEAT implementations freely\n&gt; &gt; available a=\r\nre using GPUs at all (please correct me if I&#39;m wrong). CUDA\n&gt; &gt; was notable=\r\n as being the first platform to provide a general computing\n&gt; &gt; platform/la=\r\nyer over GPUs rather than being graphics acceleration\n&gt; &gt; specific. As such=\r\n it greatly lowered the difficulty of using GPUs for\n&gt; &gt; general computing.=\r\n A notable point is that CUDA is specific to NVIDIA\n&gt; &gt; GPUs.\n&gt; &gt; \n&gt; &gt; [Ope=\r\nnCL]\n&gt; &gt; OpenCL is a more recent development that aims to provide an openly=\r\n\n&gt; &gt; defined GPGPU style platform. OpenCL then is a layer of abstraction\n&gt; =\r\n&gt; from the hardware that allows GPGPU style code to be written\n&gt; &gt; independ=\r\nently of any specific h/w and to be executed on any h/w\n&gt; &gt; suporting OpenC=\r\nL. At this time there is already a lot of support, e.g.\n&gt; &gt; there is suppor=\r\nt for NVIDIA and ATI/AMD GPUs, IBM&#39;s Cell processor\n&gt; &gt; based accelerator &#39;=\r\nblades&#39;, and you can also run OpenCL code on an\n&gt; &gt; &#39;normal&#39; Intel multicor=\r\ne CPU (which may be more useful for\n&gt; &gt; testing/development than accelerati=\r\non?).\n&gt; &gt; \n&gt; &gt; The main issue with OpenCL is that the it is an abstraction =\r\nover\n&gt; &gt; diverse hardware, thus although a program may run it may not run v=\r\nery\n&gt; &gt; fast without specific knowledge of the underlying h/w and what it&#39;s=\r\n\n&gt; &gt; strengths and weaknesses are.  E.g. if code accesses more RAM that is\n=\r\n&gt; &gt; available to each processor then OpenCL will simply compile in\n&gt; &gt; inst=\r\nructions to copy data between local and main RAM thus eliminating\n&gt; &gt; the p=\r\nerf gain of using local RAM. OpenCL does provide for querying the\n&gt; &gt; under=\r\nlying h/w for some of these factors, so you could in principle\n&gt; &gt; perform =\r\na set of checks and report that the h/w isn&#39;t suitable for\n&gt; &gt; your program=\r\n, or maybe even dynamically adjust the program code based\n&gt; &gt; on reported p=\r\narameters.\n&gt; &gt; \n&gt; &gt; On the whole though I see OpenCL as a positive developm=\r\nent and\n&gt; &gt; something the NEAT community can potentially benefit from. It i=\r\ns still\n&gt; &gt; a relatively young platform and therefore may present some chal=\r\nlenges\n&gt; &gt; to code to as it develops, but I think it&#39;s mature and stable en=\r\nough\n&gt; &gt; to consider experimenting with now.\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; [Current GPU h/w=\r\n]\n&gt; &gt; As a ballpark estimate of the sort of performance gains a GPGPU can\n&gt;=\r\n &gt; give us, ATI/AMDs current flagship card (Radeon 7970) has a peak\n&gt; &gt; thr=\r\noughput of about 3.8 TFlops, compared to 100 GFlops for a 4th\n&gt; &gt; generatio=\r\nn quad core Intel i7. So on paper we&#39;re looking at a possible\n&gt; &gt; 38x speed=\r\nup compared to top flight CPUs. However, OpenCL does support\n&gt; &gt; utilising =\r\nmutliple GPUs, e.g. in the Bitcoin mining world it&#39;s typical\n&gt; &gt; to have 4 =\r\nand sometimes 5 GPUs in one system (using PCI &#39;riser&#39; cables\n&gt; &gt; to distanc=\r\ne the GPUs from the motherboard). So for a relatively modest\n&gt; &gt; investment=\r\n you could be looking at a possible 100x speedup compared to\n&gt; &gt; current be=\r\nst CPUs.\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; [NEAT and GPUs]\n&gt; &gt; My instinct here is to modify cu=\r\nrrent NEAT code to report stats on how\n&gt; &gt; much time proportionally is bein=\r\ng spent in each stage of the NEAT\n&gt; &gt; algorithm and to target the code that=\r\n takes up the most time, this\n&gt; &gt; will be different across problem domains =\r\nand also for NEAT versus\n&gt; &gt; HyperNEAT.\n&gt; &gt; \n&gt; &gt; Certainly if a problem dom=\r\nain is known to be CPU heavy (e.g. uses a\n&gt; &gt; physics simulation) then it&#39;s=\r\n probably a no-brainer to use OpenCL for\n&gt; &gt; that in isolation from the res=\r\nt of the NEAT algorithm. For NEAT itself\n&gt; &gt; I&#39;ve observed slowdown as ANNs=\r\n grow in size and this is presumably\n&gt; &gt; mostly due to time to decode and/o=\r\nr &#39;run&#39; the ANNs, and this is of\n&gt; &gt; course a greater problem in HyperNEAT =\r\nwhere the decode stage consists\n&gt; &gt; of a NEAT decode and ANN activation. So=\r\n there might be some scope for\n&gt; &gt; using OpenCL there. One can envisage mul=\r\ntiple GPUs where one may be\n&gt; &gt; dedicated to problem domain physics, one to=\r\n ANN activation and another\n&gt; &gt; to ANN genome decoding (say).\n&gt; &gt; \n&gt; &gt; \n&gt; &gt;=\r\n [Typical GPU Architecture]\n&gt; &gt; Finally I&#39;m going to briefly describe the a=\r\nrchitecture of the Radeon\n&gt; &gt; 7970 to give an idea of what it is capable of=\r\n.\n&gt; &gt; [Mainly taken from\n&gt; &gt; http://www.techradar.com/reviews/pc-mac/pc-com=\r\nponents/graphics-cards/amd-radeon-hd-7970-1049734/review/2]\n&gt; &gt; \n&gt; &gt; The 79=\r\n70 has:\n&gt; &gt; \n&gt; &gt; 32 x Compute Units (CUs). These are completely independent=\r\n of each\n&gt; &gt; other. If you have 2x GPUs then OpenCL will see (I think) a bl=\r\nock of\n&gt; &gt; 64 compute units, hence in some cases code can be accelerated ju=\r\nst by\n&gt; &gt; adding GPUs. Each CU has:\n&gt; &gt; \n&gt; &gt; 4 x Vector Units (VUs). And ea=\r\nch VU has:\n&gt; &gt; 16 x Unified shaders (unified here just means they are no lo=\r\nnger\n&gt; &gt; specific to a task, e.g. pixel or vector shader, they are general\n=\r\n&gt; &gt; purpose processors)\n&gt; &gt; \n&gt; &gt; So in total there are 32 x 4 x 16 =3D  204=\r\n8 unified shaders.\n&gt; &gt; \n&gt; &gt; Each CU has 64kB of local RAM that all of the V=\r\nUs can access\n&gt; &gt; (typically for reading shared state data I would guess). =\r\nIn addition\n&gt; &gt; each VU has it&#39;s own 64 kB of RAM  (note. you would typical=\r\nly control\n&gt; &gt; what&#39;s in these local memories in code, that is, it&#39;s not a =\r\npassive\n&gt; &gt; CPU cache). A vector unit is basically a SIMD processor, there =\r\nis one\n&gt; &gt; set of instructions that are executed against all 16 shaders (so=\r\n e.g.\n&gt; &gt; you can &#39;shade&#39; 16 pixels at a time). So each of the 128 vector u=\r\nnits\n&gt; &gt; can execute its own instructions, and in turn those instructions a=\r\nre\n&gt; &gt; operating on 16 shaders. A shader then consists of some minimal stat=\r\ne\n&gt; &gt; data specific to it and the data it is operating on, and also\n&gt; &gt; exe=\r\ncution units for performing arithmetic, etc.\n&gt; &gt; \n&gt; &gt; An interesting thing =\r\nabout vector units is that conditional branches\n&gt; &gt; are allowed in OpenCL, =\r\nthat is, you can have some shaders executing a\n&gt; &gt; different path despite t=\r\nhere being only one set of instructions and\n&gt; &gt; one instruction pointer. Ho=\r\nwever this is merely a trick, if VU code\n&gt; &gt; contains a branch then both br=\r\nanches are executed for all shaders and\n&gt; &gt; the shaders are assigned the co=\r\nrrect final result based on which\n&gt; &gt; branch they should have followed. Hen=\r\nce it&#39;s advisable to avoid\n&gt; &gt; branches, but it&#39;s a nice feature to have av=\r\nailable so long as you\n&gt; &gt; don&#39;t abuse it.\n&gt; &gt; \n&gt; &gt; For more info see:\n&gt; &gt; =\r\n   [From Shader Code to a Tera=EF=AC=82op: How Shader Cores Work, Kayvon\n&gt; =\r\n&gt; Fatahalian, Stanford University]\n&gt; &gt;    [http://s08.idav.ucdavis.edu/fata=\r\nhalian-gpu-architecture.pdf]\n&gt; &gt; \n&gt; &gt; There&#39;s obviously a heck of a lot mor=\r\ne to this subject than I&#39;ve\n&gt; &gt; described but I thought this might be a rea=\r\nsonably good intro to\n&gt; &gt; current possibilities around GPU use in NEAT.\n&gt; &gt;=\r\n \n&gt; &gt; Colin\n&gt; &gt;\n&gt;\n\n\n\n"}}