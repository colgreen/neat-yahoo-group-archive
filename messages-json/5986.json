{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":234577593,"authorName":"Oliver Coleman","from":"Oliver Coleman &lt;oliver.coleman@...&gt;","profile":"olivercoleman04","replyTo":"LIST","senderId":"BKue_uZlDAhaXY2-jrNysO-7RFvV1rCWyZWbS_XmeBvDQwxkWYg241-KGqxjFEMpY8Vp0luoBHx0bobWiAPQWpnoeQNABmxKqY2qxxJlW8k","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [neat] Re: New paper on why modules evolve, and how to evolve modular artificial neural networks","postDate":"1360538612","msgId":5986,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PENBK2R1aW1Na3ArYTc1cnI3ck1qaVUycDNqWUFrTmdBMG9ncmozSGthaFM0WXU0RGItQUBtYWlsLmdtYWlsLmNvbT4=","inReplyToHeader":"PGtmOTI3OSs4Y3IxQGVHcm91cHMuY29tPg==","referencesHeader":"PGtmOTI3OSs4Y3IxQGVHcm91cHMuY29tPg=="},"prevInTopic":5985,"nextInTopic":5987,"prevInTime":5985,"nextInTime":5987,"topicId":5976,"numMessagesInTopic":30,"msgSnippet":"First, congrats on the paper, Jeff et al, it was fascinating and exciting, I really enjoyed reading it. I just had some comments on Ken s comments about","rawEmail":"Return-Path: &lt;oliver.coleman@...&gt;\r\nX-Sender: oliver.coleman@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 68012 invoked from network); 10 Feb 2013 23:23:33 -0000\r\nX-Received: from unknown (10.193.84.163)\n  by m7.grp.bf1.yahoo.com with QMQP; 10 Feb 2013 23:23:33 -0000\r\nX-Received: from unknown (HELO mail-we0-f181.google.com) (74.125.82.181)\n  by mta3.grp.bf1.yahoo.com with SMTP; 10 Feb 2013 23:23:33 -0000\r\nX-Received: by mail-we0-f181.google.com with SMTP id t44so4391190wey.26\n        for &lt;neat@yahoogroups.com&gt;; Sun, 10 Feb 2013 15:23:32 -0800 (PST)\r\nMIME-Version: 1.0\r\nX-Received: by 10.180.73.80 with SMTP id j16mr12755839wiv.5.1360538612495;\n Sun, 10 Feb 2013 15:23:32 -0800 (PST)\r\nX-Received: by 10.194.243.193 with HTTP; Sun, 10 Feb 2013 15:23:32 -0800 (PST)\r\nX-Received: by 10.194.243.193 with HTTP; Sun, 10 Feb 2013 15:23:32 -0800 (PST)\r\nIn-Reply-To: &lt;kf9279+8cr1@...&gt;\r\nReferences: &lt;kf9279+8cr1@...&gt;\r\nDate: Mon, 11 Feb 2013 10:23:32 +1100\r\nMessage-ID: &lt;CA+duimMkp+a75rr7rMjiU2p3jYAkNgA0ogrj3HkahS4Yu4Db-A@...&gt;\r\nTo: neat@yahoogroups.com\r\nContent-Type: multipart/alternative; boundary=f46d043bdf38b374cd04d567144b\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Oliver Coleman &lt;oliver.coleman@...&gt;\r\nSubject: Re: [neat] Re: New paper on why modules evolve, and how to evolve\n modular artificial neural networks\r\nX-Yahoo-Group-Post: member; u=234577593; y=SmqC07d5QChfI_FuosgG58Ns7pJD3DkKZ0zUNatCb89yXFOZbyfjA9DtW7Sh-NpbRCNJ7OGeWA\r\nX-Yahoo-Profile: olivercoleman04\r\n\r\n\r\n--f46d043bdf38b374cd04d567144b\r\nContent-Type: text/plain; charset=windows-1252\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\nFirst, congrats on the paper, Jeff et al, it was fascinating and exciting,\n=\r\nI really enjoyed reading it.\n\nI just had some comments on Ken&#39;s comments ab=\r\nout biasing search via fitness\nversus encoding. I get the impression that i=\r\nn nature it might not be so\nclear cut, at least for the example given of co=\r\nnnection count and length.\nWhile there may be biases provided by encoding (=\r\nI think at least in part\nindirectly via neuronal spiking and synaptic plast=\r\nicity dynamics), the\nexample of not being able to fit many connections into=\r\n current head size\nand maintaining more connections in terms of energy cons=\r\numption seems more\nlike a fitness bias: individuals with bigger heads are m=\r\nore likely to\nresult in birth complications, lowering fitness, and more ene=\r\nrgy being sunk\ninto more connections, especially if those connections provi=\r\nde little\nbenefit, would also provide a fitness hit.\n\nUnlike nearly all EAs=\r\n, in nature the encoding has also been evolved.\nI wonder if these kinds of =\r\n(fitness) pressures resulted in the encoding of\nnatural networks being evol=\r\nved to produce low connection count networks\n(directly and/or via things li=\r\nke plasticity dynamics). Perhaps mitigating\nproblems like &quot;dead-weight&quot; sub=\r\n populations.\n\nCheers,\nOliver\n On Feb 11, 2013 8:08 AM, &quot;Ken&quot; &lt;kstanley@cs.=\r\nutexas.edu&gt; wrote:\n\n&gt; **\n&gt;\n&gt;\n&gt;\n&gt;\n&gt; I realized that my thoughts on biasing s=\r\nearch were a little misleading in\n&gt; a couple ways so I wanted to clarify a =\r\ncouple points. First, I didn&#39;t do a\n&gt; good job acknowledging that the multi=\r\nobjective framework does help to\n&gt; mitigate the danger of manipulating fitn=\r\ness. I shouldn&#39;t have used the term\n&gt; &quot;fitness penalty&quot; because that sounds=\r\n like it&#39;s not a separate objective.\n&gt;\n&gt; The other thing is that I should h=\r\nave elaborated when I said that using\n&gt; the encoding to bias the search is =\r\na good idea &quot;if you can figure out a way\n&gt; to do it.&quot; The problem is that o=\r\nften it will not be entirely unclear how\n&gt; the encoding can be manipulated =\r\nto encourage a certain type of behavior or\n&gt; property, so it&#39;s not always a=\r\nn option. Sometimes fitness may be the only\n&gt; option, so it&#39;s not something=\r\n that can simply be dismissed, and I think my\n&gt; prior post made it sound li=\r\nke I was being dismissive.\n&gt;\n&gt; But hopefully my general point is still usef=\r\nul, because at least when it\n&gt; comes to modularity, it is possible to encou=\r\nrage it through the encoding\n&gt; (and arguably that&#39;s how it happened in natu=\r\nre). And also, while\n&gt; multiobjective optimization does reduce deception to=\r\n some extent, the\n&gt; problem is still there. For example, if you have a perf=\r\normance objective in\n&gt; a task and a low connectivity objective, then if you=\r\n have someone with\n&gt; (high performance, low connectivity) and someone with =\r\n(equally high\n&gt; performance, higher connectivity), the one with higher conn=\r\nectivity will be\n&gt; dominated, even in the multiobjective case, which means =\r\nthat if it is a\n&gt; deceptive stepping stone, we will not be less likely to e=\r\nxplore it. You\n&gt; also still maintain this &quot;dead-weight&quot; subpopulation in th=\r\ne multiobjective\n&gt; case, because a subpopulation can survive by having extr=\r\nemely low\n&gt; connectivity (yet performing terribly) and thereby dominating o=\r\nn the low\n&gt; connectivity objective. That subpopulation will never go away.\n=\r\n&gt;\n&gt; So the issues are still there, but I didn&#39;t do the best job acknowledgi=\r\nng\n&gt; the mitigating factors. At the least, while you could reasonably argue=\r\n for\n&gt; using fitness to provide a bias in some cases, it&#39;s helpful to think=\r\n about\n&gt; whether encoding bias is a viable alternative in any particular ca=\r\nse.\n&gt;\n&gt; Best,\n&gt;\n&gt; ken\n&gt;\n&gt; --- In neat@yahoogroups.com, &quot;Ken&quot; wrote:\n&gt; &gt;\n&gt; &gt;=\r\n\n&gt; &gt;\n&gt; &gt; I wanted to follow up on Alex&#39;s second question, on whether any ap=\r\nproach\n&gt; biased to low connectivity could work. I think that&#39;s a really int=\r\neresting\n&gt; question with a lot of deep implications and deserves some discu=\r\nssion.\n&gt; &gt;\n&gt; &gt; First let me echo others&#39; congratulations to Jeff, Jean-Bapt=\r\niste, and\n&gt; Hod for this important publication, which dispels a lot of myth=\r\ns about the\n&gt; origins of modularity. Jeff has heard some of what I&#39;m postin=\r\ng here before\n&gt; in our own private discussions, but it&#39;s something I think =\r\nworth sharing on\n&gt; this group as well given the importance of the issue.\n&gt; =\r\n&gt;\n&gt; &gt; In particular, the question of how to bias a search is significant no=\r\nt\n&gt; only for biasing towards low connectivity (for the purpose of producing=\r\n\n&gt; modularity), but potentially for all kinds of different properties we mi=\r\nght\n&gt; realize are important in the future. And I believe there are two prim=\r\nary\n&gt; options: (1) Use the fitness function to bias the search, as Jeff et =\r\nal.\n&gt; do; or (2) use the encoding to bias the search. That is, in the secon=\r\nd\n&gt; option, you would not manipulate the fitness function in any way, but\n&gt;=\r\n instead do something to the genetic encoding to make it more likely to\n&gt; p=\r\nroduce the phenotypic properties you want to see. In the case of low\n&gt; conn=\r\nectivity, one example of such a genetic bias is through an additional\n&gt; lin=\r\nk expression output (LEO) on CPPNs in HyperNEAT, which was introduced by\n&gt; =\r\nPhilip Verbancsics and myself:\n&gt; &gt;\n&gt; &gt; http://eplex.cs.ucf.edu/publications=\r\n/2011/verbancsics-gecco11\n&gt; &gt;\n&gt; &gt; To bias towards low connectivity with the=\r\n LEO, you start with a CPPN\n&gt; that tends to express short connections (inst=\r\nead of long ones), which keeps\n&gt; connectivity down overall, and also thereb=\r\ny seemed to help lead to modular\n&gt; structures.\n&gt; &gt;\n&gt; &gt; But like I said ther=\r\ne is a larger issue here than just modularity or low\n&gt; connectivity. The is=\r\nsue is how you should implement a search bias of any\n&gt; type. And I believe =\r\nthat doing it by manipulating the fitness function is\n&gt; generally dangerous=\r\n in the long run. Like with many ideas in EC, it is\n&gt; likely to work well f=\r\nor relatively simple problems, but the more we aim for\n&gt; high complexity, t=\r\nhe more I think it will burden the search with unintended\n&gt; consequences th=\r\nat increasingly warp the search off the best path.\n&gt; &gt;\n&gt; &gt; As a simple exam=\r\nple, if you directly manipulate fitness to be lower when\n&gt; connectivity is =\r\nhigh, then you create a &quot;niche&quot; for networks of low\n&gt; connectivity that oth=\r\nerwise do nothing of any substance because they will\n&gt; be able to survive b=\r\ny having a higher fitness than networks of high\n&gt; connectivity that also do=\r\n nothing of any substance. This niche of\n&gt; non-functional low-connectivity =\r\nnetworks is essentially a permanent\n&gt; dead-weight in the population that wi=\r\nll last forever. Maybe it&#39;s not enough\n&gt; to kill you in simple problems, bu=\r\nt in complex problems it&#39;s something you\n&gt; probably can&#39;t afford.\n&gt; &gt;\n&gt; &gt; M=\r\nore generally the issue is the usual problem of deception, which is\n&gt; compo=\r\nunded by anything you do with fitness. For example, in a complex\n&gt; search s=\r\npace, there is a reasonable chance that the stepping stone to a\n&gt; good low-=\r\nconnectivity solution is something with higher connectivity. By\n&gt; manipulat=\r\ning fitness, you are cutting out all chances of encountering such\n&gt; a decep=\r\ntive stepping stone. But even if you don&#39;t believe that could be\n&gt; true, th=\r\ne single-mindedness of always favoring low-connectivity could\n&gt; deceive you=\r\n from many parts of the search space that might be stepping\n&gt; stones to som=\r\nething worthwhile, relating to connection density or not.\n&gt; &gt;\n&gt; &gt; On the ot=\r\nher hand, manipulating the encoding is different because in\n&gt; effect it act=\r\nually reorganizes the structure of the search space itself,\n&gt; which seems t=\r\no me a more principled thing to do (if you can figure out a\n&gt; way to do it)=\r\n. Because the thing is, in that case, you do not need to worry\n&gt; about a pe=\r\nrmanent dead weight taking up some proportion of your population\n&gt; forever.=\r\n Instead, while the encoding may *tend* to produce e.g.\n&gt; low-connectivity =\r\nsolutions, it can still escape that tendency without any\n&gt; penalty to fitne=\r\nss. Furthermore, in reality the best situation regarding\n&gt; modularity and c=\r\nonnectivity is probably rather subtle, with most of the\n&gt; brain respecting =\r\nthe principle of low connectivity, but with a number of\n&gt; critical exceptio=\r\nns in key areas, such as major inter-module hubs. A\n&gt; sophisticated encodin=\r\ng can allow its bias to bend to make such nuanced\n&gt; exceptions (e.g. based =\r\non locations within a geometry), whereas a fitness\n&gt; penalty is a heavy han=\r\nd and blunt instrument that cannot but help always to\n&gt; demand global and h=\r\nolistic subservience to dogmatic universals (unless you\n&gt; are willing to ta=\r\nke a hit in fitness).\n&gt; &gt;\n&gt; &gt; An interesting question in nature (where our =\r\nbrains evolved modular\n&gt; structure) is whether its tendency towards low con=\r\nnectivity is a result of\n&gt; an aspect of fitness in the wild, or an aspect o=\r\nf encoding bias. I think\n&gt; there is a lot of room in this question for argu=\r\ning either way, but my\n&gt; hunch is that the bias is mostly in the encoding. =\r\nMy logic is that I think\n&gt; the reason that the connectivity of the brain is=\r\n so much lower than what it\n&gt; could be (e.g. it is a tiny fraction of every=\r\nthing-to-everything\n&gt; connectivity) is an artifact of physics rather than a=\r\nn artifact of fitness.\n&gt; It is simply physically impossible for a giant 100=\r\n-billion-to-100-billion\n&gt; connectivity to fit in a head anything close to o=\r\nur size. And physical\n&gt; impossibility is in some sense a property of encodi=\r\nng. That is, mutations\n&gt; that could step from a low-connectivity brain to a=\r\n high one are few and far\n&gt; between simply because of physical constraint. =\r\nSo high-connectivity\n&gt; structures are simply a very small part of the searc=\r\nh space of brains in\n&gt; the physical universe. However, at the same time, yo=\r\nu can still get\n&gt; long-range connections from time to time because there is=\r\n no universal\n&gt; penalty for doing so, just a lower a priori probability of =\r\nsuch mutations\n&gt; occurring.\n&gt; &gt;\n&gt; &gt; In summary, the key difference between =\r\nthe alternatives is that with\n&gt; fitness you are saying &quot;stay out of this pa=\r\nrt of the search space&quot; whereas\n&gt; with encoding you are saying &quot;this part o=\r\nf the search space is much smaller\n&gt; and hence less likely to encounter.&quot;\n&gt;=\r\n &gt;\n&gt; &gt; So, my speculation is that if you want to bias the search in highly\n=\r\n&gt; complex domains, the best way is through the encoding. Fitness is a nasty=\r\n\n&gt; quagmire that is deceptively tempting to manipulate, but never plays by =\r\nthe\n&gt; rules you wish it would. Of course, these are merely my own unproven\n=\r\n&gt; intuitions and their veracity remains to be demonstrated. But at least it=\r\n&#39;s\n&gt; something to think about.\n&gt; &gt;\n&gt; &gt; Best,\n&gt; &gt;\n&gt; &gt; ken\n&gt; &gt;\n&gt; &gt; --- In nea=\r\nt@yahoogroups.com, Alexandre Devert wrote:\n&gt; &gt; &gt;\n&gt; &gt; &gt; Hi,\n&gt; &gt; &gt;\n&gt; &gt; &gt; =C2 =\r\n Simple, clean experiment, with sharp results, congrats on that,\n&gt; definite=\r\nly\n&gt; &gt; &gt; a step forward ! Of course, it begs for more questions. I would lo=\r\nve\n&gt; to hear\n&gt; &gt; &gt; you on such (fairly open) questions\n&gt; &gt; &gt;\n&gt; &gt; &gt; =C2  =C2=\r\n 1) Do you think that selection pressure for low connectivity is\n&gt; sufficie=\r\nnt in\n&gt; &gt; &gt; itself to evolve large coherent networks, or is it just a piece=\r\n of the\n&gt; puzzle ?\n&gt; &gt; &gt; =C2  =C2 2) Do you see your work as an indication =\r\nthat any approach biased\n&gt; to low\n&gt; &gt; &gt; connectivity would reproduce the re=\r\nsult ? Or does the way you guys\n&gt; enforced\n&gt; &gt; &gt; this bias matters ?\n&gt; &gt; &gt;\n=\r\n&gt; &gt; &gt; To me=C2\n&gt; &gt; &gt; 1) =3D&gt; Part of the puzzle. Should see how well it sca=\r\nles for\n&gt; increasingly\n&gt; &gt; &gt; complex task, when the connection graph gets b=\r\nigger. A randomized=C2\n&gt; &gt; &gt; search process=C2 on large graph sounds not so=\r\n efficient, need something\n&gt; to guide it.\n&gt; &gt; &gt; I advocate construction pro=\r\ncess that have a feedback from what the\n&gt; neuron=C2\n&gt; &gt; &gt; network is comput=\r\ning. Don&#39;t know how to do it without creepling\n&gt; computational\n&gt; &gt; &gt; cost t=\r\nho...\n&gt; &gt; &gt; 2) =3D&gt; I guess that the bias alone is enough, the way to intro=\r\nduce it\n&gt; might\n&gt; &gt; &gt; not be such a big deal.=C2\n&gt; &gt; &gt;\n&gt; &gt; &gt; Again, great w=\r\nork, very helpful contribution :)\n&gt; &gt; &gt;\n&gt; &gt; &gt; Alex\n&gt; &gt; &gt; =C2\n&gt; &gt; &gt; Dr. Deve=\r\nrt Alexandre\n&gt; &gt; &gt; Researcher at the Nature Inspired Computation and Applic=\r\nations\n&gt; Laboratory (NICAL)\n&gt; &gt; &gt; Lecturer at School Of Software Engineerin=\r\ng of USTC\n&gt; &gt; &gt; ----------------------------------------------------\n&gt; &gt; &gt; =\r\nHomepage :=C2 http://www.marmakoide.org\n&gt; &gt; &gt; -----------------------------=\r\n-----------------------\n&gt; &gt; &gt; 166 Renai Road, Dushu Lake Higher Education T=\r\nown\n&gt; &gt; &gt; Suzhou Industrial Park,\n&gt; &gt; &gt; Suzhou, Jiangsu, People&#39;s Republic =\r\nof China\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt; ________________________________\n&gt; &gt; &gt; From: Jef=\r\nf Clune\n&gt; &gt; &gt; To: neat users group group\n&gt; &gt; &gt; Cc: Jean-Baptiste Mouret ; H=\r\nod Lipson\n&gt; &gt; &gt; Sent: Thursday, February 7, 2013 1:57 AM\n&gt; &gt; &gt; Subject: [ne=\r\nat] New paper on why modules evolve, and how to evolve\n&gt; modular artificial=\r\n neural networks\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt; =C2\n&gt; &gt; &gt; Hello all,\n&gt; &gt; &gt;\n&gt; &gt; &gt; I&#39;m ext=\r\nremely pleased to announce a new paper on a subject that\n&gt; many--including =\r\nmyself--think is critical to making significant progress in\n&gt; our field: th=\r\ne evolution of modularity.=C2\n&gt; &gt; &gt;\n&gt; &gt; &gt; Jean-Baptiste Mouret, Hod Lipson =\r\nand I have a new paper that=C2\n&gt; &gt; &gt;\n&gt; &gt; &gt; 1) sheds light on why modularity=\r\n may evolve in biological networks\n&gt; (e.g. neural, genetic, metabolic, prot=\r\nein-protein, etc.)\n&gt; &gt; &gt;\n&gt; &gt; &gt; 2) provides a simple technique for evolving =\r\nneural networks that are\n&gt; modular and have increased evolvability, in that=\r\n they adapt faster to new\n&gt; environments. The modules that formed solved su=\r\nbproblems in the domain.=C2\n&gt; &gt; &gt; Cite:=C2 Clune J, Mouret J-B, Lipson H (2=\r\n013) The evolutionary origins\n&gt; of modularity. Proceedings of the Royal Soc=\r\niety B. 280: 20122863.=C2\n&gt; http://dx.doi.org/10.1098/rspb.2012.2863=C2 (pd=\r\nf)\n&gt; &gt; &gt;\n&gt; &gt; &gt; Abstract: A central biological question is how natural organ=\r\nisms are\n&gt; so evolvable (capable of quickly adapting to new environments). =\r\nA key\n&gt; driver of evolvability is the widespread modularity of biological\n&gt;=\r\n networks=E2=80&quot;their organization as functional, sparsely connected\n&gt; subu=\r\nnits=E2=80&quot;but there is no consensus regarding why modularity itself\n&gt; evol=\r\nved. Although most hypotheses assume indirect selection for\n&gt; evolvability,=\r\n here we demonstrate that the ubiquitous, direct selection\n&gt; pressure to re=\r\nduce the cost of connections between network nodes causes the\n&gt; emergence o=\r\nf modular networks. Computational evolution experiments with\n&gt; selection pr=\r\nessures to maximize network performance and minimize connection\n&gt; costs yie=\r\nld networks that are significantly more modular and more evolvable\n&gt; than c=\r\nontrol experiments that only select for performance. These results\n&gt; will c=\r\natalyse research in numerous disciplines, such as neuroscience and\n&gt; geneti=\r\ncs, and enhance our ability to harness\n&gt; &gt; &gt; evolution for engineering purp=\r\noses.\n&gt; &gt; &gt;\n&gt; &gt; &gt; Video:=C2\n&gt; http://www.youtube.com/watch?feature=3Dplayer=\r\n_embedded&v=3DSG4_aW8LMng\n&gt; &gt; &gt;\n&gt; &gt; &gt; There has been some nice coverage of =\r\nthis work in the popular press,\n&gt; in case you are interested:\n&gt; &gt; &gt; Nationa=\r\nl Geographic:=C2\n&gt; http://phenomena.nationalgeographic.com/2013/01/30/the-p=\r\narts-of-life/MIT&#39;s\n&gt; Technology Review:=C2\n&gt; http://www.technologyreview.co=\r\nm/view/428504/computer-scientists-reproduce-the-evolution-of-evolvability/=\r\n=C2 Fast\n&gt; Company:=C2\n&gt; http://www.fastcompany.com/3005313/evolved-brains-=\r\nrobots-creep-closer-animal-learningCornellChronicle:=C2\n&gt; http://www.news.c=\r\nornell.edu/stories/Jan13/modNetwork.htmlScienceDaily:=C2\n&gt; http://www.scien=\r\ncedaily.com/releases/2013/01/130130082300.htm\n&gt; &gt; &gt;\n&gt; &gt; &gt; Please let me kno=\r\nw what you think and if you have any questions. I\n&gt; hope this work will hel=\r\np our field move forward!\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt; Best regards,\n&gt; &gt; &gt;=\r\n Jeff Clune\n&gt; &gt; &gt;\n&gt; &gt; &gt; Assistant Professor\n&gt; &gt; &gt; Computer Science\n&gt; &gt; &gt; Un=\r\niversity of Wyoming\n&gt; &gt; &gt; jclune@\n&gt; &gt; &gt; jeffclune.com\n&gt; &gt; &gt;\n&gt; &gt;\n&gt;\n&gt;  \n&gt;\n\r\n--f46d043bdf38b374cd04d567144b\r\nContent-Type: text/html; charset=windows-1252\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\n&lt;p&gt;First, congrats on the paper, Jeff et al, it was fascinating and excitin=\r\ng, I really enjoyed reading it.&lt;/p&gt;\n&lt;p&gt;I just had some comments on Ken&#39;=\r\ns comments about biasing search via fitness versus encoding. I get the impr=\r\nession that in nature it might not be so clear cut, at least for the exampl=\r\ne given of connection count and length. While there may be biases provided =\r\nby encoding (I think at least in part indirectly via neuronal spiking and s=\r\nynaptic plasticity dynamics), the example of not being able to fit many con=\r\nnections into current head size and maintaining more connections in terms o=\r\nf energy consumption seems more like a fitness bias: individuals with bigge=\r\nr heads are more likely to result in birth complications, lowering fitness,=\r\n and more energy being sunk into more connections, especially if those conn=\r\nections provide little benefit, would also provide a fitness hit.&lt;/p&gt;\n\n&lt;p&gt;U=\r\nnlike nearly all EAs, in nature the encoding has also been evolved. &lt;br&gt;\nI =\r\nwonder if these kinds of (fitness) pressures resulted in the encoding of na=\r\ntural networks being evolved to produce low connection count networks (dire=\r\nctly and/or via things like plasticity dynamics). Perhaps mitigating proble=\r\nms like &quot;dead-weight&quot; sub populations. &lt;/p&gt;\n\n&lt;p&gt;Cheers,&lt;br&gt;\nOlive=\r\nr&lt;br&gt;\n&lt;/p&gt;\n&lt;div class=3D&quot;gmail_quote&quot;&gt;On Feb 11, 2013 8:08 AM, &quot;Ken&qu=\r\not; &lt;&lt;a href=3D&quot;mailto:kstanley@...&quot;&gt;kstanley@...&lt;/a=\r\n&gt;&gt; wrote:&lt;br type=3D&quot;attribution&quot;&gt;&lt;blockquote class=3D&quot;gmail_quote&quot; styl=\r\ne=3D&quot;margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex&quot;&gt;\n\n\n\n&lt;u&gt;=\r\n&lt;/u&gt;\n\n\n\n\n\n\n\n\n\n\n&lt;div style&gt;\n&lt;span&gt;=A0&lt;/span&gt;\n\n\n&lt;div&gt;\n  &lt;div&gt;\n\n\n    &lt;div&gt;\n   =\r\n   \n      \n      &lt;p&gt;&lt;br&gt;\n&lt;br&gt;\nI realized that my thoughts on biasing search=\r\n were a little misleading in a couple ways so I wanted to clarify a couple =\r\npoints.  First, I didn&#39;t do a good job acknowledging that the multiobje=\r\nctive framework does help to mitigate the danger of manipulating fitness.  =\r\nI shouldn&#39;t have used the term &quot;fitness penalty&quot; because that=\r\n sounds like it&#39;s not a separate objective.  &lt;br&gt;\n\n&lt;br&gt;\nThe other thing=\r\n is that I should have elaborated when I said that using the encoding to bi=\r\nas the search is a good idea &quot;if you can figure out a way to do it.&qu=\r\not;  The problem is that often it will not be entirely unclear how the enco=\r\nding can be manipulated to encourage a certain type of behavior or property=\r\n, so it&#39;s not always an option.  Sometimes fitness may be the only opti=\r\non, so it&#39;s not something that can simply be dismissed, and I think my =\r\nprior post made it sound like I was being dismissive.&lt;br&gt;\n\n&lt;br&gt;\nBut hopeful=\r\nly my general point is still useful, because at least when it comes to modu=\r\nlarity, it is possible to encourage it through the encoding (and arguably t=\r\nhat&#39;s how it happened in nature).  And also, while multiobjective optim=\r\nization does reduce deception to some extent, the problem is still there.  =\r\nFor example, if you have a performance objective in a task and a low connec=\r\ntivity objective, then if you have someone with (high performance, low conn=\r\nectivity) and someone with (equally high performance, higher connectivity),=\r\n the one with higher connectivity will be dominated, even in the multiobjec=\r\ntive case, which means that if it is a deceptive stepping stone, we will no=\r\nt be less likely to explore it.  You also still maintain this &quot;dead-we=\r\night&quot; subpopulation in the multiobjective case, because a subpopulatio=\r\nn can survive by having extremely low connectivity (yet performing terribly=\r\n) and thereby dominating on the low connectivity objective.  That subpopula=\r\ntion will never go away.  &lt;br&gt;\n\n&lt;br&gt;\nSo the issues are still there, but I d=\r\nidn&#39;t do the best job acknowledging the mitigating factors.  At the lea=\r\nst, while you could reasonably argue for using fitness to provide a bias in=\r\n some cases, it&#39;s helpful to think about whether encoding bias is a via=\r\nble alternative in any particular case.&lt;br&gt;\n\n&lt;br&gt;\nBest,&lt;br&gt;\n&lt;br&gt;\nken&lt;br&gt;\n&lt;b=\r\nr&gt;\n--- In &lt;a href=3D&quot;mailto:neat%40yahoogroups.com&quot; target=3D&quot;_blank&quot;&gt;neat@=\r\nyahoogroups.com&lt;/a&gt;, &quot;Ken&quot;  wrote:&lt;br&gt;\n&gt;&lt;br&gt;\n&gt; &lt;br&gt;\n&gt; &lt;b=\r\nr&gt;\n&gt; I wanted to follow up on Alex&#39;s second question, on whether any=\r\n approach biased to low connectivity could work. I think that&#39;s a reall=\r\ny interesting question with a lot of deep implications and deserves some di=\r\nscussion.&lt;br&gt;\n\n&gt; &lt;br&gt;\n&gt; First let me echo others&#39; congratulations=\r\n to Jeff, Jean-Baptiste, and Hod for this important publication, which disp=\r\nels a lot of myths about the origins of modularity.  Jeff has heard some of=\r\n what I&#39;m posting here before in our own private discussions, but it&#3=\r\n9;s something I think worth sharing on this group as well given the importa=\r\nnce of the issue.&lt;br&gt;\n\n&gt; &lt;br&gt;\n&gt; In particular, the question of how to=\r\n bias a search is significant not only for biasing towards low connectivity=\r\n (for the purpose of producing modularity), but potentially for all kinds o=\r\nf different properties we might realize are important in the future.  And I=\r\n believe there are two primary options:  (1) Use the fitness function to bi=\r\nas the search, as Jeff et al. do; or (2) use the encoding to bias the searc=\r\nh.  That is, in the second option, you would not manipulate the fitness fun=\r\nction in any way, but instead do something to the genetic encoding to make =\r\nit more likely to produce the phenotypic properties you want to see.  In th=\r\ne case of low connectivity, one example of such a genetic bias is through a=\r\nn additional link expression output (LEO) on CPPNs in HyperNEAT, which was =\r\nintroduced by Philip Verbancsics and myself:&lt;br&gt;\n\n&gt; &lt;br&gt;\n&gt; &lt;a href=3D=\r\n&quot;http://eplex.cs.ucf.edu/publications/2011/verbancsics-gecco11&quot; target=3D&quot;_=\r\nblank&quot;&gt;http://eplex.cs.ucf.edu/publications/2011/verbancsics-gecco11&lt;/a&gt;&lt;br=\r\n&gt;\n&gt; &lt;br&gt;\n&gt; To bias towards low connectivity with the LEO, you start w=\r\nith a CPPN that tends to express short connections (instead of long ones), =\r\nwhich keeps connectivity down overall, and also thereby seemed to help lead=\r\n to modular structures.  &lt;br&gt;\n\n&gt; &lt;br&gt;\n&gt; But like I said there is a la=\r\nrger issue here than just modularity or low connectivity.  The issue is how=\r\n you should implement a search bias of any type.  And I believe that doing =\r\nit by manipulating the fitness function is generally dangerous in the long =\r\nrun.  Like with many ideas in EC, it is likely to work well for relatively =\r\nsimple problems, but the more we aim for high complexity, the more I think =\r\nit will burden the search with unintended consequences that increasingly wa=\r\nrp the search off the best path.  &lt;br&gt;\n\n&gt; &lt;br&gt;\n&gt; As a simple example,=\r\n if you directly manipulate fitness to be lower when connectivity is high, =\r\nthen you create a &quot;niche&quot; for networks of low connectivity that o=\r\ntherwise do nothing of any substance because they will be able to survive b=\r\ny having a higher fitness than networks of high connectivity that also do n=\r\nothing of any substance.  This niche of non-functional low-connectivity net=\r\nworks is essentially a permanent dead-weight in the population that will la=\r\nst forever.    Maybe it&#39;s not enough to kill you in simple problems, bu=\r\nt in complex problems it&#39;s something you probably can&#39;t afford.&lt;br&gt;=\r\n\n\n&gt; &lt;br&gt;\n&gt; More generally the issue is the usual problem of deception=\r\n, which is compounded by anything you do with fitness.  For example, in a c=\r\nomplex search space, there is a reasonable chance that the stepping stone t=\r\no a good low-connectivity solution is something with higher connectivity.  =\r\nBy manipulating fitness, you are cutting out all chances of encountering su=\r\nch a deceptive stepping stone.  But even if you don&#39;t believe that coul=\r\nd be true, the single-mindedness of always favoring low-connectivity could =\r\ndeceive you from many parts of the search space that might be stepping ston=\r\nes to something worthwhile, relating to connection density or not.&lt;br&gt;\n\n&gt=\r\n; &lt;br&gt;\n&gt; On the other hand, manipulating the encoding is different becau=\r\nse in effect it actually reorganizes the structure of the search space itse=\r\nlf, which seems to me a more principled thing to do (if you can figure out =\r\na way to do it).  Because the thing is, in that case, you do not need to wo=\r\nrry about a permanent dead weight taking up some proportion of your populat=\r\nion forever.  Instead, while the encoding may *tend* to produce e.g. low-co=\r\nnnectivity solutions, it can still escape that tendency without any penalty=\r\n to fitness.  Furthermore, in reality the best situation regarding modulari=\r\nty and connectivity is probably rather subtle, with most of the brain respe=\r\ncting the principle of low connectivity, but with a number of critical exce=\r\nptions in key areas, such as major inter-module hubs.  A sophisticated enco=\r\nding can allow its bias to bend to make such nuanced exceptions (e.g. based=\r\n on locations within a geometry), whereas a fitness penalty is a heavy hand=\r\n and blunt instrument that cannot but help always to demand global and holi=\r\nstic subservience to dogmatic universals (unless you are willing to take a =\r\nhit in fitness).&lt;br&gt;\n\n&gt; &lt;br&gt;\n&gt; An interesting question in nature (whe=\r\nre our brains evolved modular structure) is whether its tendency towards lo=\r\nw connectivity is a result of an aspect of fitness in the wild, or an aspec=\r\nt of encoding bias.  I think there is a lot of room in this question for ar=\r\nguing either way, but my hunch is that the bias is mostly in the encoding. =\r\n My logic is that I think the reason that the connectivity of the brain is =\r\nso much lower than what it could be (e.g. it is a tiny fraction of everythi=\r\nng-to-everything connectivity) is an artifact of physics rather than an art=\r\nifact of fitness.  It is simply physically impossible for a giant 100-billi=\r\non-to-100-billion connectivity to fit in a head anything close to our size.=\r\n  And physical impossibility is in some sense a property of encoding.  That=\r\n is, mutations that could step from a low-connectivity brain to a high one =\r\nare few and far between simply because of physical constraint.  So high-con=\r\nnectivity structures are simply a very small part of the search space of br=\r\nains in the physical universe.  However, at the same time, you can still ge=\r\nt long-range connections from time to time because there is no universal pe=\r\nnalty for doing so, just a lower a priori probability of such mutations occ=\r\nurring.&lt;br&gt;\n\n&gt; &lt;br&gt;\n&gt; In summary, the key difference between the alte=\r\nrnatives is that with fitness you are saying &quot;stay out of this part of=\r\n the search space&quot; whereas with encoding you are saying &quot;this par=\r\nt of the search space is much smaller and hence less likely to encounter.&q=\r\nuot;&lt;br&gt;\n\n&gt; &lt;br&gt;\n&gt; So, my speculation is that if you want to bias the=\r\n search in highly complex domains, the best way is through the encoding.  F=\r\nitness is a nasty quagmire that is deceptively tempting to manipulate, but =\r\nnever plays by the rules you wish it would.  Of course, these are merely my=\r\n own unproven intuitions and their veracity remains to be demonstrated.  Bu=\r\nt at least it&#39;s something to think about.&lt;br&gt;\n\n&gt; &lt;br&gt;\n&gt; Best,&lt;br&gt;=\r\n\n&gt; &lt;br&gt;\n&gt; ken&lt;br&gt;\n&gt; &lt;br&gt;\n&gt; --- In &lt;a href=3D&quot;mailto:neat%40yaho=\r\nogroups.com&quot; target=3D&quot;_blank&quot;&gt;neat@yahoogroups.com&lt;/a&gt;, Alexandre Devert  =\r\nwrote:&lt;br&gt;\n&gt; &gt;&lt;br&gt;\n&gt; &gt; Hi,&lt;br&gt;\n&gt; &gt; &lt;br&gt;\n&gt; &gt; =C2=A0 =\r\nSimple, clean experiment, with sharp results, congrats on that, definitely&lt;=\r\nbr&gt;\n&gt; &gt; a step forward ! Of course, it begs for more questions. I wou=\r\nld love to hear&lt;br&gt;\n&gt; &gt; you on such (fairly open) questions&lt;br&gt;\n&gt; =\r\n&gt; &lt;br&gt;\n&gt; &gt; =C2=A0 =C2=A01) Do you think that selection pressure fo=\r\nr low connectivity is sufficient in&lt;br&gt;\n&gt; &gt; itself to evolve large co=\r\nherent networks, or is it just a piece of the puzzle ?&lt;br&gt;\n&gt; &gt; =C2=A0=\r\n =C2=A02) Do you see your work as an indication that any approach biased to=\r\n low&lt;br&gt;\n&gt; &gt; connectivity would reproduce the result ? Or does the wa=\r\ny you guys enforced&lt;br&gt;\n&gt; &gt; this bias matters ?&lt;br&gt;\n&gt; &gt; &lt;br&gt;\n&g=\r\nt; &gt; To me=C2=A0&lt;br&gt;\n&gt; &gt; 1) =3D&gt; Part of the puzzle. Should see=\r\n how well it scales for increasingly&lt;br&gt;\n&gt; &gt; complex task, when the c=\r\nonnection graph gets bigger. A randomized=C2=A0&lt;br&gt;\n&gt; &gt; search proces=\r\ns=C2=A0on large graph sounds not so efficient, need something to guide it.&lt;=\r\nbr&gt;\n&gt; &gt; I advocate construction process that have a feedback from wha=\r\nt the neuron=C2=A0&lt;br&gt;\n&gt; &gt; network is computing. Don&#39;t know how t=\r\no do it without creepling computational&lt;br&gt;\n&gt; &gt; cost tho...&lt;br&gt;\n&gt; =\r\n&gt; 2) =3D&gt; I guess that the bias alone is enough, the way to introduce=\r\n it might&lt;br&gt;\n&gt; &gt; not be such a big deal.=C2=A0&lt;br&gt;\n&gt; &gt; &lt;br&gt;\n&g=\r\nt; &gt; Again, great work, very helpful contribution :)&lt;br&gt;\n&gt; &gt; &lt;br&gt;\n=\r\n&gt; &gt; Alex&lt;br&gt;\n&gt; &gt; =C2=A0&lt;br&gt;\n&gt; &gt; Dr. Devert Alexandre&lt;br&gt;\n=\r\n&gt; &gt; Researcher at the Nature Inspired Computation and Applications La=\r\nboratory (NICAL)&lt;br&gt;\n&gt; &gt; Lecturer at School Of Software Engineering o=\r\nf USTC&lt;br&gt;\n&gt; &gt; ----------------------------------------------------&lt;b=\r\nr&gt;\n&gt; &gt; Homepage :=C2=A0&lt;a href=3D&quot;http://www.marmakoide.org&quot; target=\r\n=3D&quot;_blank&quot;&gt;http://www.marmakoide.org&lt;/a&gt;&lt;br&gt;\n&gt; &gt; -------------------=\r\n---------------------------------&lt;br&gt;\n&gt; &gt; 166 Renai Road, Dushu Lake =\r\nHigher Education Town&lt;br&gt;\n&gt; &gt; Suzhou Industrial Park,&lt;br&gt;\n&gt; &gt; S=\r\nuzhou, Jiangsu, People&#39;s Republic of China&lt;br&gt;\n&gt; &gt; &lt;br&gt;\n&gt; &gt;=\r\n &lt;br&gt;\n&gt; &gt; ________________________________&lt;br&gt;\n&gt; &gt;  From: Jeff =\r\nClune &lt;br&gt;\n&gt; &gt; To: neat users group group  &lt;br&gt;\n&gt; &gt; Cc: Jean-Ba=\r\nptiste Mouret ; Hod Lipson  &lt;br&gt;\n&gt; &gt; Sent: Thursday, February 7, 2013=\r\n 1:57 AM&lt;br&gt;\n&gt; &gt; Subject: [neat] New paper on why modules evolve, and=\r\n how to evolve modular artificial neural networks&lt;br&gt;\n&gt; &gt;  &lt;br&gt;\n&gt; =\r\n&gt; &lt;br&gt;\n&gt; &gt; =C2=A0 &lt;br&gt;\n&gt; &gt; Hello all,&lt;br&gt;\n&gt; &gt; &lt;br&gt;\n&gt=\r\n; &gt; I&#39;m extremely pleased to announce a new paper on a subject that =\r\nmany--including myself--think is critical to making significant progress in=\r\n our field: the evolution of modularity.=C2=A0&lt;br&gt;\n&gt; &gt; &lt;br&gt;\n&gt; &gt;=\r\n Jean-Baptiste Mouret, Hod Lipson and I have a new paper that=C2=A0&lt;br&gt;\n&gt=\r\n; &gt; &lt;br&gt;\n&gt; &gt; 1) sheds light on why modularity may evolve in biolog=\r\nical networks (e.g. neural, genetic, metabolic, protein-protein, etc.)&lt;br&gt;\n=\r\n&gt; &gt; &lt;br&gt;\n&gt; &gt; 2) provides a simple technique for evolving neural=\r\n networks that are modular and have increased evolvability, in that they ad=\r\napt faster to new environments. The modules that formed solved subproblems =\r\nin the domain.=C2=A0&lt;br&gt;\n\n&gt; &gt; Cite:=C2=A0Clune J, Mouret J-B, Lipson =\r\nH (2013) The evolutionary origins of modularity. Proceedings of the Royal S=\r\nociety B. 280: 20122863.=C2=A0&lt;a href=3D&quot;http://dx.doi.org/10.1098/rspb.201=\r\n2.2863&quot; target=3D&quot;_blank&quot;&gt;http://dx.doi.org/10.1098/rspb.2012.2863&lt;/a&gt;=C2=\r\n=A0(pdf)&lt;br&gt;\n\n&gt; &gt; &lt;br&gt;\n&gt; &gt; Abstract: A central biological quest=\r\nion is how natural organisms are so evolvable (capable of quickly adapting =\r\nto new environments). A key driver of evolvability is the widespread modula=\r\nrity of biological networks=E2=80&quot;their organization as functional, sp=\r\narsely connected subunits=E2=80&quot;but there is no consensus regarding wh=\r\ny modularity itself evolved. Although most hypotheses assume indirect selec=\r\ntion for evolvability, here we demonstrate that the ubiquitous, direct sele=\r\nction pressure to reduce the cost of connections between network nodes caus=\r\nes the emergence of modular networks. Computational evolution experiments w=\r\nith selection pressures to maximize network performance and minimize connec=\r\ntion costs yield networks that are significantly more modular and more evol=\r\nvable than control experiments that only select for performance. These resu=\r\nlts will catalyse research in numerous disciplines, such as neuroscience an=\r\nd genetics, and enhance our ability to harness&lt;br&gt;\n\n&gt; &gt;  evolution fo=\r\nr engineering purposes.&lt;br&gt;\n&gt; &gt; &lt;br&gt;\n&gt; &gt; Video:=C2=A0&lt;a href=3D=\r\n&quot;http://www.youtube.com/watch?feature=3Dplayer_embedded&amp;v=3DSG4_aW8LMng=\r\n&quot; target=3D&quot;_blank&quot;&gt;http://www.youtube.com/watch?feature=3Dplayer_embedded&=\r\namp;v=3DSG4_aW8LMng&lt;/a&gt;&lt;br&gt;\n&gt; &gt; &lt;br&gt;\n&gt; &gt; There has been some ni=\r\nce coverage of this work in the popular press, in case you are interested:&lt;=\r\nbr&gt;\n&gt; &gt; National Geographic:=C2=A0&lt;a href=3D&quot;http://phenomena.nationa=\r\nlgeographic.com/2013/01/30/the-parts-of-life/MIT&quot; target=3D&quot;_blank&quot;&gt;http://=\r\nphenomena.nationalgeographic.com/2013/01/30/the-parts-of-life/MIT&lt;/a&gt;&#39;s=\r\n Technology Review:=C2=A0&lt;a href=3D&quot;http://www.technologyreview.com/view/42=\r\n8504/computer-scientists-reproduce-the-evolution-of-evolvability/&quot; target=\r\n=3D&quot;_blank&quot;&gt;http://www.technologyreview.com/view/428504/computer-scientists=\r\n-reproduce-the-evolution-of-evolvability/&lt;/a&gt;=C2=A0Fast Company:=C2=A0&lt;a hr=\r\nef=3D&quot;http://www.fastcompany.com/3005313/evolved-brains-robots-creep-closer=\r\n-animal-learningCornell&quot; target=3D&quot;_blank&quot;&gt;http://www.fastcompany.com/30053=\r\n13/evolved-brains-robots-creep-closer-animal-learningCornell&lt;/a&gt; Chronicle:=\r\n=C2=A0&lt;a href=3D&quot;http://www.news.cornell.edu/stories/Jan13/modNetwork.htmlS=\r\ncienceDaily:&quot; target=3D&quot;_blank&quot;&gt;http://www.news.cornell.edu/stories/Jan13/m=\r\nodNetwork.htmlScienceDaily:&lt;/a&gt;=C2=A0&lt;a href=3D&quot;http://www.sciencedaily.com=\r\n/releases/2013/01/130130082300.htm&quot; target=3D&quot;_blank&quot;&gt;http://www.sciencedai=\r\nly.com/releases/2013/01/130130082300.htm&lt;/a&gt;&lt;br&gt;\n\n&gt; &gt; &lt;br&gt;\n&gt; &gt; =\r\nPlease let me know what you think and if you have any questions. I hope thi=\r\ns work will help our field move forward!&lt;br&gt;\n&gt; &gt; &lt;br&gt;\n&gt; &gt; &lt;br&gt;\n=\r\n&gt; &gt; &lt;br&gt;\n&gt; &gt; &lt;br&gt;\n&gt; &gt; Best regards,&lt;br&gt;\n&gt; &gt; Jeff Cl=\r\nune&lt;br&gt;\n&gt; &gt; &lt;br&gt;\n&gt; &gt; Assistant Professor&lt;br&gt;\n&gt; &gt; Computer=\r\n Science&lt;br&gt;\n&gt; &gt; University of Wyoming&lt;br&gt;\n&gt; &gt; jclune@&lt;br&gt;\n&gt;=\r\n &gt; &lt;a href=3D&quot;http://jeffclune.com&quot; target=3D&quot;_blank&quot;&gt;jeffclune.com&lt;/a&gt;&lt;=\r\nbr&gt;\n&gt; &gt;&lt;br&gt;\n&gt;&lt;br&gt;\n&lt;br&gt;\n&lt;/p&gt;\n\n    &lt;/div&gt;\n     \n\n    \n    &lt;div style=\r\n=3D&quot;color:#fff;min-height:0&quot;&gt;&lt;/div&gt;\n\n\n&lt;/div&gt;\n\n\n\n  \n\n\n\n\n\n\n&lt;/blockquote&gt;&lt;/div=\r\n&gt;\n\r\n--f46d043bdf38b374cd04d567144b--\r\n\n"}}