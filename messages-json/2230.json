{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":60940451,"authorName":"Jeff Haynes","from":"&quot;Jeff Haynes&quot; &lt;jeff@...&gt;","profile":"jefffhaynes","replyTo":"LIST","senderId":"DeQvM-Zq4ehiQqIQ1FmuLAUb9XXh3rYOpW-GEe4SfNDmU3416qpZPXOFsMmLdgGc5Xqgvr-oufNwVof2oR91wxri6aHwQhqTwg","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [neat] Re: Introduction---recurrency question","postDate":"1125607720","msgId":2230,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDIwMDUwOTAxMjA0ODI4Lk0yMzA2N0BkZWFyZG9yZmYuY29tPg==","inReplyToHeader":"PDYuMi4wLjE0LjAuMjAwNTA5MDExMjI0MjUuMDMyODQ5YjBAcG9wLm1haWwueWFob28uY28udWs+","referencesHeader":"PGRmMzh1MCtkNGhvQGVHcm91cHMuY29tPiA8ZGY1MGtxK2UxMXJAZUdyb3Vwcy5jb20+IDw1MTdmYTZmMTA1MDgzMTE3Mjc3NTllNWNkY0BtYWlsLmdtYWlsLmNvbT4gPDYuMi4wLjE0LjAuMjAwNTA5MDExMjI0MjUuMDMyODQ5YjBAcG9wLm1haWwueWFob28uY28udWs+"},"prevInTopic":2229,"nextInTopic":2231,"prevInTime":2229,"nextInTime":2231,"topicId":2209,"numMessagesInTopic":42,"msgSnippet":"This is all very interesting to me as I hadn t really thought about the single value approach.  I think my mind briefly forayed into the possibility of a ","rawEmail":"Return-Path: &lt;jeff@...&gt;\r\nX-Sender: jeff@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 75399 invoked from network); 1 Sep 2005 20:48:43 -0000\r\nReceived: from unknown (66.218.66.167)\n  by m8.grp.scd.yahoo.com with QMQP; 1 Sep 2005 20:48:43 -0000\r\nReceived: from unknown (HELO eagle.deardorff.com) (64.92.206.84)\n  by mta6.grp.scd.yahoo.com with SMTP; 1 Sep 2005 20:48:43 -0000\r\nReceived: from eagle.deardorff.com (jeffie@localhost [127.0.0.1])\n\tby eagle.deardorff.com (8.13.3/8.13.3) with ESMTP id j81Kmebg023371\n\tfor &lt;neat@yahoogroups.com&gt;; Thu, 1 Sep 2005 16:48:40 -0400\r\nTo: neat@yahoogroups.com\r\nDate: Thu, 1 Sep 2005 16:48:40 -0400\r\nMessage-Id: &lt;20050901204828.M23067@...&gt;\r\nIn-Reply-To: &lt;6.2.0.14.0.20050901122425.032849b0@...&gt;\r\nReferences: &lt;df38u0+d4ho@...&gt; &lt;df50kq+e11r@...&gt; &lt;517fa6f10508311727759e5cdc@...&gt; &lt;6.2.0.14.0.20050901122425.032849b0@...&gt;\r\nX-Mailer: Open WebMail 2.51 20050323\r\nX-OriginatingIP: 151.203.236.223 (jeffie)\r\nMIME-Version: 1.0\r\nContent-Type: text/plain;\n\tcharset=iso-8859-1\r\nX-Spam-Status: No, Not spam. Probably whitelisted.\r\nX-Scanned-By: MIMEDefang 2.51 on 64.92.206.84\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: &quot;Jeff Haynes&quot; &lt;jeff@...&gt;\r\nSubject: Re: [neat] Re: Introduction---recurrency question\r\nX-Yahoo-Group-Post: member; u=60940451; y=SP71byLvlChWwtxzN1sYjhVYmpZymn1AWqRdaB-H1h52-TlSrCs\r\nX-Yahoo-Profile: jefffhaynes\r\n\r\nThis is all very interesting to me as I hadn&#39;t really thought about the single\nvalue approach.  I think my mind briefly forayed into the possibility of a\nmassively parallel system with threads representing signals, got very scared,\nand retreated.  Does anyone have any experience comparing the two?  Is one\n&quot;more correct&quot;?  It seems to me that the sink/source model is &quot;correct&quot; in\nthat it &quot;fairly&quot; propogates all signals and that the single source model\n*could* be equally &quot;correct&quot;.  However, the single source model also seems\nmuch more susceptible to evolution-induced sequencing.  I think I would need\nto be assured that the single value model was in practice equivelent to the\ns/s model although I&#39;m not sure why...I suppose as long as the single value\nmodel converges to a solution, I don&#39;t really care.  Perhaps I feel that any\ndeviation from the s/s behavior would lead to unstable and nonrobust networks\nthat would be overly sensitive to change.\n\n- jeff \n\nOn Thu, 01 Sep 2005 17:12:25 +0100, Ian Badcoe wrote\n&gt; Hi,\n&gt;          I&#39;m with John 100% on this, but I just thought I&#39;d try to \n&gt; give a some more perspective on the two types of update.  Call them \n&gt; single value and sink/source systems.\n&gt; \n&gt; Sink-source simulates the process of _one_ neurone, taking care to \n&gt; insulate it from any questions of whether its neighbours have been \n&gt; updated yet or not.\n&gt; \n&gt; For a non-recurrent network, there is no difference, provided you do \n&gt; enough updates to propagate the inputs all the way to the output (by \n&gt; the _slowest_ route).  Once this has happened, further updates will \n&gt; not change the output until the inputs are changed.  For a single-\n&gt; value system, if the nodes are evaluated in the right order, the \n&gt; result can get to the output in a single pass.  A sink/source system \n&gt; could also do that, if you copy the sink straight to the source \n&gt; after updating one node, but then it isn&#39;t really be acting as \n&gt; sink/source.  There is no need for sink/source in a non-recurrent \n&gt; (NR) network.  A single-value system evaluated in a different order \n&gt; will take more updates but eventually get the same result to the output.\n&gt; \n&gt; NR-networks are state-functions of their inputs, e.g. (once the \n&gt; signal has reached the output)\n&gt; \n&gt; output = F(inputs)\n&gt; \n&gt; And nothing will change the value of the output except new inputs. \n&gt;  If you want to regard the inputs as varying over time, then we have:\n&gt; \n&gt; output[t] = F(inputs[t], inputs[t-1], ..., inputs[t - n)\n&gt; \n&gt; e.g. the output responds to a range of time-slots in the inputs \n&gt; formed by a window stretching back from the present.  This is \n&gt; because the network contains no &quot;memory&quot; but it can contain time \n&gt; delays.  The length of the time-window (n) depends on the longest \n&gt; delay present in the network but is less according to how completely \n&gt; you update the network in each time step.  If you update completely \n&gt; then n will be zero and the output will depend only on the \n&gt; inputs[t].  OTOH, if it takes two passes to update your network but \n&gt; you only run one in each time step, then you will have contributions \n&gt; from inputs[t] and inputs[t-1] in the output.\n&gt; \n&gt; --\n&gt; \n&gt; For a recurrent network, the problem is not that update is harder, \n&gt; it isn&#39;t, you can do it the same way as for non-recurrent if you \n&gt; like, the problem is that the nature of update has changed.  The \n&gt; network no longer need have a final update value to converge on. \n&gt;  e.g. it is perfectly possible to design an R-network that is an \n&gt; oscillator, even with no input.  Thus you no longer have the option \n&gt; of running it to &quot;completely updated&quot; because there is no such thing \n&gt; -- it might keep changing forever.\n&gt; \n&gt; The choices you now have (after deciding on an update methodology) are:\n&gt; \n&gt; (i) how fast to run the network compared to the world.  e.g. N \n&gt; network updates per input time step.  It doesn&#39;t make sense to run \n&gt; the network slower than the world, or rather it does but what you \n&gt; are actually doing is discarding some of the input data.\n&gt; (ii) whether to reinitialise the network before each timestep.  This \n&gt; is basically saying &quot;and now erase any memories of previous steps&quot;...\n&gt; \n&gt; Ian\n&gt; \n&gt; At 01:27 01/09/2005, you wrote:\n&gt; &gt;Kevin,\n&gt; &gt;\n&gt; &gt;What you&#39;ve described here is far too complicated.  You&#39;re doing too\n&gt; &gt;much work.  Try this:\n&gt; &gt;\n&gt; &gt;For a given network topology, you only need to determine the firing\n&gt; &gt;order ONCE.  Save that list somewhere.  It will be consulted every\n&gt; &gt;time the network is activated, which may be thousands of times for a\n&gt; &gt;single fitness evaluation.  And of course a single network may be\n&gt; &gt;fitness evaluated multiple times with different scenarios presented to\n&gt; &gt;it during the course of a generation.  And if that topology survives\n&gt; &gt;into the next generation, you might as well take that list with it...\n&gt; &gt;\n&gt; &gt;When you are testing your network, you are testing it in a\n&gt; &gt;&#39;simulation&#39; where it checks the state of the world.  That determines\n&gt; &gt;the values of the &#39;input&#39; nodes.\n&gt; &gt;\n&gt; &gt;You zero out the state of all neurons only ONCE.  AT the beginning of\n&gt; &gt;the simulation.\n&gt; &gt;\n&gt; &gt;Then, for a single time-step, you fire each neuron in the order that\n&gt; &gt;it exists in the list.  The first time through, if a connection refers\n&gt; &gt;to a neuron that hasn&#39;t fired yet, the output of that neuron is 0, so\n&gt; &gt;the input value that is sent to the firing neuron is zero.\n&gt; &gt;\n&gt; &gt;At the end of that firing pass, you process the outputs, update the\n&gt; &gt;&#39;real world&#39; of the simulation, and alter the inputs accordingly.  DO\n&gt; &gt;NOT RESET ANYTHING INSIDE THE NETWORK!  This continues in an infinite\n&gt; &gt;loop until an exit condition is met, namely either failure of the\n&gt; &gt;network to meet its objective in a timely fashion, or success in\n&gt; &gt;accomplishing the desired goal.\n&gt; &gt;\n&gt; &gt;Make sense?  Again, you only reset neurons once, at the beginning of\n&gt; &gt;fitness testing (for a given scenario).  From that moment on, a\n&gt; &gt;neuron&#39;s value is retained for the next time it is accessed.\n&gt; &gt;\n&gt; &gt;Here&#39;s the difference between this approach and the &#39;canonical&#39;\n&gt; &gt;approach.  In the canonical approach you have two values for every\n&gt; &gt;neuron:  Last timestep&#39;s value, and this timestep&#39;s value.  Or\n&gt; &gt;&quot;beginning value&quot; and &quot;ending value&quot;.  For input neurons, they are\n&gt; &gt;always one and the same.  Whenever a neuron references the value of\n&gt; &gt;another neuron, it always inspects the OLD or beginning value.  Then,\n&gt; &gt;it doesn&#39;t matter what order you process the nodes in.  You process\n&gt; &gt;every node, one by one, always referencing the old value of that\n&gt; &gt;neuron.  And you write the new value of the node you are working on to\n&gt; &gt;the other or ending value.\n&gt; &gt;\n&gt; &gt;In a standard feed-forward network with 3 hidden layers, the first\n&gt; &gt;timestep will only propagate the signal one layer down.  At which\n&gt; &gt;point, in a &#39;simulation&#39;, the real world inputs may change.  Now, the\n&gt; &gt;value of the first hidden layer has been changed, so the second layer\n&gt; &gt;will be calculated using those modified values.  But the inputs may\n&gt; &gt;have changed, so the first layer will change.  But no matter, the\n&gt; &gt;second layer is being calculated using the values that were present in\n&gt; &gt;the first hidden layer at the end of the last time step.\n&gt; &gt;\n&gt; &gt;The &#39;optimized&#39; approach that I was advocating doesn&#39;t bother having a\n&gt; &gt;&#39;before&#39; and an &#39;after&#39; (or last and current) value.  It just has a\n&gt; &gt;value.  When another node references the value of a node, it\n&gt; &gt;references the current value of that node.  As soon as that node gets\n&gt; &gt;calculated, the value of the node changes.  In a recurrent network,\n&gt; &gt;some nodes my reference the value of a particular node BEFORE it was\n&gt; &gt;updated, and others may reference it AFTER.  For the puritan, that\n&gt; &gt;&#39;unpredictability&#39; would be undesirable.  But since you are evolving a\n&gt; &gt;network that WORKS, that argument becomes purely academic.\n&gt; &gt;\n&gt; &gt;Unless of course tests show that for a particular experiment it\n&gt; &gt;requires more topology to solve the same problem using the one firing\n&gt; &gt;mechanism over the other.  In that case, the academic argument against\n&gt; &gt;doing it does in fact hold weight.\n&gt; &gt;\n&gt; &gt;But again, it only matters for a controller, not for a classifier\n&gt; &gt;network that is non-recurrent.  So if you are evolving a non-recurrent\n&gt; &gt;classifier, and CPU time matters (both during evolution and in the\n&gt; &gt;final product), then this optimized routine is for you.  Otherwise,\n&gt; &gt;I&#39;d make both activation schemes available, and test evolve networks\n&gt; &gt;using both mechanisms, to find out whether or not it makes a\n&gt; &gt;difference in the ability to evolve a solution.\n&gt; &gt;\n&gt; &gt;Honestly, I can see that there might be pathological cases where this\n&gt; &gt;firing approach would hinder the evolution of a solution to a\n&gt; &gt;controller.  But only for a particular class of problem, and it would\n&gt; &gt;just mean that the successful topology will be different under the one\n&gt; &gt;approach than it would have been under the other approach.  The\n&gt; &gt;chances of it being possible to evolve a solution under one approach\n&gt; &gt;that is impossible to evolve using the other approach are very slim.\n&gt; &gt;But only experience can tell us if that is true or not.  :p\n&gt; &gt;\n&gt; &gt;On 8/31/05, maitrikaruna &lt;kevin@...&gt; wrote:\n&gt; &gt; &gt; John,\n&gt; &gt; &gt;\n&gt; &gt; &gt; thanks for the reply...wish we were all at a whiteboard together in\n&gt; &gt; &gt; which case we could come up with an elegant solution rather\n&gt; &gt; &gt; quickly...\n&gt; &gt; &gt;\n&gt; &gt; &gt; I understand all that you said, but it still creates problems.  The\n&gt; &gt; &gt; idea of recurrency is that it stores a memory that allows the net to\n&gt; &gt; &gt; anticipate and strategize based on a prior world state.\n&gt; &gt; &gt;\n&gt; &gt; &gt; Suppose I follow your idea of activating the net in several steps,\n&gt; &gt; &gt; moving my agent in those time steps if the net says to do it, but\n&gt; &gt; &gt; not checking the new world state.\n&gt; &gt; &gt;\n&gt; &gt; &gt; After these several timesteps, I assume I should re-initialize the\n&gt; &gt; &gt; inputs to all the neurons to zero.  Then I re-scan the &quot;world&quot;, get\n&gt; &gt; &gt; my inputs, and do it all again.  But this resetting and rescanning\n&gt; &gt; &gt; implies no memory of a prior world state since in our internal\n&gt; &gt; &gt; timesteps we did not consider the world inputs again until we were\n&gt; &gt; &gt; done with our net activation.  Does this make sense?\n&gt; &gt; &gt;\n&gt; &gt; &gt; I have come up with my own way to handle this and I&#39;m coding it\n&gt; &gt; &gt; as we speak.  Following is what I propose:\n&gt; &gt; &gt;\n&gt; &gt; &gt; 1) I do exactly what John suggested, I have a nice and tight\n&gt; &gt; &gt; recursive routine that quickly determines dependencies, which\n&gt; &gt; &gt; includes recursive dependencies\n&gt; &gt; &gt;\n&gt; &gt; &gt; 2) I then activate each neuron that has all its depedencies\n&gt; &gt; &gt; fulfilled/processed iteratively, unitl ALL neurons have been\n&gt; &gt; &gt; processed.  I also process recurrent links here, BUT if it is\n&gt; &gt; &gt; timestep 0, i store the recurrent value in its own place separate\n&gt; &gt; &gt; from the neurons non-recurrent, total inputs.  Importantly, at\n&gt; &gt; &gt; timestep 0 these recurrent weights are NOT considered in the\n&gt; &gt; &gt; firing/non-firing decision of the neuron.\n&gt; &gt; &gt;\n&gt; &gt; &gt; 3) if the output neurons tell the agent to move, it does so.\n&gt; &gt; &gt;\n&gt; &gt; &gt; 4) reset ALL the neurons...BUT, if its not timestep 0, then keep the\n&gt; &gt; &gt; recurrent totals in the neurons that had a recurrent link back to\n&gt; &gt; &gt; them..otherwise erset all neurons totals including recurrent totals\n&gt; &gt; &gt;\n&gt; &gt; &gt; 5) get the new state of the world..meaning read the input values..\n&gt; &gt; &gt;\n&gt; &gt; &gt; 6) process the net as above...if it is not timestep 0, then when\n&gt; &gt; &gt; making a fire/not-fire decision, we total the inputs for a neuron\n&gt; &gt; &gt; PLUS the recurrent values from the last net execution....so this is\n&gt; &gt; &gt; the memory of the prior state being used.\n&gt; &gt; &gt;\n&gt; &gt; &gt; 7) repeat all the above...\n&gt; &gt; &gt;\n&gt; &gt; &gt; This seems like a practical approach to me that should work,\n&gt; &gt; &gt; although I haven&#39;t got it working yet.. The code to fire the\n&gt; &gt; &gt; network, even knowing dependencies, is a little tricky.\n&gt; &gt; &gt;\n&gt; &gt; &gt; Hope this all makes some sense...hard to describe via written note...\n&gt; &gt; &gt;\n&gt; &gt; &gt; --Kevin\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt; Yahoo! Groups Links\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;Yahoo! Groups Links\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; \n&gt; In fifteen minutes, everybody will be in the future.\n&gt; \n&gt; \t\t\n&gt; ___________________________________________________________ \n&gt; To help you stay safe and secure online, we&#39;ve developed the all new \n&gt; Yahoo! Security Centre. http://uk.security.yahoo.com\n&gt; \n&gt; ------------------------ Yahoo! Groups Sponsor --------------------~--&gt; \n&gt; Get fast access to your favorite Yahoo! Groups. Make Yahoo! your \n&gt; home page http://us.click.yahoo.com/dpRU5A/wUILAA/yQLSAA/7brrlB/TM\n&gt; --------------------------------------------------------------------~-&gt;\n&gt; \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; \n\n\n-------------------------------------------------------------------------\nwww.greghaynes.com - check out my bro&#39;s stuff and order something! thx.\n\n\n"}}