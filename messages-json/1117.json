{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":7192225,"authorName":"Ian Badcoe","from":"Ian Badcoe &lt;ian_badcoe@...&gt;","profile":"ian_badcoe","replyTo":"LIST","senderId":"1M-DRj3tgswfPPrvf7YRwDGniCi9C4QwFA0ml3TfXXD1d3r5K_3beQ20ue9o-F3oYsGaPG7j32pOtjhU05ksVcWQY7MQugrIxCc","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] Computation Time","postDate":"1087909987","msgId":1117,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDYuMS4wLjYuMC4yMDA0MDYyMjA5MjIwOC4wMjUxMTQ2MEBwb3AubWFpbC55YWhvby5jby51az4=","inReplyToHeader":"PEJBWTItRjc0cEp4VjBIdkJnVXAwMDAwMzE0OUBob3RtYWlsLmNvbT4=","referencesHeader":"PEJBWTItRjc0cEp4VjBIdkJnVXAwMDAwMzE0OUBob3RtYWlsLmNvbT4="},"prevInTopic":1111,"nextInTopic":1118,"prevInTime":1116,"nextInTime":1118,"topicId":845,"numMessagesInTopic":99,"msgSnippet":"... Right, when we started this it was before I asked you about the picture size, if you can get a big slab of processing which operates on limited data, then","rawEmail":"Return-Path: &lt;ian_badcoe@...&gt;\r\nX-Sender: ian_badcoe@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 9127 invoked from network); 22 Jun 2004 13:27:54 -0000\r\nReceived: from unknown (66.218.66.172)\n  by m19.grp.scd.yahoo.com with QMQP; 22 Jun 2004 13:27:54 -0000\r\nReceived: from unknown (HELO smtp003.mail.ukl.yahoo.com) (217.12.11.34)\n  by mta4.grp.scd.yahoo.com with SMTP; 22 Jun 2004 13:27:48 -0000\r\nReceived: from unknown (HELO ian2k.yahoo.co.uk) (ian?badcoe@212.159.73.108 with login)\n  by smtp003.mail.ukl.yahoo.com with SMTP; 22 Jun 2004 13:27:42 -0000\r\nMessage-Id: &lt;6.1.0.6.0.20040622092208.02511460@...&gt;\r\nX-Sender: ian_badcoe@...\r\nX-Mailer: QUALCOMM Windows Eudora Version 6.1.0.6\r\nDate: Tue, 22 Jun 2004 14:13:07 +0100\r\nTo: neat@yahoogroups.com\r\nIn-Reply-To: &lt;BAY2-F74pJxV0HvBgUp00003149@...&gt;\r\nReferences: &lt;BAY2-F74pJxV0HvBgUp00003149@...&gt;\r\nMime-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;us-ascii&quot;; format=flowed\r\nX-eGroups-Remote-IP: 217.12.11.34\r\nFrom: Ian Badcoe &lt;ian_badcoe@...&gt;\r\nSubject: Re: [neat] Computation Time\r\nX-Yahoo-Group-Post: member; u=7192225\r\nX-Yahoo-Profile: ian_badcoe\r\n\r\n\n&gt;Which is why I am organizing it so that I work on the same bit of data over\n&gt;and over again until I have done everything I can with it.  The same 9x9\n&gt;input pixels will be evaluated with 1,005,720 different combinations of\n&gt;&#39;area&#39; parameters and expected results.\n\nRight, when we started this it was before I asked you about the picture \nsize, if you can get a big slab of processing which operates on limited \ndata, then that is very cache efficient.\n\n&gt;Which brings up an important question:  Suppose I have something like this:\n&gt;\n&gt;an array of function pointers to custom activation functions\n&gt;an array of inputs and expected outputs\n\n\n&lt;aside&gt;\n\nBefore we start, you have an array of function pointers...\n\nThis is potentially inefficient (at the instruction level) because calls \nthrough pointers are a type of conditional branch with the same costs as \nany conditional.  These costs are because modern cpus do not execute \ninstructions purely synchronously.  They use a pipeline, which means than \nwhen instruction N is being executed (in the middle of the pipeline), then \ninstruction N+1 has read its inputs and ready and waiting, N+2 is being \nread from the cache and N-1 is still completing execution.\n\n     (The exact range of instructions before and after the &quot;current&quot; one \nvary between cpus.)\n\n     This system is efficient for most code, but suffers two problems:\n\na) when an instruction needs the result of another already in the pipeline\n\nb) when a branch occurs\n\n     In (a) you might have:\n\n         add eax, 1\n         mov [memory], eax\n\n     The second instruction cannot read the value of eax until the first \ninstruction has cleared the pipeline.  This causes a &quot;pipeline stall&quot; where \nthe CPU just has to delay until the value is available (That&#39;s on CISC \nsystems, like X86s.  On RISC the same is true, but the CPU does not do it \nautomatically, the compiler has to insert NOPs).\n\n     in (b) what happens is that the earliest stages of the pipeline (the \n&quot;fetch&quot; stages) encounter the branch, and they have to make an assumption \n(branch prediction) about where the branch will go and start loading \ninstructions from that address.  When the assumption is correct, processing \nproceeds as usual.  When the assumption turns out wrong, the entire \ncontents of the pipeline have to be discarded and processing starts again \nfrom the correct address.\n\nSo, a loop like this:\n\nfor (i=...) {\n   (*fn_ptrs[i])(...);\n}\n\nWill cause a branch-prediction failure whenever the content of the \npointer-array fails to match the CPUs assumption, which will happen a lot \nif there is a mixture of several function addresses in the array.  Sorting \nso the addresses come in blocks would help if the blocks were long enough \n(say 3 or 4 addresses in a row).\n\n(What I described there was roughly the Pentium-I situation.  Later CPUs \nstill have pipelines, but they use them to feed more than one execution \nunit.  Which for the purposes of what we are discussing here just means \nthat there is more to flush when a branch-prediction failure occurs.\n\nThe Pentium-II used to be pretty good at handling one branch-prediction \nfailure, but would really take a bad hit if another occured whilst it was \nhandling the first.  This is probably better now, but the bottom line is: \ntry to move conditionals out of loops.\n\nn.b.  virtual functions work through function pointers.)\n\n&lt;/aside&gt;\n\n\n&gt;If the size of the activation function is such that it can fit in the code\n&gt;cache, then it is better to have that be iterated in the outer loop, and the\n&gt;inputs/outputs iterated in the inner loop.\n&gt;\n&gt;If the function is too big to stay in the code cache anyway, then it is\n&gt;faster to reverse that, right?  Let the data stay in the data cache, and\n&gt;call all the different functions with that data.\n\nDon&#39;t quite get you here.  Could you sketch out which variants on the loops \nyou mean?\n\n&gt;So, how big is too big?  /proc/cpuinfo says my laptop&#39;s cache is 256kb, and\n&gt;my servers have 512kb.  Is that shared for both code and data?\n\nYes and no.\n\nThere are actually three levels of cache on the average PC.\n\nL2 - big fast cache\n\nL1 - smaller very fast cache\n\nAnd a small amount of stuff effectively cached in the CPU itself in the \nform of temporary results.\n\nL2 is generally in the same chip as the CPU and very fast.  That said, some \ncheap and low-power CPUs don&#39;t have it (Celeron, I believe...)\n\nThose sizes you are quoting sound like the L2 size.  At that level I \nbelieve instructions and data are all mixed in together.  L1, OTOH, I seem \nto recall L1 has half reserved for instructions and half for data and the \ntwo don&#39;t mix (this kills self-modifying code, where the instructions _are_ \ndata).\n\n&gt;   How big of a\n&gt;network would I need in order for the generated activation function (no\n&gt;loops) to exceed the size of the cache?  At what point would the cost of a\n&gt;loop counter and weight lookups be less than the unrolled loop?  Is it as I\n&gt;suspect, only for networks so big that worrying about it isn&#39;t worth the\n&gt;effort?\n\nIf your program is traditionally structured (e.g. with the inner-loops \ntight and self contained) then my guess is you will not have code-caching \nproblems.  A tight maths-rich inner-loop can easily fit in 1 or 2 K.  If \nyou compile all the connection look-ups and weights into the network, the \nfunction necessarily gets bigger.  Note that (IIRC) X86 architecture does \nnot support immediate floating-point values (immediate values are written \ninto the code, not data-memory) so the weights will all be data, even if \nthey look like code.\n\nFor the network size, you must consider all reads and writes going on in \nthe loop, so the input data and whatever output process you use must also \nbe considered.  ISRT your input is 9x9 and as double?  So that&#39;s just over \n1/2 K (B+W).  Then there&#39;s the weights (8 bytes per connection), and the \ncurrent node values (8 bytes per node).  So yes, on this basis I don&#39;t see \none network growing so far as to stress the cache.\n\nOne thing, when you do output, absolutely avoid writing files (including \nstdio), or consoles in every loop (sorry if that&#39;s insultingly obvious but \nit&#39;s worth stating the obvious every now and then :) )\n\n&gt; &gt;Thus this:\n&gt; &gt;\n&gt; &gt;int a[1000000000];\n&gt; &gt;int i;\n&gt; &gt;\n&gt; &gt;for(i = 0; i &lt; 1000000000; i += 100) {\n&gt; &gt;    for(j = 0; j &lt; 100; j++) {\n&gt; &gt;      a[i+j] = 1;\n&gt; &gt;    }\n&gt; &gt;}\n&gt; &gt;\n&gt; &gt;Will be far faster than this:\n&gt; &gt;\n&gt; &gt;for(j = 0; j &lt; 100; j++) {\n&gt; &gt;    for(i = 0; i &lt; 1000000000; i += 100) {\n&gt; &gt;      a[i+j] = 1;\n&gt; &gt;    }\n&gt; &gt;}\n&gt;\n&gt;A silly example, since it walks off the end of the array, and could be done\n&gt;without an inner loop!  :)   But I understand the point.\n\nI don&#39;t see where it runs off the array.  The top access in each case will be:\n\na[9999999900 + 99] ?\n\n&gt;A better example is extracting a rectangle from a larger rectangle.  You\n&gt;want to have the Y loop on the outside, and the X loop on the inside, for\n&gt;precisely that reason.  But if your memory is stored in a one-dimensional\n&gt;array (which technically it always is), you have to calculate the offset\n&gt;based on x,y every time.  And even if you store it in a 2-dimensional array,\n&gt;you just push that job off to the compiler to do transparently for you.\n&gt;Now, if you instead calculate the offset of the first point in the row, you\n&gt;can access the next data item by just incrementing your pointer!   The\n&gt;pointer increment is much faster than the calculated memory offset.\n\nIt was once, but modern CPUs care a lot less (if at all) and modern \ncompilers will often do the optimization for you.\n\nOne thing which did catch me out.  I was used to x86 architecture (where \naccessing arrays of ints (4 bytes) is supported in hardware and I did this:\n\nfor(i=...) {\n   a = x[i]\n   b = x[i + 1]\n... many other array accesses around x[i]\n}\n\nWhen we took it to the PlayStation2 it slowed to a crawl, because the PS2 \nhas a RISC chip and was doing a multiply by 4 on every array look up.  So \nfor RISC CPUs, pointers still save you a lot, unlike x86s.\n\n         Ian Badcoe\n\n\n\nLiving@Home - Open Source Evolving Organisms - \nhttp://livingathome.sourceforge.net/\n\n\n\n"}}