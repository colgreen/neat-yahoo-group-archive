{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":283334584,"authorName":"petar_chervenski","from":"&quot;petar_chervenski&quot; &lt;petar_chervenski@...&gt;","profile":"petar_chervenski","replyTo":"LIST","senderId":"PTJd6pJskq_QEcPCJud21d6rvdt4wDLMM9xpT4lMQW-v9gF1_Qj3Zdp-k1rTlb74tqS7l7cFvwt7Zmw71IR5HpWnkoaydhqPMO8lHNxAdF3N4PZmVWM","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Substrate Evolution","postDate":"1253653840","msgId":4864,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGg5YmVnZyszODBxQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGg5YXI5NSt0aXY3QGVHcm91cHMuY29tPg=="},"prevInTopic":4863,"nextInTopic":0,"prevInTime":4863,"nextInTime":4865,"topicId":4848,"numMessagesInTopic":5,"msgSnippet":"Hi Paul, Yes, you are right about that pruning of neurons is probably not a biological performance issue. It was just some hypothesis I thought of. You are","rawEmail":"Return-Path: &lt;petar_chervenski@...&gt;\r\nX-Sender: petar_chervenski@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 36246 invoked from network); 22 Sep 2009 21:11:26 -0000\r\nX-Received: from unknown (98.137.34.46)\n  by m4.grp.sp2.yahoo.com with QMQP; 22 Sep 2009 21:11:26 -0000\r\nX-Received: from unknown (HELO n37b.bullet.mail.sp1.yahoo.com) (66.163.168.151)\n  by mta3.grp.sp2.yahoo.com with SMTP; 22 Sep 2009 21:11:26 -0000\r\nX-Received: from [69.147.65.151] by n37.bullet.mail.sp1.yahoo.com with NNFMP; 22 Sep 2009 21:10:41 -0000\r\nX-Received: from [98.137.34.72] by t5.bullet.mail.sp1.yahoo.com with NNFMP; 22 Sep 2009 21:10:41 -0000\r\nDate: Tue, 22 Sep 2009 21:10:40 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;h9begg+380q@...&gt;\r\nIn-Reply-To: &lt;h9ar95+tiv7@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;petar_chervenski&quot; &lt;petar_chervenski@...&gt;\r\nSubject: Re: Substrate Evolution\r\nX-Yahoo-Group-Post: member; u=283334584; y=n_cjy558QzAMFttkdfnyRqwrwSs-0p2l2ihOudIKYO2pp0DWJyBsjE3vrQ\r\nX-Yahoo-Profile: petar_chervenski\r\n\r\nHi Paul, \n\nYes, you are right about that pruning of neurons is probably not=\r\n a biological performance issue. It was just some hypothesis I thought of. =\r\n\n\nYou are also right that directly answering the question of how many nodes=\r\n and where even for smallest of mammals is a very hard task. This is where =\r\na good indirect encoding (like CPPNs) can be applied. But I&#39;d like to point=\r\n that we are not looking to evolve a specific class of brains, but any kind=\r\n of. There are many examples of different nervous systems in different spec=\r\nies. I think that for a specific problem, a brain may not be necessary, but=\r\n perhaps something else, like a network of ganglia or something that we hav=\r\nen&#39;t seen anywhere in nature. Just because nature solved the &quot;general intel=\r\nligence for survival&quot; problem with the evolution of a centralized brain and=\r\n a spinal cord and stuff, doesn&#39;t mean machines have to do the same exact t=\r\nhing in a &quot;computer&quot; world. \n\nYet substrate topography is very important, b=\r\necause I think it guides the search and in essence the substrate topography=\r\n is the key component that provides advantage to HyperNEAT - the ability to=\r\n inject prior knowledge about the problem into the search, with the hope th=\r\nat the underlying geometry of the problem/solution will be discovered. \n\nTh=\r\ne problem with trying to use a spatial CPPN to describe the substrate is th=\r\nis. First of all, you don&#39;t know how many nodes are required and therefore,=\r\n how to map between a CPPN picture and a complete substrate with generally =\r\nunknown number of nodes. Suppose you put some scaling, where 0.00 in the CP=\r\nPN picture means &quot;1 node&quot; and 1.00 means &quot;a trillion nodes&quot;. Then even the =\r\nslightest mutation in a CPPN can make a substrate jump in complexity from 1=\r\n0 nodes to 100 million in a single generation. Obviously we don&#39;t want that=\r\n. Plus, even the simplest CPPN can create an entirely white picture, which =\r\ntranslated to a substrate with simple mapping will result in maximum node d=\r\nensity/count, but the connections between them would mean nothing, because =\r\nthe CPPN is so simple. Like one without hidden nodes. \n\nYou are right, some=\r\n kind of Neural Darwinism and probably a learning mechanism is good to be a=\r\ndded to the substrate, but the main concern now is how do we get that subst=\r\nrate first :) \n\nA good solution is to make the phenotypes gradually increas=\r\ne in complexity over time just like their corresponding CPPN genotypes. \n\nO=\r\nne idea I have is that we shouldn&#39;t try to map substrate density directly o=\r\nff the CPPN spatial picture, but use the picture to direct where the substr=\r\nate should grow in density over generations. This is the &quot;density mutation =\r\nmap&quot;. First we start with individuals with simple substrates with low densi=\r\nty, and then fitness/evolutionary dynamics will guide the rest. It is possi=\r\nble to include overall density data to the speciation procedure as well, so=\r\n very complex individuals won&#39;t mate with very simple ones. \n\nWell there is=\r\n still plenty of room for new ideas, I will be very happy  and excited if a=\r\nn elegant and meaningful solution to substrate evolution is finally discove=\r\nred. \n\nPeter\n\n--- In neat@yahoogroups.com, &quot;spoonsx21&quot; &lt;spoonsx21@...&gt; wrot=\r\ne:\n&gt;\n&gt; Hello Peter,\n&gt; \n&gt; I absolutely agree with you that Neural Darwinism =\r\nisn&#39;t the ultimate answer, nor is it a complete picture. I thought of it as=\r\n an affirmation of the idea that life learning and brain structure are inte=\r\nrtwined.\n&gt; \n&gt; I do have to briefly disagree that the pruning of neurons is =\r\na biological performance issue. Your explanation accounts for why neurons m=\r\nake so many connections, but not for the fact that they make specific conne=\r\nctions between areas of the brain. It also leaves out an explanation for ne=\r\nuronal growth spurts. Of course I can&#39;t point you to anything conclusive ri=\r\nght now, so that&#39;s better off left as my educated guess, not fact. \n&gt; \n&gt; Bu=\r\nt that&#39;s neither here nor there. I think it is of critical importance that =\r\nthe substrate configuration isn&#39;t static for the entire experiment. It woul=\r\nd seem that the question of &quot;how many nodes and where&quot; is not yours to answ=\r\ner. In fact, if you were to examine even the smallest of mammals, answering=\r\n the question of how many neurons and where is a computationally infeasible=\r\n task. \n&gt; \n&gt; Going back to what Professor Stanley said, if you implement TN=\r\nGS ideas within HyperNEAT, it still lacks information on what topography yo=\r\nu start with and how the rules are originally created that dictate which co=\r\nnnections survive and which are eliminated. But my argument would be that i=\r\nf CPPNs in HyperNEAT are in some ways an abstraction of DNA-dictated brain =\r\nstructure, then couldn&#39;t you have an abstraction of the rule-set that dicta=\r\ntes neuronal placement and movement? The CPPNS represent a pattern in 2n-D =\r\nspace (where n is the dimension of your substrate, if I remember correctly)=\r\n. Then what&#39;s to stop the use of a CPPN to describe the substrate? Picbreed=\r\ner demonstrates the CPPNs abilitiy to paint a picture, is there a downside =\r\nto using it with the substrate (other than increasing the dimension of the =\r\nsearch space)?\n&gt; \n&gt; I still think that there should be some mechanism for m=\r\nodifying the structure within the lifetime of the individual being evaluate=\r\nd, however, that adds another layer of complexity. \n&gt; \n&gt; Perhaps a better q=\r\nuestion might be, what have been your thoughts on evolving the substrate? I=\r\n&#39;m putting this out there as the equivalent of a thinking out loud kind of =\r\nphilosophy, but perhaps you have come up with some suitable directions of y=\r\nour own. \n&gt; \n&gt; Thanks for the reply,\n&gt; -Paul\n&gt; \n&gt; \n&gt; --- In neat@yahoogroup=\r\ns.com, &quot;petar_chervenski&quot; &lt;petar_chervenski@&gt; wrote:\n&gt; &gt;\n&gt; &gt; Hi Paul, \n&gt; &gt; =\r\n\n&gt; &gt; This is a second message, because the first one was actually lost. My =\r\nYahoo e-mail was &quot;bouncing&quot; and stuff. So the group automatically &quot;kicked&quot; =\r\nme. Nevermind, \n&gt; &gt; \n&gt; &gt; I am researching the evolution of CPPNs, and in pa=\r\nrticular HyperNEAT substrate configurations, a lot of time. I still haven&#39;t=\r\n found the solution to it. The problem is really kind of simple one: &quot;how m=\r\nany nodes, and where?&quot; \n&gt; &gt; \n&gt; &gt; It is usually a good idea to see how biolo=\r\ngy does it, but the theory you point out is such that deals with phenotypes=\r\n during their lifetime. I mean, yes, the brain is born with lots of neurons=\r\n and then some process gradually filters them out, making more connections =\r\nmeanwhile. But I think this process is because of &quot;biological performance&quot; =\r\nissues. Neural tissue requires lots of oxygen. More neurons - more oxygen &=\r\n food required. It is better to have few neurons computing lots of stuff, r=\r\nequiring less oxygen, as the whole body grows in size. So this can kind of =\r\nexplain why neurons prefer to make many many connections, no matter how muc=\r\nh they are. \n&gt; &gt; \n&gt; &gt; But I can&#39;t see any connection to HyperNEAT at all. T=\r\no evolve a substrate means to have all the nodes (count of nodes and their =\r\nplacement) depend on evolutionary dynamics. Like for example, if the fittes=\r\nt individual can evolve faster using a circular substrate configuration tha=\r\nn such that displays a square config, the algorithm should make sure it has=\r\n been selected for the next generation. And if the same circular-substrate =\r\nindividual performs good with 16 nodes and another one performs equally wel=\r\nl with 64, the smaller one should be chosen as better. \n&gt; &gt; \n&gt; &gt; The count =\r\nof nodes is a big problem. Even the human brain, the chimp&#39;s brain, the rat=\r\n&#39;s brain, all started from one single cell. So I think it is more fundament=\r\nal to understand biological development than the post-development of the hu=\r\nman brain after it is born. We can actually think of the entire life cycle =\r\nas one pattern in 4D space. CPPNs can create any pattern in any space. \n&gt; &gt;=\r\n \n&gt; &gt; Peter\n&gt; &gt; \n&gt; &gt; --- In neat@yahoogroups.com, &quot;spoonsx21&quot; &lt;spoonsx21@&gt; =\r\nwrote:\n&gt; &gt; &gt;\n&gt; &gt; &gt; Hello everyone,\n&gt; &gt; &gt; \n&gt; &gt; &gt; I&#39;m new to the Neat group. =\r\nMy name is Paul, I&#39;m an undergraduate interested in evolutionary computatio=\r\nn, and obviously HyperNEAT/ NEAT. I have been thinking a lot about substrat=\r\ne evolution, and its parallel in biology. One of the fundamental questions =\r\nI&#39;ve been trying to answer is, how did brain topology evolve? And it is cle=\r\narly fundamental to the problem of substrate evolution. Our sensory informa=\r\ntion is pretty well segregated in the brain. For instance, visual areas are=\r\n broken down into separate processing locations (V1-V5), each area dealing =\r\nwith different aspects of visualization like object movement, or pattern re=\r\ncognition. There are plenty of studies and papers on current brain topology=\r\n, but it&#39;s difficult to find ideas on how brain topology evolved. \n&gt; &gt; &gt; \n&gt;=\r\n &gt; &gt; However, I did stumble upon one theory I enjoyed. It was Gerald Edelma=\r\nn&#39;s theory on what he calls neural Darwinism, or the theory of neuronal gro=\r\nup selection (TNGS). And while I attempt to truly get my head around the th=\r\neory, what I have drawn from his theory seems applicable to HyperNEAT&#39;s ext=\r\nensions. The theory states that within the brain there is first a process o=\r\nf selection in creating the brain&#39;s anatomy, with small epigenetic changes =\r\noccurring in development (Here you can imagine that the anatomy in HyperNEA=\r\nT is our substrate). Then in the postnatal stage, there is a time of neuron=\r\n selection, where some synaptic connections are strengthened, and others si=\r\nmply disappear altogether through neuron death (something with little or no=\r\n parallel in HyperNEAT). He gives as an example a chicken, which is born wi=\r\nth 20,000 neurons. At the adult stage, the chicken has 12,000 neurons, keep=\r\ning only 60% of the original neurons. There is in fact much more to this th=\r\neory, but I am in no way able to communicate it effectively. I encourage yo=\r\nu to read any of his papers of books (or a quick Wikipedia scan). My intere=\r\nst was in HyperNEAT&#39;s possible abstraction of the idea. \n&gt; &gt; &gt; \n&gt; &gt; &gt; Somet=\r\nhing HyperNEAT has yet to incorporate is the idea of intra-life learning. I=\r\n read a few of the other posts, and I think this might be in some ways rela=\r\nted to the HybrID conversation about irregularities. HybrID attempts to mak=\r\ne up for this lack of intra-life learning through the use of NEAT. At some =\r\npoint in the algorithm, HyperNEAT is stopped in favor of using NEAT to more=\r\n accurately pinpoint irregularities. But this is not really the &quot;job&quot; of ev=\r\nolution, rather this is an intralife task. Evolution can provide the framew=\r\nork (i.e. a species), but the more fit individual is able to adapt to the i=\r\nrregularities of life (i.e. through intralife learning). \n&gt; &gt; &gt; \n&gt; &gt; &gt; The =\r\npoint I&#39;m laboriously trying to bring you to is that I believe substrate ev=\r\nolution and intralife learning are related. And perhaps you could take out =\r\ntwo birds with one stone using some ideas from neural Darwinism. My idea is=\r\n a bit crude, and the details aren&#39;t ironed out, but these were some though=\r\nts. Speaking strictly about HyperNEAT, what I thought would be helpful woul=\r\nd be to generate more points then necessary within the substrate. This woul=\r\nd happen during mutation/crossover. Perhaps duplicating inputs in more than=\r\n one place on the substrate (within a certain distance from each other), an=\r\nd adding additional layers in the hidden nodes (if they exist). Through the=\r\n evaluation of the new population, essentially neuron&#39;s that fire together =\r\nwire together (as Edelman loves to say in his books) and weights can be mod=\r\nified slowly during the evaluation, additionally allowing for the removal o=\r\nf less fit neurons. What&#39;s left is an individual whose substrate isn&#39;t stri=\r\nctly identical to the original, and weight connections that might slightly =\r\ndiffer from the original CPPN. \n&gt; &gt; &gt; \n&gt; &gt; &gt; Now problems. There are a host=\r\n of them, and this is what currently makes this idea a bit clunky and inele=\r\ngant. At a very basic level, I&#39;m still uncertain how one could reconcile th=\r\ne difference between the resulting substrate and the original. Also the res=\r\nulting CPPN and the original. Also scaling issues, when trying to modify ne=\r\nuron connections, making changes to 1 connection weight at a time makes thi=\r\ns impossible when examining a neural net of 9 million connections. This mak=\r\nes it difficult to convince anyone that this is the direction that HyperNEA=\r\nT should head in. Rather, it was my idea to ping some ideas off of you guys=\r\n. And I do believe that the solutions to substrate evolution and intralife =\r\nlearning are linked, whether or not this is the best way to do it (most lik=\r\nely not). \n&gt; &gt; &gt; \n&gt; &gt; &gt; Let me know what you guys think,\n&gt; &gt; &gt; -Paul\n&gt; &gt; &gt;\n=\r\n&gt; &gt;\n&gt;\n\n\n\n"}}