{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"pZgHvolQdr_NFfeeSnWdF58u5pl-nZWpY-cZH6RKk3Kg2QAQffCPLaytukzz_stheSdNq4cnQZ-oEY8nXKwDwgn2y8aLd45GBPc-0I9noC3I","spamInfo":{"isSpam":false,"reason":"6"},"subject":"RE : [neat] Re: HyperNEAT: Creating Neural Networks with CPPNs","postDate":"1175016733","msgId":3045,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGV1YmtldCs3YWl0QGVHcm91cHMuY29tPg==","inReplyToHeader":"PGV1YjhqMCtnMmJmQGVHcm91cHMuY29tPg=="},"prevInTopic":3044,"nextInTopic":3046,"prevInTime":3044,"nextInTime":3046,"topicId":3028,"numMessagesInTopic":34,"msgSnippet":"As Alex said, the process of complexification from starting minimally has always been principles of the genotype space, which is the search space.  Thus they","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 69888 invoked from network); 27 Mar 2007 17:32:28 -0000\r\nReceived: from unknown (66.218.67.35)\n  by m45.grp.scd.yahoo.com with QMQP; 27 Mar 2007 17:32:28 -0000\r\nReceived: from unknown (HELO n19d.bullet.scd.yahoo.com) (66.218.67.244)\n  by mta9.grp.scd.yahoo.com with SMTP; 27 Mar 2007 17:32:28 -0000\r\nReceived: from [66.218.69.5] by n19.bullet.scd.yahoo.com with NNFMP; 27 Mar 2007 17:32:17 -0000\r\nReceived: from [66.218.66.77] by t5.bullet.scd.yahoo.com with NNFMP; 27 Mar 2007 17:32:15 -0000\r\nDate: Tue, 27 Mar 2007 17:32:13 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;eubket+7ait@...&gt;\r\nIn-Reply-To: &lt;eub8j0+g2bf@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: RE : [neat] Re: HyperNEAT: Creating Neural Networks with CPPNs\r\nX-Yahoo-Group-Post: member; u=54567749; y=HZRTcSqKsuRw1P2qAe8z7t0OOw3Rk6IX6VWV1UBeeZEaZG8AhgGN\r\nX-Yahoo-Profile: kenstanley01\r\n\r\nAs Alex said, the process of complexification from starting \nminimally has =\r\nalways been principles of the genotype space, which is \nthe search space.  =\r\nThus they still hold in HyperNEAT, which starts \nwith a minimal CPPN.  In f=\r\nact, the NEAT method can be applied to any \nvariable-length indirect encodi=\r\nng, including encodings that do not \nuse CPPNs or HyperNEAT.  (Joe Reisinge=\r\nr has done this)\n\nAs with other indirect encodings, the genotype space in H=\r\nyperNEAT is \nmapped to a different phenotype space.  So in effect when you =\r\nchange \na weight in the CPPN you are making a global change in the \nphenoty=\r\npe, which may have millions of connections.  The hope is that \nthe mapping =\r\nis sufficiently principled that these global variations \nare correlated acr=\r\noss the substrate in sensible ways- that&#39;s what \ngives the mapping its powe=\r\nr.\n\nPetar, I agree that it would be interesting to introduce a kind of \nmut=\r\nation that automatically bumps up the substrate density (i.e. # \nof nodes i=\r\nn the substrate).  It would also be interesting to allow \nHyperNEAT to deci=\r\nde the locations of the nodes in the substrate on \nits own.\n\nHowever, I do =\r\nnot know right now how important the latter issue \nis.   I&#39;m not sure if yo=\r\nu really need to be able to add and \nsubstract substrate automatically.  Th=\r\ne density of the substrate, I \nbelieve, will be most critical.\n\nHow to plac=\r\ne hidden nodes is an issue we have left open with our \npublications so far.=\r\n  It&#39;s clear you can place them in many ways, \nand we will be showing some =\r\nin time to come.  Note that the hidden \nnodes, since they have no a priori =\r\nrole, can simply take advantage \nof their localities.  For example, a hidde=\r\nn node near the left \nsensor is likely to become responsible for operations=\r\n having to do \nwith &quot;left.&quot;  Or two hidden nodes that are near each other c=\r\nan \ndevelop local connections.  Thus there is implicit geometric bias in \nt=\r\nhe placement of nodes in a lattice, ring, multilayer sandwich, or \nline of =\r\nany type.  This information should be possible for evolution \nto exploit.\n\n=\r\nken\n\n\n\n--- In neat@yahoogroups.com, &quot;petar_chervenski&quot; \n&lt;petar_chervenski@.=\r\n..&gt; wrote:\n&gt;\n&gt; &gt; Searching a network that reach a given fitness and\n&gt; &gt; sea=\r\nrching the minimal neural network that reach the\n&gt; &gt; same fitness are two d=\r\nifferents problems.\n&gt; \n&gt; NEAT has shown that it&#39;s better to search in a min=\r\nimal space and \nthen \n&gt; expand that space to find the right complexity for =\r\nthe task.\n&gt; What we have here is a fixed space - the substrate. We search i=\r\nn \nthis \n&gt; space only.. Even if not all connections are expressed, that spa=\r\nce \nis \n&gt; constrained - it has a limit. So here comes the need for a &quot;feel&quot;=\r\n \nof \n&gt; how many nodes are necesarry for a problem. \n&gt; The search of HyperN=\r\nEAT is actually just like the Genetic Art \nstuff.. \n&gt; It can raise complexi=\r\nty, but the pictures are still the same X/Y \n&gt; size..\n&gt; What if HyperNEAT h=\r\nas the ability to scale the substrate and add \n&gt; neurons by itself? Some ki=\r\nnd of mutation maybe.. Just like in the \n&gt; original NEAT, first start with =\r\nextremely simple substrate, then \n&gt; scale it further and add nodes to it.. =\r\nThe nodes are added in the \n&gt; same space, so it shouldn&#39;t be a problem.. I =\r\nhave to give this \nsome \n&gt; more thought.. \n&gt; \n&gt; Peter\n&gt; \n&gt; --- In neat@yaho=\r\nogroups.com, Alexandre Devert &lt;marmakoide@&gt; wrote:\n&gt; &gt;\n&gt; &gt; \n&gt; &gt; --- petar_c=\r\nhervenski &lt;petar_chervenski@&gt; a\n&gt; &gt; =E9crit :\n&gt; &gt; \n&gt; &gt; &gt; Hi, thanks for the=\r\n quick response, \n&gt; &gt; &gt; But I know that the CPPN is used like a &quot;genome&quot; fo=\r\nr\n&gt; &gt; &gt; the big network, \n&gt; &gt; &gt; and the CPPNs evolve with the NEAT method. =\r\n\n&gt; &gt; Sorry to break down open doors, that&#39;s an old habit :D\n&gt; &gt; \n&gt; &gt; &gt; I wa=\r\ns\n&gt; &gt; &gt; just wondering why \n&gt; &gt; &gt; so big phenotypes at first? Does it reall=\r\ny matter? \n&gt; &gt; &gt; I know that the  fitness function tells me which \n&gt; &gt; &gt; ge=\r\nnome is better, but consider some \n&gt; &gt; &gt; simple task.. like the &quot;go to the =\r\nfood&quot; one.. \n&gt; &gt; &gt; Can this method be applied succesfuly where direct\n&gt; &gt; &gt;=\r\n encodings like the \n&gt; &gt; &gt; original NEAT perform best? \n&gt; &gt; &gt;\n&gt; &gt; Yes, for =\r\na one bit XOR or food retrieval for a two\n&gt; &gt; sensors robots, it&#39;s a bit ov=\r\nesized. But let&#39;s for a\n&gt; &gt; 256 bits XOR, it&#39;s mandatory. It would interest=\r\ning,\n&gt; &gt; for the XOR problem, when HyperNEAT beats NEAT in time\n&gt; &gt; to reac=\r\nh a given fitness ceil. I have other problems\n&gt; &gt; in mind where the solutio=\r\nn is a big neural network.\n&gt; &gt; Then, that&#39;s only a personal intuition, it&#39;s=\r\n maybe\n&gt; &gt; much easier to find a big, regular neural network for\n&gt; &gt; a give=\r\nn task, rather than find the minimal one.\n&gt; &gt; Searching a network that reac=\r\nh a given fitness and\n&gt; &gt; searching the minimal neural network that reach t=\r\nhe\n&gt; &gt; same fitness are two differents problems.\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; &gt; Can HyperN=\r\nEAT be extended so it can alter the\n&gt; &gt; &gt; substrate (like adding \n&gt; &gt; &gt; mor=\r\ne neurons), just like the original NEAT alters\n&gt; &gt; &gt; the search space by \n&gt;=\r\n &gt; &gt; adding more nodes and connections?\n&gt; &gt; &gt; \n&gt; &gt; Developmental approaches=\r\n, here we come :D\n&gt; &gt; \n&gt; &gt; &gt; Peter.\n&gt; &gt; &gt; \n&gt; &gt; &gt; --- In neat@...=\r\nm, Alexandre Devert\n&gt; &gt; &gt; &lt;marmakoide@&gt; wrote:\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; Hi\n&gt; &gt; &gt; &gt;  =\r\nHere the phenotype is the million connections\n&gt; &gt; &gt; neural\n&gt; &gt; &gt; &gt; network,=\r\n but the genotype is a much more simple,\n&gt; &gt; &gt; &gt; initialy minimal CPNN. Add=\r\nitive mutations acts on\n&gt; &gt; &gt; the\n&gt; &gt; &gt; &gt; CPNN. So there is any violations =\r\nof the NEAT\n&gt; &gt; &gt; &gt; philosophy : minimal genotype with well conceived\n&gt; &gt; &gt;=\r\n &gt; mutations. Of course, small modifications of the\n&gt; &gt; &gt; CPNN\n&gt; &gt; &gt; &gt; lead=\r\n to huge modifications in the target\n&gt; &gt; &gt; phenotype, as\n&gt; &gt; &gt; &gt; in all com=\r\npact neural networks encodings like\n&gt; &gt; &gt; cellular\n&gt; &gt; &gt; &gt; encoding. The Hy=\r\nperNEAT seems to be a new step in\n&gt; &gt; &gt; this\n&gt; &gt; &gt; &gt; familly of encodings :=\r\nD\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; The hidden nodes of the genotype (the CPNN) are\n&gt; &gt; &gt; ad=\r\nded\n&gt; &gt; &gt; &gt; by NEAT as usual. The hidden nodes of the\n&gt; &gt; &gt; phenotype\n&gt; &gt; &gt;=\r\n &gt; (the jumbo sized neural network) are never added,\n&gt; &gt; &gt; it&#39;s\n&gt; &gt; &gt; &gt; up =\r\nto the substrate conception to provide enought\n&gt; &gt; &gt; &gt; neurons. The HyperNE=\r\nAT allows to put thousands of\n&gt; &gt; &gt; &gt; neurons, so we can be generous with t=\r\nhe substrate.\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; --- petar_chervenski &lt;petar_chervenski@&gt; a\n&gt;=\r\n &gt; &gt; &gt; =E9crit :\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; Hi, \n&gt; &gt; &gt; &gt; &gt; a have a quiestion about=\r\n this.. The evolution in\n&gt; &gt; &gt; &gt; &gt; this indirect method \n&gt; &gt; &gt; &gt; &gt; evolves =\r\nthe geometry of the connections, right?\n&gt; &gt; &gt; But\n&gt; &gt; &gt; &gt; &gt; even in the fir=\r\nst \n&gt; &gt; &gt; &gt; &gt; population there may be phenotypes with millions\n&gt; &gt; &gt; of\n&gt; &gt;=\r\n &gt; &gt; &gt; connections. Does \n&gt; &gt; &gt; &gt; &gt; this violate the third principle of NEA=\r\nT, that\n&gt; &gt; &gt; &gt; &gt; initial genomes must \n&gt; &gt; &gt; &gt; &gt; be as small as possible? =\r\n\n&gt; &gt; &gt; &gt; &gt; What about hidden nodes? In this approach, a set\n&gt; &gt; &gt; of\n&gt; &gt; &gt; =\r\n&gt; &gt; nodes has to be \n&gt; &gt; &gt; &gt; &gt; chosen and the evolution then evolves only t=\r\nhe\n&gt; &gt; &gt; &gt; &gt; connections between \n&gt; &gt; &gt; &gt; &gt; them. There are no additive mut=\r\nations for nodes\n&gt; &gt; &gt; and\n&gt; &gt; &gt; &gt; &gt; stuff. \n&gt; &gt; &gt; &gt; &gt; I know that the inne=\r\nr workings of this system is\n&gt; &gt; &gt; &gt; &gt; NEAT, starting \n&gt; &gt; &gt; &gt; &gt; minimally =\r\nand so on, but when we look at the\n&gt; &gt; &gt; actual\n&gt; &gt; &gt; &gt; &gt; phenotypes, when =\r\n\n&gt; &gt; &gt; &gt; &gt; we see how they evolve, it doesn&#39;t look like\n&gt; &gt; &gt; NEAT\n&gt; &gt; &gt; &gt; =\r\n&gt; starting from a \n&gt; &gt; &gt; &gt; &gt; minimal point and slowly adding structure.. A\n=\r\n&gt; &gt; &gt; single\n&gt; &gt; &gt; &gt; &gt; small mutation \n&gt; &gt; &gt; &gt; &gt; in the CPPN genome changes=\r\n the entire structure\n&gt; &gt; &gt; of\n&gt; &gt; &gt; &gt; &gt; the phenotype. Is \n&gt; &gt; &gt; &gt; &gt; this =\r\nright?\n&gt; &gt; &gt; &gt; &gt; Or the comlexification here is about moving to\n&gt; &gt; &gt; &gt; &gt; h=\r\nigher resolutions of \n&gt; &gt; &gt; &gt; &gt; the networks? \n&gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; Peter\n&gt;=\r\n &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; --- In neat@yahoogroups.com, &quot;Kenneth Stanley&quot;\n&gt; &gt; &gt; &gt; =\r\n&gt; &lt;kstanley@&gt; wrote:\n&gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; Hi everyone, as some of you kn=\r\now, we have been\n&gt; &gt; &gt; &gt; &gt; working for a while \n&gt; &gt; &gt; &gt; &gt; &gt; now on turning =\r\nthe patterns output by CPPNs\n&gt; &gt; &gt; into\n&gt; &gt; &gt; &gt; &gt; neural networks.  \n&gt; &gt; &gt; =\r\n&gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; I am excited to report that we have found a \n&gt; &gt; &gt; &gt; &gt; &gt;=\r\n straightforward and principled approach to\n&gt; &gt; &gt; &gt; &gt; interpreting CPPN out=\r\nput \n&gt; &gt; &gt; &gt; &gt; &gt; as a connectivity pattern rather than a\n&gt; &gt; &gt; spatial\n&gt; &gt; =\r\n&gt; &gt; &gt; pattern.    We are \n&gt; &gt; &gt; &gt; &gt; &gt; calling this method HyperNEAT, which =\r\nstands\n&gt; &gt; &gt; for\n&gt; &gt; &gt; &gt; &gt; Hypercube-based \n&gt; &gt; &gt; &gt; &gt; &gt; NEAT.  With HyperNE=\r\nAT, we have been able to\n&gt; &gt; &gt; &gt; &gt; produce working neural \n&gt; &gt; &gt; &gt; &gt; &gt; netw=\r\norks with millions of connections.\n&gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; Because we have=\r\n just recently had two\n&gt; &gt; &gt; publications\n&gt; &gt; &gt; &gt; &gt; on this method \n&gt; &gt; &gt; &gt;=\r\n &gt; &gt; accepted at GECCO-2007, we can finally\n&gt; &gt; &gt; disclose\n&gt; &gt; &gt; &gt; &gt; our wo=\r\nrk and share \n&gt; &gt; &gt; &gt; &gt; &gt; it with others.  The publications are\n&gt; &gt; &gt; avail=\r\nable on\n&gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; the &quot;Publications&quot; page at\n&gt; &gt; &gt; http://eple=\r\nx.cs.ucf.edu\n&gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; They are also available th=\r\nrough my homepage \n&gt; &gt; &gt; &gt; &gt; &gt; http://www.cs.ucf.edu/~kstanley/#publication=\r\ns\n&gt; &gt; &gt; &gt; &gt; (under Publications, \n&gt; &gt; &gt; &gt; &gt; &gt; Conference and Symposium Pape=\r\nrs), or directly\n&gt; &gt; &gt; at\n&gt; &gt; &gt; &gt; &gt; the following \n&gt; &gt; &gt; &gt; &gt; &gt; address, whi=\r\nch I am sure yahoo is going to\n&gt; &gt; &gt; mangle:\n&gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; http:=\r\n//eplex.cs.ucf.edu/index.php?\n&gt; &gt; &gt; &gt; &gt; &gt; option=3Dcom_content&task=3Dview&=\r\nid=3D14&Itemid=3D28\n&gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; The two papers, which present =\r\nseparate\n&gt; &gt; &gt; experiments\n&gt; &gt; &gt; &gt; &gt; are:\n&gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; &quot;Generat=\r\ning Large-Scale Neural Networks\n&gt; &gt; &gt; Through\n&gt; &gt; &gt; &gt; &gt; Discovering \n&gt; &gt; &gt; =\r\n&gt; &gt; &gt; Geometric Regularities&quot; by Jason J. Gauci and\n&gt; &gt; &gt; &gt; &gt; Kenneth O. St=\r\nanley\n&gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; and \n&gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; &quot;A Novel Gener=\r\native Encoding for Exploiting\n&gt; &gt; &gt; Neural\n&gt; &gt; &gt; &gt; &gt; Network Sensor \n&gt; &gt; &gt; =\r\n&gt; &gt; &gt; and Output Geometry&quot; by David B. D&#39;Ambrosio\n&gt; &gt; &gt; and\n&gt; &gt; &gt; &gt; &gt; Kenne=\r\nth O. Stanley\n&gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; David D&#39;Ambriosio and Jason Gauci ar=\r\ne both\n&gt; &gt; &gt; Ph.D.\n&gt; &gt; &gt; &gt; &gt; students in the \n&gt; &gt; &gt; &gt; &gt; &gt; EPlex lab who did=\r\n an enormous amount of work\n&gt; &gt; &gt; to\n&gt; &gt; &gt; &gt; &gt; make this possible.\n&gt; &gt; &gt; &gt; =\r\n&gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; HyperNEAT is based on a surprisingly simple\n&gt; &gt; &gt; &gt; &gt; prin=\r\nciple: Connectivity \n&gt; &gt; &gt; &gt; &gt; &gt; patterns are actually no different from\n&gt; =\r\n&gt; &gt; spatial\n&gt; &gt; &gt; &gt; &gt; patterns in a \n&gt; &gt; &gt; &gt; &gt; higher-\n&gt; &gt; &gt; &gt; &gt; &gt; dimensio=\r\nnal space.  In other words, a\n&gt; &gt; &gt; &gt; &gt; 2-dimensional connectivity \n&gt; &gt; &gt; &gt;=\r\n &gt; &gt; pattern is the same thing as a 4-dimensional\n&gt; &gt; &gt; &gt; &gt; spatial pattern=\r\n.  (The \n&gt; &gt; &gt; &gt; &gt; &gt; technical term is that they are &quot;isomorphic.&quot;)\n&gt; &gt; &gt; \n=\r\n&gt; &gt; &gt; &gt; &gt; Therefore, if we \n&gt; &gt; &gt; &gt; &gt; &gt; simply produce spatial patterns wit=\r\nhin a 4D\n&gt; &gt; &gt; &gt; &gt; hypercube instead of a \n&gt; &gt; &gt; &gt; &gt; &gt; 2D plane, they can b=\r\ne mapped directly to\n&gt; &gt; &gt; &gt; &gt; connectivity patterns with \n&gt; &gt; &gt; &gt; &gt; &gt; all =\r\nthe nice properties that we have seen from\n&gt; &gt; &gt; &gt; &gt; CPPNs.  Thus, they \n&gt; =\r\n&gt; &gt; &gt; &gt; &gt; require no special &quot;image recognition&quot; type\n&gt; &gt; &gt; &gt; &gt; tricks.  Th=\r\ne mapping is \n&gt; &gt; &gt; &gt; &gt; &gt; perfectly isomorphic.  We are calling these 4D\n&gt; =\r\n&gt; &gt; &gt; &gt; CPPNs &quot;connective \n&gt; &gt; &gt; &gt; &gt; &gt; CPPNs&quot; since they are interpreted as=\r\n\n&gt; &gt; &gt; connectivity\n&gt; &gt; &gt; &gt; &gt; patterns.\n&gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; This insig=\r\nht yields a handful of unprecedented\n&gt; &gt; &gt; &gt; &gt; capabilities.  For \n&gt; &gt; &gt; &gt; =\r\n&gt; &gt; example, with connective CPPNs, you can\n&gt; &gt; &gt; recreate\n&gt; &gt; &gt; &gt; &gt; the sa=\r\nme solution \n&gt; &gt; &gt; &gt; &gt; &gt; network at a different resolution without\n&gt; &gt; &gt; fu=\r\nrther\n&gt; &gt; &gt; &gt; &gt; evolution!  You \n&gt; &gt; &gt; &gt; &gt; &gt; can also place inputs and outp=\r\nuts in\n&gt; &gt; &gt; meaningful\n&gt; &gt; &gt; &gt; &gt; geometric \n&gt; &gt; &gt; &gt; &gt; &gt; configurations tha=\r\nt HyperNEAT can exploit for\n&gt; &gt; &gt; &gt; &gt; symmetries and \n&gt; &gt; &gt; &gt; &gt; &gt; regularit=\r\nies.  \n&gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; In any case, I am excited about this new\n&gt; =\r\n&gt; &gt; advance\n&gt; &gt; &gt; &gt; &gt; and these papers \n&gt; &gt; &gt; &gt; &gt; &gt; are a first step in exp=\r\nloring the new\n&gt; &gt; &gt; approach.\n&gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; ken\n&gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt;=\r\n &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; ___________________=\r\n____________\n&gt; &gt; &gt; &gt; Marmakoide aka Alexandre Devert \n&gt; &gt; &gt; &gt; _____________=\r\n__________________\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; \t\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; \t\n&gt; &gt; &gt; &gt; \t=\r\n\t\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt;\n&gt; &gt; \n&gt; \n__________________________________________________=\r\n___________________\n_\n&gt; &gt; &gt; _____ \n&gt; &gt; &gt; &gt; D=E9couvrez une nouvelle fa=E7on=\r\n d&#39;obtenir des\n&gt; &gt; &gt; r=E9ponses =E0 toutes vos \n&gt; &gt; &gt; questions ! \n&gt; &gt; &gt; &gt; =\r\nProfitez des connaissances, des opinions et des\n&gt; &gt; &gt; exp=E9riences des \n&gt; =\r\n&gt; &gt; internautes sur Yahoo! Questions/R=E9ponses \n&gt; &gt; &gt; &gt; http://fr.answers.=\r\nyahoo.com\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; ______________________=\r\n_________\n&gt; &gt; Marmakoide aka Alexandre Devert \n&gt; &gt; ________________________=\r\n_______\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \t\n&gt; &gt; \n&gt; &gt; \t\n&gt; &gt; \t\t\n&gt; &gt; \n&gt; \n_________________________=\r\n____________________________________________\n_\n&gt; _____ \n&gt; &gt; D=E9couvrez une=\r\n nouvelle fa=E7on d&#39;obtenir des r=E9ponses =E0 toutes vos \n&gt; questions ! \n&gt;=\r\n &gt; Profitez des connaissances, des opinions et des exp=E9riences des \n&gt; int=\r\nernautes sur Yahoo! Questions/R=E9ponses \n&gt; &gt; http://fr.answers.yahoo.com\n&gt;=\r\n &gt;\n&gt;\n\n\n\n"}}