{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":127853030,"authorName":"Colin Green","from":"Colin Green &lt;cgreen@...&gt;","profile":"alienseedpod","replyTo":"LIST","senderId":"yW1ruzSNU0Gv2Aqb3E_NRxx4i9RZa-fjHbrTX3bXdUXJ6P9VL85J-dtOUA7u6DYOUR6gcc8aP_43tUcGUdp8U3beSUVlp8a68w","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [neat] Compactness of ANN&#39;s built from indirect encodings","postDate":"1134866399","msgId":2472,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQzQTRBRkRGLjcwMDA4MDNAZHNsLnBpcGV4LmNvbT4=","inReplyToHeader":"PDYuMi4zLjQuMC4yMDA1MTIxMzIwNDMzOS4wMjdiMzc5OEBwb3AubWFpbC55YWhvby5jby51az4=","referencesHeader":"PDQzOUIzQTQ1LjYwNTA1MDdAZHNsLnBpcGV4LmNvbT4gPDYuMi4zLjQuMC4yMDA1MTIxMzIwNDMzOS4wMjdiMzc5OEBwb3AubWFpbC55YWhvby5jby51az4="},"prevInTopic":2469,"nextInTopic":0,"prevInTime":2471,"nextInTime":2473,"topicId":2466,"numMessagesInTopic":3,"msgSnippet":"... When I said invocation I meant invocation statement , or invocation instruction(JMP or whatever) within compiled code. I m not quite sure where you re","rawEmail":"Return-Path: &lt;cgreen@...&gt;\r\nX-Sender: cgreen@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 35604 invoked from network); 18 Dec 2005 00:39:50 -0000\r\nReceived: from unknown (66.218.66.172)\n  by m26.grp.scd.yahoo.com with QMQP; 18 Dec 2005 00:39:50 -0000\r\nReceived: from unknown (HELO ranger.systems.pipex.net) (62.241.162.32)\n  by mta4.grp.scd.yahoo.com with SMTP; 18 Dec 2005 00:39:50 -0000\r\nReceived: from [10.0.0.10] (81-86-161-87.dsl.pipex.com [81.86.161.87])\n\tby ranger.systems.pipex.net (Postfix) with ESMTP id 293A0E0000E3\n\tfor &lt;neat@yahoogroups.com&gt;; Sun, 18 Dec 2005 00:39:49 +0000 (GMT)\r\nMessage-ID: &lt;43A4AFDF.7000803@...&gt;\r\nDate: Sun, 18 Dec 2005 00:39:59 +0000\r\nUser-Agent: Mozilla Thunderbird 1.0.7 (Windows/20050923)\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: neat@yahoogroups.com\r\nReferences: &lt;439B3A45.6050507@...&gt; &lt;6.2.3.4.0.20051213204339.027b3798@...&gt;\r\nIn-Reply-To: &lt;6.2.3.4.0.20051213204339.027b3798@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: Colin Green &lt;cgreen@...&gt;\r\nSubject: Re: [neat] Compactness of ANN&#39;s built from indirect encodings\r\nX-Yahoo-Group-Post: member; u=127853030; y=GLc-CZjObE6io_TruxF0LC3P9Ow7uxbLuOyC3H6u83qaRRjJVgTz\r\nX-Yahoo-Profile: alienseedpod\r\n\r\nIan Badcoe wrote:\n\n&gt;At 20:27 10/12/2005, you wrote:\n&gt;  \n&gt;\n&gt;&gt;Here&#39;s an idea I got while reading some GP papers.\n&gt;&gt;\n&gt;&gt;Often in GP work the use of ADF&#39;s (Automatically Defined Functions) is\n&gt;&gt;mentioned, as far as I can see this is just a fancy term for a\n&gt;&gt;sub-routine. In tree based GP there is usually a main program tree and\n&gt;&gt;optionally a seperate set of ADF&#39;s than can be invoked (from the main\n&gt;&gt;tree) with parameters, just as any other sub-routine. What this means is\n&gt;&gt;that each ADF is only defined once in memory, just as sub-routines (or\n&gt;&gt;the CPU instructions that make up a function) in C++, Java or whatever\n&gt;&gt;only exist once in computer memory regardless of how many times the\n&gt;&gt;function is called. Each invocation simply pushes its parameters onto\n&gt;&gt;the stack and jumps to the function&#39;s address in memory.\n&gt;&gt;\n&gt;&gt;Now consider, say, modular-NEAT. To me sub-networks or modules are the\n&gt;&gt;equivalent of sub-routines within GP, each instance of a module within\n&gt;&gt;an ANN (following decoding of the genome) is like a seperate invocation\n&gt;&gt;of a sub-routine with the parameters coming from the connections feeding\n&gt;&gt;signals into the module.\n&gt;&gt;    \n&gt;&gt;\n&gt;\n&gt;Yes and no (and you get into this a bit yourself).  A subroutine is \n&gt;(usually) entirely discrete, e.g. invocation#1 can have no influence \n&gt;on invocation#2.\n&gt;  \n&gt;\nWhen I said &#39;invocation&#39; I meant &#39;invocation statement&#39;, or invocation \ninstruction(JMP or whatever) within compiled code. I&#39;m not quite sure \nwhere you&#39;re coming from with this point, perhaps you&#39;re refering to the \nvery nature of ANNs whereby signals move through every part of the \nnetwork at all times, rather than sub-nets being activated independently \nof the larger containing net. In that respect I can see that data can \npass between sub-nets and that the analogy breaks down from the point of \nview of following the data flow. This is an important point but I still \nthink my analogy holds water...\n\n&gt;You can change this in some languages by adding static data to the \n&gt;subroutine (in which case you have arguably turned it into simplistic \n&gt;pure-static class).\n&gt;\n&gt;You have the same choice in ANN, a sub-net can be shared.  In which \n&gt;case you still have a load of questions to answer about how the \n&gt;wiring is done, but the nets that use it are to some extent sharing \n&gt;data.  Or the sub-net can be uniquely cloned into each place where it \n&gt;is used, in which case it is completely insulated in each case.\n&gt;  \n&gt;\nIf a sub-net is shared then surely that is equivalent to a single \ninstance of the sub-net receiving inputs from different neurons?\n\n&gt;So during instantiation, you have two separate choices to make:\n&gt;\n&gt;1) instantiate one definition into one or more sub-routines\n&gt;2) wire each subroutine into one or more places in the net\n&gt;\n&gt;  \n&gt;\nI&#39;m losing the thread here slightly, I think we&#39;re on slightly different \nwavelengths! I&#39;ll try and pick up below...\n\n&gt;&gt;Of course a fully decoded ANN would normally (I guess) contain\n&gt;&gt;duplicates of the modules, because each copy must maintain it&#39;s own set\n&gt;&gt;of neuron states (activation signal).\n&gt;&gt;    \n&gt;&gt;\n&gt;\n&gt;Only a recurrent net needs that, a non-recurrant net has the useful \n&gt;property that you could use the same copy of the sub-net in multiple \n&gt;places, and as long as you chase the sub-net&#39;s activation to \n&gt;completion in each context where it is used, you are not leaking data \n&gt;from one instance to another.  Now that&#39;s saving memory!\n&gt;  \n&gt;\nYes good point. I always tend to forget about feedforward nets as I \nthink of them as a sub-type of the more general recursive nets, which of \ncourse they are. Fair point though.\n\n&gt;Non-recurrent sub-nets also have the property that they are \n&gt;completely asynchronous from the net that uses them, so as long as \n&gt;you wait until all the sub-nets inputs are available, you can then \n&gt;process it to completion as a discrete action, knowing it won&#39;t need \n&gt;any other input from the outer net.\n&gt;  \n&gt;\nHmm I&#39;m not sure. What about control experiments whereby at each \ntimestep you generally feed in a set of inputs and read a set of \noutputs. You don&#39;t [normally] want to effectively perform several \ntimesteps in a sub-net in the context of a single activation of the net \nas a whole. So at any one time you generally have signals sloshing \naround within a sub-net from several sets of inputs from several \nprevious timesteps - even for feedforward networks.\n\n&gt;UNLESS, of course, the outer net _is_ recurrent, in which case what I \n&gt;said is still true but you might be concerned about the time-delay \n&gt;between setting sub-net inputs and the result working its way through...\n&gt;  \n&gt;\nAhh yes, which is similar to the scenario I just described.\n\n&gt;&gt; This is distinct from\n&gt;&gt;(traditional) GP because GP trees (or graphs) traditionally describe\n&gt;&gt;control flow and so each invocation of a routine occurs in sequence,\n&gt;&gt;whereas ANN&#39;s describe data flow and all instances of an ANN module\n&gt;&gt;therefore have data flowing through them concurrently.\n&gt;&gt;\n&gt;&gt; However this level of duplication is still potentially wasteful (of\n&gt;&gt;main memory) because only the neuron signals are unique to each copy,\n&gt;&gt;not the structure. The structure is equivalent to the CPU instructions\n&gt;&gt;of a normal sub-routine and as such doesn&#39;t actually need to be\n&gt;&gt;duplicated. Instead each module could be decoded into it&#39;s own sub-ANN\n&gt;&gt;that the main ANN connects to, but in addition to a normal connection we\n&gt;&gt;must specify the module &#39;instance&#39; we are connecting to, each neuron\n&gt;&gt;then contains an array (for example) of states and it operates on the\n&gt;&gt;relevant state data.\n&gt;&gt;\n&gt;&gt;Of course a sub-ANN could refer to other sub-ANN&#39;s, so overall the\n&gt;&gt;memory saving could potentially be very large. Such an ANN\n&gt;&gt;representation may not be as compact as the original genome where\n&gt;&gt;indirect encodings are in use, but it could help keep a lid on the\n&gt;&gt;memory usage of decoded ANN&#39;s. Of course functionaly the ANN&#39;s could\n&gt;&gt;still grow out of control - they are still the same ANN, just\n&gt;&gt;represented more compactly.\n&gt;&gt;    \n&gt;&gt;\n&gt;\n&gt;If you can wangle your network to fit a &quot;stateless&quot; or &quot;lightweight&quot; \n&gt;patterns, then potentially whole slews of neurones can refer to the \n&gt;same object.  Of course it depends whether your indirect encoding \n&gt;produces slews of identical neurones, but that&#39;s kinda the point...\n&gt;  \n&gt;\nAhh ok, this is essentially what I was describing then - a &#39;stateless&#39; \npattern where the invoker has to provide the state to work on.\n\n&gt;I have a prototype GP system that works this way, but I didn&#39;t touch \n&gt;it for a couple of years...\n&gt;  \n&gt;\nI believe you ;)\n\nColin.\n\n\n"}}