{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":487025037,"authorName":"afcarl2","from":"&quot;afcarl2&quot; &lt;afcarl2@...&gt;","profile":"afcarl2","replyTo":"LIST","senderId":"ykLH2boJQDqELJqJ8_-dAE-_3v8yMc-_XEi0LXUwW6uwc9KJ52lSuRwvr6Z5s4RIqywlGuQ6XgcrL7UIFF3ciESbFwo","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Python NEAT","postDate":"1318539433","msgId":5659,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGo3N2piOStxbDYxQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGl2NjY1Zyt1MnZxQGVHcm91cHMuY29tPg=="},"prevInTopic":5618,"nextInTopic":0,"prevInTime":5658,"nextInTime":5660,"topicId":535,"numMessagesInTopic":47,"msgSnippet":"Peter, How is your python implementation going? Still excited to see what you have done! Andy","rawEmail":"Return-Path: &lt;afcarl2@...&gt;\r\nX-Sender: afcarl2@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 60250 invoked from network); 13 Oct 2011 20:57:17 -0000\r\nX-Received: from unknown (98.137.35.161)\n  by m3.grp.sp2.yahoo.com with QMQP; 13 Oct 2011 20:57:17 -0000\r\nX-Received: from unknown (HELO ng1-ip1.bullet.mail.ne1.yahoo.com) (98.138.215.55)\n  by mta5.grp.sp2.yahoo.com with SMTP; 13 Oct 2011 20:57:17 -0000\r\nX-Received: from [98.138.217.182] by ng1.bullet.mail.ne1.yahoo.com with NNFMP; 13 Oct 2011 20:57:16 -0000\r\nX-Received: from [69.147.65.148] by tg7.bullet.mail.ne1.yahoo.com with NNFMP; 13 Oct 2011 20:57:16 -0000\r\nX-Received: from [98.137.35.12] by t11.bullet.mail.sp1.yahoo.com with NNFMP; 13 Oct 2011 20:57:15 -0000\r\nDate: Thu, 13 Oct 2011 20:57:13 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;j77jb9+ql61@...&gt;\r\nIn-Reply-To: &lt;iv665g+u2vq@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;afcarl2&quot; &lt;afcarl2@...&gt;\r\nSubject: Re: Python NEAT\r\nX-Yahoo-Group-Post: member; u=487025037; y=olxCZ9hJvoMFTWM4AMmbVrhImtv4-OO7kvl2xju6DRBI8A\r\nX-Yahoo-Profile: afcarl2\r\n\r\nPeter,\n\n    How is your python implementation going? Still excited to see w=\r\nhat you have done!\n\nAndy\n\n--- In neat@yahoogroups.com, &quot;petar_chervenski&quot; &lt;=\r\npetar_chervenski@...&gt; wrote:\n&gt;\n&gt; Well, exactly. I didn&#39;t mention anything a=\r\nbout evaluation, though, this is simply a function that takes arbitrary gra=\r\nph (a genome) and returns a fitness value. A domain-dependent black box. Gr=\r\naphs are perfect for complexification. Most (or all) implementations of NEA=\r\nT are designed for evolution of neural networks, and modifying existing cod=\r\ne to evolve any graph becomes tricky. For example, recurrence is treated li=\r\nke a special case sometimes and no two nodes can have more than one (or 2 w=\r\nith recurrence) connections. Inputs and outputs are fixed and inputs don&#39;t =\r\nget connected with links pointing to them. Break one of these rules and NEA=\r\nT becomes less effective at evolution of neural networks - you may get what=\r\n you want, but at the cost of damaging the existing code that previously wo=\r\nrked fine. It&#39;s better to start from scratch and make the code as general a=\r\ns possible. NN evolution will be derived from it. \n&gt; \n&gt; Peter\n&gt; \n&gt; --- In n=\r\neat@yahoogroups.com, &quot;afcarl2&quot; &lt;afcarl2@&gt; wrote:\n&gt; &gt;\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; Pet=\r\ner,\n&gt; &gt; \n&gt; &gt; A couple of thoughts. First, IMO one of the primary values of =\r\nNEAT are the various methods of managing complexification of structure, not=\r\n how the infrastructure is constrained to match NN evaluation. Many profoun=\r\ndly useful applications are entirely non-NN in nature, and the genome itsel=\r\nf is the answer, not the NN evaluation or activation output. The genome is =\r\npassed thru a translator and the evaluation is handled by a separate analys=\r\nis code. An example being the propulsion schematic of a rocket, where the g=\r\nenome directly represents the schematic, and the objective function is defi=\r\nned by how closely the resulting prediction matches desired performance on =\r\na weight and/or cost basis. The ability to seed the initial population with=\r\n previous &quot;simular-to&quot; designs/genomes, allows the incorporation human expe=\r\nrt input directly, and an obvious interpretation of best resulting genomes =\r\noutput. In this instance, the determination of directed/non-directed/multig=\r\nraph is moot within NEAT, the appropriate interpretation is made within the=\r\n translator prior to execution by the external analysis code, given proper =\r\nand adequate meta-data.\n&gt; &gt; \n&gt; &gt; In the other instance of direct evaluation=\r\n of the network, the generality on varying number of node inputs/output, ev=\r\naluation functions of nodes and embedded networks within a node can be addr=\r\nessed by interpretation of node type and genome definition nomenclature. It=\r\n would seem that the explosion in size of the search space would tend to be=\r\n addressed via the complexification process as additional dimensionality is=\r\n justified as a consequence of incremental improved objective function valu=\r\ne.\n&gt; &gt; \n&gt; &gt; Andy\n&gt; &gt; \n&gt; &gt; --- In neat@yahoogroups.com, &quot;petar_chervenski&quot; &lt;=\r\npetar_chervenski@&gt; wrote:\n&gt; &gt; &gt;\n&gt; &gt; &gt; Hi Andy, \n&gt; &gt; &gt; \n&gt; &gt; &gt; The implementa=\r\ntion is not yet complete, but things are really good so far. NetworkX is a =\r\nvery good choice for the genomes, as it gives me a lot of freedom, many bon=\r\nus functions for the graphs, and saves me debugging time. Basically this im=\r\nplementation allows any kind of graph to be evolved. For example, nodes and=\r\n edges can represent numeric data (integer & float) or objects from a set (=\r\nlike characters from an alphabet, class instances or functions, etc.). When=\r\n you define the graph type, you have to write a distance function if you ha=\r\nve objects from a set in the graph - like the distance between characters i=\r\nn the alphabet), and also a few more functions like mutators, which, of cou=\r\nrse, will change the objects randomly or the way you like. You can have any=\r\n number of properties for nodes/edges. You can also have an undirected grap=\r\nh, or a multigraph where many edges connect the same nodes. You can even ha=\r\nve nested genomes, where genomes are the objects from a set, which are atta=\r\nched to nodes or edges. This makes things mind-blowing and lifts NEAT to a =\r\nmuch broader set of domains. I even think that the N in NEAT is somehow unn=\r\necessary here, as the primary objective in this implementation is not neura=\r\nl networks. Neural networks will be derived from a special function that wi=\r\nll translate the graph and then build a C++ object from it. They will be di=\r\nrected graphs with sigmoid or whatever functions attached to nodes and floa=\r\nts attached to edges. (Node types also attached to nodes - to know what is =\r\ninput and output). Perhaps I&#39;ll make a separate project designed for neural=\r\n networks that will use the core module. \n&gt; &gt; &gt; I will release the first ve=\r\nrsion of the code soon, which will probably not have rtNEAT and novelty sea=\r\nrch built in. CPPNs and HyperNEAT are just special cases of graph evolution=\r\n and interpretation, like neural networks. The special code about them will=\r\n be added later as the project evolves. Perhaps the community will like it =\r\nand contribute some code. I can&#39;t promise a release date, but work is progr=\r\nessing. Any ideas to minimize the search space (which blows up as you add m=\r\nore properties to nodes and edges) are appreciated. Also I could use some h=\r\nelp about innovation numbers and crossover between undirected and multi gra=\r\nphs. I&#39;m so afraid of bugs in these cases that I haven&#39;t even started to th=\r\nink about it. :D\n&gt; &gt; &gt; \n&gt; &gt; &gt; Peter\n&gt; &gt; &gt; \n&gt; &gt; &gt; --- In neat@...=\r\nm, &quot;afcarl2&quot; &lt;afcarl2@&gt; wrote:\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; Hi Peter,\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; H=\r\now is your python implementation going? Took a look at the NetworkX module.=\r\n It looks very interesting! In my C++ version, I had added variable input/o=\r\nutput connections, network within a node and seeding of the initial populat=\r\nion with the required infrastructure updates and a distributed processing b=\r\nackend. But what you are doing goes so much farther, that I am eager to get=\r\n a look at it!\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; Andy\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; --- In neat@yahoogrou=\r\nps.com, &quot;petar_chervenski&quot; &lt;petar_chervenski@&gt; wrote:\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; H=\r\ni all, \n&gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; I am almost done with the basic code and I&#39;ll =\r\nmention some of its features now. I decided to use NetworkX for the genomes=\r\n, because this module has lots of useful algorithms and allows any python o=\r\nbject to be a node and edges can be associated with anything. This makes th=\r\ne evolution of neural networks a tiny part of what&#39;s really possible. Any g=\r\nraph can be evolved, including undirected graphs and nodes/edges containing=\r\n discrete one-of-N values (integers, lists of python objects, etc). So give=\r\nn that an evaluation function exists for any kind of graph, you can quickly=\r\n setup evolution. Neural networks are a particular kind of graphs and the p=\r\nackage will have built in code necessary to evolve neural networks - the in=\r\nitialization functions, mutators, and a C++ interface to a class that repre=\r\nsents the phenotypes. CPPNs support is trivial to make, and given that pyth=\r\non functions themselves can be attached to nodes, it&#39;s possible to have alg=\r\norithmic nodes working with more than one variable and .. well, infinite st=\r\nuff. OK, I gotta go. Wish me luck debugging. Talk to you soon. :) \n&gt; &gt; &gt; &gt; =\r\n&gt; \n&gt; &gt; &gt; &gt; &gt; Peter\n&gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; --- In neat@yahoogroups.com, Jan va=\r\nn der Lugt &lt;janlugt@&gt; wrote:\n&gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; Hi Peter,\n&gt; &gt; &gt; &gt; &gt; &gt; =\r\n\n&gt; &gt; &gt; &gt; &gt; &gt; Sound like an ambitious and noble plan. Good luck coding, I&#39;m =\r\nlooking\n&gt; &gt; &gt; &gt; &gt; &gt; forward to seeing your results!\n&gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; =\r\n&gt; Regards,\n&gt; &gt; &gt; &gt; &gt; &gt; Jan\n&gt; &gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &gt; On Mon, Mar 14, 2011 at=\r\n 12:43, petar_chervenski\n&gt; &gt; &gt; &gt; &gt; &gt; &lt;petar_chervenski@&gt;wrote:\n&gt; &gt; &gt; &gt; &gt; &gt; =\r\n\n&gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; Hi people,\n&gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;=\r\n &gt; &gt; &gt; For about a month I&#39;ll be writing a Python implementation of NEAT, w=\r\nhich\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; includes all advances in the recent years, including rtN=\r\nEAT, phased\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; searching, leaky integrators, HyperNEAT, HyperNEA=\r\nT with evolving substrates,\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; and novelty search. Coevolution c=\r\node and visualizations will be included.\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; Optimized C++ code f=\r\nor running the NNs too. This code will be free and I\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; promise =\r\nthis will be the best NEAT code I can write. No bugs, no meaningless\n&gt; &gt; &gt; =\r\n&gt; &gt; &gt; &gt; NNs, etc.\n&gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; Peter\n&gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt;=\r\n &gt; &gt;  \n&gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt;\n&gt;\n\n\n\n"}}