{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":37465196,"authorName":"Ken Lloyd","from":"&quot;Ken Lloyd&quot; &lt;kalloyd@...&gt;","profile":"kalloyd2","replyTo":"LIST","senderId":"5I-Lfja4R44khvjAPgHnRHbDjo10yIuRJXo8kfYWpQJXX4ABB-drrzg7HL9y2SpNwORn4iCO2rEtp0d6E4_-NUheQNpFJjSA","spamInfo":{"isSpam":false,"reason":"12"},"subject":"RE: [neat] New Publication Introduces Adaptive HyperNEAT (with Synaptic Plasticity)","postDate":"1276172119","msgId":5258,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDU1RDhDMjkxNzhFNjQxOTFCNUJBMjgzNTMwMjM4NTAwQHdhdHRkZXYxPg==","inReplyToHeader":"PEFBTkxrVGlrZTFlenVlTUVvX3lxOFRoT2xtN2dFMGpZSlZGNEF2TVVfNGJuMkBtYWlsLmdtYWlsLmNvbT4=","referencesHeader":"PEFBTkxrVGlrZTFlenVlTUVvX3lxOFRoT2xtN2dFMGpZSlZGNEF2TVVfNGJuMkBtYWlsLmdtYWlsLmNvbT4="},"prevInTopic":5257,"nextInTopic":0,"prevInTime":5257,"nextInTime":5259,"topicId":5202,"numMessagesInTopic":4,"msgSnippet":"Sebastian, You might want to take a look at this research. http://www.sciencedaily.com/releases/2010/06/100609122828.htm The ramifications in ANNs may be seen","rawEmail":"Return-Path: &lt;kalloyd@...&gt;\r\nX-Sender: kalloyd@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 74878 invoked from network); 10 Jun 2010 12:15:25 -0000\r\nX-Received: from unknown (98.137.34.45)\n  by m13.grp.re1.yahoo.com with QMQP; 10 Jun 2010 12:15:25 -0000\r\nX-Received: from unknown (HELO qmta03.emeryville.ca.mail.comcast.net) (76.96.30.32)\n  by mta2.grp.sp2.yahoo.com with SMTP; 10 Jun 2010 12:15:24 -0000\r\nX-Received: from omta04.emeryville.ca.mail.comcast.net ([76.96.30.35])\n\tby qmta03.emeryville.ca.mail.comcast.net with comcast\n\tid UC4h1e0030lTkoCA3CFQEe; Thu, 10 Jun 2010 12:15:24 +0000\r\nX-Received: from wattdev1 ([174.56.66.94])\n\tby omta04.emeryville.ca.mail.comcast.net with comcast\n\tid UCFP1e001221HGW8QCFP3T; Thu, 10 Jun 2010 12:15:24 +0000\r\nTo: &lt;neat@yahoogroups.com&gt;\r\nReferences: &lt;AANLkTike1ezueMEo_yq8ThOlm7gE0jYJVF4AvMU_4bn2@...&gt;\r\nDate: Thu, 10 Jun 2010 06:15:19 -0600\r\nMessage-ID: &lt;55D8C29178E64191B5BA283530238500@wattdev1&gt;\r\nMIME-Version: 1.0\r\nContent-Type: multipart/alternative;\n\tboundary=&quot;----=_NextPart_000_080F_01CB0864.4DF206D0&quot;\r\nX-Mailer: Microsoft Office Outlook 11\r\nX-MimeOLE: Produced By Microsoft MimeOLE V6.00.2900.5931\r\nIn-Reply-To: &lt;AANLkTike1ezueMEo_yq8ThOlm7gE0jYJVF4AvMU_4bn2@...&gt;\r\nThread-Index: AcsIWOREsj5qnFjwTZOTziWvJ4+9PwAPTydA\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: &quot;Ken Lloyd&quot; &lt;kalloyd@...&gt;\r\nSubject: RE: [neat] New Publication Introduces Adaptive HyperNEAT (with Synaptic Plasticity)\r\nX-Yahoo-Group-Post: member; u=37465196; y=2YgJ6r1XZSxBaK59GlqN7eAOBv7ieLXb2wGUceHLkECgy1c\r\nX-Yahoo-Profile: kalloyd2\r\n\r\n\r\n------=_NextPart_000_080F_01CB0864.4DF206D0\r\nContent-Type: text/plain;\n\tcharset=&quot;iso-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\nSebastian,\n \nYou might want to take a look at this research.\n \nhttp://www.s=\r\nciencedaily.com/releases/2010/06/100609122828.htm\n \nThe ramifications in AN=\r\nNs may be seen in recurrent input nodes where no\nhidden node seems to be ne=\r\ncessary (conjecture).\n\n  _____  \n\nFrom: neat@yahoogroups.com [mailto:neat@y=\r\nahoogroups.com] On Behalf Of\nSebastian Risi\nSent: Wednesday, June 09, 2010 =\r\n10:53 PM\nTo: neat@yahoogroups.com\nSubject: Re: [neat] New Publication Intro=\r\nduces Adaptive HyperNEAT (with\nSynaptic Plasticity)\n\n\n  \n\nHi Jeff,\n\nsorry f=\r\nor the late response. Let me try to adress your questions:\n\n1) You mention =\r\na few times that in natural brains different regions have\ndifferent plastic=\r\nity rules, but you do not provide a cite. I am guessing you\nare correct, bu=\r\nt I was just wondering if you could tell us more about how\nyou know that (i=\r\n.e., what the evidence is, or a cite where I can read about\nit, etc.). I do=\r\n know that different neuromodulatory centers control\ndifferent regions of t=\r\nhe brain, so that could potentially be an\nexample...but I was just wonderin=\r\ng if you had other sources. If so, I would\nbe interested to hear about them=\r\n.\n\nOne example of different plasticity rules is long-term potentiation\n(htt=\r\np://en.wikipedia &lt;http://en.wikipedia.org/wiki/Long-term_potentiation&gt;\n.org=\r\n/wiki/Long-term_potentiation) that is mostly observed\nin the hippocampus an=\r\nd not so much in other areas of the brain, I believe. I\nwould recommend to =\r\ntake a look at &quot;Principles of Neural Science&quot; by Kandel.\nIt&#39;s a very good b=\r\nook but kind of a hard read.\n\n2) You mention that &quot;for all hyperneat models=\r\n synaptic strength is bound\nwithin the range [-1.0, 1.0].&quot; Why did you make=\r\n this choice? Isn&#39;t the range\nnormally [-3, 3]?\n\nBecause the delta weight c=\r\nhange of the iterated model is bound to be within\n[-1.0, 1.0] we decided th=\r\nat the weights should also be bounded between\n[-1.0, 1.0]. I haven&#39;t tried =\r\nit with [-3, 3] but it should still work in\nthat case I believe.\n\n3) In the=\r\n second paragraph of the results, you list that the iterated model\ntook 89 =\r\ngens. Do you mean 189? Otherwise the plots tell a different story,\nunless I=\r\n am misunderstanding something.\n\nThe iterated model took indeed on average =\r\n89 generations to find a solution.\nAt around 200 generations all of the run=\r\ns found a solution which means\nthat the average reaches 395 fitness which c=\r\nan be seen in the figure when\nthe graph reaches the horizontal line. Does t=\r\nhat make sense? \n\n4) Do you know why the plain Hebbian rule cannot solve th=\r\ne task?\n\nThe plain Hebbian rule strengthens the synaptic weights if pre and=\r\n\npostsynaptic activity correlate. Because the reward in the maze \nis given =\r\nwith a delay from the action of choosing which way to turn\ncorrelation alon=\r\ne might not be sufficient to solve this domain. Andrea \nSoltoggio actually =\r\nhas an interesting paper about the type of learning rules\nthat are necessar=\r\ny to solve the T-Maze and bee domain, in case you&#39;re\ninterested. http://lis=\r\n.epfl.\n&lt;http://lis.epfl.ch/~soltoggio/Papers/SoltoggioHIS2008.pdf&gt;\nch/~solt=\r\noggio/Papers/SoltoggioHIS2008.pdf\n\n5) You mention that it might be the case=\r\n that an ANN *with hidden nodes*\ncould have solved the task without the lea=\r\nrning rule performing the\ncomplicated (xor-equivalent?) computation. Out of=\r\n curiosity, did you try\nthat? \n\nWe actually never tried that.\n\n6) Can you e=\r\nxplain (or speculate) how the CPPN network is solving this task?\nI assume (=\r\nsince there is no recurrence), that it is simply memorizing which\nnumbers r=\r\nepresent a low reward (e.g. 0 or .8) instead of comparing the last\nreward t=\r\no the current reward. Does that seem right? To compare to the\nprevious tria=\r\nl, would they need recurrence, or is it possible to embed\ninformation in th=\r\ne connections via learning that can function like\nrecurrence in terms of st=\r\noring information? Do you know if something like\nthat is going on?\n\nThat is=\r\n a tricky question :) The weight change actually seems to allow the\nagent t=\r\no remember information from one trial to the next. All the evolved\nANNs are=\r\n notrecurrent so they actually depend on this kind of plasticity. At\nthis p=\r\noint I&#39;m not 100% sure how the evolved CPPN rule separates the\nnonlinear se=\r\nparable reward signatures but that&#39;s definitely an interesting\nquestion for=\r\n future research.\n\n7) You say that the evolved rules resemble postsynaptic-=\r\nbased learning rules\nthat have been shown essential in the T-Maze domain, a=\r\nnd cite Andrea. Can\nyou elaborate on this a bit more? How were they similar=\r\n? Wouldn&#39;t good\npostsynaptic learning rules depend on presynaptic and corre=\r\nlation rules\n(values) as well? Is it surprising to just see similar postyna=\r\nptic values\nand not similar values for the other parameters?\n\nAndrea actual=\r\nly tested different ABC rules and found that the C, AC, BC and\nABC rule can=\r\n solve the T-Maze task. So only C is in fact necessary, at least\nfor that p=\r\narticular domain. I think that the correlation term is not really\nimportant=\r\n in the T-Maze because action and reward intake can&#39;t be directly\ncorrelate=\r\nd. But there are other domains, like the foraging bee domain, that\ncritical=\r\nly depend on the term A. So it will be interesting to try the\niterated mode=\r\nl in one of those other domains to see what kind of learning\nrules it evolv=\r\nes.\n\nCheers,\nSebastian\n\n\n\n\n\r\n------=_NextPart_000_080F_01CB0864.4DF206D0\r\nContent-Type: text/html;\n\tcharset=&quot;iso-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\n&lt;!DOCTYPE HTML PUBLIC &quot;-//W3C//DTD HTML 4.01 Transitional//EN&quot; &quot;http://www.=\r\nw3c.org/TR/1999/REC-html401-19991224/loose.dtd&quot;&gt;\n&lt;HTML&gt;&lt;HEAD&gt;\n&lt;META http-eq=\r\nuiv=3DContent-Type content=3D&quot;text/html; charset=3Diso-8859-1&quot;&gt;\n&lt;META conte=\r\nnt=3D&quot;MSHTML 6.00.6000.17063&quot; name=3DGENERATOR&gt;&lt;/HEAD&gt;\n&lt;BODY style=3D&quot;BACKG=\r\nROUND-COLOR: #fff&quot;&gt;\n&lt;DIV dir=3Dltr align=3Dleft&gt;&lt;SPAN class=3D828031212-100=\r\n62010&gt;&lt;FONT face=3DArial \ncolor=3D#0000ff size=3D2&gt;Sebastian,&lt;/FONT&gt;&lt;/SPAN&gt;=\r\n&lt;/DIV&gt;\n&lt;DIV dir=3Dltr align=3Dleft&gt;&lt;SPAN class=3D828031212-10062010&gt;&lt;FONT f=\r\nace=3DArial \ncolor=3D#0000ff size=3D2&gt;&lt;/FONT&gt;&lt;/SPAN&gt;&nbsp;&lt;/DIV&gt;\n&lt;DIV dir=\r\n=3Dltr align=3Dleft&gt;&lt;SPAN class=3D828031212-10062010&gt;&lt;FONT face=3DArial \nco=\r\nlor=3D#0000ff size=3D2&gt;You might want to take a look at this \nresearch.&lt;/FO=\r\nNT&gt;&lt;/SPAN&gt;&lt;/DIV&gt;\n&lt;DIV dir=3Dltr align=3Dleft&gt;&lt;SPAN class=3D828031212-100620=\r\n10&gt;&lt;FONT face=3DArial \ncolor=3D#0000ff size=3D2&gt;&lt;/FONT&gt;&lt;/SPAN&gt;&nbsp;&lt;/DIV&gt;\n=\r\n&lt;DIV dir=3Dltr align=3Dleft&gt;&lt;SPAN class=3D828031212-10062010&gt;&lt;FONT face=3DA=\r\nrial \ncolor=3D#0000ff size=3D2&gt;&lt;A \nhref=3D&quot;http://www.sciencedaily.com/rele=\r\nases/2010/06/100609122828.htm&quot;&gt;http://www.sciencedaily.com/releases/2010/06=\r\n/100609122828.htm&lt;/A&gt;&lt;/FONT&gt;&lt;/SPAN&gt;&lt;/DIV&gt;\n&lt;DIV dir=3Dltr align=3Dleft&gt;&lt;SPAN=\r\n class=3D828031212-10062010&gt;&lt;FONT face=3DArial \ncolor=3D#0000ff size=3D2&gt;&lt;/=\r\nFONT&gt;&lt;/SPAN&gt;&nbsp;&lt;/DIV&gt;\n&lt;DIV dir=3Dltr align=3Dleft&gt;&lt;SPAN class=3D82803121=\r\n2-10062010&gt;&lt;FONT face=3DArial \ncolor=3D#0000ff size=3D2&gt;The ramifications i=\r\nn ANNs may be seen in recurrent input \nnodes where no hidden node seems to =\r\nbe necessary \n(conjecture).&lt;/FONT&gt;&lt;/SPAN&gt;&lt;/DIV&gt;&lt;BR&gt;\n&lt;DIV class=3DOutlookMes=\r\nsageHeader lang=3Den-us dir=3Dltr align=3Dleft&gt;\n&lt;HR tabIndex=3D-1&gt;\n&lt;FONT fa=\r\nce=3DTahoma size=3D2&gt;&lt;B&gt;From:&lt;/B&gt; neat@yahoogroups.com \n[mailto:neat@yahoog=\r\nroups.com] &lt;B&gt;On Behalf Of &lt;/B&gt;Sebastian Risi&lt;BR&gt;&lt;B&gt;Sent:&lt;/B&gt; \nWednesday, J=\r\nune 09, 2010 10:53 PM&lt;BR&gt;&lt;B&gt;To:&lt;/B&gt; \nneat@yahoogroups.com&lt;BR&gt;&lt;B&gt;Subject:&lt;/B=\r\n&gt; Re: [neat] New Publication Introduces \nAdaptive HyperNEAT (with Synaptic =\r\nPlasticity)&lt;BR&gt;&lt;/FONT&gt;&lt;BR&gt;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;/DIV&gt;&lt;SPAN style=3D&quot;DISPLAY: none&quot;&gt;&=\r\nnbsp;&lt;/SPAN&gt; \n&lt;DIV id=3Dygrp-text&gt;\n&lt;P&gt;Hi Jeff,&lt;BR&gt;&lt;BR&gt;sorry for the late re=\r\nsponse. Let me try to adress your \nquestions:&lt;BR&gt;&lt;BR&gt;1) You mention a few t=\r\nimes that in natural brains different \nregions have&lt;BR&gt;different plasticity=\r\n rules, but you do not provide a cite. I am \nguessing you&lt;BR&gt;are correct, b=\r\nut I was just wondering if you could tell us more \nabout how&lt;BR&gt;you know th=\r\nat (i.e., what the evidence is, or a cite where I can \nread about&lt;BR&gt;it, et=\r\nc.). I do know that different neuromodulatory centers \ncontrol&lt;BR&gt;different=\r\n regions of the brain, so that could potentially be \nan&lt;BR&gt;example...but I =\r\nwas just wondering if you had other sources. If so, I \nwould&lt;BR&gt;be interest=\r\ned to hear about them.&lt;BR&gt;&lt;BR&gt;One example of different \nplasticity rules is=\r\n long-term potentiation (&lt;A \nhref=3D&quot;http://en.wikipedia.org/wiki/Long-term=\r\n_potentiation&quot;&gt;http://en.wikipedia&lt;WBR&gt;.org/wiki/&lt;WBR&gt;Long-term_&lt;WBR&gt;potent=\r\niation&lt;/A&gt;) \nthat is mostly observed&lt;BR&gt;in the hippocampus and not so much =\r\nin other areas of \nthe brain, I believe. I would recommend to take a look a=\r\nt &quot;Principles of Neural \nScience&quot; by Kandel. It&#39;s a very good book but kind=\r\n of a hard read.&lt;BR&gt;&lt;BR&gt;2) You \nmention that &quot;for all hyperneat models syna=\r\nptic strength is bound&lt;BR&gt;within the \nrange [-1.0, 1.0].&quot; Why did you make =\r\nthis choice? Isn&#39;t the range&lt;BR&gt;normally \n[-3, 3]?&lt;BR&gt;&lt;BR&gt;Because the delta=\r\n weight change of the iterated model is bound \nto be within&lt;BR&gt;[-1.0, 1.0] =\r\nwe decided that the weights should also be bounded \nbetween [-1.0, 1.0]. I =\r\nhaven&#39;t tried it with [-3, 3] but it should still work in \nthat case I beli=\r\neve.&lt;BR&gt;&lt;BR&gt;3) In the second paragraph of the results, you list \nthat the i=\r\nterated model&lt;BR&gt;took 89 gens. Do you mean 189? Otherwise the plots \ntell a=\r\n different story,&lt;BR&gt;unless I am misunderstanding something.&lt;BR&gt;&lt;BR&gt;The \nit=\r\nerated model took indeed on average 89 generations to find a solution.&lt;BR&gt;A=\r\nt \naround 200 generations all of the runs found a solution which means&lt;BR&gt;t=\r\nhat the \naverage reaches 395 fitness which can be seen in the figure when&lt;B=\r\nR&gt;the graph \nreaches the horizontal line. Does that make sense? &lt;BR&gt;&lt;BR&gt;4) =\r\nDo you know why \nthe plain Hebbian rule cannot solve the task?&lt;BR&gt;&lt;BR&gt;The p=\r\nlain Hebbian rule \nstrengthens the synaptic weights if pre and postsynaptic=\r\n activity correlate. \nBecause the reward in the maze &lt;BR&gt;is given with a de=\r\nlay from the action of \nchoosing which way to turn&lt;BR&gt;correlation alone mig=\r\nht not be sufficient to solve \nthis domain. Andrea &lt;BR&gt;Soltoggio actually h=\r\nas an interesting paper about the \ntype of learning rules&lt;BR&gt;that are neces=\r\nsary to solve the T-Maze and bee domain, \nin case you&#39;re interested. &lt;A \nhr=\r\nef=3D&quot;http://lis.epfl.ch/~soltoggio/Papers/SoltoggioHIS2008.pdf&quot;&gt;http://lis=\r\n.epfl.&lt;WBR&gt;ch/~soltoggio/&lt;WBR&gt;Papers/Soltoggio&lt;WBR&gt;HIS2008.pdf&lt;/A&gt;&lt;BR&gt;&lt;BR&gt;5=\r\n) \nYou mention that it might be the case that an ANN *with hidden nodes*&lt;BR=\r\n&gt;could \nhave solved the task without the learning rule performing the&lt;BR&gt;co=\r\nmplicated \n(xor-equivalent?&lt;WBR&gt;) computation. Out of curiosity, did you tr=\r\ny&lt;BR&gt;that? \n&lt;BR&gt;&lt;BR&gt;We actually never tried that.&lt;BR&gt;&lt;BR&gt;6) Can you explain=\r\n (or speculate) \nhow the CPPN network is solving this task?&lt;BR&gt;I assume (si=\r\nnce there is no \nrecurrence), that it is simply memorizing which&lt;BR&gt;numbers=\r\n represent a low \nreward (e.g. 0 or .8) instead of comparing the last&lt;BR&gt;re=\r\nward to the current \nreward. Does that seem right? To compare to the&lt;BR&gt;pre=\r\nvious trial, would they \nneed recurrence, or is it possible to embed&lt;BR&gt;inf=\r\normation in the connections \nvia learning that can function like&lt;BR&gt;recurre=\r\nnce in terms of storing \ninformation? Do you know if something like&lt;BR&gt;that=\r\n is going on?&lt;BR&gt;&lt;BR&gt;That is a \ntricky question :) The weight change actual=\r\nly seems to allow the agent to \nremember information from one trial to the =\r\nnext. All the evolved ANNs are \nnotrecurrent so they actually depend on thi=\r\ns kind of plasticity. At this point \nI&#39;m not 100% sure how the evolved CPPN=\r\n rule separates the nonlinear separable \nreward signatures but that&#39;s defin=\r\nitely an interesting question for future \nresearch.&lt;BR&gt;&lt;BR&gt;7) You say that =\r\nthe evolved rules resemble \npostsynaptic-&lt;WBR&gt;based learning rules&lt;BR&gt;that =\r\nhave been shown essential in the \nT-Maze domain, and cite Andrea. Can&lt;BR&gt;yo=\r\nu elaborate on this a bit more? How \nwere they similar? Wouldn&#39;t good&lt;BR&gt;po=\r\nstsynaptic learning rules depend on \npresynaptic and correlation rules&lt;BR&gt;(=\r\nvalues) as well? Is it surprising to just \nsee similar postynaptic values&lt;B=\r\nR&gt;and not similar values for the other \nparameters?&lt;BR&gt;&lt;BR&gt;Andrea actually =\r\ntested different ABC rules and found that the \nC, AC, BC and ABC rule can s=\r\nolve the T-Maze task. So only C is in fact \nnecessary, at least for that pa=\r\nrticular domain. I think that the correlation \nterm is not really important=\r\n in the T-Maze because action and reward intake \ncan&#39;t be directly correlat=\r\ned. But there are other domains, like the foraging bee \ndomain, that critic=\r\nally depend on the term A. So it will be interesting to try \nthe iterated m=\r\nodel in one of those other domains to see what kind of learning \nrules it e=\r\nvolves.&lt;BR&gt;&lt;BR&gt;Cheers,&lt;BR&gt;Sebastian&lt;BR&gt;&lt;BR&gt;&lt;/P&gt;&lt;/DIV&gt;&lt;!-- end group email -=\r\n-&gt;&lt;/BODY&gt;&lt;/HTML&gt;\n\r\n------=_NextPart_000_080F_01CB0864.4DF206D0--\r\n\n"}}