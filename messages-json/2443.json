{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":181688725,"authorName":"yurifromtomsk","from":"&quot;yurifromtomsk&quot; &lt;neuroevolution@...&gt;","profile":"yurifromtomsk","replyTo":"LIST","senderId":"76d0UDJLYg_MuYBEqa6Jm15dCRYm_bsoGiy2qOG9-OH_WMfqn6uVGnMfnWNAGG_kEL_nbQ2FQ6yXhH6aRuQaMQELdq4o4niRW-9B5Epb3Q","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: question about the topology of ANN","postDate":"1133228551","msgId":2443,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGRtZ2JtNytzNW1iQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDQzOEI2RDE3LjgwMjA0MDVAZHNsLnBpcGV4LmNvbT4="},"prevInTopic":2442,"nextInTopic":2444,"prevInTime":2442,"nextInTime":2444,"topicId":2441,"numMessagesInTopic":7,"msgSnippet":"Hi! First of all, sorry if I ll do many mistakes in my English. I just hope that they won t be crucial for understanding. And now, the ... Since ... fact ... ","rawEmail":"Return-Path: &lt;neuroevolution@...&gt;\r\nX-Sender: neuroevolution@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 41028 invoked from network); 29 Nov 2005 01:43:09 -0000\r\nReceived: from unknown (66.218.66.167)\n  by m25.grp.scd.yahoo.com with QMQP; 29 Nov 2005 01:43:09 -0000\r\nReceived: from unknown (HELO n1a.bulk.scd.yahoo.com) (66.94.237.35)\n  by mta6.grp.scd.yahoo.com with SMTP; 29 Nov 2005 01:43:09 -0000\r\nComment: DomainKeys? See http://antispam.yahoo.com/domainkeys\r\nReceived: from [66.218.69.5] by n1.bullet.scd.yahoo.com with NNFMP; 29 Nov 2005 01:42:34 -0000\r\nReceived: from [66.218.66.80] by mailer5.bulk.scd.yahoo.com with NNFMP; 29 Nov 2005 01:42:34 -0000\r\nDate: Tue, 29 Nov 2005 01:42:31 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;dmgbm7+s5mb@...&gt;\r\nIn-Reply-To: &lt;438B6D17.8020405@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: &quot;yurifromtomsk&quot; &lt;neuroevolution@...&gt;\r\nSubject: Re: question about the topology of ANN\r\nX-Yahoo-Group-Post: member; u=181688725; y=I-DoJEZfQs6AlaTjfVnj8o7EmrjLHwzPfOUgTzFmmJBa6EuODmAeTA\r\nX-Yahoo-Profile: yurifromtomsk\r\n\r\nHi!\n\nFirst of all, sorry if I&#39;ll do many mistakes in my English. I just \nho=\r\npe that they won&#39;t be crucial for understanding. And now, the \nanswer:\n\n---=\r\n In neat@yahoogroups.com, Colin Green &lt;cgreen@d...&gt; wrote:\n&gt;\n&gt; myhabalixyp =\r\nwrote:\n&gt; \n&gt; &gt;As we know, NEAT gets a abnormity topology structure of a ANN =\r\nfor a \n&gt; &gt;special function not similar with the traditional BP networks. \nS=\r\nince \n&gt; &gt;the traditional BP networks are constructed with clear layers.\n&gt; &gt;=\r\n\n&gt; Firstly I don&#39;t think networks *have* to be arranged in layers for \n&gt; ba=\r\nck-propagation to work, it&#39;s just traditional and easy to do. In \nfact \n&gt; I=\r\n&#39;m not sure if they even have to be feedforward. As such it would \nbe \n&gt; en=\r\ntirely feasible to use something like NEAT to search topolgies and \n&gt; then =\r\nuse back-prop to find connection weights.\n\nYes, backprop can be easily appl=\r\nied to train not only multi-layered \nnets but also ANN with an arbitrary fe=\r\nedforward topology. And moreover \nI also read elsewhere that recurrent ANNS=\r\n can be trained using \ngradient-based algorithms too.\n \n&gt; &gt; Although the ne=\r\ntworks \n&gt; &gt;with layers have more &quot;complex&quot; topology, them have regular \nalg=\r\norithm \n&gt; &gt;and can workout a output more quickly with less memory. What \nbe=\r\nnefit do \n&gt; &gt;the abnormity topology structure networks bring to us? Only be=\r\ncaues \n&gt; &gt;they are &quot;simpler&quot; only when we see them with our eyes?\n&gt; &gt;  \n&gt; &gt;=\r\n\n&gt; The benefit is that they represent the complete set of possible \n&gt; topol=\r\ngies. Layered feed forward networks are just a sub-set of the \n&gt; possible t=\r\nopologies and as such are such are incapable of performing \n&gt; some tasks. I=\r\n think I see your point though, one argument for using \nNEAT \n&gt; is to find =\r\ncompact topologies, but in searching for a compact \ntopology \n&gt; we actually=\r\n can end up with a lot of redundant structure. Now if you \n&gt; know approxima=\r\ntely the structure required to solve a given problem \nthen \n&gt; it will be mo=\r\nre efficient to constrain the toplogy search somehow, \nwhich \n&gt; might be an=\r\n intersting research angle. In the general case though we \n&gt; have no idea w=\r\nhat topology is required (competetive co-evolution \n&gt; experiments for examp=\r\nle) and so the open ended-ness of NEAT makes \nsense.\n\nAs far as I know, net=\r\nworks with simpler topology tend to have better \ngeneralization abilities w=\r\nhen solving classification tasks. That means \nthat such networks efficientl=\r\ny &quot;extract&quot; some general features and \ndependencies of the training data an=\r\nd thus produce more stable output \nsignal in presence of noisy test data. W=\r\nhen applied to neurocontrol \ntask it means that &quot;complex&quot; ANN is likely to =\r\nproduce unstable signal \nwhich can destabilize the object under control.\n\nB=\r\nut note that &quot;simpler&quot; network doesn&#39;t always lead to better \nperformance b=\r\necause the main goal of the ANN training is to obtain an \nadequate neuro-mo=\r\ndel. The point here is that every time when one is \ntrying to apply ANNs to=\r\n solve some problem, it is assumed that there \nare some functional dependen=\r\ncies between input and output signals. I.e \nthat there is some input-output=\r\n mapping which is to be approximated by \nANN in result of training. It&#39;s ob=\r\nvious that topology of ANN have \ngreat influence on the resulting I-O mappi=\r\nng, specifically, on the \nnon-linear features of this mapping. Multi-layer =\r\nnets tend to be \nredundant (because of &quot;oversimplification&quot; of topology sea=\r\nrch problem) \nand thus tend to produce highly non-linear mapping. That&#39;s wh=\r\ny the \nsearch for the simpler ANN&#39;s topology is promising (at least as I \nu=\r\nnderstand it). There are some techniques to simplify the topology of \nANNs,=\r\n which I think would be interesting for readers of this group. \nThe most kn=\r\nown technique is called Optimal Brain Damage (OBD) by Yann \nLeCun, which ca=\r\nn be found here:\nhttp://yann.lecun.com/\n(there are also several interesting=\r\n humour notes :))\n\nYuri\n\n\n\n\n"}}