{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":181688725,"authorName":"yurifromtomsk","from":"&quot;yurifromtomsk&quot; &lt;neuroevolution@...&gt;","profile":"yurifromtomsk","replyTo":"LIST","senderId":"hRX3ukPhxnCNql31T6L56ateqhVrC1THzcxnIlKx5mrR6a5GMHgU0JWN8ezKN-Skw_E9sRaU5gdS9i4GmYX4wkd4WESj1uaQnKkloQnxrQ","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Overlearning","postDate":"1166239126","msgId":2901,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGVsdm9pbStmdGRpQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDAxYzkwMWM3MWZlMSRjMGRiMDNmMCQ2Zjk5Zjc4Y0BTTUFSVENISVA+"},"prevInTopic":2900,"nextInTopic":0,"prevInTime":2900,"nextInTime":2902,"topicId":2895,"numMessagesInTopic":3,"msgSnippet":"Hi, Richard, Overlearning is a general problem in ANN s training so if someone could solve it he would be worth given a medal :) You can find information in","rawEmail":"Return-Path: &lt;neuroevolution@...&gt;\r\nX-Sender: neuroevolution@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 88342 invoked from network); 16 Dec 2006 03:19:20 -0000\r\nReceived: from unknown (66.218.67.36)\n  by m24.grp.scd.yahoo.com with QMQP; 16 Dec 2006 03:19:20 -0000\r\nReceived: from unknown (HELO n31a.bullet.scd.yahoo.com) (209.73.160.90)\n  by mta10.grp.scd.yahoo.com with SMTP; 16 Dec 2006 03:19:20 -0000\r\nReceived: from [66.218.69.1] by n31.bullet.scd.yahoo.com with NNFMP; 16 Dec 2006 03:18:46 -0000\r\nReceived: from [66.218.66.81] by t1.bullet.scd.yahoo.com with NNFMP; 16 Dec 2006 03:18:46 -0000\r\nDate: Sat, 16 Dec 2006 03:18:46 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;elvoim+ftdi@...&gt;\r\nIn-Reply-To: &lt;01c901c71fe1$c0db03f0$6f99f78c@SMARTCHIP&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0\r\nFrom: &quot;yurifromtomsk&quot; &lt;neuroevolution@...&gt;\r\nSubject: Re: Overlearning\r\nX-Yahoo-Group-Post: member; u=181688725; y=HFUCiZ0ulgjmolzRVa_i_782bUA77z3ZqoxDFaVp9FLJxdCjBaoubA\r\nX-Yahoo-Profile: yurifromtomsk\r\n\r\nHi, Richard,\n\nOverlearning is a general problem in ANN&#39;s training so if som=\r\neone \ncould solve it he would be worth given a medal :) You can find \ninfor=\r\nmation in papers and books that ANNs with compact structures are \ngood gene=\r\nralizators, but, in my opinion, this question is still open \nbecause there =\r\nare papers with right opposite results and in real tasks \nin most cases nob=\r\nody can tell your apriory exactly what structure will \nbe the best. This is=\r\n just because separation bounds of different \nclasses are unknown and in la=\r\nrge ANNs extra neurons can be used to \nstabilize &quot;working&quot; neurons. There a=\r\nre some results from statistical \nlearning theory concerning toplogical com=\r\nplexity of ANN, for example \nVCDim (Vapnik-Chervonenkis&#39;s Dimension):\n\n2[K/=\r\n2]N &lt;=3D VCDim &lt;=3D 2N_w(1+lg{N_n}),\n\nwhere [] -- means convertion to integ=\r\ner, K -- numer of nodes in hidden \nlayer, N -- dimension of input vector, N=\r\n_w -- overall number of \nweights in ANN, N_n -- overall number of neurons.\n=\r\n\nBut this is a heuristic for SVM and RBF. Moreover VCDim doesn&#39;t \nconsider =\r\nshorcut connections which tend to appear in result of NEAT or \nanother NE a=\r\nlgorithms that search topology of ANN without strict \nconstraints. I&#39;m sure=\r\n that there should be novel mathematical results \nin this field but I suspe=\r\nct they don&#39;t solve the whole problem due to \ngreat variety of tasks and un=\r\ncertainity of data in many of them.\n\nAnyway, you can try early stopping tec=\r\nhnique. For this a validation \nset of data is needed (in addition to traini=\r\nng and test sets). You can \nform validation set simply by picking some samp=\r\nles from training set \nand some samples from the test set. But ensure that =\r\nresulting \ntraining, validation and test sets are representative enough for=\r\n \ncompetent learning.\n\nThe early stopping implementation have the following=\r\n steps:\n1. ANN training for X generations using training data set.\n2. Valid=\r\nation of current population using validation set. If average \nfitness is no=\r\nt worse than after the training step than proceed to Step \n1 otherwise we c=\r\nonsider that ANN is overlearning the training data and \nproceed to Step 3.\n=\r\n3. Stop\n\nGood luck,\nYury\n\n\n--- In neat@yahoogroups.com, &quot;Richard Gong&quot; &lt;gon=\r\ngrichard@...&gt; wrote:\n&gt;\n&gt; Hi,\n&gt; \n&gt; I&#39;m not sure how to prevent NEAT from ove=\r\nrtraining. Are there any \nideas about this? Thanks a lot.\n&gt; \n&gt; -Richard\n&gt;\n\n=\r\n\n\n"}}