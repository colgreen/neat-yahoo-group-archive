{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":467342474,"authorName":"Jeff Clune","from":"Jeff Clune &lt;jeffclune@...&gt;","profile":"jeffreyclune","replyTo":"LIST","senderId":"oI4rOdSJykP9Flmly4mPqB8X6tzi4yk973e0ffbYBEajSlXi5WjMLZbpSVzSWIvSGxrRQNfHBkduHYrVZmBqLpcjJSh3Se6wLg","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] New Paper Comparing a New Encoding to HyperNEAT w.r.t. Network Properties (modularity, scale-free, regularity, scalability, etc.)","postDate":"1308170960","msgId":5591,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PENBMUQ5RTM0LjNCNEFFJWplZmZjbHVuZUBjb3JuZWxsLmVkdT4=","inReplyToHeader":"PDRERjJCNjJFLjQwNTA1MDlAZS1nZWVrLmNvbS5hdT4="},"prevInTopic":5587,"nextInTopic":0,"prevInTime":5590,"nextInTime":5592,"topicId":5559,"numMessagesInTopic":5,"msgSnippet":"Hello Oliver, Thanks for the kind words about our paper. I m glad you liked it.  Please see my responses to your questions below. ... Marcin was the one who","rawEmail":"Return-Path: &lt;jclune@...&gt;\r\nX-Sender: jclune@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 86360 invoked from network); 15 Jun 2011 20:49:40 -0000\r\nX-Received: from unknown (66.196.94.106)\n  by m10.grp.re1.yahoo.com with QMQP; 15 Jun 2011 20:49:40 -0000\r\nX-Received: from unknown (HELO mail-vw0-f49.google.com) (209.85.212.49)\n  by mta2.grp.re1.yahoo.com with SMTP; 15 Jun 2011 20:49:40 -0000\r\nX-Received: by mail-vw0-f49.google.com with SMTP id 8so899814vws.8\n        for &lt;neat@yahoogroups.com&gt;; Wed, 15 Jun 2011 13:49:40 -0700 (PDT)\r\nX-Received: by 10.52.176.74 with SMTP id cg10mr75600vdc.242.1308170980083;\n        Wed, 15 Jun 2011 13:49:40 -0700 (PDT)\r\nReturn-Path: &lt;jclune@...&gt;\r\nX-Received: from [128.84.62.117] (rrdhcp62-117.redrover.cornell.edu [128.84.62.117])\n        by mx.google.com with ESMTPS id j4sm327096vdu.19.2011.06.15.13.49.31\n        (version=TLSv1/SSLv3 cipher=OTHER);\n        Wed, 15 Jun 2011 13:49:37 -0700 (PDT)\r\nUser-Agent: Microsoft-MacOutlook/14.0.0.100825\r\nDate: Wed, 15 Jun 2011 16:49:20 -0400\r\nTo: &quot;neat@yahoogroups.com&quot; &lt;neat@yahoogroups.com&gt;\r\nMessage-ID: &lt;CA1D9E34.3B4AE%jeffclune@...&gt;\r\nThread-Topic: [neat] New Paper Comparing a New Encoding to HyperNEAT w.r.t.\n Network Properties (modularity, scale-free, regularity, scalability, etc.)\r\nIn-Reply-To: &lt;4DF2B62E.4050509@...&gt;\r\nMime-version: 1.0\r\nContent-type: multipart/alternative;\n\tboundary=&quot;B_3391001374_713881&quot;\r\nFrom: Jeff Clune &lt;jeffclune@...&gt;\r\nSubject: Re: [neat] New Paper Comparing a New Encoding to HyperNEAT w.r.t.\n Network Properties (modularity, scale-free, regularity, scalability, etc.)\r\nX-Yahoo-Group-Post: member; u=467342474; y=VJMNuagxTQKzBLqP8zBy4-mO-TgKzG357esTfaIZ80u7znKSNPSA\r\nX-Yahoo-Profile: jeffreyclune\r\n\r\n\r\n--B_3391001374_713881\r\nContent-type: text/plain;\n\tcharset=&quot;ISO-8859-1&quot;\r\nContent-transfer-encoding: quoted-printable\r\n\r\nHello Oliver, \n\nThanks for the kind words about our paper. I&#39;m glad you lik=\r\ned it.  Please\nsee my responses to your questions below.\n\n&gt; \n&gt;  Thanks for =\r\none of the most intriguing papers I&#39;ve read for a while. :) I&#39;ve\n&gt; also bee=\r\nn wondering (mostly idly as I&#39;m not actively researching at the\n&gt; moment) a=\r\nbout ways of making HyperNEAT produce more modular networks and\n&gt; handle sp=\r\necial cases (ie required irregularities in the substrate); DSE looks\n&gt; like=\r\n an excellent step forward.\n&gt;  \n&gt;  It looks like there&#39;s probably a lot of =\r\ndesign decisions you must have had to\n&gt; make about how the encoding would w=\r\nork; did you try many different set-ups\n&gt; before settling on the one presen=\r\nted in the paper (eg cellular encoding\n&gt; instruction sets and argument type=\r\ns).\n\nMarcin was the one who developed the encoding, so he would have to rel=\r\nate\nthe history of it. I have forwarded this email to him, and will report =\r\nback\nwhen I hear from him. He is actually traveling for a few months in rem=\r\note\nareas of South America, so it may be a while before he responds.\n\n&gt; It =\r\nlooks like there&#39;s no easy way (eg a single instruction) for the system to\n=\r\n&gt; add a large swath of neurons and weights; I get the feeling such an\n&gt; ins=\r\ntruction could be very useful when the task requires dealing with large\n&gt; n=\r\numbers of inputs (which HyperNEAT can excel at if the inputs are\n&gt; geometri=\r\ncally correlated).\nDSE actually does allow the addition of a large swath of=\r\n neurons and weights\nwith a single instruction. It allows &quot;subroutines&quot; (th=\r\ne term we use in the\npaper if you want to search for it), which can apply a=\r\n series of operations\nall at once. A subroutine, which can be thought of as=\r\n a genetic module, can\nencode a large network swath and then weave it into =\r\nthe network.\n\n&gt;  \n&gt;  It would be great to see more results from this type o=\r\nf encoding; what plans\n&gt; do you have? I expect I might try to make use of t=\r\nhis encoding or something\n&gt; like it in my PhD (starting next year), in whic=\r\nh I&#39;ll probably be looking at\n&gt; evolving networks with plastic weights.\nI w=\r\nould recommend checking out a journal-length paper on the DSE encoding\nthat=\r\n Marcin recently had accepted:\n\nhttp://www.springerlink.com/content/5483822=\r\n350176668/\n\nAs for future work, we&#39;ll have to see. I am actively working in=\r\n the area of\nimproving the modularity and regularity in evolved networks. I=\r\n hope to have\nmore to announce on this front in the coming months.\n\nThanks =\r\nagain Oliver. Where are you going to be doing your Ph.D? Good luck\nselectin=\r\ng your area of focus, and let me know if I can be of any help.\n\nBest regard=\r\ns,\nJeff Clune\n\nPostdoctoral Fellow\nHod Lipson&#39;s Creative Machines Laborator=\r\ny\nCornell University\njeffclune@...\nwww.msu.edu/~jclune\n&gt;  \n&gt;  Cheer=\r\ns,\n&gt;  Oliver\n&gt;  \n&gt;  On 24/04/11 17:29, Jeff Clune wrote:\n&gt;&gt;     \n&gt;&gt;  \n&gt;&gt;  \n=\r\n&gt;&gt;  \n&gt;&gt; Hello all-\n&gt;&gt;  \n&gt;&gt; \n&gt;&gt;  \n&gt;&gt;  \n&gt;&gt; I am pleased to announce a paper b=\r\ny Marcin Suchorzewski and myself that\n&gt;&gt; describes a new generative encodin=\r\ng called the Developmental Symbolic\n&gt;&gt; Encoding (DSE). \n&gt;&gt;  \n&gt;&gt; \n&gt;&gt;  \n&gt;&gt;  \n=\r\n&gt;&gt; DSE combines a key innovation from HyperNEAT (making phenotypic properti=\r\nes a\n&gt;&gt; function of their geometric location) with an encoding inspired by =\r\nCellular\n&gt;&gt; Encoding.  We investigate whether the types of neural networks =\r\nproduced by\n&gt;&gt; HyperNEAT are different from those produced by DSE (an itera=\r\ntive,\n&gt;&gt; growth-based encoding). Specifically, we compare DSE to HyperNEAT =\r\nwith\n&gt;&gt; respect to each encoding&#39;s tendency to produce networks that are mo=\r\ndular,\n&gt;&gt; regular, scale-free, and scalable. In general, we find that DSE p=\r\nroduced\n&gt;&gt; networks that were more modular and scale-free than HyperNEAT. W=\r\ne also found\n&gt;&gt; that DSE outperformed HyperNEAT on a pattern-recognition pr=\r\noblem.\n&gt;&gt;  \n&gt;&gt; \n&gt;&gt;  \n&gt;&gt;  \n&gt;&gt; I think you&#39;ll enjoy looking at the visualizat=\r\nions of the networks evolved by\n&gt;&gt; DSE and HyperNEAT, and noting their diff=\r\nerences. Overall, I think this work\n&gt;&gt; raises interesting questions as to w=\r\nhat types of encodings produce what types\n&gt;&gt; of network properties. For exa=\r\nmple, are iterative growth/rewrite encodings\n&gt;&gt; more likely to produce modu=\r\nlar networks? Does HyperNEAT need growth or\n&gt;&gt; recursion to increase its mo=\r\ndularity? Etc. What do you think?\n&gt;&gt;  \n&gt;&gt; \n&gt;&gt;  \n&gt;&gt;  \n&gt;&gt; The DSE encoding is=\r\n also an interesting and promising encoding in its own\n&gt;&gt; right. \n&gt;&gt;  \n&gt;&gt; \n=\r\n&gt;&gt;  \n&gt;&gt;  \n&gt;&gt; Here is a link to the paper and the abstract:\n&gt;&gt;  \n&gt;&gt; \n&gt;&gt;  \n&gt;&gt;=\r\n  \n&gt;&gt;  \n&gt;&gt; https://www.msu.edu/~jclune/webfiles/publications/2011-Suchorzew=\r\nskiAndClune-D\n&gt;&gt; SE-Gecco.pdf \n&gt;&gt; &lt;https://www.msu.edu/%7Ejclune/webfiles/p=\r\nublications/2011-SuchorzewskiAndClun\n&gt;&gt; e-DSE-Gecco.pdf&gt;\n&gt;&gt;  \n&gt;&gt; \n&gt;&gt;  \n&gt;&gt;  =\r\n\n&gt;&gt; \n&gt;&gt;  \n&gt;&gt;  \n&gt;&gt;  \n&gt;&gt; \n&gt;&gt; A Novel Generative Encoding for Evolving Modular=\r\n, Regular and Scalable\n&gt;&gt; Networks\n&gt;&gt;   \n&gt;&gt; \n&gt;&gt; Suchorzewski M and Clune J.=\r\n To appear in the Proceedings of the Genetic and\n&gt;&gt; Evolutionary Computatio=\r\nn Conference.  2011.\n&gt;&gt;   \n&gt;&gt; \n&gt;&gt; \n&gt;&gt;  \n&gt;&gt;  \n&gt;&gt; \n&gt;&gt; Abstract: In this paper=\r\n we introduce the Developmental Symbolic Encoding\n&gt;&gt; (DSE), a new generativ=\r\ne encoding for evolving networks (e.g. neural or\n&gt;&gt; boolean). DSE combines =\r\nelements of two powerful generative encodings,\n&gt;&gt; Cellular Encoding and Hyp=\r\nerNEAT, in order to evolve networks that are\n&gt;&gt; modular, regular, scale-fre=\r\ne, and scalable. Generating networks with these\n&gt;&gt; properties is important =\r\nbecause they can enhance performance and\n&gt;&gt; evolvability. We test DSE=B9s a=\r\nbility to generate scale-free and modular\n&gt;&gt; networks by explicitly rewardi=\r\nng these properties and seeing whether\n&gt;&gt; evolution can produce networks th=\r\nat possess them. We compare the networks DSE\n&gt;&gt; evolves to those of HyperNE=\r\nAT. The results show that both encodings can\n&gt;&gt; produce scale-free networks=\r\n, although DSE performs slightly, but\n&gt;&gt; significantly, better on this obje=\r\nctive. DSE networks are far more modular\n&gt;&gt; than HyperNEAT networks. Both e=\r\nncodings produce regular networks. We further\n&gt;&gt; demonstrate that individua=\r\nl DSE genomes during development can scale up a\n&gt;&gt; network pattern to accom=\r\nmodate different numbers of inputs. We also compare\n&gt;&gt; DSE to HyperNEAT on =\r\na pattern recognition problem. DSE significantly\n&gt;&gt; outperforms HyperNEAT, =\r\nsuggesting that its potential lay not just in the\n&gt;&gt; properties of the netw=\r\norks it produces, but also because it can compete with\n&gt;&gt; leading encodings=\r\n at solving challenging problems. These preliminary results\n&gt;&gt; imply that D=\r\nSE is an interesting new encoding worthy of additional study. The\n&gt;&gt; result=\r\ns also raise questions about which network properties are more likely\n&gt;&gt; to=\r\n be produced by different types of generative encodings.\n&gt;&gt;  \n&gt;&gt;  \n&gt;&gt; \n&gt;&gt;  =\r\n\n&gt;&gt;  \n&gt;&gt; \n&gt;&gt;  \n&gt;&gt;  \n&gt;&gt; PS. Because there is not a current working distribut=\r\nion of Cellular Encoding,\n&gt;&gt; this encoding can serve as a descendent of CE =\r\nif people wish to compare\n&gt;&gt; against this type of encoding. We&#39;d be happy t=\r\no make the code available.\n&gt;&gt;  \n&gt;&gt;  Best regards,\n&gt;&gt;  Jeff Clune\n&gt;&gt;  \n&gt;&gt;  P=\r\nostdoctoral Fellow\n&gt;&gt;  Hod Lipson&#39;s Creative Machines Laboratory\n&gt;&gt;  Cornel=\r\nl University\n&gt;&gt;  jeffclune@...\n&gt;&gt;  www.msu.edu/~jclune &lt;http://www.=\r\nmsu.edu/~jclune&gt;\n&gt;&gt;  \n&gt;&gt;  \n&gt;&gt;  \n&gt;&gt;     \n&gt;  \n&gt; \n&gt;  \n&gt;    \n&gt; \n&gt;  \n\n\n\r\n--B_3391001374_713881\r\nContent-type: text/html;\n\tcharset=&quot;ISO-8859-1&quot;\r\nContent-transfer-encoding: quoted-printable\r\n\r\n&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body style=3D&quot;color: rgb(0, 0, 0); font-size: 14px; fon=\r\nt-family: Calibri, sans-serif; word-wrap: break-word; -webkit-nbsp-mode: sp=\r\nace; -webkit-line-break: after-white-space; &quot;&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;Hello Oliver,&=\r\nnbsp;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Thanks for the kind words about our paper. I=\r\n&#39;m glad you liked it. &nbsp;Please see my responses to your questions below=\r\n.&nbsp;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;span id=3D&quot;OLK_SRC_BODY_SECTION&quot;&gt;&lt;=\r\nblockquote id=3D&quot;MAC_OUTLOOK_ATTRIBUTION_BLOCKQUOTE&quot; style=3D&quot;BORDER-LEFT: =\r\n#b5c4df 5 solid; PADDING:0 0 0 5; MARGIN:0 0 0 5;&quot;&gt;&lt;div&gt;&lt;div style=3D&quot;backg=\r\nround-color: #fff;&quot;&gt;&lt;div id=3D&quot;ygrp-mlmsg&quot; style=3D&quot;position:relative;&quot;&gt;&lt;di=\r\nv id=3D&quot;ygrp-msg&quot; style=3D&quot;z-index: 1;&quot;&gt;&lt;div id=3D&quot;ygrp-text&quot;&gt;&lt;p&gt;&lt;font clas=\r\ns=3D&quot;Apple-style-span&quot; face=3D&quot;Calibri,sans-serif&quot; size=3D&quot;4&quot;&gt;&lt;span class=\r\n=3D&quot;Apple-style-span&quot; style=3D&quot;font-size: 14px; line-height: normal;&quot;&gt;&lt;br&gt;&lt;=\r\n/span&gt;&lt;/font&gt;\n    Thanks for one of the most intriguing papers I&#39;ve read fo=\r\nr a while.\n    :) I&#39;ve also been wondering (mostly idly as I&#39;m not actively=\r\n\n    researching at the moment) about ways of making HyperNEAT produce\n    =\r\nmore modular networks and handle special cases (ie required\n    irregularit=\r\nies in the substrate); DSE looks like an excellent step\n    forward.&lt;br&gt;\n  =\r\n  &lt;br&gt;\n    It looks like there&#39;s probably a lot of design decisions you mus=\r\nt\n    have had to make about how the encoding would work; did you try many\n=\r\n    different set-ups before settling on the one presented in the paper\n   =\r\n (eg cellular encoding instruction sets and argument types). &lt;/p&gt;&lt;/div&gt;&lt;/di=\r\nv&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/blockquote&gt;&lt;/span&gt;&lt;div&gt;Marcin was the one who develop=\r\ned the encoding, so he would have to relate the history of it. I have forwa=\r\nrded this email to him, and will report back when I hear from him. He is ac=\r\ntually traveling for a few months in remote areas of South America, so it m=\r\nay be a while before he responds.&nbsp;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;span id=3D&quot;OLK=\r\n_SRC_BODY_SECTION&quot;&gt;&lt;blockquote id=3D&quot;MAC_OUTLOOK_ATTRIBUTION_BLOCKQUOTE&quot; st=\r\nyle=3D&quot;BORDER-LEFT: #b5c4df 5 solid; PADDING:0 0 0 5; MARGIN:0 0 0 5;&quot;&gt;&lt;div=\r\n&gt;&lt;div style=3D&quot;background-color: #fff;&quot;&gt;&lt;div id=3D&quot;ygrp-mlmsg&quot; style=3D&quot;pos=\r\nition:relative;&quot;&gt;&lt;div id=3D&quot;ygrp-msg&quot; style=3D&quot;z-index: 1;&quot;&gt;&lt;div id=3D&quot;ygrp=\r\n-text&quot;&gt;&lt;p&gt;It looks\n    like there&#39;s no easy way (eg a single instruction) f=\r\nor the system to\n    add a large swath of neurons and weights; I get the fe=\r\neling such an\n    instruction could be very useful when the task requires d=\r\nealing with\n    large numbers of inputs (which HyperNEAT can excel at if th=\r\ne inputs\n    are geometrically correlated).&lt;br&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;=\r\n&lt;/div&gt;&lt;/blockquote&gt;&lt;/span&gt;&lt;div&gt;DSE actually does allow the addition of a la=\r\nrge swath of neurons and weights with a single instruction. It allows &quot;subr=\r\noutines&quot; (the term we use in the paper if you want to search for it), which=\r\n can apply a series of operations all at once. A subroutine, which can be t=\r\nhought of as a genetic module, can encode a large network swath and then we=\r\nave it into the network.&nbsp;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;span id=3D&quot;OLK_SRC_BODY=\r\n_SECTION&quot;&gt;&lt;blockquote id=3D&quot;MAC_OUTLOOK_ATTRIBUTION_BLOCKQUOTE&quot; style=3D&quot;BO=\r\nRDER-LEFT: #b5c4df 5 solid; PADDING:0 0 0 5; MARGIN:0 0 0 5;&quot;&gt;&lt;div&gt;&lt;div sty=\r\nle=3D&quot;background-color: #fff;&quot;&gt;&lt;div id=3D&quot;ygrp-mlmsg&quot; style=3D&quot;position:rel=\r\native;&quot;&gt;&lt;div id=3D&quot;ygrp-msg&quot; style=3D&quot;z-index: 1;&quot;&gt;&lt;div id=3D&quot;ygrp-text&quot;&gt;&lt;p=\r\n&gt;\n    &lt;br&gt;\n    It would be great to see more results from this type of enco=\r\nding;\n    what plans do you have? I expect I might try to make use of this\n=\r\n    encoding or something like it in my PhD (starting next year), in\n    wh=\r\nich I&#39;ll probably be looking at evolving networks with plastic\n    weights.=\r\n&lt;br&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/blockquote&gt;&lt;/span&gt;&lt;div&gt;I would reco=\r\nmmend checking out a journal-length paper on the DSE encoding that Marcin r=\r\necently had accepted:&nbsp;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;&lt;a href=3D&quot;http://www.=\r\nspringerlink.com/content/5483822350176668/&quot;&gt;http://www.springerlink.com/con=\r\ntent/5483822350176668/&lt;/a&gt;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;As for future work, we&#39;=\r\nll have to see. I am actively working in the area of improving the modulari=\r\nty and regularity in evolved networks. I hope to have more to announce on t=\r\nhis front in the coming months.&nbsp;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Thanks again=\r\n Oliver. Where are you going to be doing your Ph.D? Good luck selecting you=\r\nr area of focus, and let me know if I can be of any help.&nbsp;&lt;/div&gt;&lt;div&gt;&lt;=\r\nbr&gt;&lt;/div&gt;&lt;div&gt;Best regards,&lt;br&gt;&lt;font color=3D&quot;#007F00&quot;&gt;Jeff Clune&lt;br&gt;&lt;/font=\r\n&gt;&lt;br&gt;Postdoctoral Fellow&lt;br&gt;Hod Lipson&#39;s Creative Machines Laboratory&lt;br&gt;Co=\r\nrnell University&lt;br&gt;&lt;a href=3D&quot;jeffclune@...&quot;&gt;jeffclune@...=\r\n&lt;/a&gt;&lt;br&gt;www.msu.edu/~jclune&lt;/div&gt;&lt;span id=3D&quot;OLK_SRC_BODY_SECTION&quot;&gt;&lt;blockqu=\r\note id=3D&quot;MAC_OUTLOOK_ATTRIBUTION_BLOCKQUOTE&quot; style=3D&quot;BORDER-LEFT: #b5c4df=\r\n 5 solid; PADDING:0 0 0 5; MARGIN:0 0 0 5;&quot;&gt;&lt;div&gt;&lt;div style=3D&quot;background-c=\r\nolor: #fff;&quot;&gt;&lt;div id=3D&quot;ygrp-mlmsg&quot; style=3D&quot;position:relative;&quot;&gt;&lt;div id=3D=\r\n&quot;ygrp-msg&quot; style=3D&quot;z-index: 1;&quot;&gt;&lt;div id=3D&quot;ygrp-text&quot;&gt;&lt;p&gt;\n    &lt;br&gt;\n    Che=\r\ners,&lt;br&gt;\n    Oliver&lt;br&gt;\n    &lt;br&gt;\n    On 24/04/11 17:29, Jeff Clune wrote:\n =\r\n   &lt;/p&gt;&lt;blockquote cite=3D&quot;mid:C9D760C7.1E2D9%25jeffclune@...&quot; type=\r\n=3D&quot;cite&quot;&gt;\n      &lt;span&gt;&nbsp;&lt;/span&gt;\n      \n          &lt;div id=3D&quot;ygrp-text&quot;=\r\n&gt;\n            &lt;div&gt;\n              &lt;div&gt;\n                &lt;div&gt;Hello all-&lt;/di=\r\nv&gt;\n                &lt;div&gt;&lt;br&gt;\n                &lt;/div&gt;\n                &lt;div&gt;I =\r\nam pleased to announce a paper by Marcin\n                  Suchorzewski and=\r\n myself that describes a new\n                  generative encoding called t=\r\nhe Developmental Symbolic\n                  Encoding (DSE).&nbsp;&lt;/div&gt;\n   =\r\n             &lt;div&gt;&lt;br&gt;\n                &lt;/div&gt;\n                &lt;div&gt;DSE comb=\r\nines a key innovation from HyperNEAT\n                  (making phenotypic p=\r\nroperties a function of their\n                  geometric location) with an=\r\n encoding inspired by\n                  Cellular Encoding. &nbsp;We investi=\r\ngate whether the &lt;i&gt;types&lt;/i&gt;\n                  of neural networks produced=\r\n by&nbsp;HyperNEAT are different\n                  from those produced by&n=\r\nbsp;DSE (an iterative, growth-based\n                  encoding). Specifical=\r\nly, we compare DSE to HyperNEAT\n                  with respect to each enco=\r\nding&#39;s tendency to produce\n                  networks that are modular, reg=\r\nular, scale-free, and\n                  scalable. In general, we find that =\r\nDSE produced\n                  networks that were more modular and scale-fr=\r\nee than\n                  HyperNEAT. We also found that DSE outperformed\n  =\r\n                HyperNEAT on a pattern-recognition problem.&nbsp;&lt;/div&gt;\n   =\r\n             &lt;div&gt;&lt;br&gt;\n                &lt;/div&gt;\n                &lt;div&gt;I think =\r\nyou&#39;ll enjoy looking at the visualizations\n                  of the network=\r\ns evolved by DSE and HyperNEAT, and\n                  noting their differen=\r\nces. Overall, I think this work\n                  raises interesting questi=\r\nons as to what types of\n                  encodings produce what types of n=\r\network properties.\n                  For example, are iterative growth/rewr=\r\nite encodings\n                  more likely to produce modular networks? Do=\r\nes\n                  HyperNEAT need growth or recursion to increase its\n   =\r\n               modularity? Etc. What do you think?&lt;/div&gt;\n                &lt;d=\r\niv&gt;&lt;br&gt;\n                &lt;/div&gt;\n                &lt;div&gt;The DSE encoding is als=\r\no an interesting and\n                  promising encoding in its own right.=\r\n&nbsp;&lt;/div&gt;\n                &lt;div&gt;&lt;br&gt;\n                &lt;/div&gt;\n             =\r\n   &lt;div&gt;Here is a link to the paper and the abstract:&lt;/div&gt;\n               =\r\n &lt;div&gt;&lt;br&gt;\n                &lt;/div&gt;\n                &lt;div&gt;\n                  &lt;=\r\ndiv&gt;&lt;a moz=3D&quot;true&quot; href=3D&quot;https://www.msu.edu/%7Ejclune/webfiles/publicat=\r\nions/2011-SuchorzewskiAndClune-DSE-Gecco.pdf&quot;&gt;https://www.msu.edu/~jclune/w=\r\nebfiles/publications/2011-SuchorzewskiAndClune-DSE-Gecco.pdf&lt;/a&gt;&lt;/div&gt;\n    =\r\n              &lt;div&gt;&lt;br&gt;\n                  &lt;/div&gt;\n                  &lt;div&gt;&lt;br=\r\n&gt;\n                  &lt;/div&gt;\n                  &lt;div&gt;\n                    &lt;p s=\r\ntyle=3D&quot;font: 17.9px Helvetica;&quot;&gt;&lt;b&gt;A Novel\n                        Generat=\r\nive Encoding for Evolving Modular,\n                        Regular and Scal=\r\nable Networks&lt;/b&gt;&lt;/p&gt;\n                    &lt;b&gt;\n                      &lt;p styl=\r\ne=3D&quot;font: 12px Helvetica;&quot;&gt;&lt;span class=3D&quot;Apple-style-span&quot; style=3D&quot;font-=\r\nsize: 14px; &quot;&gt;Suchorzewski M and Clune J. To appear\n                       =\r\n   in the Proceedings of the Genetic and\n                          Evolutio=\r\nnary Computation Conference. &nbsp;2011.&lt;/span&gt;&lt;/p&gt;\n                    &lt;/b=\r\n&gt;\n                    &lt;p style=3D&quot;font: 17.9px Helvetica;&quot;&gt;&lt;font class=3D&quot;A=\r\npple-style-span&quot; size=3D&quot;3&quot;&gt;&lt;span class=3D&quot;Apple-style-span&quot; style=3D&quot;font-=\r\nsize:\n                          12px;&quot;&gt;&lt;br&gt;\n                        &lt;/span&gt;=\r\n&lt;/font&gt;&lt;/p&gt;\n                    &lt;p style=3D&quot;font: 17.9px Helvetica;&quot;&gt;&lt;font =\r\nclass=3D&quot;Apple-style-span&quot; size=3D&quot;4&quot;&gt;&lt;span class=3D&quot;Apple-style-span&quot; styl=\r\ne=3D&quot;font-size:\n                          14px;&quot;&gt;Abstract:&nbsp;In this pap=\r\ner we introduce\n                          the Developmental Symbolic Encodi=\r\nng (DSE), a\n                          new generative encoding for evolving =\r\nnetworks\n                          (e.g. neural or boolean). DSE combines\n =\r\n                         elements of two powerful generative encodings,\n   =\r\n                       Cellular Encoding and HyperNEAT, in order to\n       =\r\n                   evolve networks that are modular, regular,\n             =\r\n             scale-free, and scalable. Generating networks\n                =\r\n          with these properties is important because\n                      =\r\n    they can enhance performance and evolvability.\n                        =\r\n  We test DSE&#8217;s ability to generate scale-free\n                      =\r\n    and modular networks by explicitly rewarding\n                          =\r\nthese properties and seeing whether evolution\n                          can=\r\n produce networks that possess them. We\n                          compare t=\r\nhe networks DSE evolves to those of\n                          HyperNEAT. Th=\r\ne results show that both\n                          encodings can produce sc=\r\nale-free networks,\n                          although DSE performs slightly=\r\n, but\n                          significantly, better on this objective. DS=\r\nE\n                          networks are far more modular than HyperNEAT\n  =\r\n                        networks. Both encodings produce regular\n          =\r\n                networks. We further demonstrate that\n                     =\r\n     individual DSE genomes during development can\n                        =\r\n  scale up a network pattern to accommodate\n                          diffe=\r\nrent numbers of inputs. We also compare\n                          DSE to Hy=\r\nperNEAT on a pattern recognition\n                          problem. DSE sig=\r\nnificantly outperforms\n                          HyperNEAT, suggesting that=\r\n its potential lay\n                          not just in the properties of =\r\nthe networks it\n                          produces, but also because it can=\r\n compete with\n                          leading encodings at solving challe=\r\nnging\n                          problems. These preliminary results imply t=\r\nhat\n                          DSE is an interesting new encoding worthy of\n=\r\n                          additional study. The results also raise\n        =\r\n                  questions about which network properties are\n            =\r\n              more likely to be produced by different types\n               =\r\n           of generative encodings.&lt;/span&gt;&lt;/font&gt;&lt;/p&gt;\n                  &lt;/d=\r\niv&gt;\n                  &lt;div&gt;&lt;br&gt;\n                  &lt;/div&gt;\n                  =\r\n&lt;div&gt;&lt;br&gt;\n                  &lt;/div&gt;\n                  &lt;div&gt;PS.&nbsp;Because =\r\nthere is not a current working\n                    distribution of Cellular=\r\n Encoding, this encoding can\n                    serve as a descendent of C=\r\nE if people wish to\n                    compare against this type of encodi=\r\nng. We&#39;d be happy\n                    to make the code available.&nbsp;&lt;/di=\r\nv&gt;\n                  &lt;br&gt;\n                  Best regards,&lt;br&gt;\n             =\r\n     &lt;font color=3D&quot;#007f00&quot;&gt;Jeff Clune&lt;br&gt;\n                  &lt;/font&gt;&lt;br&gt;\n =\r\n                 Postdoctoral Fellow&lt;br&gt;\n                  Hod Lipson&#39;s Cre=\r\native Machines Laboratory&lt;br&gt;\n                  Cornell University&lt;br&gt;\n    =\r\n              &lt;a moz=3D&quot;true&quot; href=3D&quot;jeffclune@...&quot;&gt;jeffclune@corn=\r\nell.edu&lt;/a&gt;&lt;br&gt;\n                  &lt;a class=3D&quot;moz-txt-link-abbreviated&quot; hre=\r\nf=3D&quot;http://www.msu.edu/~jclune&quot;&gt;www.msu.edu/~jclune&lt;/a&gt;&lt;/div&gt;\n            =\r\n  &lt;/div&gt;\n            &lt;/div&gt;\n          &lt;/div&gt;\n          \n      \n      &lt;!-- e=\r\nnd group email --&gt;\n    &lt;/blockquote&gt;\n  \n\n&lt;p&gt;&lt;/p&gt;\n\n    &lt;/div&gt;\n     \n\n    &lt;/d=\r\niv&gt;&lt;/div&gt;&lt;!-- end group email --&gt;&lt;/blockquote&gt;&lt;/span&gt;&lt;/body&gt;&lt;/html&gt;\n\r\n--B_3391001374_713881--\r\n\n"}}