{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"Z2wHo5B2wSH92Xy8I8y9Y3GRTfTagz3ahR7qbkwMdzYsx1gDDIGxksaBHcgqIZo9MLL_cTBc-xOQYb8rgDIoXwXOWyaf_CWf7I8atwDPO97n","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Question about hyperneat, NEVH and hidden layers","postDate":"1213102150","msgId":4143,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGcybHQ4NisyNGU1QGVHcm91cHMuY29tPg==","inReplyToHeader":"PGcybDNodis4Z2cxQGVHcm91cHMuY29tPg=="},"prevInTopic":4140,"nextInTopic":0,"prevInTime":4142,"nextInTime":4144,"topicId":4138,"numMessagesInTopic":4,"msgSnippet":"Peter B, just a few notes on substrate layouts: If you do designate some nodes as inputs, some as hidden and some as outputs, then given those designations it","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 26486 invoked from network); 10 Jun 2008 12:49:10 -0000\r\nX-Received: from unknown (66.218.67.94)\n  by m49.grp.scd.yahoo.com with QMQP; 10 Jun 2008 12:49:10 -0000\r\nX-Received: from unknown (HELO n20b.bullet.sp1.yahoo.com) (69.147.64.134)\n  by mta15.grp.scd.yahoo.com with SMTP; 10 Jun 2008 12:49:10 -0000\r\nX-Received: from [216.252.122.219] by n20.bullet.sp1.yahoo.com with NNFMP; 10 Jun 2008 12:49:10 -0000\r\nX-Received: from [66.218.69.1] by t4.bullet.sp1.yahoo.com with NNFMP; 10 Jun 2008 12:49:10 -0000\r\nX-Received: from [66.218.66.83] by t1.bullet.scd.yahoo.com with NNFMP; 10 Jun 2008 12:49:10 -0000\r\nDate: Tue, 10 Jun 2008 12:49:10 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;g2lt86+24e5@...&gt;\r\nIn-Reply-To: &lt;g2l3hv+8gg1@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: Re: Question about hyperneat, NEVH and hidden layers\r\nX-Yahoo-Group-Post: member; u=54567749; y=x67ddr8SuASIqjWgyUV1ZHXRjgZaOyUwtIxBW0WJnzww-P3IDKDI\r\nX-Yahoo-Profile: kenstanley01\r\n\r\nPeter B, just a few notes on substrate layouts:\n\nIf you do designate some n=\r\nodes as inputs, some as hidden and some as\noutputs, then given those design=\r\nations it is true that in practice a\nCPPN can express any possible connnect=\r\nivity.  However, you can also\nrestrict the connectivity however you want, i=\r\n.e. simply by not\nquerying connections that you a priori wish to exclude fr=\r\nom being\npossible.  Perhaps in some cases those may include recurrent\nconne=\r\nctions or self-recurrent connections (which are queried by a CPPN\nwith inpu=\r\nts wherein x1=3Dx2 and y1=3Dy2).\n\nThe other issue that is involved is the n=\r\number of spatial dimensions\nin the substrate.  It is possible to create a 3=\r\n-dimensional spatial\nstructure with a CPPN that takes (x1,y1,z1,x2,y2,z2). =\r\n Then you could\nhave an arbitrary number of hidden layers (although, as Pet=\r\ner C notes,\nif you do not restrict connectivity, the word &quot;layer&quot; may be\nmi=\r\nsleading).  But in effect you can create as many layers as you want\nthat wa=\r\ny.\n\nHowever, as you note, in my AAAI-2008 paper with Jason Gauci, we used\na=\r\n kind of shortcut representation that allows us to represent a\n3-layer 3D s=\r\ntructure with only (x1,y1,x2,y2) as inputs.  The trick\nthat let us do that =\r\nwas to have a second output on the CPPN for\nexpressing connection weights b=\r\netween the hidden layer and the output\nlayer.\n\nThis trick could potentially=\r\n be extended to any number of layers\nsimply by adding more outputs to the C=\r\nPPN.  However, it seems that at\nsome point it will not scale as well as sim=\r\nply introducing z, because\nthe CPPN keeps getting bigger every time you add=\r\n a new output, but the\nCPPN that takes (x1,y1,z1,x2,y2,z2) has the same num=\r\nber of inputs and\noutputs no matter how the 3D substrate is configured.\n\nTh=\r\ne purpose of a 3D substrate, by the way, is to allow us to exploit\ngeometri=\r\nc relationships that may not be possible to represent in 2D. \nBecause a che=\r\nckers board is itself 2D, it made sense to have the\nsubstrate be 3D that re=\r\nlates the board to the hidden layer and the output.\n\nThe question of which =\r\nway is better in any particular case is\nempirical, so it is difficult to pr=\r\novide a single rule.  But I am\nguessing that it would often work reasonably=\r\n in more than one way, so\nthat this choice is not likely to make or break t=\r\nhe setup as long as\nit is reasonable.  It does however leave a lot of room =\r\nfor creativity.\n\nOf course, some people (like Jeff Clune) are interested in=\r\n automating\nthe substrate configuration, which is an interesting future pos=\r\nsibility.\n\nken\n\n--- In neat@yahoogroups.com, &quot;peterberrington&quot; &lt;peterberrin=\r\ngton@...&gt;\nwrote:\n&gt;\n&gt; Although the papers on hyperneat propose some unique w=\r\nays of\n&gt; representing the substrate geometrically, I got the impression tha=\r\nt\n&gt; the resulting generated net was always going to be 2 layer\n&gt; feed-forwa=\r\nrd network (isn&#39;t the cppn used to construct this?). In the\n&gt; paper on hype=\r\nrneat checkers however, Jason Gauci and Ken Stanley query\n&gt; a single cppn t=\r\no get connection weights for the input/hidden layer,\n&gt; and the hidden/outpu=\r\nt layer. (A Case Study on the Critical Role of\n&gt; Geometric Regularity in Ma=\r\nchine Learning)\n&gt; \n&gt; Given that single layer feedforward networks were foun=\r\nd incapable of\n&gt; learning linearly inseperable  patterns, it seems like the=\r\n\n&gt; functionality of NEVH could be severely hindered by limiting itself to\n&gt;=\r\n search the space of single-layer perceptrons. \n&gt; \n&gt; Is there a principled =\r\nway of extending the method used by Gauci and\n&gt; Stanley in the checkers pap=\r\ner to making arbitrary amounts hidden\n&gt; layers? Can anyone provide any insi=\r\nghts on whether limiting the\n&gt; generated nets to single layer actually inhi=\r\nbits their ability to\n&gt; learn in the domain of humanoid bipedal walking?\n&gt;\n=\r\n\n\n\n"}}