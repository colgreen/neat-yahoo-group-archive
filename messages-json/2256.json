{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":60940451,"authorName":"Jeff Haynes","from":"&quot;Jeff Haynes&quot; &lt;jeff@...&gt;","profile":"jefffhaynes","replyTo":"LIST","senderId":"RdZEsUtoE_QofSi3UHrqQBSmgTse_SouVxW-TuSRfxNMvJrZ7H1JP1arcgwRXlRVWJZz6aKYJFQbHjQZg9Ti_N51A23HP8tBDA","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [neat] Re: Introduction---recurrency question","postDate":"1126050974","msgId":2256,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDIwMDUwOTA2MjM0MjU5Lk04MjIyN0BkZWFyZG9yZmYuY29tPg==","inReplyToHeader":"PGRmbDM1dis5b2hqQGVHcm91cHMuY29tPg==","referencesHeader":"PGRmamkxZitqNjY5QGVHcm91cHMuY29tPiA8ZGZsMzV2KzlvaGpAZUdyb3Vwcy5jb20+"},"prevInTopic":2255,"nextInTopic":2257,"prevInTime":2255,"nextInTime":2257,"topicId":2209,"numMessagesInTopic":42,"msgSnippet":"At the risk of offending the gods, I m going to humbly add 2 cents. While I fully agree with everything you ve said here, I think there is an important","rawEmail":"Return-Path: &lt;jeff@...&gt;\r\nX-Sender: jeff@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 59218 invoked from network); 6 Sep 2005 23:56:15 -0000\r\nReceived: from unknown (66.218.66.217)\n  by m31.grp.scd.yahoo.com with QMQP; 6 Sep 2005 23:56:15 -0000\r\nReceived: from unknown (HELO eagle.deardorff.com) (64.92.206.84)\n  by mta2.grp.scd.yahoo.com with SMTP; 6 Sep 2005 23:56:15 -0000\r\nReceived: from eagle.deardorff.com (jeffie@localhost [127.0.0.1])\n\tby eagle.deardorff.com (8.13.3/8.13.3) with ESMTP id j86NuEm8026102\n\tfor &lt;neat@yahoogroups.com&gt;; Tue, 6 Sep 2005 19:56:14 -0400\r\nTo: neat@yahoogroups.com\r\nDate: Tue, 6 Sep 2005 19:56:14 -0400\r\nMessage-Id: &lt;20050906234259.M82227@...&gt;\r\nIn-Reply-To: &lt;dfl35v+9ohj@...&gt;\r\nReferences: &lt;dfji1f+j669@...&gt; &lt;dfl35v+9ohj@...&gt;\r\nX-Mailer: Open WebMail 2.51 20050323\r\nX-OriginatingIP: 69.143.110.253 (jeffie)\r\nMIME-Version: 1.0\r\nContent-Type: text/plain;\n\tcharset=iso-8859-1\r\nX-Spam-Status: No, Not spam. Probably whitelisted.\r\nX-Scanned-By: MIMEDefang 2.51 on 64.92.206.84\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: &quot;Jeff Haynes&quot; &lt;jeff@...&gt;\r\nSubject: Re: [neat] Re: Introduction---recurrency question\r\nX-Yahoo-Group-Post: member; u=60940451; y=rW5hTVAmNGInx4N3ydt904-DpE-1cmxx76j_nSzBCzjfV2B65Xk\r\nX-Yahoo-Profile: jefffhaynes\r\n\r\nAt the risk of offending the gods, I&#39;m going to humbly add 2 cents.\n\nWhile I fully agree with everything you&#39;ve said here, I think there is an\nimportant fundamental truth to all this.  And, it is simply that which I was\ntrying and failed to convey.\n\nI think the fact that it is possible to build a network capable of learning\n(no further classical training required!) is, in a manner of speaking, the\nwhole kit and kaboodle.\n\nUltimately, isn&#39;t it the creation of a true learning machine that we strive to\naccomplish?  I am not in any way purporting to know the best path to this\nmachine and as to the specifics (such as representation of memory) I have yet\nless insight.\n\nThat being said, I think any confusion as to what is ultimately possible, and\nwhat is not, can inadvertantly close the best doors.  At the very least, I do\nnot believe it is an insignificant conclusion.\n\nRegards,\n\nJeff\n\n\nOn Tue, 06 Sep 2005 21:54:07 -0000, Kenneth Stanley wrote\n&gt; John (caloyannis) is right when he says the &quot;trick is to find it!&quot;\n&gt; \n&gt; I think it&#39;s important to keep in mind in discussions such as this \n&gt; one that theoretical facts are usually of little practical \n&gt; consequence.  That recurrent networks can represent any Turing \n&gt; machine is about as practically useful as knowing that you can \n&gt; theoretically build a fully-functioning computer out of water pipes \n&gt; and water.  The real question with respect to recurrent NN&#39;s is \n&gt; what&#39;s reasonably easy to find and represent, not what&#39;s \n&gt; theoretically possible.  Also note that what&#39;s easy to find is not \n&gt; always so much a property of the neural network alone as a property \n&gt; of the search process used to generate it combined with the type of \n&gt; neural network.\n&gt; \n&gt; In any case, my point is that it certainly merits long and deep \n&gt; discussion what can or cannot be done with a recurrent network, \n&gt; regardless of what is possible in theory, even if all things are \n&gt; possible in theory.  We are generally interested in practice, which \n&gt; is much more complicated, which is also why it&#39;s fun: There is a lot \n&gt; of room for creativity and in fact standard recurrent structures may \n&gt; indeed be more difficult to discover than other memory-based \n&gt; realizations such as Hebbian networks.  For this reason, we should \n&gt; keep an open mind.\n&gt; \n&gt; ken\n&gt; \n&gt; --- In neat@yahoogroups.com, &quot;johncaloyannis&quot; &lt;johncaloyannis@y...&gt; \n&gt; wrote:\n&gt; &gt; Hello everybody\n&gt; &gt; \n&gt; &gt; I started dabbling in AI abouit twenty years ago on and off. For a\n&gt; &gt; while  I thought that genetic programming could be an avenue to \n&gt; follow\n&gt; &gt; (Koza). However lately I came across the NEAT concept which I found\n&gt; &gt; fascinating since it combines the best of both worlds (neural \n&gt; networks\n&gt; &gt; + evolution).\n&gt; &gt; I have followed the discussions you guys have with interest. I \n&gt; think\n&gt; &gt; that since the recurrent neural networks are intractable\n&gt; &gt; mathematically, apart from some very special cases (hopfield), one \n&gt; can\n&gt; &gt; only speculate and experiment to learn things.\n&gt; &gt; Regarding the discussion about the recurrency question and what a \n&gt; RNN\n&gt; &gt; can or can not do there is a very interesting paper which proves \n&gt; that\n&gt; &gt; there is a turing machine for every RNN and vice-versa. Since \n&gt; anything\n&gt; &gt; that is computable can be achieved with a turing machine so can it\n&gt; &gt; through an RNN. The trick is to find it :)\n&gt; &gt; http://www.uwasa.fi/stes/step96/step96/hyotyniemi1\n&gt; \n&gt; ------------------------ Yahoo! Groups Sponsor --------------------~--&gt; \n&gt; Get fast access to your favorite Yahoo! Groups. Make Yahoo! your \n&gt; home page http://us.click.yahoo.com/dpRU5A/wUILAA/yQLSAA/7brrlB/TM\n&gt; --------------------------------------------------------------------~-&gt;\n&gt; \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; \n\n\n-------------------------------------------------------------------------\nwww.greghaynes.com - check out my bro&#39;s stuff and order something! thx.\n\n\n"}}