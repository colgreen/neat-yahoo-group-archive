{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":211599040,"authorName":"Jeff Clune","from":"Jeff Clune &lt;jclune@...&gt;","profile":"jeffreyclune","replyTo":"LIST","senderId":"6W_CPGYVvRZ0tSo5OYfNpmMI9sAOfGGI8QzVuNV-UPvlnnIeXdt9b8RPxqZvfFBxPj93guEMfM2I2vNI2q1oXpAr","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [neat] New Publication Introduces Adaptive HyperNEAT (with Synaptic Plasticity)","postDate":"1274762307","msgId":5240,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PEM4MjBDQzgzLjMyQjgzJWpjbHVuZUBtc3UuZWR1Pg==","inReplyToHeader":"PGhxYW5kdSthZXR2QGVHcm91cHMuY29tPg=="},"prevInTopic":5202,"nextInTopic":5257,"prevInTime":5239,"nextInTime":5241,"topicId":5202,"numMessagesInTopic":4,"msgSnippet":"Hello Ken and Sebastian- Congrats on this paper. It is a really nice piece of research. I especially liked the non-linearly separable version of the problem","rawEmail":"Return-Path: &lt;jclune@...&gt;\r\nX-Sender: jclune@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 60384 invoked from network); 25 May 2010 05:01:40 -0000\r\nX-Received: from unknown (66.196.94.105)\n  by m14.grp.re1.yahoo.com with QMQP; 25 May 2010 05:01:40 -0000\r\nX-Received: from unknown (HELO mail-gw0-f50.google.com) (74.125.83.50)\n  by mta1.grp.re1.yahoo.com with SMTP; 25 May 2010 05:01:40 -0000\r\nX-Received: by gwaa18 with SMTP id a18so2340401gwa.37\n        for &lt;neat@yahoogroups.com&gt;; Mon, 24 May 2010 22:01:40 -0700 (PDT)\r\nX-Received: by 10.150.251.6 with SMTP id y6mr7234800ybh.328.1274762314879;\n        Mon, 24 May 2010 21:38:34 -0700 (PDT)\r\nReturn-Path: &lt;jclune@...&gt;\r\nX-Received: from [192.168.1.16] (c-76-20-190-42.hsd1.mi.comcast.net [76.20.190.42])\n        by mx.google.com with ESMTPS id 22sm3598131ywh.5.2010.05.24.21.38.31\n        (version=TLSv1/SSLv3 cipher=RC4-MD5);\n        Mon, 24 May 2010 21:38:33 -0700 (PDT)\r\nUser-Agent: Microsoft-Entourage/12.13.0.080930\r\nDate: Tue, 25 May 2010 00:38:27 -0400\r\nTo: &quot;neat@yahoogroups.com&quot; &lt;neat@yahoogroups.com&gt;\r\nMessage-ID: &lt;C820CC83.32B83%jclune@...&gt;\r\nThread-Topic: [neat] New Publication Introduces Adaptive HyperNEAT (with\n Synaptic Plasticity)\r\nThread-Index: Acr7xB4PePeDoO+NE0yZjzuF7AZYsg==\r\nIn-Reply-To: &lt;hqandu+aetv@...&gt;\r\nMime-version: 1.0\r\nContent-type: text/plain;\n\tcharset=&quot;US-ASCII&quot;\r\nContent-transfer-encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Jeff Clune &lt;jclune@...&gt;\r\nSubject: Re: [neat] New Publication Introduces Adaptive HyperNEAT (with\n Synaptic Plasticity)\r\nX-Yahoo-Group-Post: member; u=211599040; y=CmVXbtLXHA6bYUTtvFa6eP4RGqggIYgbiT6ktx804MnXSpzbd27O\r\nX-Yahoo-Profile: jeffreyclune\r\n\r\nHello Ken and Sebastian-\n\nCongrats on this paper. It is a really nice piece of research. I especially\nliked the non-linearly separable version of the problem (very creative) and\nthe cool result that went with it.\n\nThe paper really piqued my curiosity and, as a result, I have a few\nquestions. \n\n1) You mention a few times that in natural brains different regions have\ndifferent plasticity rules, but you do not provide a cite. I am guessing you\nare correct, but I was just wondering if you could tell us more about how\nyou know that (i.e., what the evidence is, or a cite where I can read about\nit, etc.). I do know that different neuromodulatory centers control\ndifferent regions of the brain, so that could potentially be an\nexample...but I was just wondering if you had other sources. If so, I would\nbe interested to hear about them.\n\n2) You mention that &quot;for all hyperneat models synaptic strength is bound\nwithin the range [-1.0, 1.0].&quot; Why did you make this choice? Isn&#39;t the range\nnormally [-3, 3]?\n\n3) In the second paragraph of the results, you list that the iterated model\ntook 89 gens. Do you mean 189? Otherwise the plots tell a different story,\nunless I am misunderstanding something.\n\n4) Do you know why the plain Hebbian rule cannot solve the task?\n\n5) You mention that it might be the case that an ANN *with hidden nodes*\ncould have solved the task without the learning rule performing the\ncomplicated (xor-equivalent?) computation. Out of curiosity, did you try\nthat? \n\n6) Can you explain (or speculate) how the CPPN network is solving this task?\nI assume (since there is no recurrence), that it is simply memorizing which\nnumbers represent a low reward (e.g. 0 or .8) instead of comparing the last\nreward to the current reward. Does that seem right? To compare to the\nprevious trial, would they need recurrence, or is it possible to embed\ninformation in the connections via learning that can function like\nrecurrence in terms of storing information? Do you know if something like\nthat is going on?\n\n7) You say that the evolved rules resemble postsynaptic-based learning rules\nthat have been shown essential in the T-Maze domain, and cite Andrea. Can\nyou elaborate on this a bit more? How were they similar? Wouldn&#39;t good\npostsynaptic learning rules depend on presynaptic and correlation rules\n(values) as well? Is it surprising to just see similar postynaptic values\nand not similar values for the other parameters?\n\nThanks for putting this exciting paper out there, and congrats again. I\nthink it will be the first in a long line of papers on this subject.\n\nBest regards,\nJeff Clune\n\nDigital Evolution Lab, Michigan State University\njclune@...\nwww.msu.edu/~jclune\n\n\n\n\n&gt; From: Ken &lt;kstanley@...&gt;\n&gt; Reply-To: &quot;neat@yahoogroups.com&quot; &lt;neat@yahoogroups.com&gt;\n&gt; Date: Fri, 16 Apr 2010 22:13:18 -0000\n&gt; To: &quot;neat@yahoogroups.com&quot; &lt;neat@yahoogroups.com&gt;\n&gt; Subject: [neat] New Publication Introduces Adaptive HyperNEAT (with Synapatic\n&gt; Plasticity)\n&gt; \n&gt; Sebastian Risi and I are pleased to announce our new publication, &quot;Indirectly\n&gt; Encoding Neural Plasticity as a Pattern of Local Rules,&quot; which will appear in\n&gt; the Proceedings of the 11th International Conference on Simulation of Adaptive\n&gt; Behavior (SAB 2010).  The paper is available here:\n&gt; \n&gt; http://eplex.cs.ucf.edu/publications/2010/risi.sab10.html\n&gt; Direct Link: http://eplex.cs.ucf.edu/papers/risi_sab10.pdf\n&gt; \n&gt; The idea introduced in this paper is that HyperNEAT can encode a pattern of\n&gt; plasticity rules across a network rather than simply encoding a pattern of\n&gt; static weights.  These encoded rules give HyperNEAT the ability to evolve\n&gt; networks that learn over their lifetime.  The paper explores this idea and how\n&gt; it can be implemented at different levels of generality (with a trade-off in\n&gt; computational cost).\n&gt; \n&gt; Of course, our hope is that this approach will later be applied to evolving\n&gt; large-scale plastic networks that move neuroevolution closer to evolving\n&gt; brain-like dynamical systems rather than just static (fixed-weight) networks.\n&gt; \n&gt; ken\n&gt; \n\n\n\n"}}