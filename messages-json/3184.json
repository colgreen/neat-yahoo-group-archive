{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":292213213,"authorName":"shanemcdonaldryan","from":"&quot;shanemcdonaldryan&quot; &lt;shanemcdonaldryan@...&gt;","profile":"shanemcdonaldryan","replyTo":"LIST","senderId":"Xfu-nCq0R0PC-2svYQ5gUNwgVsggao123GERzE_95pAOkjYNKVx0SL6LU1EgcAJ1aBIpuLWDg8maOV4W4X2pBM7ZLk4cfSPyeEO4euwHkZio2wvsLRst_A","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Computation Time","postDate":"1177162901","msgId":3184,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGYwZDRhbCszZ2VsQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDIwMzczMTA2NTQ5NDY2QG1haWwuaWRuZXQubmV0LnVrPg=="},"prevInTopic":3183,"nextInTopic":0,"prevInTime":3183,"nextInTime":3185,"topicId":845,"numMessagesInTopic":99,"msgSnippet":"Hi Ian, ... Yes in theory but in practice the random number generators in computers are generally only pseudo-random. If you give them the same seed they","rawEmail":"Return-Path: &lt;shanemcdonaldryan@...&gt;\r\nX-Sender: shanemcdonaldryan@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 91300 invoked from network); 21 Apr 2007 13:45:12 -0000\r\nReceived: from unknown (66.218.66.71)\n  by m44.grp.scd.yahoo.com with QMQP; 21 Apr 2007 13:45:12 -0000\r\nReceived: from unknown (HELO n29a.bullet.scd.yahoo.com) (66.94.237.31)\n  by mta13.grp.scd.yahoo.com with SMTP; 21 Apr 2007 13:45:12 -0000\r\nReceived: from [66.218.69.4] by n29.bullet.scd.yahoo.com with NNFMP; 21 Apr 2007 13:41:42 -0000\r\nReceived: from [66.218.66.73] by t4.bullet.scd.yahoo.com with NNFMP; 21 Apr 2007 13:41:42 -0000\r\nDate: Sat, 21 Apr 2007 13:41:41 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;f0d4al+3gel@...&gt;\r\nIn-Reply-To: &lt;20373106549466@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0\r\nFrom: &quot;shanemcdonaldryan&quot; &lt;shanemcdonaldryan@...&gt;\r\nSubject: Re: Computation Time\r\nX-Yahoo-Group-Post: member; u=292213213; y=raPJGFTRq3lS_4341SJU3Zt80q0QZL0vbPE46jTnfNOSv6GBuAPWduN9DPU\r\nX-Yahoo-Profile: shanemcdonaldryan\r\n\r\nHi Ian,\n\n&gt;Well there&#39;s no reason to suppose that giving different simulatio=\r\nns\n&gt;the same random seed is in any sense setting them on the same\n&gt;track. e=\r\n.g. there should be a chaotic (in the strict mathematical\n&gt;sense) relations=\r\nhip between all starting conditions (especially the\n&gt;random seed) and the p=\r\nath the system takes and which possible\n&gt;solution it produces.\n\nYes in theo=\r\nry but in practice the random number generators in\ncomputers are generally =\r\nonly pseudo-random. If you give them the same\nseed they produce the same se=\r\nquence of &quot;random&quot; numbers. That is the\nreason why seeds exist as I underst=\r\nand it. Unfortunately you have to\nuse something else other than c++ code to=\r\n make a truly random number\ngenerator. This in some ways is good because it=\r\n makes the experiment\nrepeatable as it follows a deterministic path as long=\r\n as you use all\nthe same inputs including the same seed. I don&#39;t actually t=\r\nhink that\nthe experiment is totally the same if I use the same seed and a\nd=\r\nifferent activation function. But I was just trying to reduce the\nnumber of=\r\n different variables between experiments. However I did see a\nstrong correl=\r\nation between the results of different experiments where\nI used the same se=\r\ned but switched the activation function from the\nsigmoid to my polynomial a=\r\npproximation and ran the experiment for 500\ngenerations.The final fitness w=\r\nas less than 1% different. But if I\nkept the activation function constant a=\r\nnd changed the seed there were\ntypically much larger differences (sometime =\r\norders of magnitude\ndifferent) after just 500 generations.\n\n&gt;Otherwise I wo=\r\nuld expect either my inv-abs function or else the\n&gt;piece-wise function that=\r\n... errr... if was either Mitchell or John\n&gt;came up with. BOth of those hug=\r\nely out-performed any exponent-based\n&gt;method in various tests.\nInteresting.=\r\n...I will do more research into those! Yes I agree\nthat there is probably b=\r\netter functions than the sigmoid but I like it\nbecause it allows me to easi=\r\nly combine other methods like\nback-propagation with NEAT. Plus because it i=\r\ns traditional most\nresearch involving ANNs generally uses the Sigmoid. Swit=\r\nching to a\ndifferent activation function would require plenty of extra work=\r\n in\norder to adapt this research. I would have to see a pretty convincing\na=\r\nrgument for another type of activation function before switching from\nthe s=\r\nigmoid. \n\n&gt;I&#39;d be very worried about what the curve does outside the usual =\r\nrange\n&gt;of inputs, unless you are going to cap the absolute range of inputs\n=\r\n&gt;permitted to a neuron? Otherwise you risk a neuron with unusually\n&gt;large i=\r\nnputs producing even larger outputs and chaos (in the\n&gt;mathematical and oth=\r\ner senses) ensues, especially in a recurrent\n&gt;network...\nI was worried too =\r\n;) So I made it a stepped function outside of a\ncertain range the function =\r\npegs the values at 0 or 1. I think this is\nequivalent to your idea of cappi=\r\nng the range of inputs.\n\nSorry as usual for being so verbose!\n\nThanks for t=\r\nhe feedback,\n\nShane\n\n\n\n--- In neat@yahoogroups.com, Ian Badcoe &lt;ian_badcoe@=\r\n...&gt; wrote:\n&gt;\n&gt; At 11:21 19/04/2007, you wrote:\n&gt; \n&gt; &gt;Hello Ian,\n&gt; &gt;\n&gt; &gt;Tha=\r\nnks yes I browsed through those threads. I am most interested in\n&gt; &gt;optimiz=\r\ning the sigmoid for now. I have implemented many of the other\n&gt; &gt;great opti=\r\nmizations mentioned. I saw the other sigmoid approximations\n&gt; &gt;and some of =\r\nthem were quite fast but this speed was offset by poor\n&gt; &gt;learning characte=\r\nristics of the resulting programs.\n&gt; &gt;\n&gt; &gt;I ran my code with the standard s=\r\nigmoid for a given number of\n&gt; &gt;generations and kept track of the random se=\r\ned, the final fitness and\n&gt; &gt;the run time. I replaced the standard sigmoid =\r\nwith an approximated\n&gt; &gt;sigmoid. Then I evolved a net using the same seed a=\r\nnd measured how\n&gt; &gt;long it took to arrive at the same fitness as the net wi=\r\nth the\n&gt; &gt;standard sigmoid ( although when the approximated sigmoids when r=\r\nun\n&gt; &gt;in isolation they were much faster than the standard sigmoid). When\n&gt;=\r\n &gt;trying to evolve a net with the same level of fitness in a given\n&gt; &gt;amoun=\r\nt of time they actually negatively impacted performance! I think\n&gt; &gt;this is=\r\n a more accurate way to measure real performance of the\n&gt; &gt;application. I t=\r\nhink it doesn&#39;t really matter how fast the parts of\n&gt; &gt;the program run but =\r\nhow fast the program completes the task it was\n&gt; &gt;created for.\n&gt; \n&gt; Well th=\r\nere&#39;s no reason to suppose that giving different simulations \n&gt; the same ra=\r\nndom seed is in any sense setting them on the same \n&gt; track.  e.g. there sh=\r\nould be a chaotic (in the strict mathematical \n&gt; sense) relationship betwee=\r\nn all starting conditions (especially the \n&gt; random seed) and the path the =\r\nsystem takes and which possible \n&gt; solution it produces.\n&gt; \n&gt; Other than th=\r\nat I wonder whether you have something else going on \n&gt; here which is damag=\r\ning you results in with the other sigmoids?  What \n&gt; type of problem were y=\r\nou running.  Bear in mind that there is AFAIK \n&gt; no theoretical justificati=\r\non for the use of the traditional sigmoid \n&gt; what-so-ever.  People just use=\r\n it because it is traditional.  The \n&gt; only reason it was introduced is bec=\r\nause it is differentiable for the \n&gt; purposes of back-propagation, but sinc=\r\ne none of us use \n&gt; back-propagation we really don&#39;t _need_ to be tradition=\r\nal.  Like I \n&gt; said, I know no theory for the standard expensive sigmoid to=\r\n be better.\n&gt; \n&gt; _BUT_ I&#39;d be very interested if anybody does have such a t=\r\nheory.  I \n&gt; guess it&#39;s trancendental?  So is that enabling the system to f=\r\nind a \n&gt; great range of power functions within it (e.g by focusing on parts=\r\n of \n&gt; the curve)?\n&gt; \n&gt; Otherwise I would expect either my inv-abs function=\r\n or else the \n&gt; piece-wise function that... errr... if was either Mitchell =\r\nor John \n&gt; came up with.  BOth of those hugely out-performed any exponent-b=\r\nased \n&gt; method in various tests.\n&gt; \n&gt; &gt;Because of the problems I saw with t=\r\nhe other approximations I decided\n&gt; &gt;to try and create my own sigmoid appro=\r\nximation with good learning\n&gt; &gt;characteristics and a fast runtime. To do th=\r\nis I created an\n&gt; &gt;approximation that closely follows the curve of the orig=\r\ninal sigmoid\n&gt; &gt;and that is relatively smooth and continuous. I think the r=\r\neason why\n&gt; &gt;the other sigmoid approximations has such poor learning\n&gt; &gt;cha=\r\nracteristics is because they were either to simple, or not smooth.\n&gt; &gt;I thi=\r\nnk if they are too simple you can&#39;t combine them in very may\n&gt; &gt;ways to arr=\r\nive at a good approximation of a given function. This is\n&gt; &gt;why a straight =\r\nline or a stepped function generally makes a poor\n&gt; &gt;activation function. I=\r\nf they aren&#39;t continuous small changes in the\n&gt; &gt;weights can cause big jump=\r\ns or erratic jumps. Which can make it hard\n&gt; &gt;to converge on an optimal sol=\r\nution.\n&gt; \n&gt; I&#39;d be very worried about what the curve does outside the usual=\r\n range \n&gt; of inputs, unless you are going to cap the absolute range of inpu=\r\nts \n&gt; permitted to a neuron?  Otherwise you risk a neuron with unusually \n&gt;=\r\n large inputs producing even larger outputs and chaos (in the \n&gt; mathematic=\r\nal and other senses) ensues, especially in a recurrent\nnetwork...\n&gt; \n&gt; &gt;I h=\r\nave only run my sigmoid approx on the problems I am interested in\n&gt; &gt;so my =\r\nhigh regard for it is probably unfounded and naive. But it\n&gt; &gt;definitely se=\r\nems to improve the rate of learning for the problems I\n&gt; &gt;am interested in =\r\nright now.\n&gt; &gt;\n&gt; &gt;My next major optimization project is going to be impleme=\r\nnting a\n&gt; &gt;GPGPU ANN implementation. I am hopefully hiring a chap in a few =\r\ndays\n&gt; &gt;to work on this project. It will be exciting to see if I can get th=\r\ne\n&gt; &gt;same performance gains (30 fold) from the GPGPU as others have in the\n=\r\n&gt; &gt;papers I have read. Anyone have any experience with this?\n&gt; \n&gt; What is a=\r\n GPGPGPGPGANN?  Oh, a graphics-processor-geometry-pipeline, \n&gt; yes good ide=\r\na.\n&gt; \n&gt; Did any body see intel demonstrated (I think it was) a 80-core CPU =\r\n\n&gt; which was effectively 80 floating point units on one chip?\n&gt; \n&gt;         =\r\n Ian \n&gt; \n&gt; \n&gt; -- \n&gt; No virus found in this outgoing message.\n&gt; Checked by A=\r\nVG Free Edition. \n&gt; Version: 7.5.463 / Virus Database: 269.5.5/769 - Releas=\r\ne Date:\n19/04/2007 17:56\n&gt;\n\n\n\n"}}