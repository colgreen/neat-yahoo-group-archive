{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":281645563,"authorName":"afcarl2","from":"&quot;afcarl2&quot; &lt;a.carl@...&gt;","profile":"afcarl2","replyTo":"LIST","senderId":"zqgtkp_8lfYIbn3nPBB7Wg5cIs040VTUqFLRbI4b_GIeOee_Yy7d5nogaYM2LGaI0jpZ82UGSqtFJ0P2haBQJUQ","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Tile Coding and HyperNEAT","postDate":"1179023896","msgId":3273,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGYyNXRtbytxYXM0QGVHcm91cHMuY29tPg==","inReplyToHeader":"PGYyNWZwNCs5YjVkQGVHcm91cHMuY29tPg=="},"prevInTopic":3272,"nextInTopic":3274,"prevInTime":3272,"nextInTime":3274,"topicId":3214,"numMessagesInTopic":27,"msgSnippet":"The extreme cited was your referenced example of cutting up a chess board. I completely agree with your statement that the divisions should occur at the","rawEmail":"Return-Path: &lt;a.carl@...&gt;\r\nX-Sender: a.carl@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 4697 invoked from network); 13 May 2007 02:39:19 -0000\r\nReceived: from unknown (66.218.67.36)\n  by m35.grp.scd.yahoo.com with QMQP; 13 May 2007 02:39:19 -0000\r\nReceived: from unknown (HELO n27b.bullet.sp1.yahoo.com) (209.131.38.244)\n  by mta10.grp.scd.yahoo.com with SMTP; 13 May 2007 02:39:19 -0000\r\nReceived: from [216.252.122.219] by n27.bullet.sp1.yahoo.com with NNFMP; 13 May 2007 02:38:16 -0000\r\nReceived: from [66.218.69.2] by t4.bullet.sp1.yahoo.com with NNFMP; 13 May 2007 02:38:16 -0000\r\nReceived: from [66.218.66.77] by t2.bullet.scd.yahoo.com with NNFMP; 13 May 2007 02:38:16 -0000\r\nDate: Sun, 13 May 2007 02:38:16 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;f25tmo+qas4@...&gt;\r\nIn-Reply-To: &lt;f25fp4+9b5d@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0\r\nFrom: &quot;afcarl2&quot; &lt;a.carl@...&gt;\r\nSubject: Re: Tile Coding and HyperNEAT\r\nX-Yahoo-Group-Post: member; u=281645563; y=bkg53J5UobLW_hjwchobXVYKIPVfAIX4msJeCLiWjkdZWQ\r\nX-Yahoo-Profile: afcarl2\r\n\r\nThe extreme cited was your referenced example of cutting up a chess \nboard.=\r\n I completely agree with your statement that the divisions \nshould occur at=\r\n the discontinuities. It just appears that you are \nfocusing on extremes to=\r\n prove your point, but at the same time \nappears to undermine your position=\r\n. &quot;Tile coding&quot; focuses on \npartitioning the design space to facilitate the=\r\n functional \nrepresentation. You seem to acknowledge the need to make divis=\r\nions at \ndiscontinuities in the design space. It appears that you take issu=\r\ne \nas to the degree or multiplicity of divisions, or the rational upon \nwhi=\r\nch the divisions were based upon. Yet the example problems \nreferenced to d=\r\nate for HyperNEAT exclusively deal with lower \ndimensional geometry which a=\r\nppear to not contain the types of \ndiscontinuities which would necessitate =\r\ndifferent functional \nrepresentation in different sub-regions of the design=\r\n space. \nFurthermore, to date, no mechanism appears to be implemented which=\r\n \nwould permit co-evolution of differing functional representations \nalong =\r\nwith their necessary mapping functions/data that would identify \nthe applic=\r\nable sub-portion of the design space addressed by a given \nsub-region funct=\r\nional representation.\n\nNow I realize that it is early on in the development=\r\n cycle for \nHyperNEAT, and that the simple examples provided in the papers =\r\nwere \nselected to best illustrate the ability and importance of geometric \n=\r\ninformation, but it appears to be premature to degrade an approach \nwhich f=\r\nocuses on divisions and HyperNEAT focuses on simplistic sub-\nregions withou=\r\nt capacity for divisions.\n\nYour point is well taken that divisions should o=\r\nccur at \ndiscontinuities. Equally important is the ability handle divisions=\r\n \nwithin the design space. I&#39;m just of the opinion that it is not as \nblack=\r\n and white as you have represented. If &quot;tile coding&quot; is guilty \nof irration=\r\nal division, HyperNEAT is guilty of simplistic \nrepresentation by omission =\r\nof division, thus far.\n\n\n--- In neat@yahoogroups.com, &quot;Kenneth Stanley&quot; &lt;ks=\r\ntanley@...&gt; wrote:\n&gt;\n&gt; I&#39;m not certain what &quot;the extreme cited&quot; refers to, =\r\nbut we should \nnot \n&gt; confuse tile coding with the classic &quot;divide and conq=\r\nuer&quot; approach \nto \n&gt; problem solving.  No one who follows divide and conque=\r\nr cuts a \n&gt; problem up into completely arbitrary chunks.  In fact, divide a=\r\nnd \n&gt; conquer is based on the idea that when you do divide, you divide \n*at=\r\n* \n&gt; the discontinuities that occur along dimensions of design.  Thus it \n&gt;=\r\n implies an initial assessment of the overall geometry of the \n&gt; problem.  =\r\nOnce again, that&#39;s exactly what tile coding throws away.\n&gt; \n&gt; By the way, &quot;=\r\ngeometry&quot; does not imply a necessary analogue with the \n&gt; dimensions of phy=\r\nsical space.  It only implies that there are \nindeed \n&gt; orthogonal dimensio=\r\nns.  So the &quot;higher design space dimensionality&quot; \n&gt; is part of the geometry=\r\n.\n&gt; \n&gt; ken\n&gt; \n&gt; --- In neat@yahoogroups.com, &quot;afcarl2&quot; &lt;a.carl@&gt; wrote:\n&gt; &gt;=\r\n\n&gt; &gt; IMHO, it appears that the extreme cited dismisses an element that \n&gt; &gt;=\r\n holds value. The sole focus on geometry (i.e. 1d, 2d or 3d), is a \n&gt; &gt; sim=\r\nplified subset of a problem dimensionality. Most useful \nproblems \n&gt; &gt; have=\r\n a higher design space dimensionality in which global \nfunctions \n&gt; &gt; are c=\r\nomprised of a collection of regional &quot;global functions&quot; \nalong \n&gt; &gt; with th=\r\neir individual application sub-domain definitions, which \n&gt; taken \n&gt; &gt; toge=\r\nther makeup the whole.\n&gt; &gt; \n&gt; &gt; An approach which works well within a regio=\r\nnal sub-domain isn&#39;t \nbad \n&gt; &gt; because it cannot adequately address all of =\r\nthe design space, \n&gt; unless \n&gt; &gt; of course it can co-evolve different funct=\r\nions in different sub-\n&gt; &gt; regions along with the corresponding regional \na=\r\npplication /mapping \n&gt; &gt; between them.\n&gt; &gt; \n&gt; &gt; Also, as your example illus=\r\ntrates, it would be seriously \n&gt; &gt; counterproductive to throw away proximit=\r\ny-based information, \n&gt; whether \n&gt; &gt; geometry or other higher dimensional i=\r\nnformation.\n&gt; &gt; \n&gt; &gt; Many useful problems are comprised of discontinuities =\r\nthat have \nto \n&gt; be \n&gt; &gt; dealt with individually. &quot;Divide-and-conquer&quot; is a=\r\n powerful \n&gt; approach \n&gt; &gt; when properly applied.\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; --- In neat=\r\n@yahoogroups.com, &quot;Kenneth Stanley&quot; &lt;kstanley@&gt; wrote:\n&gt; &gt; &gt;\n&gt; &gt; &gt; Joe, I a=\r\ngree this is a revealing discussion.  Tile coding to me \na \n&gt; &gt; is \n&gt; &gt; &gt; a=\r\n telling example of a significant misdirection of effort in \n&gt; &gt; machine \n&gt;=\r\n &gt; &gt; learning right now.  \n&gt; &gt; &gt; \n&gt; &gt; &gt; Most of what you said is factually =\r\ntrue.  But the spin you put \non \n&gt; &gt; it \n&gt; &gt; &gt; is wrong.  It is correct tha=\r\nt tile coding breaks up the \n&gt; &gt; state/action \n&gt; &gt; &gt; space into little piec=\r\nes to make the right behavior for each \n&gt; little \n&gt; &gt; &gt; region easier to co=\r\nmpute.  As you put it, &quot;subtiles can better \n&gt; fit \n&gt; &gt; &gt; the value functio=\r\nn being learned. Note that there is very \nlittle \n&gt; &gt; &gt; generalization desi=\r\nred here.&quot;  You say that like it&#39;s a good \n&gt; thing.\n&gt; &gt; &gt; \n&gt; &gt; &gt; However, t=\r\nhe fact that there is a need to do something like \nthat \n&gt; is \n&gt; &gt; &gt; more a=\r\n symptom of a serious disease in RL than an \naccomplishment \n&gt; we \n&gt; &gt; &gt; sh=\r\nould be congratulating ourselves for.  It&#39;s like using \ncocaine \n&gt; to \n&gt; &gt; =\r\n&gt; stay awake at work and claiming that it was a good idea because \n&gt; you \n&gt;=\r\n &gt; &gt; were more alert.  The fact is the whole approach is sick to \nbegin \n&gt; =\r\n&gt; &gt; with if it needs cocaine to function properly.\n&gt; &gt; &gt; \n&gt; &gt; &gt; And that&#39;s =\r\nwhat tile coding is really indicating: Much of RL is \n&gt; &gt; DOA.  \n&gt; &gt; &gt; Tile=\r\n coding is a symptom of a larger sickness.  You said it \n&gt; &gt; &gt; yourself: &quot;[=\r\nmost] RL is inherently incapable of performing \nmodel \n&gt; &gt; &gt; selection.&quot;  W=\r\nell, if what that means is that you can&#39;t exploit \n&gt; &gt; &gt; geometry, it&#39;s all=\r\n a dead end.  I am not certain that RL (aside \n&gt; &gt; from \n&gt; &gt; &gt; NEAT+Q-type =\r\nstuff) is really incapable of optimizing the model \n&gt; &gt; &gt; because who knows=\r\n what we might realize how to do in the \nfuture.  \n&gt; &gt; &gt; However, for now, =\r\nRL is falling back on tile coding because it \nis \n&gt; &gt; &gt; moving in the wrong=\r\n direction.\n&gt; &gt; &gt; \n&gt; &gt; &gt; Here is what is really going on:  Each variable in=\r\n the \n&gt; state/action \n&gt; &gt; &gt; space is a dimension along which the value func=\r\ntion varies.  A \n&gt; good \n&gt; &gt; &gt; learning algorithm would represent how the v=\r\nalue function \nvaries \n&gt; &gt; with \n&gt; &gt; &gt; respect to each state variable.  How=\r\never, such variation may be \n&gt; &gt; &gt; complex, i.e. the function could be pret=\r\nty complicated.  The \n&gt; &gt; learning \n&gt; &gt; &gt; methods (i.e. supervised function=\r\n approximators) inside RL are \n&gt; &gt; &gt; sufficiently bad that they cannot hand=\r\nle approximating \nfunctions \n&gt; &gt; like \n&gt; &gt; &gt; that.  So what do we do?  We b=\r\nreak the whole space into \nchunks.  \n&gt; &gt; Now \n&gt; &gt; &gt; the appropriate action =\r\nfor each little chunk requires a much \n&gt; &gt; simpler \n&gt; &gt; &gt; function, so we h=\r\nave a chance with our poor learning algorithm \nto \n&gt; &gt; &gt; maybe get all thes=\r\ne little simple functions right instead of \nonly \n&gt; a \n&gt; &gt; &gt; few big compli=\r\ncated functions.  \n&gt; &gt; &gt; \n&gt; &gt; &gt; In other words, we have a poor algorithm an=\r\nd the cure is to \n&gt; destroy \n&gt; &gt; &gt; what variational structure there was to =\r\nbegin with so that we \ncan \n&gt; &gt; &gt; look at every little bit of the problem s=\r\neparately.   So we \nhave \n&gt; &gt; now \n&gt; &gt; &gt; lost the ability to exploit all th=\r\ne useful relationships that \n&gt; &gt; &gt; initially existed in the space.  States =\r\nthat are related are \nnow \n&gt; &gt; &gt; broken apart and must be learned separatel=\r\ny, that is, the \n&gt; geometry \n&gt; &gt; &gt; has been destroyed! The fact that many s=\r\nee such an operation as \na \n&gt; &gt; &gt; step in the right direction is symptomati=\r\nc of serious \n&gt; misdirection \n&gt; &gt; in \n&gt; &gt; &gt; the field.  If that&#39;s the best =\r\nwe can do to make RL easier, \nthan \n&gt; RL \n&gt; &gt; &gt; is in serious trouble!\n&gt; &gt; =\r\n&gt; \n&gt; &gt; &gt; Think of it like this:  Take a game like chess, which I learned \n&gt;=\r\n as \n&gt; &gt; a \n&gt; &gt; &gt; little kid.  Now take the 64 squares and cut each piece o=\r\nut of \n&gt; the \n&gt; &gt; &gt; board individually.  Now sprinkle them all randomly all=\r\n over \nyour \n&gt; &gt; &gt; living room.  Each square still represents the same loca=\r\ntion it \n&gt; was \n&gt; &gt; &gt; originally taken from in the board.  It&#39;s just you ca=\r\nn&#39;t see \n&gt; where \n&gt; &gt; &gt; they were anymore.  Now place the chess pieces in t=\r\nhe right \n&gt; &gt; starting \n&gt; &gt; &gt; squares and teach a little kid to play chess.=\r\n  Think he or she \n&gt; &gt; would \n&gt; &gt; &gt; learn anything at all?\n&gt; &gt; &gt; \n&gt; &gt; &gt; Wel=\r\nl, that&#39;s exactly what tile coding is!  A method that learns \n&gt; &gt; chess \n&gt; =\r\n&gt; &gt; (or anything else that has implicit or explicit geometry) needs \n&gt; to \n=\r\n&gt; &gt; &gt; know how the positions relate to each other geometrically \nbecause \n&gt;=\r\n &gt; &gt; there is massive regularity being lost without that \ninformation.  \n&gt; =\r\n&gt; &gt; What kind of crazy algorithm would purposely put a chess board \n&gt; into =\r\n\n&gt; &gt; a \n&gt; &gt; &gt; meaningless order before learning begins?  A method \n&gt; that &quot;=\r\nbenefits&quot; \n&gt; &gt; &gt; from such an approach is clearly DOA.  RL researchers shou=\r\nld be \n&gt; &gt; &gt; seriously concerned about tile coding being necessary at all, =\r\n\nnot \n&gt; &gt; &gt; happy about it.\n&gt; &gt; &gt; \n&gt; &gt; &gt; So I stick to my position: Tile co=\r\nding is anti-geometry and \nanti-\n&gt; &gt; &gt; representation.  It deserves no cred=\r\nit whatsoever \n&gt; for &quot;respecting&quot; \n&gt; &gt; &gt; anything.\n&gt; &gt; &gt; \n&gt; &gt; &gt; ken\n&gt; &gt; &gt; \n=\r\n&gt; &gt; &gt; --- In neat@yahoogroups.com, Joseph Reisinger &lt;joeraii@&gt; wrote:\n&gt; &gt; &gt;=\r\n &gt;\n&gt; &gt; &gt; &gt; I&#39;ve been aching to reply to this post for a while, and I \n&gt; &gt; f=\r\ninally  \n&gt; &gt; &gt; &gt; have enough free time to do so. I think we could have a \nr=\r\neally  \n&gt; &gt; &gt; &gt; interesting discussion here, hopefully at least more \n&gt; int=\r\neresting \n&gt; &gt; &gt; than  \n&gt; &gt; &gt; &gt; the NFL tangent.\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt;&gt; Sure, b=\r\nut tile-coding does respect at least one form of \n&gt; &gt; geometry:\n&gt; &gt; &gt; &gt; &gt;&gt; =\r\nNearby elements in the state space are known to be nearby, \n&gt; and \n&gt; &gt; &gt; th=\r\nus\n&gt; &gt; &gt; &gt; &gt;&gt; are grouped in the same tile.\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; I have to d=\r\nispute this characterization of tile coding\n&gt; &gt; &gt; &gt; &gt; as &quot;respecting at lea=\r\nst one form of geometry.&quot; I think you \nare\n&gt; &gt; &gt; &gt; &gt; being unnecessarily eq=\r\nuitable toward tile coding.\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; What you are saying is that=\r\n in effect taking a nice \nsculpture \n&gt; &gt; and\n&gt; &gt; &gt; &gt; &gt; cutting it into piec=\r\nes &quot;respects&quot; its geometry because \nthose \n&gt; &gt; &gt; little\n&gt; &gt; &gt; &gt; &gt; pieces ar=\r\ne not broken up any further than that. It&#39;s like \n&gt; saying\n&gt; &gt; &gt; &gt; &gt; that s=\r\nomeone who cut your head off &quot;respected&quot; your head by \n&gt; &gt; &gt; keeping\n&gt; &gt; &gt; =\r\n&gt; &gt; its internal integrity intact. In fact, tile coding is \n&gt; &gt; peforming a=\r\n\n&gt; &gt; &gt; &gt; &gt; grievous violation against the existing geometry of the \n&gt; domai=\r\nn, \n&gt; &gt; &gt; and\n&gt; &gt; &gt; &gt; &gt; does not deserve to be credited with respecting geo=\r\nmetry\n&gt; &gt; &gt; &gt; &gt; whatsoever. I&#39;m hard pressed to imagine how one could do \nw=\r\norse\n&gt; &gt; &gt; &gt; &gt; beyond cutting things up into even tinier and tinier bits; \n=\r\n&gt; but \n&gt; &gt; &gt; even\n&gt; &gt; &gt; &gt; &gt; then, those bits still contain &quot;nearby elements=\r\n in the state\n&gt; &gt; &gt; &gt; &gt; space.&quot; So that isn&#39;t saying much.\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt;=\r\n Yeah, from the way your framing this argument, e.g. tile-\ncoding \n&gt; &gt; &gt; us=\r\ned  \n&gt; &gt; &gt; &gt; in the GA model-selection sense, you&#39;re absolutely right. \nI&#39;l=\r\nl \n&gt; &gt; get  \n&gt; &gt; &gt; &gt; back to exactly what I mean by that in a bit. For now =\r\nlets \ntry \n&gt; &gt; to  \n&gt; &gt; &gt; &gt; reframe the issue from an RL perspective, which=\r\n is where tile-\n&gt; &gt; &gt; codings  \n&gt; &gt; &gt; &gt; are predominantly used. In RL, the =\r\ntile-coding is just a  \n&gt; &gt; &gt; &gt; representation for a function approximator =\r\n(in a sense its \nsort \n&gt; &gt; of  \n&gt; &gt; &gt; &gt; like a really simple spline cure) t=\r\nhat learns in a supervised \n&gt; &gt; &gt; manner.  \n&gt; &gt; &gt; &gt; Tile coding makes a lot=\r\n of sense in this domain because you \n&gt; can  \n&gt; &gt; &gt; &gt; calculate with a good=\r\n deal of precision how much some \n&gt; particular \n&gt; &gt; &gt; tile  \n&gt; &gt; &gt; &gt; differ=\r\ns from the expected value of the function being \n&gt; &gt; approximated  \n&gt; &gt; &gt; &gt;=\r\n (in this case the Bellman error).\n&gt; &gt; &gt; &gt; Tiles hat is cover a broad area =\r\nwhere the value function \n&gt; changes \n&gt; &gt; a  \n&gt; &gt; &gt; &gt; lot (&quot;have bad fit&quot;, &quot;=\r\nare too general&quot;, etc) are then split \nso \n&gt; &gt; &gt; that  \n&gt; &gt; &gt; &gt; the subtile=\r\ns can better fit the value function being learned. \n&gt; &gt; Note  \n&gt; &gt; &gt; &gt; that=\r\n there is very little generalization desired here; the \nbest \n&gt; &gt; &gt; thing  =\r\n\n&gt; &gt; &gt; &gt; given infinite computational resources would be to have a \nwhole \n=\r\n&gt; &gt; &gt; ton  \n&gt; &gt; &gt; &gt; of itty-bitty tiles that fit the value function perfect=\r\nly.\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; Anyway, since we&#39;re in the standard RL framework, ther=\r\ne is \n&gt; really \n&gt; &gt; &gt; no  \n&gt; &gt; &gt; &gt; way of learning the &quot;geometry&quot; of a valu=\r\ne function (well, \n&gt; &gt; &gt; technically  \n&gt; &gt; &gt; &gt; there is, but thats a long t=\r\nangent towards a really \n&gt; interesting  \n&gt; &gt; &gt; &gt; research area). Maybe if t=\r\nhe geometry was given by the \n&gt; &gt; &gt; experimenter  \n&gt; &gt; &gt; &gt; beforehand (this=\r\n would also lead to an interesting extension \nof \n&gt; &gt; &gt; tile- \n&gt; &gt; &gt; &gt; codi=\r\nngs that you might like a little better). But in any \ncase, \n&gt; &gt; &gt; since  \n=\r\n&gt; &gt; &gt; &gt; all we&#39;re trying to do in RL is supervised function \n&gt; &gt; approximat=\r\nion,  \n&gt; &gt; &gt; &gt; the lack of geometry isn&#39;t bad.\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; =\r\nI&#39;m obviously not a big fan of tile coding :) I&#39;m not really\n&gt; &gt; &gt; &gt; &gt; conc=\r\nerned whether it might do better in some cases; the \n&gt; problem \n&gt; &gt; &gt; with\n=\r\n&gt; &gt; &gt; &gt; &gt; it is that it is a dead end for future progress because it \nis \n&gt;=\r\n &gt; &gt; about\n&gt; &gt; &gt; &gt; &gt; ruining our ability to exploit geometric relationships=\r\n.\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; Ok, this is where the discussion gets really interesting=\r\n. \n&gt; &gt; Remember  \n&gt; &gt; &gt; &gt; when I mentioned GA&#39;s &quot;performing model selection=\r\n&quot; or \nsomething \n&gt; &gt; &gt; like  \n&gt; &gt; &gt; &gt; that before? Thats a fundamental diff=\r\nerence in the GA \napproach \n&gt; &gt; and  \n&gt; &gt; &gt; &gt; RL. So what do I mean by mode=\r\nl selection: roughly speaking, \nin  \n&gt; &gt; &gt; &gt; Bayesian inference you have th=\r\nis idea of some separation of \n&gt; the  \n&gt; &gt; &gt; &gt; parameters you are optimizin=\r\ng (e.g. the weights in an NN) and \n&gt; &gt; the  \n&gt; &gt; &gt; &gt; model that generates t=\r\nhose parameters (e.g. the topology of \nthe \n&gt; &gt; &gt; NN,  \n&gt; &gt; &gt; &gt; or even whe=\r\nther you use an NN or decision tree or something). \n&gt; RL \n&gt; &gt; &gt; is  \n&gt; &gt; &gt; =\r\n&gt; inherently incapable of performing model selection (at least \n&gt; &gt; &gt; outsi=\r\nde  \n&gt; &gt; &gt; &gt; of NEAT+Q and some others). Once you start learning with  a \n&gt;=\r\n &gt; given  \n&gt; &gt; &gt; &gt; value function representation, you can no longer switch =\r\nto a  \n&gt; &gt; &gt; &gt; different representation without throwing away everything \n&gt;=\r\n you&#39;ve \n&gt; &gt; &gt; just  \n&gt; &gt; &gt; &gt; learned.  GAs on the other hand learn one par=\r\nameterized model \n&gt; &gt; per  \n&gt; &gt; &gt; &gt; individual. This is an important distin=\r\nction.\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; Now, what does this have to do with tile coding and=\r\n learning  \n&gt; &gt; &gt; &gt; geometry?  When you talk about &quot;cutting up different \nv=\r\nariables&quot; \n&gt; &gt; &gt; you  \n&gt; &gt; &gt; &gt; are inherently making an argument from the s=\r\ntandpoint of \nmodel  \n&gt; &gt; &gt; &gt; selection: i.e. what is the best representati=\r\non for this \n&gt; &gt; learning  \n&gt; &gt; &gt; &gt; problem? This is a valid question in th=\r\ne GA world, and I \nagree \n&gt; &gt; &gt; with  \n&gt; &gt; &gt; &gt; you tile coding wouldn&#39;t wor=\r\nk at all for learning good  \n&gt; &gt; &gt; &gt; representations that allow good future=\r\n learning. But from the \n&gt; RL  \n&gt; &gt; &gt; &gt; standpoint, since all tile-coding i=\r\ns used for is function  \n&gt; &gt; &gt; &gt; approximation, I don&#39;t think they are as p=\r\nroblematic as you \n&gt; &gt; imagine.\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; -- Joe\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt;\n&gt;=\r\n\n\n\n\n"}}