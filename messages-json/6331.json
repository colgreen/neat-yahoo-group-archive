{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":360532607,"authorName":"Sebastian Risi","from":"Sebastian Risi &lt;sebastian.risi@...&gt;","profile":"sebastian.risi","replyTo":"LIST","senderId":"A25IIrJvS_UI3eRBdgPHdyWAEgtayemY7_UMlsAr4O622_UyuqgXJ9P2vd27zkidg7TnLeE_qXBiE09VmhOQQofa5RhThjdPdQ2fIZvit2E","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] New Paper on Evolving Topographic Maps with HyperNEAT","postDate":"1399927903","msgId":6331,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PENBSm42PWRyRW81TkZHVm5Bd3J0PTZTeFNEaUZTRXUxTlNLYk4wZlo3a2tkOS11X3hLd0BtYWlsLmdtYWlsLmNvbT4=","inReplyToHeader":"PENBTnRYaG12WnpCTGlRdjllVUhUYUc1eGktblFqS1pjTEJiNTU5dlFNWmFucmg9aDNyd0BtYWlsLmdtYWlsLmNvbT4=","referencesHeader":"PENBSm42PWRwK3E9V3ZTeHJHUkE9eEpjQ0I1Q3h4QXNaMThUWVBxeUJ2XzNqU1FjNHR2Z0BtYWlsLmdtYWlsLmNvbT4JPENBTnRYaG12WnpCTGlRdjllVUhUYUc1eGktblFqS1pjTEJiNTU5dlFNWmFucmg9aDNyd0BtYWlsLmdtYWlsLmNvbT4="},"prevInTopic":6324,"nextInTopic":0,"prevInTime":6330,"nextInTime":6332,"topicId":6273,"numMessagesInTopic":9,"msgSnippet":"Hi Vassilis, Martin and Jeff, Thank you all for your kind words and sorry for my slow response to the questions. I ll try to answer them below: * 1) Network","rawEmail":"Return-Path: &lt;sebastian.risi@...&gt;\r\nX-Sender: sebastian.risi@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 10504 invoked by uid 102); 12 May 2014 20:51:43 -0000\r\nX-Received: from unknown (HELO mtaq6.grp.bf1.yahoo.com) (10.193.84.37)\n  by m3.grp.bf1.yahoo.com with SMTP; 12 May 2014 20:51:43 -0000\r\nX-Received: (qmail 3811 invoked from network); 12 May 2014 20:51:43 -0000\r\nX-Received: from unknown (HELO mail-qc0-f182.google.com) (209.85.216.182)\n  by mtaq6.grp.bf1.yahoo.com with SMTP; 12 May 2014 20:51:43 -0000\r\nX-Received: by mail-qc0-f182.google.com with SMTP id e16so8630265qcx.13\n        for &lt;neat@yahoogroups.com&gt;; Mon, 12 May 2014 13:51:43 -0700 (PDT)\r\nMIME-Version: 1.0\r\nX-Received: by 10.140.94.179 with SMTP id g48mr39910868qge.58.1399927903559;\n Mon, 12 May 2014 13:51:43 -0700 (PDT)\r\nX-Received: by 10.96.109.69 with HTTP; Mon, 12 May 2014 13:51:43 -0700 (PDT)\r\nIn-Reply-To: &lt;CANtXhmvZzBLiQv9eUHTaG5xi-nQjKZcLBb559vQMZanrh=h3rw@...&gt;\r\nReferences: &lt;CAJn6=dp+q=WvSxrGRA=xJcCB5CxxAsZ18TYPqyBv_3jSQc4tvg@...&gt;\n\t&lt;CANtXhmvZzBLiQv9eUHTaG5xi-nQjKZcLBb559vQMZanrh=h3rw@...&gt;\r\nDate: Mon, 12 May 2014 22:51:43 +0200\r\nMessage-ID: &lt;CAJn6=drEo5NFGVnAwrt=6SxSDiFSEu1NSKbN0fZ7kkd9-u_xKw@...&gt;\r\nTo: neat@yahoogroups.com\r\nContent-Type: multipart/alternative; boundary=001a113ab3ec672d7704f93a1de3\r\nSubject: Re: [neat] New Paper on Evolving Topographic Maps with HyperNEAT\r\nX-Yahoo-Group-Post: member; u=360532607; y=2SGfXUOf8dAppkdLtnsIUh4UQrnCl0zJ4QidXMDT-tPf0ew2VhoP8bo\r\nX-Yahoo-Profile: sebastian.risi\r\nFrom: Sebastian Risi &lt;sebastian.risi@...&gt;\r\n\r\n\r\n--001a113ab3ec672d7704f93a1de3\r\nContent-Type: text/plain; charset=UTF-8\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\nHi Vassilis, Martin and Jeff,\n\nThank you all for your kind words and sorry =\r\nfor my slow response to the\nquestions. I&#39;ll try to answer them below:\n\n*&quot;1)=\r\n Network activation: I think there are two ways of updating the states\nof t=\r\nhe neurons: (i) synchronous and (ii) asynchronous. *\n\n*The asynchronous upd=\r\nate is basically what is done in networks with layers,\nwhere a signal start=\r\ning at the input layer at time t arrives at the output\nlayer at the same ti=\r\nmestep. The synchronous update is what most people\nimplement in networks wi=\r\nth a free topology, where the neuron inputs at time\nt produce the output at=\r\n time t+1. This delay can be interpreted as the time\nfor a signal to be pro=\r\npagated from a presynaptic neuron to a postsynaptic\none. Consequently, in o=\r\nrder to express causality of firing, in the hebbian\nrule the presynaptic ac=\r\ntivity should be taken from time t-1, while the\npostsynaptic activity shoul=\r\nd be taken from time t.*\n\n*I was wondering whether you experimented with th=\r\nis. In your paper you say\nthat the network is activated 15 times, so are yo=\r\nu using the synchronous\nstate update?&quot;*\n\nYes, the network is updated synchr=\r\nonously and we did not try an\nasynchronous variation.\n\n*&quot;2) Have you experi=\r\nmented with learning rules other than the standard\nHebbian one? For example=\r\n, the rules of the original Adaptive HyperNEAT\npaper, or by adding a weight=\r\n decay to the equation of the standard Hebbian\nrule.&quot;*\n\nNot in this paper a=\r\nlthough I think it would be very interesting to do so in\nthe future. One re=\r\nason we didn&#39;t add more complicated learning rules was to\nisolate the effec=\r\nt of indirectly encoded weights and learning rates by\nthemselves.\n\n*&quot;3) I w=\r\nas wondering whether you thought of any other tasks to evaluate\nyour approa=\r\nch. For example, line orientation maps develop in the primary\nvisual cortex=\r\n, and as you say in the paper, these topographic maps are\nfound in all sens=\r\nory systems. So, would it be sensible to have a task in\nwhich you &quot;use&quot; oth=\r\ner feature detector cells, let&#39;s say from the primary\nauditory cortex, and =\r\na topographic map is then self-organized to encode\nsound frequencies?&quot;*\n\nTh=\r\ne approach should be applicable to other domains as well, especially\ndomain=\r\ns in which self-organizing maps have shown promising results. Here is\nan ex=\r\nample of SOMs applied to modelling the auditory cortex (\nhttp://www.inb.uni=\r\n-luebeck.de/publikationen/pdfs/MaRiSc89a.pdf). What I\nlike about this appro=\r\nach is that the same general learning algorithm should\nbe able to learn a v=\r\nariety of different input modalities (similar to how\nour brain seem to func=\r\ntion). Evolution can then fine-tune this general\nlearning algorithm to tail=\r\nor it more to a specific domain.\n\n*&quot;4) I believe line orientation maps exis=\r\nt at the lowest level of a\nhierarchical architecture, and as information mo=\r\nves to higher levels, more\nabstract concepts can be encoded. These abstract=\r\n concepts can fuse\ninformation from all sensory systems, e.g., how does a d=\r\nog looks like,\nsmells like, sounds like, etc. I believe that using HyperNEA=\r\nT with such\nhierarchical architectures (and not just for developing a contr=\r\noller) is a\npromising approach and I look forward to seeing it mature.&quot;*\n\nM=\r\ne too! Ultimately I see a combination of the topographic map seed,\ntogether=\r\n with a method that can evolve the HyperNEAT substrate (like\nES-HyperNEAT; =\r\nhttp://eplex.cs.ucf.edu/papers/risi_alife12.pdf) and a method\nsuch as novel=\r\nty search as the basis to evolve such an architecture.\n\n*&quot; However, when I =\r\nunderstand your paper correctly, already the search for\na &#39;mexican hat&#39; tur=\r\nns out to be too deceptive to evolve it from\nscratch!?&quot; Now the question is=\r\n of course whether the deceptiveness lies in\nthe fitness function or in the=\r\n nature of CPPNs. But it is at least\ndiscouraging to see that CPPNs cannot =\r\neasily be used in an objective search\nto evolve networks that we want to se=\r\ne.&quot;*\n\nThe mexican hat is an activation function that was added to the avail=\r\nable\nCPPN-activation functions. The hard part is to re-evolve something tha=\r\nt\nshows the same self-organizing dynamics as the CPPN seed (Figure 2).  I\nb=\r\nelieve (and others here will probably agree) that this is not a problem of\n=\r\nthe CPPNs themselves but a general problem of objective-based search\nmethod=\r\ns, which was also nicely shown in the following paper:\nhttp://eplex.cs.ucf.=\r\nedu/papers/woolley_gecco11.pdf\nUltimately, it will likely be beneficial to =\r\nuse a method like novelty\nsearch instead of a fitness-based approach to evo=\r\nlve something like\ntopographic maps from scratch.\n\n*&quot;There were two points =\r\nin your paper that I did not understand.  The\ndefinition of the fitness-fun=\r\nction (formula 3) implicates that fitness\nvalues are between 0-1. But figur=\r\ne 5 and others show values between 0-150.\nHow are these values related to t=\r\nhe fitness-function?&quot;*\n\nAt each timestep the network receives a fitness bet=\r\nween 0-1 but they are\nsummed over all timesteps, so ultimately they can rea=\r\nch higher values.\n\n*&quot;And what changes in an evolutionary run in HyperNEAT w=\r\nith a Seeded\nUniform approach. From what I understand, it means that the Se=\r\neded part is\nkept constant, the learning rate is kept constant to 1.0 and t=\r\nhe weights\nfrom the input to the output are set to 0.5. So what can be alte=\r\nred in this\nencoding during an evoluationary run?&quot;*\n\nThe CPPN-Seed is free =\r\nto evolve and there are no restrictions imposed on it\n(e.g. links and nodes=\r\n can be added, weights can change, etc.) Seeding in\nthis case just means th=\r\nat instead of initial random CPPNs, the initial\npopulation is composed of m=\r\nutated versions of the seed CPPN. While the\nlearning rate and weights from =\r\nthe input to the hidden layer are kept\nconstant for the Seeded Uniform appr=\r\noach, the hidden layer weights\n(determined by CPPN BB) can still change.\n\n*=\r\n&quot;Are the nodes in the CPPN drawing in Fig. 2 Magic Hat activation nodes?\nTh=\r\ne drawing makes them look like absolute value nodes. &quot;*\n\n The BB output nod=\r\ne is a mexican hat function\n\n*&quot;Is it right to characterize the problem in t=\r\nhe experiments where you\nevolve the seed as =E2=80=9CTarget Weights=E2=80=\r\n=9D (i.e. the same problem from [1])? In\nother words, you give it a specifi=\r\nc neural network and its goal is to match\nthat exact network (though, in th=\r\nis case, after a period of\nlearning/self-organization)?&quot;*\n\nThe weight patte=\r\nrn that the CPPN outputs has to match a predefined weight\npattern. So I gue=\r\nss you could say they have to match the exact network.\n\n*&quot;If so, it seems l=\r\nater in the paper that you say that the target network\nit is supposed to ma=\r\ntch is the one shown in Fig. 3b. Is that right, or did\nyou challenge it wit=\r\nh a different randomly-created target network each\nrun?&quot;*\n\nIt was also supp=\r\nosed to match the exact same one.\n\n*&quot;If you did challenge it with the netwo=\r\nrk in Fig. 3b, then I don=E2=80=99t get why\nit takes any time at all for mo=\r\nst treatments to evolve this pattern. Don=E2=80=99t\nyou show initially (e.g=\r\n. Fig. 3) that the seed itself produces this exact\npattern with no further =\r\nevolution? I apologize if I am missing something. &quot;*\n\nThe seed does produce=\r\n the exact same pattern after the original training\nphase of 250 input repr=\r\nesentations. However, the idea in this paper was to\nevolve this initial see=\r\nd further, ultimately allowing the network to reach\nthe same final state in=\r\n fewer training steps (because of the initial\nCPPN-determined weight patter=\r\nn). Such an accelerated self-organization is\ne.g. shown in Figure 8.\n\n*&quot;I s=\r\nhare Martin=E2=80=99s question about exactly what is free to evolve in the\n=\r\nSeeded Uniform treatment. My guess is it is the B to B connections (i.e.\nco=\r\nnnections from nodes within the output layer to other nodes in the output\nl=\r\nayer), but I wasn=E2=80=99t sure.&quot;*\n\nI hope I was able to answer that above=\r\n :)\n\n*&quot;Finally, why do you think Random does worse on the test set than the=\r\n\ntraining set? It seems competitive on the training set, just not on the\nte=\r\nst (i.e. generalization) set. &quot;*\n\nI think during training Random might just=\r\n sometimes get lucky and produce\nthe correct pattern in all 3 training tria=\r\nls. Because the generalisation\ntest is based on 30 trials, producing the co=\r\nrrect pattern just based on\nchance is less likely.\n\nPlease let me know if y=\r\nou have any other questions. I&#39;m glad evolving\nplastic neural is becoming m=\r\nore popular, since it is the future of NE (at\nleast in my opinion :)\n\nCheer=\r\ns,\nSebastian\n\n\n\n\nOn Thu, May 1, 2014 at 6:20 PM, Vassilis Vassiliades\n&lt;vass=\r\nilisvas@...&gt;wrote:\n\n&gt;\n&gt;\n&gt; I would also like to congratulate you on th=\r\nis paper! Very interesting\n&gt; indeed! I have a few questions and comments as=\r\n well:\n&gt;\n&gt; 1) Network activation: I think there are two ways of updating th=\r\ne states\n&gt; of the neurons: (i) synchronous and (ii) asynchronous.\n&gt;\n&gt; The a=\r\nsynchronous update is basically what is done in networks with layers,\n&gt; whe=\r\nre a signal starting at the input layer at time t arrives at the output\n&gt; l=\r\nayer at the same timestep. The synchronous update is what most people\n&gt; imp=\r\nlement in networks with a free topology, where the neuron inputs at time\n&gt; =\r\nt produce the output at time t+1. This delay can be interpreted as the time=\r\n\n&gt; for a signal to be propagated from a presynaptic neuron to a postsynapti=\r\nc\n&gt; one. Consequently, in order to express causality of firing, in the hebb=\r\nian\n&gt; rule the presynaptic activity should be taken from time t-1, while th=\r\ne\n&gt; postsynaptic activity should be taken from time t.\n&gt;\n&gt; I was wondering =\r\nwhether you experimented with this. In your paper you say\n&gt; that the networ=\r\nk is activated 15 times, so are you using the synchronous\n&gt; state update?\n&gt;=\r\n\n&gt; 2) Have you experimented with learning rules other than the standard\n&gt; H=\r\nebbian one? For example, the rules of the original Adaptive HyperNEAT\n&gt; pap=\r\ner, or by adding a weight decay to the equation of the standard Hebbian\n&gt; r=\r\nule.\n&gt;\n&gt; 3) I was wondering whether you thought of any other tasks to evalu=\r\nate your\n&gt; approach. For example, line orientation maps develop in the prim=\r\nary visual\n&gt; cortex, and as you say in the paper, these topographic maps ar=\r\ne found in\n&gt; all sensory systems. So, would it be sensible to have a task i=\r\nn which you\n&gt; &quot;use&quot; other feature detector cells, let&#39;s say from the primar=\r\ny auditory\n&gt; cortex, and a topographic map is then self-organized to encode=\r\n sound\n&gt; frequencies?\n&gt;\n&gt; 4) I believe line orientation maps exist at the l=\r\nowest level of a\n&gt; hierarchical architecture, and as information moves to h=\r\nigher levels, more\n&gt; abstract concepts can be encoded. These abstract conce=\r\npts can fuse\n&gt; information from all sensory systems, e.g., how does a dog l=\r\nooks like,\n&gt; smells like, sounds like, etc. I believe that using HyperNEAT =\r\nwith such\n&gt; hierarchical architectures (and not just for developing a contr=\r\noller) is a\n&gt; promising approach and I look forward to seeing it mature.\n&gt;\n=\r\n&gt;\n&gt; Vassilis\n&gt;\n&gt;\n&gt; On Mon, Apr 28, 2014 at 12:47 PM, Sebastian Risi &lt;sebast=\r\nian.risi@...\n&gt; &gt; wrote:\n&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; Kenneth Stanley and I are pleased =\r\nto announce our new GECCO paper (to\n&gt;&gt; appear), &quot;Guided Self-Organization i=\r\nn Indirectly Encoded and Evolving\n&gt;&gt; Topographic Maps&quot;\n&gt;&gt;\n&gt;&gt; *PDF: *http://=\r\neplex.cs.ucf.edu/papers/risi_gecco14.pdf\n&gt;&gt;\n&gt;&gt; This paper introduces the id=\r\nea that HyperNEAT can be seeded with a\n&gt;&gt; lateral inhibitory connectivity p=\r\nattern, which then allows the weights from\n&gt;&gt; the inputs to the hidden laye=\r\nr of an ANN to self-organize to form a genuine\n&gt;&gt; topographic map of the in=\r\nput space. The benefit of this new approach is\n&gt;&gt; that the initial seed can=\r\n be evolved further to accelerate the\n&gt;&gt; self-organizing process and to bia=\r\ns it towards a specific target\n&gt;&gt; configuration. An interesting implication=\r\n is that enabling HyperNEAT to\n&gt;&gt; capture this key feature of natural syste=\r\nms might now allow us to evolve\n&gt;&gt; architectures resembling deep learning n=\r\networks, SOMs, or other\n&gt;&gt; self-organizing processes that also rely on unsu=\r\npervised learning processes.\n&gt;&gt;\n&gt;&gt; *Abstract: *An important phenomenon seen=\r\n in many areas of biological\n&gt;&gt; brains and recently in deep learning archit=\r\nectures is a process known as\n&gt;&gt; self-organization. For example, in the pri=\r\nmary visual cortex, color and\n&gt;&gt; orientation maps develop based on lateral =\r\ninhibitory connectivity patterns\n&gt;&gt; and Hebbian learning dynamics. These to=\r\npographic maps, which are found in\n&gt;&gt; all sensory systems, are thought to b=\r\ne a key factor in enabling abstract\n&gt;&gt; cognitive representations. This pape=\r\nr shows for the first time that the\n&gt;&gt; Hypercube-based NeuroEvolution of Au=\r\ngmenting Topologies (HyperNEAT) method\n&gt;&gt; can be seeded to begin evolution =\r\n with such lateral connectivity, enabling\n&gt;&gt; genuine self-organizing dynami=\r\ncs. The proposed approach draws on\n&gt;&gt; HyperNEAT&#39;s ability to generate a pat=\r\ntern of weights across the\n&gt;&gt; connectivity of an artificial neural network =\r\n(ANN) based on a function of\n&gt;&gt; its geometry. Validating this approach, the=\r\n afferent weights of an ANN\n&gt;&gt; self-organize in this paper to form a genuin=\r\ne topographic map of the input\n&gt;&gt; space for a simple line orientation task.=\r\n Most interestingly, this seed can\n&gt;&gt; then be evolved further, providing a =\r\nmethod to guide the self-organization\n&gt;&gt; of weights in a specific way, much=\r\n as evolution likely guided the\n&gt;&gt; self-organizing trajectories of biologic=\r\nal brains.\n&gt;&gt;\n&gt;&gt; Cheers,\n&gt;&gt; Sebastian\n&gt;&gt;\n&gt;&gt; --\n&gt;&gt; Dr. Sebastian Risi\n&gt;&gt; Ass=\r\nistant Professor\n&gt;&gt; IT University of Copenhagen, Room 5D08\n&gt;&gt; Rued Langgaar=\r\nds Vej 7, 2300 Copenhagen, Denmark\n&gt;&gt; email: sebastian.risi@..., web:=\r\n www.sebastianrisi.com\n&gt;&gt; mobile: +45-50250355, office: +45-7218-5127\n&gt;&gt;\n&gt;&gt;=\r\n\n&gt;  \n&gt;\n\n\n\n-- \nDr. Sebastian Risi\nAssistant Professor\nIT University of Copen=\r\nhagen, Room 5D08\nRued Langgaards Vej 7, 2300 Copenhagen, Denmark\nemail: seb=\r\nastian.risi@..., web: www.sebastianrisi.com\nmobile: +45-50250355, off=\r\nice: +45-7218-5127\n\r\n--001a113ab3ec672d7704f93a1de3\r\nContent-Type: text/html; charset=UTF-8\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\n&lt;div dir=3D&quot;ltr&quot;&gt;Hi=C2=A0&lt;span style=3D&quot;font-family:arial,sans-serif;font-s=\r\nize:12.727272033691406px&quot;&gt;Vassilis, Martin and Jeff,&lt;/span&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;=\r\ndiv&gt;Thank you all for your kind words and sorry for my slow response to the=\r\n questions. I&#39;ll try to answer them below:&lt;/div&gt;\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;&lt;d=\r\niv style=3D&quot;font-family:arial,sans-serif;font-size:12.727272033691406px&quot;&gt;&lt;i=\r\n&gt;&quot;1) Network activation: I think there are two ways of updating the st=\r\nates of the neurons: (i) synchronous and (ii) asynchronous.=C2=A0&lt;/i&gt;&lt;/div&gt;=\r\n\n&lt;div style=3D&quot;font-family:arial,sans-serif;font-size:12.727272033691406px&quot;=\r\n&gt;&lt;i&gt;&lt;br&gt;&lt;/i&gt;&lt;/div&gt;&lt;div style=3D&quot;font-family:arial,sans-serif;font-size:12.7=\r\n27272033691406px&quot;&gt;&lt;i&gt;The asynchronous update is basically what is done in n=\r\networks with layers, where a signal starting at the input layer at time t a=\r\nrrives at the output layer at the same timestep. The synchronous update is =\r\nwhat most people implement in networks with a free topology, where the neur=\r\non inputs at time t produce the output at time t+1. This delay can be inter=\r\npreted as the time for a signal to be propagated from a presynaptic neuron =\r\nto a postsynaptic one. Consequently, in order to express causality of firin=\r\ng, in the hebbian rule the presynaptic activity should be taken from time t=\r\n-1, while the postsynaptic activity should be taken from time t.&lt;/i&gt;&lt;/div&gt;\n=\r\n&lt;div style=3D&quot;font-family:arial,sans-serif;font-size:12.727272033691406px&quot;&gt;=\r\n&lt;i&gt;&lt;br&gt;&lt;/i&gt;&lt;/div&gt;&lt;div style=3D&quot;font-family:arial,sans-serif;font-size:12.72=\r\n7272033691406px&quot;&gt;&lt;i&gt;I was wondering whether you experimented with this. In =\r\nyour paper you say that the network is activated 15 times, so are you using=\r\n the synchronous state update?&quot;&lt;/i&gt;&lt;/div&gt;\n&lt;div style=3D&quot;font-family:ar=\r\nial,sans-serif;font-size:12.727272033691406px&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div style=3D&quot;font=\r\n-family:arial,sans-serif;font-size:12.727272033691406px&quot;&gt;Yes, the network i=\r\ns updated synchronously and we did not try an asynchronous variation.&lt;/div&gt;=\r\n\n&lt;div style=3D&quot;font-family:arial,sans-serif;font-size:12.727272033691406px&quot;=\r\n&gt;&lt;br&gt;&lt;/div&gt;&lt;div style=3D&quot;font-family:arial,sans-serif;font-size:12.72727203=\r\n3691406px&quot;&gt;&lt;i&gt;&quot;2) Have you experimented with learning rules other than=\r\n the standard Hebbian one? For example, the rules of the original Adaptive =\r\nHyperNEAT paper, or by adding a weight decay to the equation of the standar=\r\nd Hebbian rule.&quot;&lt;/i&gt;&lt;/div&gt;\n&lt;div style=3D&quot;font-family:arial,sans-serif;=\r\nfont-size:12.727272033691406px&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div style=3D&quot;font-family:arial,s=\r\nans-serif;font-size:12.727272033691406px&quot;&gt;Not in this paper although I thin=\r\nk it would be very interesting to do so in the future. One reason we didn&#=\r\n39;t add more complicated learning rules was to isolate the effect of indir=\r\nectly encoded weights and learning rates by themselves.&lt;/div&gt;\n&lt;div style=3D=\r\n&quot;font-family:arial,sans-serif;font-size:12.727272033691406px&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;di=\r\nv style=3D&quot;font-family:arial,sans-serif;font-size:12.727272033691406px&quot;&gt;&lt;i&gt;=\r\n&quot;3) I was wondering whether you thought of any other tasks to evaluate=\r\n your approach. For example, line orientation maps develop in the primary v=\r\nisual cortex, and as you say in the paper, these topographic maps are found=\r\n in all sensory systems. So, would it be sensible to have a task in which y=\r\nou &quot;use&quot; other feature detector cells, let&#39;s say from the pri=\r\nmary auditory cortex, and a topographic map is then self-organized to encod=\r\ne sound frequencies?&quot;&lt;/i&gt;&lt;/div&gt;\n&lt;div style=3D&quot;font-family:arial,sans-s=\r\nerif;font-size:12.727272033691406px&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div style=3D&quot;font-family:ar=\r\nial,sans-serif;font-size:12.727272033691406px&quot;&gt;The approach should be appli=\r\ncable to other domains as well, especially domains in which self-organizing=\r\n maps have shown promising results. Here is an example of SOMs applied to m=\r\nodelling the auditory cortex (&lt;a href=3D&quot;http://www.inb.uni-luebeck.de/publ=\r\nikationen/pdfs/MaRiSc89a.pdf&quot;&gt;http://www.inb.uni-luebeck.de/publikationen/p=\r\ndfs/MaRiSc89a.pdf&lt;/a&gt;). What I like about this approach is that the same ge=\r\nneral learning algorithm should be able to learn a variety of different inp=\r\nut modalities (similar to how our brain seem to function). Evolution can th=\r\nen fine-tune this general learning algorithm to tailor it more to a specifi=\r\nc domain.&lt;br&gt;\n&lt;/div&gt;&lt;div style=3D&quot;font-family:arial,sans-serif;font-size:12=\r\n.727272033691406px&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div style=3D&quot;font-family:arial,sans-serif;fo=\r\nnt-size:12.727272033691406px&quot;&gt;&lt;i&gt;&quot;4) I believe line orientation maps e=\r\nxist at the lowest level of a hierarchical architecture, and as information=\r\n moves to higher levels, more abstract concepts can be encoded. These abstr=\r\nact concepts can fuse information from all sensory systems, e.g., how does =\r\na dog looks like, smells like, sounds like, etc. I believe that using Hyper=\r\nNEAT with such hierarchical architectures (and not just for developing a co=\r\nntroller) is a promising approach and I look forward to seeing it mature.&q=\r\nuot;&lt;/i&gt;&lt;/div&gt;\n&lt;/div&gt;&lt;div style=3D&quot;font-family:arial,sans-serif;font-size:1=\r\n2.727272033691406px&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div style=3D&quot;font-family:arial,sans-serif;f=\r\nont-size:12.727272033691406px&quot;&gt;Me too! Ultimately I see a combination of th=\r\ne topographic map seed, together with a method that can evolve the HyperNEA=\r\nT substrate (like ES-HyperNEAT; &lt;a href=3D&quot;http://eplex.cs.ucf.edu/papers/r=\r\nisi_alife12.pdf&quot;&gt;http://eplex.cs.ucf.edu/papers/risi_alife12.pdf&lt;/a&gt;) and a=\r\n method such as novelty search as the basis to evolve such an architecture.=\r\n=C2=A0&lt;/div&gt;\n&lt;div style=3D&quot;font-family:arial,sans-serif;font-size:12.727272=\r\n033691406px&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div style=3D&quot;font-family:arial,sans-serif;font-size=\r\n:12.727272033691406px&quot;&gt;&lt;div&gt;&lt;i&gt;&quot; However, when I understand your paper=\r\n correctly, already the search for a &#39;mexican hat&#39; turns out to be =\r\ntoo deceptive to evolve it from scratch!?&quot;=C2=A0Now the question is of=\r\n course whether the deceptiveness lies in the fitness function or in the na=\r\nture of CPPNs. But it is at least discouraging to see that CPPNs cannot eas=\r\nily be used in an objective search to evolve networks that we want to see.&=\r\nquot;&lt;/i&gt;&lt;/div&gt;\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;The mexican hat is an activation functi=\r\non that was added to the available CPPN-activation functions. The hard part=\r\n is to re-evolve something that shows the same self-organizing dynamics as =\r\nthe CPPN seed (Figure 2). =C2=A0I believe (and others here will probably ag=\r\nree) that this is not a problem of the CPPNs themselves but a general probl=\r\nem of objective-based search methods, which was also nicely shown in the fo=\r\nllowing paper: &lt;a href=3D&quot;http://eplex.cs.ucf.edu/papers/woolley_gecco11.pd=\r\nf&quot;&gt;http://eplex.cs.ucf.edu/papers/woolley_gecco11.pdf&lt;/a&gt;=C2=A0&lt;/div&gt;\n&lt;div&gt;=\r\nUltimately, it will likely be beneficial to use a method like novelty searc=\r\nh instead of a fitness-based approach to evolve something like topographic =\r\nmaps from scratch.&lt;/div&gt;&lt;div&gt;&lt;i&gt;&lt;br&gt;&lt;/i&gt;&lt;/div&gt;&lt;div&gt;&lt;i&gt;&quot;There were two =\r\npoints in your paper that I did not understand. =C2=A0The definition of the=\r\n fitness-function (formula 3) implicates that fitness values are between 0-=\r\n1. But figure 5 and others show values between 0-150. How are these values =\r\nrelated to the fitness-function?&quot;&lt;/i&gt;&lt;/div&gt;\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;At eac=\r\nh timestep the network receives a fitness between 0-1 but they are summed o=\r\nver all timesteps, so ultimately they can reach higher values.&lt;/div&gt;&lt;div&gt;&lt;b=\r\nr&gt;&lt;/div&gt;&lt;div&gt;&lt;i&gt;&quot;And what changes in an evolutionary run in HyperNEAT =\r\nwith a Seeded Uniform approach. From what I understand, it means that the S=\r\needed part is kept constant, the learning rate is kept constant to 1.0 and =\r\nthe weights from the input to the output are set to 0.5. So what can be alt=\r\nered in this encoding during an evoluationary run?&quot;&lt;/i&gt;&lt;/div&gt;\n&lt;div&gt;&lt;br=\r\n&gt;&lt;/div&gt;&lt;div&gt;The CPPN-Seed is free to evolve and there are no restrictions i=\r\nmposed on it (e.g. links and nodes can be added, weights can change, etc.) =\r\nSeeding in this case just means that instead of initial random CPPNs, the i=\r\nnitial population is composed of mutated versions of the seed CPPN. While t=\r\nhe learning rate and weights from the input to the hidden layer are kept co=\r\nnstant for the Seeded Uniform approach, the hidden layer weights (determine=\r\nd by CPPN BB) can still change.&lt;/div&gt;\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;&lt;i&gt;&quot;Are the =\r\nnodes in the CPPN drawing in Fig. 2 Magic Hat activation nodes? The drawing=\r\n makes them look like absolute value nodes. &quot;&lt;/i&gt;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;=\r\n&lt;div&gt;=C2=A0The BB output node is a mexican hat function&lt;/div&gt;\n&lt;div&gt;&lt;br&gt;&lt;/di=\r\nv&gt;&lt;div&gt;&lt;i&gt;&quot;Is it right to characterize the problem in the experiments =\r\nwhere you evolve the seed as =E2=80=9CTarget Weights=E2=80=9D (i.e. the sam=\r\ne problem from [1])? In other words, you give it a specific neural network =\r\nand its goal is to match that exact network (though, in this case, after a =\r\nperiod of learning/self-organization)?&quot;&lt;/i&gt;&lt;/div&gt;\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;=\r\nThe weight pattern that the CPPN outputs has to match a predefined weight p=\r\nattern. So I guess you could say they have to match the exact network.&lt;/div=\r\n&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;&lt;i&gt;&quot;If so, it seems later in the paper that you s=\r\nay that the target network it is supposed to match is the one shown in Fig.=\r\n 3b. Is that right, or did you challenge it with a different randomly-creat=\r\ned target network each run?&quot;&lt;/i&gt;&lt;/div&gt;\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;It was also=\r\n supposed to match the exact same one.&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;&lt;i&gt;&quot;If=\r\n you did challenge it with the network in Fig. 3b, then I don=E2=80=99t get=\r\n why it takes any time at all for most treatments to evolve this pattern. D=\r\non=E2=80=99t you show initially (e.g. Fig. 3) that the seed itself produces=\r\n this exact pattern with no further evolution? I apologize if I am missing =\r\nsomething. &quot;&lt;/i&gt;&lt;/div&gt;\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;The seed does produce the e=\r\nxact same pattern after the original training phase of 250 input representa=\r\ntions. However, the idea in this paper was to evolve this initial seed furt=\r\nher, ultimately allowing the network to reach the same final state in fewer=\r\n training steps (because of the initial CPPN-determined weight pattern). Su=\r\nch an accelerated self-organization is e.g. shown in Figure 8.&lt;/div&gt;\n&lt;div&gt;&lt;=\r\nbr&gt;&lt;/div&gt;&lt;div&gt;&lt;i&gt;&quot;I share Martin=E2=80=99s question about exactly what=\r\n is free to evolve in the Seeded Uniform treatment. My guess is it is the B=\r\n to B connections (i.e. connections from nodes within the output layer to o=\r\nther nodes in the output layer), but I wasn=E2=80=99t sure.&quot;&lt;/i&gt;&lt;/div&gt;=\r\n\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;I hope I was able to answer that above :)&lt;/div&gt;&lt;div&gt;&lt;b=\r\nr&gt;&lt;/div&gt;&lt;div&gt;&lt;i&gt;&quot;Finally, why do you think Random does worse on the te=\r\nst set than the training set? It seems competitive on the training set, jus=\r\nt not on the test (i.e. generalization) set. &quot;&lt;/i&gt;&lt;/div&gt;\n&lt;div&gt;&lt;i&gt;&lt;br&gt;&lt;=\r\n/i&gt;&lt;/div&gt;&lt;div&gt;I think during training Random might just sometimes get lucky=\r\n and produce the correct pattern in all 3 training trials. Because the gene=\r\nralisation test is based on 30 trials, producing the correct pattern just b=\r\nased on chance is less likely.=C2=A0&lt;/div&gt;\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Please let m=\r\ne know if you have any other questions. I&#39;m glad evolving plastic neura=\r\nl is becoming more popular, since it is the future of NE (at least in my op=\r\ninion :)&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Cheers,&lt;/div&gt;\n&lt;div&gt;Sebastian&lt;/div&gt;&lt;div&gt;&lt;d=\r\niv&gt;&lt;br&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;b=\r\nr&gt;&lt;br&gt;&lt;div class=3D&quot;gmail_quote&quot;&gt;On Thu, May 1, 2014 at 6:20 PM, Vassilis V=\r\nassiliades &lt;span dir=3D&quot;ltr&quot;&gt;&lt;&lt;a href=3D&quot;mailto:vassilisvas@...&quot; t=\r\narget=3D&quot;_blank&quot;&gt;vassilisvas@...&lt;/a&gt;&gt;&lt;/span&gt; wrote:&lt;br&gt;\n&lt;blockquot=\r\ne class=3D&quot;gmail_quote&quot; style=3D&quot;margin:0 0 0 .8ex;border-left:1px #ccc sol=\r\nid;padding-left:1ex&quot;&gt;\n\n\n&lt;u&gt;&lt;/u&gt;\n\n\n\n\n\n\n\n\n\n \n&lt;div style=3D&quot;background-color:#=\r\nfff&quot;&gt;\n&lt;span&gt;=C2=A0&lt;/span&gt;\n\n\n&lt;div&gt;\n  &lt;div&gt;\n\n\n    &lt;div&gt;\n      \n      \n      &lt;=\r\np&gt;&lt;/p&gt;&lt;div dir=3D&quot;ltr&quot;&gt;&lt;div&gt;I would also like to congratulate you on this p=\r\naper! Very interesting indeed! I have a few questions and comments as well:=\r\n&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;1) Network activation: I think there are two ways=\r\n of updating the states of the neurons: (i) synchronous and (ii) asynchrono=\r\nus.=C2=A0&lt;/div&gt;\n\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;The asynchronous update is basically w=\r\nhat is done in networks with layers, where a signal starting at the input l=\r\nayer at time t arrives at the output layer at the same timestep. The synchr=\r\nonous update is what most people implement in networks with a free topology=\r\n, where the neuron inputs at time t produce the output at time t+1. This de=\r\nlay can be interpreted as the time for a signal to be propagated from a pre=\r\nsynaptic neuron to a postsynaptic one. Consequently, in order to express ca=\r\nusality of firing, in the hebbian rule the presynaptic activity should be t=\r\naken from time t-1, while the postsynaptic activity should be taken from ti=\r\nme t.&lt;/div&gt;\n\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;I was wondering whether you experimented w=\r\nith this. In your paper you say that the network is activated 15 times, so =\r\nare you using the synchronous state update?&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;2) Hav=\r\ne you experimented with learning rules other than the standard Hebbian one?=\r\n For example, the rules of the original Adaptive HyperNEAT paper, or by add=\r\ning a weight decay to the equation of the standard Hebbian rule.&lt;/div&gt;\n\n&lt;di=\r\nv&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;3) I was wondering whether you thought of any other tasks =\r\nto evaluate your approach. For example, line orientation maps develop in th=\r\ne primary visual cortex, and as you say in the paper, these topographic map=\r\ns are found in all sensory systems. So, would it be sensible to have a task=\r\n in which you &quot;use&quot; other feature detector cells, let&#39;s say f=\r\nrom the primary auditory cortex, and a topographic map is then self-organiz=\r\ned to encode sound frequencies?&lt;/div&gt;\n\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;4) I believe lin=\r\ne orientation maps exist at the lowest level of a hierarchical architecture=\r\n, and as information moves to higher levels, more abstract concepts can be =\r\nencoded. These abstract concepts can fuse information from all sensory syst=\r\nems, e.g., how does a dog looks like, smells like, sounds like, etc. I beli=\r\neve that using HyperNEAT with such hierarchical architectures (and not just=\r\n for developing a controller) is a promising approach and I look forward to=\r\n seeing it mature.&lt;/div&gt;\n\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Vassilis&lt;/div&gt;=\r\n&lt;/div&gt;&lt;div&gt;&lt;div class=3D&quot;h5&quot;&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;br&gt;&lt;br&gt;&lt;div class=\r\n=3D&quot;gmail_quote&quot;&gt;On Mon, Apr 28, 2014 at 12:47 PM, Sebastian Risi &lt;span dir=\r\n=3D&quot;ltr&quot;&gt;&lt;&lt;a href=3D&quot;mailto:sebastian.risi@...&quot; target=3D&quot;_blank&quot;&gt;=\r\nsebastian.risi@...&lt;/a&gt;&gt;&lt;/span&gt; wrote:&lt;br&gt;\n\n&lt;blockquote class=3D&quot;gm=\r\nail_quote&quot; style=3D&quot;border-left:1px #ccc solid&quot;&gt;\n\n\n&lt;u&gt;&lt;/u&gt;\n\n\n\n\n\n\n\n\n\n \n&lt;div&gt;=\r\n\n&lt;span&gt;=C2=A0&lt;/span&gt;\n\n\n&lt;div&gt;\n  &lt;div&gt;\n\n\n    &lt;div&gt;\n      \n      \n      &lt;p&gt;&lt;/p=\r\n&gt;&lt;div dir=3D&quot;ltr&quot;&gt;Kenneth Stanley and I are pleased to announce our new GEC=\r\nCO paper (to appear), &quot;Guided Self-Organization in Indirectly Encoded =\r\nand Evolving Topographic Maps&quot;&lt;br&gt;&lt;br&gt;&lt;b&gt;PDF: &lt;/b&gt;&lt;a href=3D&quot;http://ep=\r\nlex.cs.ucf.edu/papers/risi_gecco14.pdf&quot; target=3D&quot;_blank&quot;&gt;http://eplex.cs.u=\r\ncf.edu/papers/risi_gecco14.pdf&lt;/a&gt;&lt;br&gt;\n\n\n&lt;br&gt;This paper introduces the idea=\r\n that HyperNEAT can be seeded with a lateral inhibitory connectivity patter=\r\nn, which then allows the weights from the inputs to the hidden layer=C2=A0o=\r\nf an ANN to self-organize to form a genuine topographic map of the input=C2=\r\n=A0space. The benefit of this new approach is that the initial seed can=C2=\r\n=A0be evolved further to accelerate the self-organizing process and to=C2=\r\n=A0bias it towards a specific target configuration. An interesting implicat=\r\nion=C2=A0is that enabling HyperNEAT to capture this key feature of=C2=A0nat=\r\nural systems might now allow us to evolve architectures resembling=C2=A0dee=\r\np learning networks, SOMs, or other self-organizing processes that also rel=\r\ny on unsupervised learning processes.&lt;br&gt;\n\n\n&lt;br&gt;&lt;b&gt;Abstract: &lt;/b&gt;An importa=\r\nnt phenomenon seen in many areas of biological brains and recently in deep =\r\nlearning architectures is a process known as self-organization. For example=\r\n, in the primary visual cortex, color and orientation maps develop based on=\r\n lateral inhibitory connectivity patterns and Hebbian learning dynamics. Th=\r\nese topographic maps, which are found in all sensory systems, are thought t=\r\no be a key factor in enabling abstract cognitive representations. This pape=\r\nr shows for the first time that the Hypercube-based NeuroEvolution of Augme=\r\nnting Topologies (HyperNEAT) method can be seeded to begin evolution =C2=A0=\r\nwith such lateral connectivity, enabling genuine self-organizing dynamics. =\r\nThe proposed approach draws on HyperNEAT&#39;s ability to generate a patter=\r\nn of weights across the connectivity of an artificial neural network (ANN) =\r\nbased on a function of its geometry. Validating this approach, the afferent=\r\n weights of an ANN self-organize in this paper to form a genuine topographi=\r\nc map of the input space for a simple line orientation task. Most interesti=\r\nngly, this seed can then be evolved further, providing a method to guide th=\r\ne self-organization of weights in a specific way, much as evolution likely =\r\nguided the self-organizing trajectories of biological brains.&lt;br&gt;\n\n\n&lt;br&gt;Che=\r\ners,&lt;br&gt;Sebastian&lt;br&gt;&lt;br&gt;--&lt;br&gt;Dr. Sebastian Risi&lt;br&gt;Assistant Professor &lt;b=\r\nr&gt;IT University of Copenhagen, Room 5D08&lt;br&gt;Rued Langgaards Vej 7, 2300 Cop=\r\nenhagen, Denmark&lt;br&gt;email: &lt;a href=3D&quot;mailto:sebastian.risi@...&quot; targ=\r\net=3D&quot;_blank&quot;&gt;sebastian.risi@...&lt;/a&gt;, web: &lt;a href=3D&quot;http://www.seba=\r\nstianrisi.com&quot; target=3D&quot;_blank&quot;&gt;www.sebastianrisi.com&lt;/a&gt;&lt;br&gt;\n\n\nmobile: &lt;a=\r\n href=3D&quot;tel:%2B45-50250355&quot; value=3D&quot;+4550250355&quot; target=3D&quot;_blank&quot;&gt;+45-50=\r\n250355&lt;/a&gt;, office: &lt;a href=3D&quot;tel:%2B45-7218-5127&quot; value=3D&quot;+4572185127&quot; t=\r\narget=3D&quot;_blank&quot;&gt;+45-7218-5127&lt;/a&gt;\n&lt;/div&gt;\n&lt;p&gt;&lt;/p&gt;\n\n    &lt;/div&gt;\n     \n\n    \n =\r\n   &lt;div style=3D&quot;color:#fff;min-height:0&quot;&gt;&lt;/div&gt;\n\n\n&lt;/div&gt;\n\n\n\n  \n\n\n\n\n\n\n&lt;/div=\r\n&gt;&lt;/div&gt;&lt;/blockquote&gt;&lt;/div&gt;&lt;br&gt;&lt;/div&gt;\n&lt;/div&gt;&lt;/div&gt;&lt;p&gt;&lt;/p&gt;\n\n    &lt;/div&gt;\n     \n=\r\n\n    \n    &lt;div style=3D&quot;color:#fff;min-height:0&quot;&gt;&lt;/div&gt;\n\n\n&lt;/div&gt;\n\n\n\n  \n\n\n\n\n=\r\n\n\n&lt;/blockquote&gt;&lt;/div&gt;&lt;br&gt;&lt;br clear=3D&quot;all&quot;&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;-- &lt;br&gt;&lt;div dir=\r\n=3D&quot;ltr&quot;&gt;Dr. Sebastian Risi&lt;br&gt;Assistant Professor=C2=A0&lt;br&gt;IT University o=\r\nf Copenhagen, Room 5D08&lt;br&gt;Rued Langgaards Vej 7, 2300 Copenhagen, Denmark&lt;=\r\nbr&gt;email: &lt;a href=3D&quot;mailto:sebastian.risi@...&quot; target=3D&quot;_blank&quot;&gt;seb=\r\nastian.risi@...&lt;/a&gt;, web:=C2=A0&lt;a href=3D&quot;http://www.sebastianrisi.co=\r\nm&quot; target=3D&quot;_blank&quot;&gt;www.sebastianrisi.com&lt;/a&gt;&lt;div&gt;\nmobile: +45-50250355, o=\r\nffice: +45-7218-5127&lt;br&gt;&lt;/div&gt;&lt;/div&gt;\n&lt;/div&gt;\n\r\n--001a113ab3ec672d7704f93a1de3--\r\n\n"}}