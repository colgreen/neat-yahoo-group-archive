{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":211599040,"authorName":"Jeff Clune","from":"Jeff Clune &lt;jclune@...&gt;","profile":"jeffreyclune","replyTo":"LIST","senderId":"M5BM6zuL0HyZSxyyrnRqtHpo65nk37C-nyxoyMfinz3xTTx_VzzOgUxW6pDTC8mProKKs_tniJrK2sdxJc5azGP9","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [neat] Re: Machine Learning and the Long View of AI","postDate":"1209424108","msgId":4004,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PEM0M0JEMTJDLjIyN0ZDJWpjbHVuZUBtc3UuZWR1Pg==","inReplyToHeader":"PGZ2NWhiZit1YmJkQGVHcm91cHMuY29tPg=="},"prevInTopic":4003,"nextInTopic":4005,"prevInTime":4003,"nextInTime":4005,"topicId":3955,"numMessagesInTopic":49,"msgSnippet":"... This would be a very interesting study to see. Hornby did similar comparisons when he used L-systems to evolve tables, creatures and neural nets. For those","rawEmail":"Return-Path: &lt;jclune@...&gt;\r\nX-Sender: jclune@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 7546 invoked from network); 28 Apr 2008 23:08:37 -0000\r\nX-Received: from unknown (66.218.67.96)\n  by m36.grp.scd.yahoo.com with QMQP; 28 Apr 2008 23:08:37 -0000\r\nX-Received: from unknown (HELO py-out-1112.google.com) (64.233.166.178)\n  by mta17.grp.scd.yahoo.com with SMTP; 28 Apr 2008 23:08:37 -0000\r\nX-Received: by py-out-1112.google.com with SMTP id a73so7189735pye.2\n        for &lt;neat@yahoogroups.com&gt;; Mon, 28 Apr 2008 16:08:34 -0700 (PDT)\r\nX-Received: by 10.65.110.11 with SMTP id n11mr15903052qbm.39.1209424113072;\n        Mon, 28 Apr 2008 16:08:33 -0700 (PDT)\r\nReturn-Path: &lt;jclune@...&gt;\r\nX-Received: from ?192.168.2.2? ( [67.167.130.112])\n        by mx.google.com with ESMTPS id f14sm8648872qba.25.2008.04.28.16.08.31\n        (version=TLSv1/SSLv3 cipher=OTHER);\n        Mon, 28 Apr 2008 16:08:32 -0700 (PDT)\r\nUser-Agent: Microsoft-Entourage/12.1.0.080305\r\nDate: Mon, 28 Apr 2008 19:08:28 -0400\r\nTo: &quot;neat@yahoogroups.com&quot; &lt;neat@yahoogroups.com&gt;,\n\tKenneth Stanley &lt;kstanley@...&gt;\r\nMessage-ID: &lt;C43BD12C.227FC%jclune@...&gt;\r\nThread-Topic: [neat] Re: Machine Learning and the Long View of AI\r\nThread-Index: AciphMSl9Z7/PhXIy0SjnUawNzS/5A==\r\nIn-Reply-To: &lt;fv5hbf+ubbd@...&gt;\r\nMime-version: 1.0\r\nContent-type: text/plain;\n\tcharset=&quot;US-ASCII&quot;\r\nContent-transfer-encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Jeff Clune &lt;jclune@...&gt;\r\nSubject: Re: [neat] Re: Machine Learning and the Long View of AI\r\nX-Yahoo-Group-Post: member; u=211599040; y=t3kWtbW-m9fM4_a5P77SewKI8v_NbG9zFVYsYf64kpPDZyUs24Qn\r\nX-Yahoo-Profile: jeffreyclune\r\n\r\n&gt; However, I do not believe that the mutations in HyperNEAT are actually\n&gt; more destructive than those in NEAT.  It is true that HyperNEAT\n&gt; mutations have holistic effects, but those holistic effects are\n&gt; orderly, that is, it is not the equivalent of randomizing the weights\n&gt; of all the connections involved.  Rather, it is a warping of the\n&gt; weight distribution along a dimension of regularity that was selected\n&gt; by evolution.   I think there is no a priori reason to believe that\n&gt; such changes are more or less destructive than single-weight\n&gt; mutations, as long as the magnitude of the overall change is kept\n&gt; within a reasonable limit, just as with any kind of mutation.\n\nThis would be a very interesting study to see. Hornby did similar\ncomparisons when he used L-systems to evolve tables, creatures and neural\nnets. For those of you interested in this subject, I recommend looking at\nhis analyses. That said, I know we would all be interested to see similar\ninvestigations comparing HyperNEAT to P-NEAT, and on a few different\nproblems to see how that affects things.\n\n \n\n\n\n\n&gt; \n&gt; ken\n&gt; \n&gt; --- In neat@yahoogroups.com, &quot;petar_chervenski&quot; &lt;petar_chervenski@...&gt;\n&gt; wrote:\n&gt;&gt; \n&gt;&gt; Great post, Ken! I really enjoyed reading it. It is just all true.\n&gt;&gt; \n&gt;&gt; There is really a difference between building a brain and evolving\n&gt;&gt; one. No matter if the brain learns in its lifetime or not. In the\n&gt;&gt; case you build a brain yourself that doesn&#39;t learn, it is just\n&gt;&gt; conventional programming at all.\n&gt;&gt; \n&gt;&gt; I think the constraints for evolution should be very sharp, so to\n&gt;&gt; say, because in EC in general the fitness function is the most\n&gt;&gt; important thing as well as the representation/mapping. You can&#39;t just\n&gt;&gt; say &quot;be smart!&quot; to an EC algorithm. You have to model\n&gt;&gt; its &quot;environment&quot; as well, and the process of evaluation usually\n&gt;&gt; takes a lot of computation time for the most interesting problems.\n&gt;&gt; \n&gt;&gt; There is a kind of.. Hm I guess I can&#39;t express myself in english\n&gt;&gt; well. The more complex the task is, the more computation time is\n&gt;&gt; required for a proper evaluation.\n&gt;&gt; I am maybe not saying anything new to you, Ken, but I just mention I\n&gt;&gt; understand it. \n&gt;&gt; \n&gt;&gt; HyperNEAT and CPPNs in general opened up an entire new field of\n&gt;&gt; research, that is, the evolution of mathematical compositions\n&gt;&gt; describing phenotypes of any kind. What I think about it is, that\n&gt;&gt; mutations are mostly destructive to the networks, while in a robotics\n&gt;&gt; experiments with direct representation, one weight change is not that\n&gt;&gt; bad, so to say. But change one weight of a CPPN and you get a totally\n&gt;&gt; different thing. In HyperNEAT this is not just a minor change, but a\n&gt;&gt; total change of the network&#39;s behaviour. So there is a great deal of\n&gt;&gt; computation time required to discover some concepts. In fact the\n&gt;&gt; fitness landscape in CPPN-based evolution is totally different than\n&gt;&gt; other approaches to the same problem.\n&gt;&gt; \n&gt;&gt; Another thing is that the geometry itself does not provide\n&gt;&gt; information about the phenotype complexity at all. I mean that even a\n&gt;&gt; network of 1000000000 connections can be generated by a connective\n&gt;&gt; CPPN but the bias is usually towards minimal solutions. I know that\n&gt;&gt; complexification is a property of the genotype space, but why to\n&gt;&gt; waste computation time evaluating individuals with millions of\n&gt;&gt; connections that actually are bad solutions?\n&gt;&gt; \n&gt;&gt; So, you may provide the geometry to the search, but you still can&#39;t\n&gt;&gt; provide the complexity. You need a priori that the complexity of the\n&gt;&gt; substrate is big enough.\n&gt;&gt; \n&gt;&gt; That 0.2 treshold is like a hard-coded hack to me. It may be able to\n&gt;&gt; represent any kind of connectivity, but I think the effort for\n&gt;&gt; discovering it is bigger than discovering the actual regularities at\n&gt;&gt; all. \n&gt;&gt; \n&gt;&gt; There should be a way to map complexity of the genotype to the\n&gt;&gt; phenotype, but not in such a constrained way. It should be\n&gt;&gt; increasing. Did you ever see an animal as simple as a worm but big as\n&gt;&gt; a whale? OK size doesn&#39;t matter. :) This comparison was not a good\n&gt;&gt; one. \n&gt;&gt; \n&gt;&gt; I know there is an option that HyperNEAT can evolve the substrate by\n&gt;&gt; itself, but how to control it? The dynamics of the neural networks\n&gt;&gt; has to be taken into account.\n&gt;&gt; \n&gt;&gt; Sorry about my scattered around thoughts. That was just a stream of\n&gt;&gt; conciosness. \n&gt;&gt; \n&gt;&gt; Peter\n&gt;&gt; \n&gt;&gt; --- In neat@yahoogroups.com, &quot;Kenneth Stanley&quot; &lt;kstanley@&gt; wrote:\n&gt;&gt;&gt; \n&gt;&gt;&gt; --- In neat@yahoogroups.com, &quot;Derek James&quot; &lt;djames@&gt; wrote:\n&gt;&gt;&gt; \n&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;&gt;  In RL, in contrast, the long view is almost the opposite: They\n&gt;&gt;&gt; want to\n&gt;&gt;&gt;&gt;&gt;  remove all constraints and still learn nevertheless.\n&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt; I&#39;m not sure what you mean by this, Ken. Could you elaborate a\n&gt;&gt; little?\n&gt;&gt;&gt;&gt; \n&gt;&gt;&gt; \n&gt;&gt;&gt; Sure.  I think the problem is that I can&#39;t find a way to explain my\n&gt;&gt;&gt; point concisely.  As I try to explain it, it starts taking up too\n&gt;&gt; much\n&gt;&gt;&gt; text so I shorten it and then it loses its meaning.  Let me give it\n&gt;&gt; a\n&gt;&gt;&gt; try again...\n&gt;&gt;&gt; \n&gt;&gt;&gt; I think the difference between the goals of RL and NE is an\n&gt;&gt;&gt; interesting topic because they are almost always conflated, as if\n&gt;&gt; they\n&gt;&gt;&gt; are trying to solve the same problem.\n&gt;&gt;&gt; \n&gt;&gt;&gt; The RL community (e.g. value-function approaches) is trying to build\n&gt;&gt;&gt; something that learns like a natural brain.  They are saying,\n&gt;&gt; through\n&gt;&gt;&gt; analytic means we can deduce how a brain can learn from sparse\n&gt;&gt;&gt; reinforcement and formalize that process in an algorithm.  The\n&gt;&gt; hope, I\n&gt;&gt;&gt; would think, is to eventually build the &quot;general intelligence&quot; that\n&gt;&gt;&gt; aligns with the holy grail of AI.  So each step along the way is an\n&gt;&gt;&gt; improvement in that general ability.\n&gt;&gt;&gt; \n&gt;&gt;&gt; So if that is your goal, then the benchmarks you choose have to be\n&gt;&gt;&gt; designed to measure progress to that goal.  So what they need to do\n&gt;&gt; is\n&gt;&gt;&gt; show that their designed intelligence can work largely independently\n&gt;&gt;&gt; of a priori &quot;cheats&quot; that provide the meat of the solution.\n&gt;&gt; Because,\n&gt;&gt;&gt; after all, how can it be a general intelligence if it needs you to\n&gt;&gt;&gt; tell it something that it is supposed to be able to figure out?\n&gt;&gt; This\n&gt;&gt;&gt; perspective, I believe, is aligned with Jeff&#39;s view.\n&gt;&gt;&gt; \n&gt;&gt;&gt; However, NE as a long-term pursuit is involved in something\n&gt;&gt; different,\n&gt;&gt;&gt; even though it can be applied to the same problems.  NE is not an\n&gt;&gt;&gt; attempt to formalize how people learn with sparse reinforcement.\n&gt;&gt;&gt; Rather, it is an attempt to formalize how evolution can build a\n&gt;&gt; brain.\n&gt;&gt;&gt;  So RL is formalizing the brain itself and NE is formalizing how\n&gt;&gt;&gt; evolution succeeds in creating a brain.  NE is therefore one step\n&gt;&gt; removed.\n&gt;&gt;&gt; \n&gt;&gt;&gt; This difference is ultimately a philosophical difference on the best\n&gt;&gt;&gt; approach to creating a full-blown AI.  The instrumental issue is\n&gt;&gt;&gt; whether you think it&#39;s easier to build it yourself or to design an\n&gt;&gt;&gt; algorithm that can build it.  The confusion and hence conflation of\n&gt;&gt;&gt; the two approaches arises in part because they do indeed both aim at\n&gt;&gt;&gt; the same long view goal: a general AI.  But they are coming at it\n&gt;&gt; from\n&gt;&gt;&gt; very different angles.\n&gt;&gt;&gt; \n&gt;&gt;&gt; And because of this stark difference, the *metric* of progress\n&gt;&gt; should\n&gt;&gt;&gt; be quite different.  We cannot measure our progress in building a\n&gt;&gt;&gt; general intelligence directly in the same way that we measure our\n&gt;&gt;&gt; progress in creating an evolutionary algorithm that itself will\n&gt;&gt;&gt; someday output one.\n&gt;&gt;&gt; \n&gt;&gt;&gt; This distinction is potentially subtle and confusing so let me try\n&gt;&gt; to\n&gt;&gt;&gt; make it clearer:  Human brains aren&#39;t designed to build yet more\n&gt;&gt; human\n&gt;&gt;&gt; brains.  We are good at a lot of things, and we learn generally, but\n&gt;&gt;&gt; we do not build 100-trillion part devices that are more complex than\n&gt;&gt;&gt; any known object in the universe.  I&#39;m not saying we won&#39;t ever be\n&gt;&gt;&gt; able to do it, but if you want to simulate a human brain, your first\n&gt;&gt;&gt; thought would not be that it needs to be capable of designing yet\n&gt;&gt;&gt; another brain by itself.  Your first thought is about things like\n&gt;&gt;&gt; object recognition or pursuit and evasion.\n&gt;&gt;&gt; \n&gt;&gt;&gt; In contrast, building brains is exactly what natural evolution did,\n&gt;&gt;&gt; and it did it quite well.  Natural evolution does not perform object\n&gt;&gt;&gt; recognition; it does not communicate with language; it does not run\n&gt;&gt;&gt; away from predators or hunt for prey.  Yet it does build brains that\n&gt;&gt;&gt; themselves do those things.  And that is the aspect of it we wish to\n&gt;&gt;&gt; harness- a very specific niche kind of skill (though radically\n&gt;&gt;&gt; impressive)- not a general skill.\n&gt;&gt;&gt; \n&gt;&gt;&gt; So the two pursuits are really quite different.  And therefore they\n&gt;&gt;&gt; deserve different metrics to judge their progress with respect to\n&gt;&gt; the\n&gt;&gt;&gt; long term goal.  That is, unless we conflate them to be the same\n&gt;&gt;&gt; thing, which we often do without thinking about it.\n&gt;&gt;&gt; \n&gt;&gt;&gt; For example, we could just say, well, both NE and RL are learning\n&gt;&gt;&gt; techniques, and after all, we can apply them to the same problems,\n&gt;&gt; so\n&gt;&gt;&gt; why make a big distinction in how we judge them?  Let&#39;s just compare\n&gt;&gt;&gt; them directly on the same benchmarks and get on with it.\n&gt;&gt;&gt; \n&gt;&gt;&gt; That&#39;s fine for the short-term view, i.e. let&#39;s just improve our\n&gt;&gt;&gt; ability to tackle practical problems, but for the long view, they\n&gt;&gt;&gt; cannot be judged in the same way.  If I improve at my ability to\n&gt;&gt;&gt; balance on one foot is that a sign that I will be able to build a\n&gt;&gt;&gt; brain someday?  If evolution evolves a brain that plays checkers, is\n&gt;&gt;&gt; that a sign that evolution *itself* is on the road to performing\n&gt;&gt;&gt; object recognition?  These are totally different pursuits.\n&gt;&gt;&gt; \n&gt;&gt;&gt; So in that context, how should they be judged with respect to long\n&gt;&gt;&gt; term goals?  Well, I think RL deserves to be judged based on its\n&gt;&gt;&gt; increasing ability to learn more generally.  And in that sense,\n&gt;&gt;&gt; exactly Jeff&#39;s criteria should apply to it: We should be interested\n&gt;&gt; in\n&gt;&gt;&gt; whether it &quot;needs&quot; a priori information to learn.  In other words,\n&gt;&gt; the\n&gt;&gt;&gt; less we need to constrain the problem for the learner, the more\n&gt;&gt;&gt; impressed we deserve to be.  That shows progress towards more and\n&gt;&gt; more\n&gt;&gt;&gt; general AI and ML.\n&gt;&gt;&gt; \n&gt;&gt;&gt; But if evolution is not *itself* supposed to be a general learner\n&gt;&gt;&gt; (rather, we just want it to concentrate on one very specific skill:\n&gt;&gt;&gt; brain building), then those considerations are orthogonal to its\n&gt;&gt;&gt; greatest promise.  Its promise is to evolve a brain itself, and as\n&gt;&gt;&gt; such, neuroevolutionary algorithms deserve to be judged on our\n&gt;&gt; ability\n&gt;&gt;&gt; to *constrain* the problem so that they can accomplish exactly\n&gt;&gt; that. \n&gt;&gt;&gt; In other words, the problem NE *algorithms* face is leaps and bounds\n&gt;&gt;&gt; beyond what RL algorithms face.  RL algorithms just need to be able\n&gt;&gt; to\n&gt;&gt;&gt; do as well as brains; NE has to be able to discover brains\n&gt;&gt; themselves.\n&gt;&gt;&gt;  Therefore, progress is NE should in part be measured with respect\n&gt;&gt; to\n&gt;&gt;&gt; progress in constraining the problem to make such a discovery more\n&gt;&gt;&gt; likely.  When an NE algorithm is improved to allow us to tell it\n&gt;&gt; more\n&gt;&gt;&gt; about the world in which its output will be situated, that is good\n&gt;&gt;&gt; news for the long view.  In short, we don&#39;t care at all how NE\n&gt;&gt;&gt; produced a brain as long as it really does.  Will anyone complain\n&gt;&gt; if a\n&gt;&gt;&gt; human brain pops out of a system that was a priori given the concept\n&gt;&gt;&gt; of symmetry?  Rather, we should be glad that such a priori context\n&gt;&gt; was\n&gt;&gt;&gt; possible to provide in the first place, because it may have saved\n&gt;&gt; us a\n&gt;&gt;&gt; year of wasted computation in figuring it out needlessly.\n&gt;&gt;&gt; \n&gt;&gt;&gt; This distinction is almost completely ignored when NE and RL are\n&gt;&gt;&gt; compared directly.  Therefore, the implications of any such\n&gt;&gt; comparison\n&gt;&gt;&gt; are fuzzy and lacking context with respect to the long view.  I am\n&gt;&gt; not\n&gt;&gt;&gt; sure if I should care or not if RL solves something better than NE,\n&gt;&gt; or\n&gt;&gt;&gt; vice versa, because the author doesn&#39;t explain how the result aligns\n&gt;&gt;&gt; with the long-term goals of the fields.  Long term goals seem like\n&gt;&gt;&gt; unwelcome guests these days in AI, which is why I probably won&#39;t be\n&gt;&gt;&gt; writing about any of this in a publication any time soon.\n&gt;&gt;&gt; \n&gt;&gt;&gt; ...\n&gt;&gt;&gt; \n&gt;&gt;&gt; So Derek what you are saying about NE being good at &quot;hard-wired&quot;\n&gt;&gt;&gt; solutions and RL being appropriate for ontogenetic lifetime\n&gt;&gt; learning,\n&gt;&gt;&gt; while true, is not what I think of as the primary long-view issue.\n&gt;&gt;&gt; \n&gt;&gt;&gt; In the long view, NE will be used to evolve structures that do learn\n&gt;&gt;&gt; over their lifetime, i.e. not hardwired at all.  The only reason\n&gt;&gt; that\n&gt;&gt;&gt; it tends to be used to evolve hardwired solutions today is because\n&gt;&gt; we\n&gt;&gt;&gt; are trying to get a foothold on how to evolve certain types of\n&gt;&gt; complex\n&gt;&gt;&gt; structures.   Once we get very good at it, focus will naturally\n&gt;&gt; shift\n&gt;&gt;&gt; to evolving dynamic brains (and of course there is already work\n&gt;&gt; along\n&gt;&gt;&gt; these lines today, much from Floreano).  I do not even think that we\n&gt;&gt;&gt; will need to include stock learning algorithms like Hebbian\n&gt;&gt; learning.\n&gt;&gt;&gt;  When we achieve our long-term goals, those *themselves* will be\n&gt;&gt; left\n&gt;&gt;&gt; up to evolution because after all there may be something even\n&gt;&gt; better.\n&gt;&gt;&gt;  \n&gt;&gt;&gt;&gt;&gt; My aim is to design an\n&gt;&gt;&gt;&gt;&gt;  algorithm that will output a brain, not to design the brain\n&gt;&gt; itself.\n&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt; But what kind of brain are you wanting to output?\n&gt;&gt;&gt;&gt; \n&gt;&gt;&gt; \n&gt;&gt;&gt; Note that I&#39;m speaking purely about the long view for these\n&gt;&gt; different\n&gt;&gt;&gt; fields here.  Of course on a day-to-day basis I am not solely\n&gt;&gt; focused\n&gt;&gt;&gt; on what will happen 100 years from now.  On a practical day-to-day\n&gt;&gt;&gt; basis, of course I want to make NE better capable to tackle problems\n&gt;&gt;&gt; that e.g. RL tackles.  So in the short-term context, I just want to\n&gt;&gt;&gt; output something that works for the problem at hand.\n&gt;&gt;&gt; \n&gt;&gt;&gt; But in the long view, which we were talking about, I think the\n&gt;&gt;&gt; ultimate goal would be to output a full-fledged adaptive system with\n&gt;&gt;&gt; astronomical complexity and the power and subtlety of human\n&gt;&gt; reasoning.\n&gt;&gt;&gt;  On that path, constraint is the only hope, unless you want to wait\n&gt;&gt;&gt; three billion years and just hope in the meantime that the initial\n&gt;&gt;&gt; conditions were set up correctly.  Therefore, demonstrations of the\n&gt;&gt;&gt; power of constraint deserve to be judged as evidence of the promise\n&gt;&gt; of\n&gt;&gt;&gt; and progress towards the long term goal in NE.\n&gt;&gt;&gt; \n&gt;&gt;&gt; ken\n&gt;&gt;&gt; \n&gt;&gt; \n&gt; \n&gt; \n\n\n\n"}}