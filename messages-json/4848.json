{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":413744767,"authorName":"spoonsx21","from":"&quot;spoonsx21&quot; &lt;spoonsx21@...&gt;","profile":"spoonsx21","replyTo":"LIST","senderId":"YIjmpJ9OdjHijl7JybXGyAbYWPI__2Vr6kz7rghUgYHXzAV-AGIdTXk_yoRP9oi8KkTvg06Jy9Xb8-iI8_yu7HRmEEh9pJ7J","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Substrate Evolution","postDate":"1252509076","msgId":4848,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGg4OGdpaysyc2M4QGVHcm91cHMuY29tPg=="},"prevInTopic":0,"nextInTopic":4853,"prevInTime":4847,"nextInTime":4849,"topicId":4848,"numMessagesInTopic":5,"msgSnippet":"Hello everyone, I m new to the Neat group. My name is Paul, I m an undergraduate interested in evolutionary computation, and obviously HyperNEAT/ NEAT. I have","rawEmail":"Return-Path: &lt;spoonsx21@...&gt;\r\nX-Sender: spoonsx21@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 32602 invoked from network); 9 Sep 2009 15:11:22 -0000\r\nX-Received: from unknown (98.137.34.46)\n  by m1.grp.re1.yahoo.com with QMQP; 9 Sep 2009 15:11:22 -0000\r\nX-Received: from unknown (HELO n37b.bullet.mail.sp1.yahoo.com) (66.163.168.151)\n  by mta3.grp.sp2.yahoo.com with SMTP; 9 Sep 2009 15:11:21 -0000\r\nX-Received: from [69.147.65.172] by n37.bullet.mail.sp1.yahoo.com with NNFMP; 09 Sep 2009 15:11:18 -0000\r\nX-Received: from [98.137.34.35] by t14.bullet.mail.sp1.yahoo.com with NNFMP; 09 Sep 2009 15:11:18 -0000\r\nDate: Wed, 09 Sep 2009 15:11:16 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;h88gik+2sc8@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;spoonsx21&quot; &lt;spoonsx21@...&gt;\r\nSubject: Substrate Evolution\r\nX-Yahoo-Group-Post: member; u=413744767; y=-Pm5slGVfyXpOyEoy9HqLW84Ae6Urq4889MZaq0EXl7T8bxp\r\nX-Yahoo-Profile: spoonsx21\r\n\r\nHello everyone,\n\nI&#39;m new to the Neat group. My name is Paul, I&#39;m an undergr=\r\naduate interested in evolutionary computation, and obviously HyperNEAT/ NEA=\r\nT. I have been thinking a lot about substrate evolution, and its parallel i=\r\nn biology. One of the fundamental questions I&#39;ve been trying to answer is, =\r\nhow did brain topology evolve? And it is clearly fundamental to the problem=\r\n of substrate evolution. Our sensory information is pretty well segregated =\r\nin the brain. For instance, visual areas are broken down into separate proc=\r\nessing locations (V1-V5), each area dealing with different aspects of visua=\r\nlization like object movement, or pattern recognition. There are plenty of =\r\nstudies and papers on current brain topology, but it&#39;s difficult to find id=\r\neas on how brain topology evolved. \n\nHowever, I did stumble upon one theory=\r\n I enjoyed. It was Gerald Edelman&#39;s theory on what he calls neural Darwinis=\r\nm, or the theory of neuronal group selection (TNGS). And while I attempt to=\r\n truly get my head around the theory, what I have drawn from his theory see=\r\nms applicable to HyperNEAT&#39;s extensions. The theory states that within the =\r\nbrain there is first a process of selection in creating the brain&#39;s anatomy=\r\n, with small epigenetic changes occurring in development (Here you can imag=\r\nine that the anatomy in HyperNEAT is our substrate). Then in the postnatal =\r\nstage, there is a time of neuron selection, where some synaptic connections=\r\n are strengthened, and others simply disappear altogether through neuron de=\r\nath (something with little or no parallel in HyperNEAT). He gives as an exa=\r\nmple a chicken, which is born with 20,000 neurons. At the adult stage, the =\r\nchicken has 12,000 neurons, keeping only 60% of the original neurons. There=\r\n is in fact much more to this theory, but I am in no way able to communicat=\r\ne it effectively. I encourage you to read any of his papers of books (or a =\r\nquick Wikipedia scan). My interest was in HyperNEAT&#39;s possible abstraction =\r\nof the idea. \n\nSomething HyperNEAT has yet to incorporate is the idea of in=\r\ntra-life learning. I read a few of the other posts, and I think this might =\r\nbe in some ways related to the HybrID conversation about irregularities. Hy=\r\nbrID attempts to make up for this lack of intra-life learning through the u=\r\nse of NEAT. At some point in the algorithm, HyperNEAT is stopped in favor o=\r\nf using NEAT to more accurately pinpoint irregularities. But this is not re=\r\nally the &quot;job&quot; of evolution, rather this is an intralife task. Evolution ca=\r\nn provide the framework (i.e. a species), but the more fit individual is ab=\r\nle to adapt to the irregularities of life (i.e. through intralife learning)=\r\n. \n\nThe point I&#39;m laboriously trying to bring you to is that I believe subs=\r\ntrate evolution and intralife learning are related. And perhaps you could t=\r\nake out two birds with one stone using some ideas from neural Darwinism. My=\r\n idea is a bit crude, and the details aren&#39;t ironed out, but these were som=\r\ne thoughts. Speaking strictly about HyperNEAT, what I thought would be help=\r\nful would be to generate more points then necessary within the substrate. T=\r\nhis would happen during mutation/crossover. Perhaps duplicating inputs in m=\r\nore than one place on the substrate (within a certain distance from each ot=\r\nher), and adding additional layers in the hidden nodes (if they exist). Thr=\r\nough the evaluation of the new population, essentially neuron&#39;s that fire t=\r\nogether wire together (as Edelman loves to say in his books) and weights ca=\r\nn be modified slowly during the evaluation, additionally allowing for the r=\r\nemoval of less fit neurons. What&#39;s left is an individual whose substrate is=\r\nn&#39;t strictly identical to the original, and weight connections that might s=\r\nlightly differ from the original CPPN. \n\nNow problems. There are a host of =\r\nthem, and this is what currently makes this idea a bit clunky and inelegant=\r\n. At a very basic level, I&#39;m still uncertain how one could reconcile the di=\r\nfference between the resulting substrate and the original. Also the resulti=\r\nng CPPN and the original. Also scaling issues, when trying to modify neuron=\r\n connections, making changes to 1 connection weight at a time makes this im=\r\npossible when examining a neural net of 9 million connections. This makes i=\r\nt difficult to convince anyone that this is the direction that HyperNEAT sh=\r\nould head in. Rather, it was my idea to ping some ideas off of you guys. An=\r\nd I do believe that the solutions to substrate evolution and intralife lear=\r\nning are linked, whether or not this is the best way to do it (most likely =\r\nnot). \n\nLet me know what you guys think,\n-Paul \n\n\n\n\n"}}