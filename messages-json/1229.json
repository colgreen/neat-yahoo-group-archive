{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":82117382,"authorName":"Jim O&#39;Flaherty, Jr.","from":"&quot;Jim O&#39;Flaherty, Jr.&quot; &lt;jim_oflaherty_jr@...&gt;","profile":"jim_oflaherty_jr","replyTo":"LIST","senderId":"zj1ytyDaNRsUQkzrz2eSz8ZMtDoJjWa8ljTFkTroqmY21LekioJjVzxPDgp_ApLi0X7r_yRROqKuuB93VoxYD9x1-T7DkosefxO17kUqUrBvqSKtH0Q2as0","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] An ANN design question...","postDate":"1090603594","msgId":1229,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDIwMDQwNzIzMTcyNjM0Ljk2MjYxLnFtYWlsQHdlYjUyODA5Lm1haWwueWFob28uY29tPg==","inReplyToHeader":"PDIwMDQwNzIzMTQwNDU0LjEyNzQ2LnFtYWlsQHdlYjUyODA2Lm1haWwueWFob28uY29tPg=="},"prevInTopic":1228,"nextInTopic":1230,"prevInTime":1228,"nextInTime":1230,"topicId":1226,"numMessagesInTopic":19,"msgSnippet":"Ken, In re-reading this, I realized that I did not accurately describe the relationship of the input data to the bounds of the activation function - I realize","rawEmail":"Return-Path: &lt;jim_oflaherty_jr@...&gt;\r\nX-Sender: jim_oflaherty_jr@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 20460 invoked from network); 23 Jul 2004 17:26:35 -0000\r\nReceived: from unknown (66.218.66.216)\n  by m22.grp.scd.yahoo.com with QMQP; 23 Jul 2004 17:26:35 -0000\r\nReceived: from unknown (HELO web52809.mail.yahoo.com) (206.190.39.173)\n  by mta1.grp.scd.yahoo.com with SMTP; 23 Jul 2004 17:26:35 -0000\r\nMessage-ID: &lt;20040723172634.96261.qmail@...&gt;\r\nReceived: from [205.158.160.209] by web52809.mail.yahoo.com via HTTP; Fri, 23 Jul 2004 10:26:34 PDT\r\nDate: Fri, 23 Jul 2004 10:26:34 -0700 (PDT)\r\nTo: neat@yahoogroups.com\r\nIn-Reply-To: &lt;20040723140454.12746.qmail@...&gt;\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=us-ascii\r\nX-eGroups-Remote-IP: 206.190.39.173\r\nFrom: &quot;Jim O&#39;Flaherty, Jr.&quot; &lt;jim_oflaherty_jr@...&gt;\r\nSubject: Re: [neat] An ANN design question...\r\nX-Yahoo-Group-Post: member; u=82117382\r\nX-Yahoo-Profile: jim_oflaherty_jr\r\n\r\nKen,\n\nIn re-reading this, I realized that I did not accurately describe the relationship of the input\ndata to the bounds of the activation function - I realize the activation function is only bounded\non the y axis.  The spread of range of x values where the corresponding y value changes the most\nrapidly (which results in the most distinction) typically occurs when x is between -1..1, which\nhappens to also be the y bounds of the hyperbolic tangent, which is the activation function I\nalmost exclusively use.\n\nAnyway, it is when the y value changes are infintesimal (x is less than -10 and x is greater than\n+10) is where I am concerned about data meaning being lost by the input node having an activation\nfunction.\n\nHope this clarifies.\n\n\nJim\n\n\n--- &quot;Jim O&#39;Flaherty, Jr.&quot; &lt;jim_oflaherty_jr@...&gt; wrote:\n&gt; Ken,\n&gt; \n&gt; As I have written previously here, I am working on an optimized ANN implementation in Java which\n&gt; I\n&gt; have named SEMIANN (Sparsely Evaluated Matrix Interace Artificial Neural Network).\n&gt; \n&gt; In a meeting with Derek and Philip, we were reviewing my design and comparing it with the design\n&gt; they are currently using derived from your NEAT ANN design.  What showed up was a small\n&gt; difference\n&gt; in how I am handling the input data versus how it is being handled in their NEAT implementation.\n&gt; \n&gt; It is my understanding from the small number of ANN implementations I have seen (around 5)\n&gt; including that of David Fogel (author of book titled &quot;Blondie24&quot; from which I am duplicating\n&gt; experiments), the input data is placed directly into the input node.  The data is not bounded\n&gt; (other than the actual limits of a float or double).  And the input node does *not* have an\n&gt; activation function.  The unbounded data present in the input node is then used in the\n&gt; activation\n&gt; of the hidden nodes (simple 3 layer feed forward network).  Any sort of altering the input data\n&gt; is\n&gt; then handled by the weight attached to that input node.  The GA process will then drift the\n&gt; weights around such that inputs which are not so valuable are muted with smaller weight values. \n&gt; And inputs which are important are magnified with higher weight values.  And all of these\n&gt; weights\n&gt; will eventually form a function over which the input data is �normalized� based on each input�s\n&gt; relative importance, a sort of first approximation of the input data�s inter-relatedness.\n&gt; \n&gt; In contrast, Philip and Derek indicated an input data point entering their ANN implementation is\n&gt; actually being pushed through the input node&#39;s activation function.  Then the &quot;modified&quot; data\n&gt; point is now placed into the input node.  It is then used in the activation of the &quot;hidden&quot;\n&gt; nodes.\n&gt; \n&gt; In talking through the difference, we talked about how that might impact the efficacy of the\n&gt; evolving ANN.  In other words, by having the input data go through an activation function\n&gt; without\n&gt; their being a weight involved, it seems the input data is being skewed, meaningful data is lost \n&gt; with no opportunity for the GA to compensate prior to the data loss.  Essentially, some data is\n&gt; lost.  In pure mathematical terms, this implmentation provides a weight of 1.0 multiplied by the\n&gt; unbounded input value which is then submitted to the input node&#39;s activation function with the\n&gt; result of the function being placed into the input node.\n&gt; \n&gt; My immediate response was this: the skewing seems like it would make it more difficult for the\n&gt; ANN\n&gt; to generate associations to the inputs that range outside of the bounds of the activation\n&gt; function.  For example, in replicating Fogel&#39;s experiments, I am using the hyperbolic tangent \n&gt; bounded -1..1, and the following input values are used: a red checker has the value of 1.0, a\n&gt; black check has the value of -1.0, a red king has the value of 1.5, and a black king has the\n&gt; value\n&gt; of -1.5.\n&gt; \n&gt; Now, I know that Fogel was expecting the inputs to be related directly, as a ratio, as he\n&gt; discusses this at some length in his book.  He left it up to the GA/ANN to work out the optimal\n&gt; ratio relationship.  Additionally, the king&#39;s value was a GA parameter which was bounded between\n&gt; 1.0 and 3.0 and could randomly change by +/- 0.1 when a parent was generating a descendant.\n&gt; \n&gt; With the approach Philip and Derek have taken (and they said theirs is modeled after your\n&gt; design),\n&gt; it seems like the ratio gets perverted by the activation function on the input node.  So as\n&gt; input\n&gt; values fall further and further from the activation function bounds, relationships between\n&gt; inputs\n&gt; outside of the bounds are eventually lost due to approximation/rounding errors in the IEEE float\n&gt; or double.  Or so it seems to me.\n&gt; \n&gt; So my questions are this:\n&gt; A) What is the theoretical or mathematical explanation as to why the input values for NEAT are\n&gt; pushed through an input node�s activation function as opposed to being used directly?\n&gt; B) Does some form of assumption exist in which to provide input to a NEAT ANN, the input data\n&gt; point for each input node must be scaled such that the data point�s relevant range of values\n&gt; falls\n&gt; between the upper and lower bounds of the input node�s activation function?\n&gt; C) What kinds of different types of activation functions on an input node might possibly handle\n&gt; this differently and/or more effectively?\n&gt; \n&gt; Sidenote: In SEMIANN, I do not allow a connection to have a destination of an input node.  So\n&gt; there is no need for an activation function at an input node.  And input node is treated as just\n&gt; an unbounded data point.  It was my understanding that if there was to be feedback to the\n&gt; �input�,\n&gt; it would occur as new nodes and connections around the hidden/output nodes from which the\n&gt; particular input node was connected.\n&gt; \n&gt; Well, that sort of took much longer to present than I initially thought.  Hmmm�\n&gt; \n&gt; Thank you for any clarification(s) you can offer on this.\n&gt; \n&gt; \n&gt; Jim O�Flaherty, Jr.\n&gt; \n&gt; \n&gt; \n&gt; \n&gt; \n&gt;  \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; \n&gt;  \n&gt; \n&gt; \n\n\n"}}