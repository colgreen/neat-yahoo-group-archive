{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":422167445,"authorName":"Nemo 136","from":"Nemo 136 &lt;nemo136@...&gt;","profile":"nemo136p","replyTo":"LIST","senderId":"uq7jHEZqn7FKfNNxIZonljtbkWb7eT7u6hDdxkRVT83tupEoLgdVfPqc2pA0c_SJdlL7VNAJTRRHH7TGQDfVfCpiMA","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [neat] Trying to reproduce novelty results.","postDate":"1260381750","msgId":4996,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDJhMGQyYTRjMDkxMjA5MTAwMm81NDZkZDI3YnhjYjBiYjAyNzJiZDIxMDM2QG1haWwuZ21haWwuY29tPg==","inReplyToHeader":"PGU4ZjJiYWM4MDkxMjA2MjE0NGczZGUzNWU0YWlmZGE3ZjVjNjlmMDIxZTU4QG1haWwuZ21haWwuY29tPg==","referencesHeader":"PDJhMGQyYTRjMDkxMjA0MDk1MG43ODhjNWMzZnBhMjk3MjAwOTFkZjM3NTJlQG1haWwuZ21haWwuY29tPgkgPGU4ZjJiYWM4MDkxMjA2MjE0NGczZGUzNWU0YWlmZGE3ZjVjNjlmMDIxZTU4QG1haWwuZ21haWwuY29tPg=="},"prevInTopic":4976,"nextInTopic":5008,"prevInTime":4995,"nextInTime":4997,"topicId":4973,"numMessagesInTopic":4,"msgSnippet":"Hi Sebastian, I wasn t able to evolve the learning behavior with a regular fitness, even after quite long simulation times. Though, the modulatory Neurons are","rawEmail":"Return-Path: &lt;nemo136@...&gt;\r\nX-Sender: nemo136@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 6271 invoked from network); 9 Dec 2009 18:02:32 -0000\r\nX-Received: from unknown (98.137.34.44)\n  by m1.grp.sp2.yahoo.com with QMQP; 9 Dec 2009 18:02:32 -0000\r\nX-Received: from unknown (HELO mail-bw0-f228.google.com) (209.85.218.228)\n  by mta1.grp.sp2.yahoo.com with SMTP; 9 Dec 2009 18:02:32 -0000\r\nX-Received: by bwz28 with SMTP id 28so6418980bwz.37\n        for &lt;neat@yahoogroups.com&gt;; Wed, 09 Dec 2009 10:02:31 -0800 (PST)\r\nMIME-Version: 1.0\r\nX-Received: by 10.204.175.20 with SMTP id v20mr10032885bkz.213.1260381750551; \n\tWed, 09 Dec 2009 10:02:30 -0800 (PST)\r\nIn-Reply-To: &lt;e8f2bac80912062144g3de35e4aifda7f5c69f021e58@...&gt;\r\nReferences: &lt;2a0d2a4c0912040950n788c5c3fpa29720091df3752e@...&gt;\n\t &lt;e8f2bac80912062144g3de35e4aifda7f5c69f021e58@...&gt;\r\nDate: Wed, 9 Dec 2009 19:02:30 +0100\r\nMessage-ID: &lt;2a0d2a4c0912091002o546dd27bxcb0bb0272bd21036@...&gt;\r\nTo: neat@yahoogroups.com\r\nContent-Type: multipart/alternative; boundary=00032555aa7685f9ba047a4f7e28\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Nemo 136 &lt;nemo136@...&gt;\r\nSubject: Re: [neat] Trying to reproduce novelty results.\r\nX-Yahoo-Group-Post: member; u=422167445; y=PtHV6Gn8gsedvZJTOneQFYKKfOFLDLRtNwEsnOf1JHxJ1is\r\nX-Yahoo-Profile: nemo136p\r\n\r\n\r\n--00032555aa7685f9ba047a4f7e28\r\nContent-Type: text/plain; charset=windows-1252\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\nHi Sebastian,\n\nI wasn&#39;t able to evolve the learning behavior with a regular=\r\n fitness, even\nafter quite long simulation times. Though, the modulatory Ne=\r\nurons are used,\nas I have behaviors which change along the time and dependi=\r\nng on the absence\nor presence of reward.\n\nMy adaptation is the same as the =\r\none described in\nhttp://alifexi.alife.org/papers/ALIFExi_pp569-576.pdf\n\nWhe=\r\nn I mutate a link into a node, the node can either be modulatory or\nnormal.=\r\n\n\nAs the first paper cited in the article for the tmaze (\nhttp://www.spring=\r\nerlink.com/index/46ATV1CN3FEM1E67.pdf) was continuous, I\nassumed your exper=\r\niment was also in a continuous environment, and in these\nenvironment, the r=\r\nobot manages to find all the possible behaviors but not\nthe learning ones.\n=\r\n\n I am now trying in a discrete T maze as in Soltoggio&#39;s experiment.\n\nI und=\r\nerstand that using NEAT could make a difference and that&#39;s\nexactly one of t=\r\nhe things we are trying to understand by re-implementing your\n\nexperiment.\n=\r\n\nCould you publish the source code (as for most NEAT experiment from\nthe EP=\r\nLEX group) so as we can compare the evolutionary path between\nour implement=\r\nations ?\n\nBest regards,\n\nPaul\n\n\n\n2009/12/7 Sebastian Risi &lt;sebastian.risi@g=\r\nmail.com&gt;\n\n&gt;\n&gt;\n&gt; Hi Paul,\n&gt;\n&gt; As you&#39;ve already pointed out it could make a=\r\n difference that you&#39;re not\n&gt; using NEAT. We believe that NEAT is beneficia=\r\nl to Novelty Search because it\n&gt; gradually complexifies behaviors which red=\r\nuces the novelty search space.\n&gt; Your novelty metric seems similar to the o=\r\nne I used so I think that\n&gt; shouldn&#39;t make a big difference.\n&gt;\n&gt; Were you a=\r\nble to evolve learning behavior with a regular fitness function?\n&gt; What kin=\r\nd of adaptation mechanism are you using?\n&gt;\n&gt; Cheers,\n&gt; Sebastian\n&gt;\n&gt;\n&gt;\n&gt; On=\r\n Fri, Dec 4, 2009 at 12:50 PM, Nemo 136 &lt;nemo136@...&gt; wrote:\n&gt;\n&gt;&gt;\n&gt;&gt;\n=\r\n&gt;&gt; Hello,\n&gt;&gt;\n&gt;&gt; I&#39;m currently trying to reproduce the results in\n&gt;&gt; http://=\r\neplex.cs.ucf.edu/publications/2009/risi.gecco09.html and finding\n&gt;&gt; out if =\r\na NEAT algorithm is necessary to obtain theses results. Right now, I\n&gt;&gt; man=\r\nage without too many problems to get behaviors changing depending on the\n&gt;&gt;=\r\n trial / deploy.\n&gt;&gt;\n&gt;&gt;  Though, the novelty keeps trying new behaviors base=\r\nd upon crashing / not\n&gt;&gt; crashing or getting one reward or not getting it, =\r\nand returning or not\n&gt;&gt; returning to the origin (no reward). This fills up =\r\nthe archive quite fast\n&gt;&gt; without any actual &quot;learning&quot; appearing (the best=\r\n scores show 60% to 70% of\n&gt;&gt; all high reward reached without any clear &quot;le=\r\narning&quot; pattern).\n&gt;&gt;\n&gt;&gt; I would like to know if some code is provided onlin=\r\ne for this experiment,\n&gt;&gt; or if you could send me the code used in this exp=\r\neriment, if only to try to\n&gt;&gt; see which parameters are different in the two=\r\n of them, so that I can try to\n&gt;&gt; pinpoint the differences.\n&gt;&gt;\n&gt;&gt; Right now=\r\n, the differences I can see are :\n&gt;&gt; - simple network evolution algorithm v=\r\ns NEAT mechanisms\n&gt;&gt; - possible difference in the way of counting for novel=\r\nty score : I am\n&gt;&gt; using a bitset of size 4*nb_deploy*nb_trials with each b=\r\nit standing for : a\n&gt;&gt; crash, a high reward, a low reward, returning to the=\r\n starting point wit the\n&gt;&gt; reward. I do a hamming distance between behavior=\r\ns to get my novelty score.\n&gt;&gt; But, it seems to be the same mechanisms as in=\r\n the paper\n&gt;&gt; - generation based algorithm while the paper uses a a steady =\r\nstate\n&gt;&gt; approach in which only one individual is generated at each generat=\r\nion\n&gt;&gt;\n&gt;&gt; Cheers.\n&gt;&gt;\n&gt;&gt; Paul Tonelli\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;  \n&gt;\n\r\n--00032555aa7685f9ba047a4f7e28\r\nContent-Type: text/html; charset=windows-1252\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\nHi Sebastian,&lt;br&gt;&lt;br&gt;I wasn&#39;t able to evolve the learning behavior with=\r\n a regular fitness, even after quite long simulation times. Though, the mod=\r\nulatory Neurons are used, as I have behaviors which change along the time a=\r\nnd depending on the absence or presence of reward.&lt;br&gt;\n\n&lt;br&gt;My adaptation i=\r\ns the same as the one described in &lt;a href=3D&quot;http://alifexi.alife.org/pape=\r\nrs/ALIFExi_pp569-576.pdf&quot; target=3D&quot;_blank&quot;&gt;http://alifexi.alife.org/papers=\r\n/ALIFExi_pp569-576.pdf&lt;/a&gt;&lt;br&gt;&lt;br&gt;When I mutate a link into a node, the nod=\r\ne can either be modulatory or normal. &lt;br&gt;\n&lt;br&gt;As the first paper cited in =\r\nthe article for the tmaze (&lt;a href=3D&quot;http://www.springerlink.com/index/46A=\r\nTV1CN3FEM1E67.pdf&quot;&gt;http://www.springerlink.com/index/46ATV1CN3FEM1E67.pdf&lt;/=\r\na&gt;) was continuous, I assumed your experiment was also in a continuous envi=\r\nronment, and in these environment, the robot manages to find all the possib=\r\nle behaviors but not the learning ones.&lt;br&gt;\n&lt;br&gt;=A0I am now trying in a dis=\r\ncrete T maze as in Soltoggio&#39;s experiment.&lt;br&gt;&lt;br&gt;\n&lt;pre style=3D&quot;font-f=\r\namily: arial,helvetica,sans-serif;&quot;&gt;I understand that using NEAT could make=\r\n a difference and that&#39;s&lt;br&gt;exactly one of the things we are trying to =\r\nunderstand by re-implementing your&lt;br&gt;\nexperiment. &lt;br&gt;&lt;br&gt;Could you publis=\r\nh the source code (as for most NEAT experiment from&lt;br&gt;the EPLEX group) so =\r\nas we can compare the evolutionary path between&lt;br&gt;our implementations ?&lt;br=\r\n&gt;&lt;br&gt;Best regards,&lt;br&gt;&lt;br&gt;Paul&lt;br&gt;\n&lt;/pre&gt;\n&lt;br&gt;&lt;br&gt;&lt;div class=3D&quot;gmail_quote=\r\n&quot;&gt;2009/12/7 Sebastian Risi &lt;span dir=3D&quot;ltr&quot;&gt;&lt;&lt;a href=3D&quot;mailto:sebastia=\r\nn.risi@...&quot; target=3D&quot;_blank&quot;&gt;sebastian.risi@...&lt;/a&gt;&gt;&lt;/span&gt;=\r\n&lt;br&gt;&lt;blockquote class=3D&quot;gmail_quote&quot; style=3D&quot;border-left: 1px solid rgb(2=\r\n04, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;&quot;&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n=\r\n\n\n\n&lt;div style=3D&quot;background-color: rgb(255, 255, 255);&quot;&gt;\n&lt;span&gt;=A0&lt;/span&gt;\n\n=\r\n\n&lt;div&gt;\n  &lt;div&gt;\n\n\n    &lt;div&gt;\n      \n      \n      &lt;p&gt;Hi Paul,&lt;br&gt;&lt;br&gt;As you&#3=\r\n9;ve already pointed out it could make a difference that you&#39;re not usi=\r\nng NEAT. We believe that NEAT is beneficial to Novelty Search because it gr=\r\nadually complexifies behaviors which reduces the novelty search space. Your=\r\n novelty metric seems similar to the one I used so I think that shouldn&#39=\r\n;t make a big difference.&lt;br&gt;\n\n\n&lt;br&gt;Were you able to evolve learning behavi=\r\nor with a regular fitness function? What kind of adaptation mechanism are y=\r\nou using? &lt;br&gt;&lt;br&gt;Cheers,&lt;br&gt;Sebastian&lt;/p&gt;&lt;div&gt;&lt;div&gt;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;=\r\n&lt;div class=3D&quot;gmail_quote&quot;&gt;\nOn Fri, Dec 4, 2009 at 12:50 PM, Nemo 136 &lt;span=\r\n dir=3D&quot;ltr&quot;&gt;&lt;&lt;a href=3D&quot;mailto:nemo136@...&quot; target=3D&quot;_blank&quot;&gt;nem=\r\no136@...&lt;/a&gt;&gt;&lt;/span&gt; wrote:&lt;br&gt;\n&lt;blockquote class=3D&quot;gmail_quote&quot; =\r\nstyle=3D&quot;border-left: 1px solid rgb(204, 204, 204);&quot;&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;div sty=\r\nle=3D&quot;background-color: rgb(255, 255, 255);&quot;&gt;\n&lt;span&gt;=A0&lt;/span&gt;\n\n\n&lt;div&gt;\n  &lt;d=\r\niv&gt;\n\n\n    &lt;div&gt;\n      \n      \n      &lt;p&gt;&lt;/p&gt;&lt;div class=3D&quot;gmail_quote&quot;&gt;\nHell=\r\no, &lt;br&gt;&lt;br&gt;I&#39;m currently trying to reproduce the results in &lt;a href=3D&quot;=\r\nhttp://eplex.cs.ucf.edu/publications/2009/risi.gecco09.html&quot; target=3D&quot;_bla=\r\nnk&quot;&gt;http://eplex.cs.ucf.edu/publications/2009/risi.gecco09.html&lt;/a&gt; and fin=\r\nding out if a NEAT algorithm is necessary to obtain theses results. Right n=\r\now, I manage without too many problems to get behaviors changing depending =\r\non the trial / deploy.&lt;br&gt;\n\n\n\n\n&lt;br&gt;=A0Though, the novelty keeps trying new =\r\nbehaviors based upon crashing / not crashing or getting one reward or not g=\r\netting it, and returning or not returning to the origin (no reward). This f=\r\nills up the archive quite fast without any actual &quot;learning&quot; appe=\r\naring (the best scores show 60% to 70% of all high reward reached without a=\r\nny clear &quot;learning&quot; pattern).&lt;br&gt;\n\n\n\n\n&lt;br&gt;I would like to know if=\r\n some code is provided online for this experiment, or if you could send me =\r\nthe code used in this experiment, if only to try to see which parameters ar=\r\ne different in the two of them, so that I can try to pinpoint the differenc=\r\nes. &lt;br&gt;\n\n\n\n\n&lt;br&gt;Right now, the differences I can see are :&lt;br&gt;- simple net=\r\nwork evolution algorithm vs NEAT mechanisms&lt;br&gt;- possible difference in the=\r\n way of counting for novelty score : I am using a bitset of size 4*nb_deplo=\r\ny*nb_trials with each bit standing for : a crash, a high reward, a low rewa=\r\nrd, returning to the starting point wit the reward. I do a hamming distance=\r\n between behaviors to get my novelty score. But, it seems to be the same me=\r\nchanisms as in the paper&lt;br&gt;\n\n\n\n\n- generation based algorithm while the pap=\r\ner uses a a steady state approach in\nwhich only one individual is generated=\r\n at each generation&lt;br&gt;&lt;br&gt;Cheers.&lt;br&gt;&lt;br&gt;Paul Tonelli&lt;br&gt;&lt;br&gt;\n&lt;/div&gt;&lt;br&gt;\n\n=\r\n\n    &lt;/div&gt;\n     \n\n    \n    &lt;div style=3D&quot;color: rgb(255, 255, 255);&quot;&gt;&lt;/div=\r\n&gt;\n\n\n&lt;/div&gt;\n\n\n\n  \n\n\n\n\n\n\n&lt;/div&gt;&lt;/div&gt;&lt;/blockquote&gt;&lt;/div&gt;&lt;br&gt;\n&lt;/div&gt;&lt;/div&gt;\n\n  =\r\n  &lt;/div&gt;\n     \n\n    \n    &lt;div style=3D&quot;color: rgb(255, 255, 255); min-heigh=\r\nt: 0pt;&quot;&gt;&lt;/div&gt;\n\n\n&lt;/div&gt;\n\n\n\n  \n\n\n\n\n\n\n&lt;/blockquote&gt;&lt;/div&gt;&lt;br&gt;\n\r\n--00032555aa7685f9ba047a4f7e28--\r\n\n"}}