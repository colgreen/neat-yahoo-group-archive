{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":434634266,"authorName":"Vassilis Vassiliades","from":"Vassilis Vassiliades &lt;vassilisvas@...&gt;","profile":"v.vassiliades","replyTo":"LIST","senderId":"3TZN93YqeDnZztQiUvkc6gVnni1y7sKtSh345MIglKsGD1ouR9PmpyC7GIUBlsA-KJzxOvbPr0eXOqHLE4MFFifvxXfF2_0yPmeoBVJwbFjWMQE","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [neat] New paper: Automated Generation of Environments to Test the General Learning Capabilities of AI Agents","postDate":"1398955540","msgId":6291,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PENBTnRYaG1zdk5oK2Jpcz1zZUV1cG53eTRVaUZ0Z3FjTndoQ0I4VGkxYW50QURrQ2VWd0BtYWlsLmdtYWlsLmNvbT4=","inReplyToHeader":"PENBK2R1aW1QQ29VRnkxVzVXRWQrRnJpZ1haWlJ1ckZROVJHeE81LUt4QURaR2dVcExCQUBtYWlsLmdtYWlsLmNvbT4=","referencesHeader":"PENBK2R1aW1PMjRzYWtPWFNNVnVxYkVleDgremlCbVFIdmVjb1kza3dBZCt6QUI1Wmt3UUBtYWlsLmdtYWlsLmNvbT4JPENBTnRYaG12dUpHMkxkWXpSRGVXRldpU01HNW1iK3pmQkxWZ0VhQm10dHkyV0ZQaXhFd0BtYWlsLmdtYWlsLmNvbT4JPENBK2R1aW1ONCtZVTMtelN4ZnV1ek9OLVAtcnI4NVBTcHMrMDFDMW5Wa2twSkxjUmR0d0BtYWlsLmdtYWlsLmNvbT4JPENBTnRYaG10eG9oTzRSZmhVYzBCUzRhMmZXMTlKY2pEYmUtOHEwdzlNSDFXalhjbzh6UUBtYWlsLmdtYWlsLmNvbT4JPEY1MkE3MUQ3LURGMzUtNEEzMC1BM0ZDLTk3RjY0NjIwMzg0N0BnbWFpbC5jb20+CTxDQUpuNj1kckVOcjJzaFlEZmJLWmkzWEViQ3g2NDBwUUgxMWZhSnRYaENaeXZxSkd3THdAbWFpbC5nbWFpbC5jb20+CTxDQStkdWltUENvVUZ5MVc1V0VkK0ZyaWdYWlpSdXJGUTlSR3hPNS1LeEFEWkdnVXBMQkFAbWFpbC5nbWFpbC5jb20+"},"prevInTopic":6290,"nextInTopic":6296,"prevInTime":6290,"nextInTime":6292,"topicId":6279,"numMessagesInTopic":11,"msgSnippet":"Hi Oliver, From the paper: Where not specified, the number of actions and the length ... Yes, I read that part, but I was wondering about the number of","rawEmail":"Return-Path: &lt;vassilisvas@...&gt;\r\nX-Sender: vassilisvas@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 42910 invoked by uid 102); 1 May 2014 14:45:41 -0000\r\nX-Received: from unknown (HELO mtaq1.grp.bf1.yahoo.com) (10.193.84.32)\n  by m5.grp.bf1.yahoo.com with SMTP; 1 May 2014 14:45:41 -0000\r\nX-Received: (qmail 23902 invoked from network); 1 May 2014 14:45:41 -0000\r\nX-Received: from unknown (HELO mail-pa0-f46.google.com) (209.85.220.46)\n  by mtaq1.grp.bf1.yahoo.com with SMTP; 1 May 2014 14:45:41 -0000\r\nX-Received: by mail-pa0-f46.google.com with SMTP id kx10so979732pab.33\n        for &lt;neat@yahoogroups.com&gt;; Thu, 01 May 2014 07:45:40 -0700 (PDT)\r\nMIME-Version: 1.0\r\nX-Received: by 10.66.192.225 with SMTP id hj1mr21813733pac.142.1398955540668;\n Thu, 01 May 2014 07:45:40 -0700 (PDT)\r\nX-Received: by 10.70.50.103 with HTTP; Thu, 1 May 2014 07:45:40 -0700 (PDT)\r\nIn-Reply-To: &lt;CA+duimPCoUFy1W5WEd+FrigXZZRurFQ9RGxO5-KxADZGgUpLBA@...&gt;\r\nReferences: &lt;CA+duimO24sakOXSMVuqbEex8+ziBmQHvecoY3kwAd+zAB5ZkwQ@...&gt;\n\t&lt;CANtXhmvuJG2LdYzRDeWFWiSMG5mb+zfBLVgEaBmtty2WFPixEw@...&gt;\n\t&lt;CA+duimN4+YU3-zSxfuuzON-P-rr85PSps+01C1nVkkpJLcRdtw@...&gt;\n\t&lt;CANtXhmtxohO4RfhUc0BS4a2fW19JcjDbe-8q0w9MH1WjXco8zQ@...&gt;\n\t&lt;F52A71D7-DF35-4A30-A3FC-97F646203847@...&gt;\n\t&lt;CAJn6=drENr2shYDfbKZi3XEbCx640pQH11faJtXhCZyvqJGwLw@...&gt;\n\t&lt;CA+duimPCoUFy1W5WEd+FrigXZZRurFQ9RGxO5-KxADZGgUpLBA@...&gt;\r\nDate: Thu, 1 May 2014 17:45:40 +0300\r\nMessage-ID: &lt;CANtXhmsvNh+bis=seEupnwy4UiFtgqcNwhCB8Ti1antADkCeVw@...&gt;\r\nTo: neat@yahoogroups.com\r\nContent-Type: multipart/alternative; boundary=047d7bdc91460ee57304f857b8c7\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Vassilis Vassiliades &lt;vassilisvas@...&gt;\r\nSubject: Re: [neat] New paper: Automated Generation of Environments to Test\n the General Learning Capabilities of AI Agents\r\nX-Yahoo-Group-Post: member; u=434634266; y=6oGia34d6zJmR0VFD50CiIDZ8JyAMG8Y04jVtYvgbfJevbCRlzjHHw\r\nX-Yahoo-Profile: v.vassiliades\r\n\r\n\r\n--047d7bdc91460ee57304f857b8c7\r\nContent-Type: text/plain; charset=UTF-8\r\n\r\nHi Oliver,\n\nFrom the paper: &quot;Where not specified, the number of actions and the length\n&gt; of trials are set to 4 and the number of environment configurations per run\n&gt; is 8.&quot;\n&gt;\n\nYes, I read that part, but I was wondering about the number of *states*,\nnot the number of actions or the length of the trials. :)\n\nThe reason I am asking is because there exist practically optimal\nalgorithms in reinforcement learning that have a sample complexity of ~\nO(SA), where S is the size of the state space and A the number of actions\nper state; however, they behave optimally in (1) stationary and (2) fully\nobservable environments. The environments you present are fully observable,\nbut nonstationary, so it is harder to perform optimally. Since you are\ngenerating *random* MDPs per configuration, I believe that the length of\ntrials must be *at least* equal to S*A, since you want to allow the\nnetworks to fully explore the current environment configuration, before\nmoving on to a new one.\n\n\nVassilis\n\r\n--047d7bdc91460ee57304f857b8c7\r\nContent-Type: text/html; charset=UTF-8\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\n&lt;div dir=3D&quot;ltr&quot;&gt;Hi Oliver,&lt;div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;div class=3D&quot;gm=\r\nail_quote&quot;&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;blockquote class=3D&quot;gmail_quote&quot; style=3D&quot;margin=\r\n:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex&quot;&gt;&lt;div style&gt;&lt;div&gt;&lt;d=\r\niv&gt;&lt;div&gt;\n&lt;div dir=3D&quot;ltr&quot;&gt;&lt;div&gt;&lt;/div&gt;&lt;div&gt;From the paper: &quot;Where not s=\r\npecified, the number of actions and the length of trials are set to 4 and t=\r\nhe number of environment configurations per run is 8.&quot;&lt;/div&gt;&lt;/div&gt;&lt;/di=\r\nv&gt;&lt;/div&gt;\n&lt;/div&gt;&lt;/div&gt;&lt;/blockquote&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Yes, I read that part=\r\n, but I was wondering about the number of &lt;b&gt;states&lt;/b&gt;, not the number of =\r\nactions or the length of the trials. :)=C2=A0&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;The =\r\nreason I am asking is because there exist practically optimal algorithms in=\r\n reinforcement learning that have a sample complexity of ~ O(SA), where S i=\r\ns the size of the state space and A the number of actions per state; howeve=\r\nr, they behave optimally in (1) stationary and (2) fully observable environ=\r\nments. The environments you present are fully observable, but nonstationary=\r\n, so it is harder to perform optimally. Since you are generating &lt;b&gt;random&lt;=\r\n/b&gt; MDPs per configuration, I believe that the length of trials must be &lt;b&gt;=\r\nat least&lt;/b&gt; equal to S*A, since you want to allow the networks to fully ex=\r\nplore the current environment configuration, before moving on to a new one.=\r\n&lt;/div&gt;\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Vassilis&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;=\r\n/div&gt;\n\r\n--047d7bdc91460ee57304f857b8c7--\r\n\n"}}