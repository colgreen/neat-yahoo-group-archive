{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":292213213,"authorName":"shanemcdonaldryan","from":"&quot;shanemcdonaldryan&quot; &lt;shanemcdonaldryan@...&gt;","profile":"shanemcdonaldryan","replyTo":"LIST","senderId":"6F5MIZ16wwtxsowXj3UtM7wObGi6ZemDXsTQWGWQ6W_xbiMWztPG_33AZyXJfJpRviparsGbZ4hY1A1h5k51H710KSkTpY5uSc7ovZFfPd7seroMKvjMrw","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Mix of Identity and Sigmoid activation functions","postDate":"1176375949","msgId":3123,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGV2bDNxZCtsbzIwQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGV2ajVtZSt1N284QGVHcm91cHMuY29tPg=="},"prevInTopic":3120,"nextInTopic":0,"prevInTime":3122,"nextInTime":3124,"topicId":3114,"numMessagesInTopic":8,"msgSnippet":"Well I am not sure if I would personally add in specific operators like add multiply, std dev etc. This makes the problem start to stray towards Genetic","rawEmail":"Return-Path: &lt;shanemcdonaldryan@...&gt;\r\nX-Sender: shanemcdonaldryan@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 4614 invoked from network); 12 Apr 2007 11:06:24 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m45.grp.scd.yahoo.com with QMQP; 12 Apr 2007 11:06:24 -0000\r\nReceived: from unknown (HELO n4b.bullet.sp1.yahoo.com) (69.147.64.183)\n  by mta5.grp.scd.yahoo.com with SMTP; 12 Apr 2007 11:06:24 -0000\r\nReceived: from [216.252.122.216] by n4.bullet.sp1.yahoo.com with NNFMP; 12 Apr 2007 11:05:52 -0000\r\nReceived: from [66.218.69.1] by t1.bullet.sp1.yahoo.com with NNFMP; 12 Apr 2007 11:05:51 -0000\r\nReceived: from [66.218.66.92] by t1.bullet.scd.yahoo.com with NNFMP; 12 Apr 2007 11:05:51 -0000\r\nDate: Thu, 12 Apr 2007 11:05:49 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;evl3qd+lo20@...&gt;\r\nIn-Reply-To: &lt;evj5me+u7o8@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0\r\nFrom: &quot;shanemcdonaldryan&quot; &lt;shanemcdonaldryan@...&gt;\r\nSubject: Re: Mix of Identity and Sigmoid activation functions\r\nX-Yahoo-Group-Post: member; u=292213213; y=fxFHYb1L_vrIMxru3vifeA-gsOFKqN5gOTJLU95CdDfrECd8oYXnhszQYE0\r\nX-Yahoo-Profile: shanemcdonaldryan\r\n\r\nWell I am not sure if I would personally add in specific operators \nlike ad=\r\nd multiply, std dev etc. This makes the problem start to stray \ntowards Gen=\r\netic Programming. Also it increases the size of the search \nspace. Plus the=\r\n choice of what operators to use becomes pretty \narbitrary. The nice thing =\r\nin my opinion about ANNs is their \ntheoretical ability to act as a universa=\r\nl function approximators. \nPlus a recurrent ANN with an identity operator a=\r\ns the activation \nfunction and the proper topology will create the add, sub=\r\ntract, and \nmultiply operators automatically (I think).\n\nThe problem that I=\r\n was trying to overcome is Sigmoid activated ANNs \npoor ability to approxim=\r\nate linear functions. But now that I think \nabout it more, a better solutio=\r\nn might be to use NEAT to develop two \nseperate networks one using the sigm=\r\noid and the other the identity \nfunction. Then feed the output of these two=\r\n networks into a simple \nNEAT evolved net with two inputs and one output. T=\r\nhis third net would \nbe responsible for composing the output of the two net=\r\nworks into one \noutput. I am not sure if it should use the sigmoid or the i=\r\ndentity \nfunction as an activation function. Essentially it would decide wh=\r\nen \nand how much to use either of the two input networks. I think this \nmig=\r\nht be a good way to quickly evolve a network that approximates the \nlinear =\r\nand non-linear aspects of a function.\n\n--Shane\n\n\n--- In neat@...=\r\nm, &quot;petar_chervenski&quot; \n&lt;petar_chervenski@...&gt; wrote:\n&gt;\n&gt; Well, in some poin=\r\nt of view, Neuro-Evolution is a genetic \nprogramming \n&gt; scheme, composing m=\r\nany many &quot;1/(1+exp(-(weighted_inputs * x)))&quot; in \n&gt; such a way, that the com=\r\nposition will compute something at all :) \n&gt; In fact, the basic principles =\r\nof NEAT are important, not the thing \n&gt; they are applied on.\n&gt; \n&gt; --- In ne=\r\nat@yahoogroups.com, &quot;Rafael C.P.&quot; &lt;kurama.youko.br@&gt; \n&gt; wrote:\n&gt; &gt;\n&gt; &gt; Hey,=\r\n all that things will simply take us to genetic programming \nat \n&gt; the\n&gt; &gt; =\r\nend...\n&gt; &gt; \n&gt; &gt; On 4/11/07, Rafael C.P. &lt;kurama.youko.br@&gt; wrote:\n&gt; &gt; &gt;\n&gt; &gt;=\r\n &gt; Totally agreed! Your solution seems good for the problem. About \n&gt; the 4=\r\n\n&gt; &gt; &gt; functions (MIN, MAX, ADD (default), MUL), do you mean apply \nthem \n&gt;=\r\n before the\n&gt; &gt; &gt; activation function? It seems good too, one more generali=\r\nzation \n&gt; to neural\n&gt; &gt; &gt; nets. AVG (average), SD (standard deviation) and =\r\nother \n&gt; statistical functions\n&gt; &gt; &gt; would be fine too.\n&gt; &gt; &gt;\n&gt; &gt; &gt; And som=\r\nething must be clear about it: adding all that things \n&gt; doesn&#39;t\n&gt; &gt; &gt; incr=\r\neases the computational power of a NN since every operation \n&gt; here can be\n=\r\n&gt; &gt; &gt; simulated. It could be better in other ways (like using less \n&gt; neuro=\r\nns).\n&gt; &gt; &gt;\n&gt; &gt; &gt; On 4/11/07, petar_chervenski &lt;petar_chervenski@&gt; wrote:\n&gt; =\r\n&gt; &gt; &gt;\n&gt; &gt; &gt; &gt;   This is the Identity function? Heh, I call it linear.. So I=\r\n \n&gt; have this\n&gt; &gt; &gt; &gt; already :)\n&gt; &gt; &gt; &gt; You&#39;re right that a simple mutatio=\r\nn will have a significant \n&gt; impact,\n&gt; &gt; &gt; &gt; but what if we adjust the impo=\r\nrtance factors for speciation? \nI \n&gt; mean\n&gt; &gt; &gt; &gt; that if activation functi=\r\non types are so important to \n&gt; performace, we\n&gt; &gt; &gt; &gt; can increase that mu=\r\nltiplier and every time such a mutation \n&gt; occurs, a\n&gt; &gt; &gt; &gt; new species wi=\r\nll be created that will further optimize its \n&gt; weights\n&gt; &gt; &gt; &gt; and connect=\r\nions, not changing the activation functions..\n&gt; &gt; &gt; &gt; I think the best thin=\r\ng to do is to supply the system with \n&gt; everything,\n&gt; &gt; &gt; &gt; MIN,MAX,ADD and=\r\n MUL summing functions, and let evolution \ndecide \n&gt; what\n&gt; &gt; &gt; &gt; is best. =\r\nThere may be some heuristics, of course.\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; --- In neat@yahoog=\r\nroups.com &lt;neat%40yahoogroups.com&gt;, &quot;Rafael \n&gt; C.P.&quot; &lt;\n&gt; &gt; &gt; &gt; kurama.youko=\r\n.br@&gt;\n&gt; &gt; &gt; &gt; wrote:\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; Identity function is f(x) =3D x, s=\r\nimple =3D)\n&gt; &gt; &gt; &gt; &gt; I&#39;ve thought about multi-activation-functions populati=\r\nons \nas \n&gt; you 2\n&gt; &gt; &gt; &gt; proposed.\n&gt; &gt; &gt; &gt; &gt; My contribution here is to add=\r\n the sine function too, \nallowing\n&gt; &gt; &gt; &gt; recognition\n&gt; &gt; &gt; &gt; &gt; of periodic=\r\n patterns.\n&gt; &gt; &gt; &gt; &gt; The problem I see with that approach is that a simple =\r\n\nmutation\n&gt; &gt; &gt; &gt; &gt; including/modifying an activation function would cause =\r\na \nbig \n&gt; change\n&gt; &gt; &gt; &gt; (with\n&gt; &gt; &gt; &gt; &gt; undefined direction) to the output=\r\ns. There must be a way to \n&gt; cluster\n&gt; &gt; &gt; &gt; similar\n&gt; &gt; &gt; &gt; &gt; functions in=\r\n order to make always the smallest change \n&gt; possible and\n&gt; &gt; &gt; &gt; to the\n&gt; =\r\n&gt; &gt; &gt; &gt; right direction.\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; PS: you could use the MIN func=\r\ntion instead of AND (and MAX \n&gt; for OR),\n&gt; &gt; &gt; &gt; turning\n&gt; &gt; &gt; &gt; &gt; it into =\r\nfuzzy logic.\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; On 4/11/07, petar_chervenski &lt;petar_cherve=\r\nnski@&gt; wrote:\n&gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; Hi Shane,\n&gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; I d=\r\non&#39;t know what the identity function is exactly.. I \nneed \n&gt; to\n&gt; &gt; &gt; &gt; lea=\r\nrn\n&gt; &gt; &gt; &gt; &gt; &gt; that.\n&gt; &gt; &gt; &gt; &gt; &gt; But I suppose you can also try to split th=\r\ne neuron \n&gt; activation into\n&gt; &gt; &gt; &gt; &gt; &gt; stages, introduce another function =\r\nthat sums the weighted \n&gt; inputs\n&gt; &gt; &gt; &gt; in\n&gt; &gt; &gt; &gt; &gt; &gt; different ways, you=\r\n know that this is usually addition, \nbut\n&gt; &gt; &gt; &gt; &gt; &gt; multiplication is als=\r\no promising, since it is like the \nAND \n&gt; logical\n&gt; &gt; &gt; &gt; &gt; &gt; operation on =\r\nthose inputs. When this is combined with \n&gt; linear and\n&gt; &gt; &gt; &gt; &gt; &gt; other ac=\r\ntivation functions, it turns NEAT into actually \na ..\n&gt; &gt; &gt; &gt; &gt; &gt; something=\r\n like a math equation evolving system.. I don&#39;t \n&gt; think\n&gt; &gt; &gt; &gt; this\n&gt; &gt; &gt;=\r\n &gt; &gt; &gt; will enhance performace in most tasks, but interesting \n&gt; results\n&gt; =\r\n&gt; &gt; &gt; may be\n&gt; &gt; &gt; &gt; &gt; &gt; obtained.\n&gt; &gt; &gt; &gt; &gt; &gt; There may be also a MAX summ=\r\ning function, that returns \nthe \n&gt; maximum\n&gt; &gt; &gt; &gt; &gt; &gt; input of all. This w=\r\nay the neurons can have winner-take-\nall\n&gt; &gt; &gt; &gt; &gt; &gt; behaviour, I guess. I =\r\nhave implemented this and I see it \n&gt; gives\n&gt; &gt; &gt; &gt; &gt; &gt; results.\n&gt; &gt; &gt; &gt; &gt; =\r\n&gt;\n&gt; &gt; &gt; &gt; &gt; &gt; Peter\n&gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; --- In neat@yahoogroups.com &lt;ne=\r\nat%40yahoogroups.com&gt; \n&lt;neat%\n&gt; &gt; &gt; &gt; 40yahoogroups.com&gt;, &quot;shanemcdonaldrya=\r\nn&quot;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &lt;shanemcdonaldryan@&gt; wrote:\n&gt; &gt; &gt; &gt; &gt; &gt; =\r\n&gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; I am sure this must have been done but I can&#39;t find any \n&gt; =\r\npapers\n&gt; &gt; &gt; &gt; on\n&gt; &gt; &gt; &gt; &gt; &gt; it.\n&gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; Has anyone tr=\r\nied starting out with an initial \npopulation \n&gt; that\n&gt; &gt; &gt; &gt; &gt; &gt; connects\n&gt;=\r\n &gt; &gt; &gt; &gt; &gt; &gt; all the inputs to the outputs with a parallel set of \n&gt; neuron=\r\ns\n&gt; &gt; &gt; &gt; using\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; identity as the activation function? Then add=\r\n a new \ntype \n&gt; of\n&gt; &gt; &gt; &gt; mutation\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; that can add new neurons =\r\nwith the identity activation \n&gt; function.\n&gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; Basic=\r\nally what I am proposing is starting with the same \n&gt; initial\n&gt; &gt; &gt; &gt; NEAT\n=\r\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; configuration of all the inputs connected to the \noutputs \n&gt; =\r\nvia\n&gt; &gt; &gt; &gt; &gt; &gt; sigmoids.\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; But doubling up this configuration =\r\nwith an identical net\n&gt; &gt; &gt; &gt; connected\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; via the identity acti=\r\nvation function.\n&gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; The whole point of this would =\r\nbe to create a hybrid \n&gt; network\n&gt; &gt; &gt; &gt; that is\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; good at capt=\r\nuring the linear aspects (A+B, cross \nproduct) \n&gt; of the\n&gt; &gt; &gt; &gt; &gt; &gt; data\n&gt;=\r\n &gt; &gt; &gt; &gt; &gt; &gt; and composing it with the non-linear aspects.\n&gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; =\r\n&gt; &gt; &gt; &gt; &gt; &gt; But now that I think about it. I guess a good starting \n&gt; quest=\r\nion\n&gt; &gt; &gt; &gt; &gt; &gt; would\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; be is a Neural Net using only the Ident=\r\nity activation \n&gt; function\n&gt; &gt; &gt; &gt; good\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; at approximating line=\r\nar functions?\n&gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; Something like this might be good=\r\n at approximating time \n&gt; series\n&gt; &gt; &gt; &gt; with\n&gt; &gt; &gt; &gt; &gt; &gt; a\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; s=\r\nimple linear trend. So you don&#39;t have to detrend the \n&gt; data.\n&gt; &gt; &gt; &gt; &gt; &gt; &gt;=\r\n\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; Thanks,\n&gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; Shane\n&gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; =\r\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; --\n=\r\n&gt; &gt; &gt; &gt; &gt; =3D=3D=3D=3D=3D=3D=3D=3D=3D\n&gt; &gt; &gt; &gt; &gt; Rafael C.P.\n&gt; &gt; &gt; &gt; &gt; =3D=\r\n=3D=3D=3D=3D=3D=3D=3D=3D\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;  \n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; =\r\n&gt; &gt;\n&gt; &gt; &gt; --\n&gt; &gt; &gt; =3D=3D=3D=3D=3D=3D=3D=3D=3D\n&gt; &gt; &gt; Rafael C.P.\n&gt; &gt; &gt; =3D=\r\n=3D=3D=3D=3D=3D=3D=3D=3D\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; -- \n&gt; &gt; =3D=3D=3D=3D=3D=3D=\r\n=3D=3D=3D\n&gt; &gt; Rafael C.P.\n&gt; &gt; =3D=3D=3D=3D=3D=3D=3D=3D=3D\n&gt; &gt;\n&gt;\n\n\n\n"}}