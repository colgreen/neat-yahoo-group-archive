{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"H2ECNtokELjnSmn1hVURMlodiFbuX_mA_9ZI_JGUpTZpvXlOi5LY4aRC3z5C3ywJWm4uR-gLhTx1Yqo1if0k_rpgF1uw1CPRRfed70RKULEM","spamInfo":{"isSpam":false,"reason":"6"},"subject":"FW: [neat] Re: Machine Learning and the Long View of AI","postDate":"1209579097","msgId":4023,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGZ2YWNvcSs4MzMwQGVHcm91cHMuY29tPg==","inReplyToHeader":"PEM0M0QyRjhDLjIyODc0JWpjbHVuZUBtc3UuZWR1Pg=="},"prevInTopic":4022,"nextInTopic":4025,"prevInTime":4022,"nextInTime":4024,"topicId":3955,"numMessagesInTopic":49,"msgSnippet":"Jeff, I know we ve reached a kind of natural conclusion to our discussion, but I think it s a good opportunity to point out one more interesting issue to think","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 28120 invoked from network); 30 Apr 2008 18:11:39 -0000\r\nX-Received: from unknown (66.218.67.95)\n  by m53.grp.scd.yahoo.com with QMQP; 30 Apr 2008 18:11:39 -0000\r\nX-Received: from unknown (HELO n42a.bullet.mail.sp1.yahoo.com) (66.163.168.136)\n  by mta16.grp.scd.yahoo.com with SMTP; 30 Apr 2008 18:11:39 -0000\r\nX-Received: from [216.252.122.216] by n42.bullet.mail.sp1.yahoo.com with NNFMP; 30 Apr 2008 18:11:39 -0000\r\nX-Received: from [66.218.69.5] by t1.bullet.sp1.yahoo.com with NNFMP; 30 Apr 2008 18:11:39 -0000\r\nX-Received: from [66.218.66.74] by t5.bullet.scd.yahoo.com with NNFMP; 30 Apr 2008 18:11:39 -0000\r\nDate: Wed, 30 Apr 2008 18:11:37 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;fvacoq+8330@...&gt;\r\nIn-Reply-To: &lt;C43D2F8C.22874%jclune@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: FW: [neat] Re: Machine Learning and the Long View of AI\r\nX-Yahoo-Group-Post: member; u=54567749; y=JeIJZBXL5WC5HErx8pBIPtnVmVsROF5b34Ew3EgdCjRaQBFD2eMU\r\nX-Yahoo-Profile: kenstanley01\r\n\r\nJeff,\n\nI know we&#39;ve reached a kind of natural conclusion to our discussion,=\r\n\nbut I think it&#39;s a good opportunity to point out one more interesting\nissu=\r\ne to think about:  \n\nTo what extent is it possible to improve in general at=\r\n being &quot;good at\ndiscovering and exploiting regularities?&quot;  I mean, let&#39;s re=\r\nally think\nabout what the hope would be with that idea.  \n\nThere is no doub=\r\nt that an indirect encoding can *represent*\nregularities. For example, CPPN=\r\ns can represent symmetry.  But does\nbeing able to represent something equat=\r\ne to being good at discovering\nit, and if not, what else could make somethi=\r\nng more &quot;discoverable?&quot;\n\nWhat I&#39;m getting at is that I think the issue is f=\r\nairly deep and\ncomplicated (not to imply that you are implying that it is a=\r\n simple\nmatter either- I just think it&#39;s interesting to discuss these issue=\r\ns).\n\nIf being good at discovering just means that you tend to discover the\n=\r\nright regularity quickly, then it&#39;s easy to see what kind of\nregularity a p=\r\narticular encoding will be good at discovering.  For\nexample, CPPNs are qui=\r\nte good at discovering bilateral symmetry (in\nthis sense) because all it ne=\r\neds to do is add a single Gaussian node\nto the CPPN early in evolution, whi=\r\nch has relatively high probability.\n\nYet does that mean HyperNEAT is not go=\r\nod at discovering things that\nwould take longer to build up, such as five-f=\r\nold radial symmetry with\ntwo of the repetitions slightly different than the=\r\n others? Certainly\nit *can* represent such a structure, but because it does=\r\n not have a\nhidden node with that exact pattern in it, it would have to bui=\r\nld it\nup.  So it seems like the &quot;time to discovery&quot; metric is not the full\n=\r\nstory on how good something is at discovering.  After all, it *should*\ntake=\r\n longer to discover some regularities than others.\n\nSo really what we&#39;re ta=\r\nlking about here are biases towards certain\ntypes of regularities appearing=\r\n sooner or later in the search.  In\nthat view, we need to be careful about =\r\nwhat we mean by good because I\nam not sure if a system that is fast at disc=\r\novering a particular\nbizarre sort of regularity really deserves to be calle=\r\nd good in\ngeneral, when 99% of the time that regularity is going to interfe=\r\nre\nwhenever it is instantiated.\n\nYet the problem is even more complicated t=\r\nhan that.  In fact, just\nbecause a particular regularity is &quot;discovered&quot; in=\r\nside the CPPN (or\nany other encoding) does not mean that the fitness functi=\r\non rewards\nthat discovery.  Therefore, there is a more fundamental problem.=\r\n  For\nexample, imagine discovering a symmetric blob when we are searching\nf=\r\nor an airplane.  Well, blobs don&#39;t fly, but symmetry is the right\nregularit=\r\ny.  But because it can&#39;t fly, the blob doesn&#39;t get rewarded\nand just dies o=\r\nut, so what then do we say about the goodness of the\ndiscovery?\n\nThen we mi=\r\nght say, well, it should be smarter about things.  It should\nsomehow make s=\r\nure it doesn&#39;t lose the right regularity when it\ndiscovers it.  And in fact=\r\n there are some protections that can be put\nin place to maintain diversity =\r\nand try to help with that issue (like\nspeciation).  But at the end of the d=\r\nay, if you&#39;ve got a blob\ncompeting to be an airplane, and it doesn&#39;t get lu=\r\ncky enough to make\nsome more mutations on the path to airplane, it&#39;s not go=\r\ning to cut it,\nand it is eventually going to die out.\n\nSo then we might say=\r\n that what we really need is something that would\nsay, &quot;Ah! Even though I h=\r\nave a useless blob I can see that the\ndiscovery of symmetry might be useful=\r\n and therefore we will hold on to\nit and see what we can do with it.&quot;  Yet =\r\nthat seems way outside the\nscope of evolutionary computation.  If you could=\r\n make that kind of\ndeduction (based on an intelligent insight), why have ev=\r\nolution at\nall?  You could just build the thing based on intellect and dedu=\r\nction.\n Or at least you would have a system well beyond evolutionary\ncomput=\r\nation alone (perhaps not a bad idea if it&#39;s possible).\n\nSo the point is jus=\r\nt that being good at discovering regularities is a\nfairly tricky topic.  It=\r\n&#39;s easy to set up expectations that cannot be\nmet.  No Free Lunch is also l=\r\neaking through the cracks all over the\nplace.  What I think is &quot;good&quot; about=\r\n CPPNs in terms of discovering\nregularities is that we are able to encode t=\r\nhe primary types of\nregularities into the activation functions, so we know =\r\nwhat we&#39;re\nbuilding out of and biasing towards, and those regularities are\n=\r\nmotivated by nature and engineering designs.  Also, we get to forgo\nthe pro=\r\ncess of developmental unfolding, which can be expensive and\nharder to bias =\r\nin a desired way.  So what I&#39;m saying is that part of\nwhat&#39;s good about CPP=\r\nNs is that they can be biased towards different\ntypes of patterns relativel=\r\ny easily.  But that does not mean they\nnecessarily discover any arbitrary p=\r\nattern quickly.  No encoding will\never be able to do that, unless it is und=\r\ner the control of a higher\nintellect. CPPNs in their present form are based=\r\n on an assumption\nabout what kinds of regularities are useful.  If the assu=\r\nmptions turn\nout wrong, the encoding is relatively flexible enough to be bi=\r\nased\ndifferently, which is another thing you could say is good about its\nab=\r\nility to discover regularities.\n\nOn the other hand, I think &quot;exploiting&quot; re=\r\ngularities (which you also\nmention) is easier to pin down.  It means, given=\r\n that you found\nsymmetry, you can now elaborate on that and make something =\r\nmore\ncomplex with even more regularities, but that still respects the\niniti=\r\nal symmetric regularity.  In other words, we&#39;re not judging on\nthe regulari=\r\nty being good or bad, just that it can be elaborated.  I\nbelieve it is easi=\r\ner to see that CPPNs can do that well, since it is\ndirectly related to comp=\r\nlexification.\n\nAnyway, I just thought these would be interesting to conside=\r\nr all the\nnuances in the discussion of effective pattern discovery.\n\nken\n\n\n=\r\n--- In neat@yahoogroups.com, Jeff Clune &lt;jclune@...&gt; wrote:\n&gt;\n&gt; Hello Ken-\n=\r\n&gt; \n&gt; You make a very strong defense of your position. I have been entirely\n=\r\n&gt; persuaded that in the multi-agent paper it was appropriate to use\nthe r(x=\r\n)\n&gt; strategy. However, I continue to think that the issue comes down to wha=\r\nt\n&gt; one&#39;s goal is. Your goal here was to evolve sophisticated multi-agent\n&gt;=\r\n controllers. Given that goal, you are right that adding the extra\nchalleng=\r\ne\n&gt; of figuring out where one brain begins and ends is not necessary.\nHowev=\r\ner,\n&gt; were one&#39;s goal to test how good HyperNEAT is at discovering and\nexpl=\r\noiting\n&gt; regularities in the problem space, in order to know how well it wi=\r\nll\nperform\n&gt; when it encounters unanticipated regularities, it could be\nwor=\r\nthwhile to see\n&gt; how it does without the injected information.\n&gt; \n&gt; While I=\r\n do think that evolution is cool, and like to show its\nprowess, I too\n&gt; am =\r\nmainly interested in using evolution to produce general AI. My\n&gt; suppositio=\r\nn was that figuring out to what extent our algorithms are\nable to\n&gt; exploit=\r\n problem regularities may facilitate improvements on that font. I\n&gt; also th=\r\nink such improvements will be necessary steps in the path to\n&gt; producing ev=\r\nolutionary algorithms capable of creating general AI.\n&gt; \n&gt; Nevertheless, I =\r\nbenefited from hearing your perspective and\nappreciate the\n&gt; exchange. \n&gt; \n=\r\n&gt; \n&gt; Cheers,\n&gt; Jeff Clune\n&gt; \n&gt; Digital Evolution Lab, Michigan State Univer=\r\nsity\n&gt; \n&gt; jclune@...\n&gt; \n&gt; \n&gt; \n&gt; ------ Forwarded Message\n&gt; &gt; From: Kenneth =\r\nStanley &lt;kstanley@...&gt;\n&gt; &gt; Reply-To: &quot;neat@yahoogroups.com&quot; &lt;neat@yahoogrou=\r\nps.com&gt;\n&gt; &gt; Date: Tue, 29 Apr 2008 02:30:13 -0000\n&gt; &gt; To: &quot;neat@yahoogroups=\r\n.com&quot; &lt;neat@yahoogroups.com&gt;\n&gt; &gt; Subject: [neat] Re: Machine Learning and t=\r\nhe Long View of AI\n&gt; &gt; \n&gt; &gt; Jeff,\n&gt; &gt; \n&gt; &gt; I think it would be interesting =\r\nto step back and look at the\n&gt; &gt; assumptions that underly your straw man ar=\r\ngument.  In particular, why\n&gt; &gt; are we using evolution in the first place?\n=\r\n&gt; &gt; \n&gt; &gt; Your straw man implies a world view wherein we are using evolution=\r\n\n&gt; &gt; because we like evolution and want it to succeed.  Under that\n&gt; &gt; phil=\r\nosophy, then indeed, creating a super-powerful neural network\n&gt; &gt; (DeepNet)=\r\n by hand and then creating a faux-mutation operator that\n&gt; &gt; simply makes a=\r\n neural network turn into DeepNet would be a\n&gt; &gt; disappointment, because it=\r\n would mean that evolution didn&#39;t really do\n&gt; &gt; what we wanted, and we woul=\r\nd be being disingenuous (and unimpressed).\n&gt; &gt; \n&gt; &gt; However, my world view =\r\nis different.  I am not using evolution because\n&gt; &gt; I like evolution and wa=\r\nnt to prove that it is impressive. (Note that I\n&gt; &gt; *do* like evolution, bu=\r\nt that&#39;s not the reason I use it.)  Rather, I\n&gt; &gt; am using evolution becaus=\r\ne I genuinely believe that full-fledged AI\n&gt; &gt; likely *cannot* be construct=\r\ned by hand, and that evolution is the best\n&gt; &gt; alternative.  If someone wen=\r\nt ahead and built a general-AI neural\n&gt; &gt; network by hand, it would simply =\r\nprove me wrong.  But it would mean\n&gt; &gt; nothing with respect to how we shoul=\r\nd go about doing the same with\n&gt; &gt; evolution (which would however be a moot=\r\n point at that point anyway\n&gt; &gt; because why bother when someone figured out=\r\n a better way?).  So as\n&gt; &gt; soon as someone did what you said, evolution wo=\r\nuld be thrown out\n&gt; &gt; anyway, at least in terms of being the best path to g=\r\neneral AI.\n&gt; &gt; \n&gt; &gt; So I am looking at things in a kind of reverse perspect=\r\nive from you.\n&gt; &gt; To me, the point is not to bolster up evolution and show =\r\nhow powerful\n&gt; &gt; it is.  Rather, the point is that I believe it *is* powerf=\r\nul and\n&gt; &gt; therefore I am using it.  If I can do something to boost it furt=\r\nher,\n&gt; &gt; good.  What else is there to prove?\n&gt; &gt; \n&gt; &gt; However, I am a reali=\r\nst and I doubt that evolution alone will get the\n&gt; &gt; job done in the long v=\r\niew.  It&#39;s just too gigantic a search space and\n&gt; &gt; the problem is too poor=\r\nly specified.  Therefore, I think there will be\n&gt; &gt; a lot of biases and man=\r\nipulations along the way.\n&gt; &gt; \n&gt; &gt; I&#39;m way off speculating about the far of=\r\nf future here, but my guess is\n&gt; &gt; that those manipulations will come mostl=\r\ny at the genetic level rather\n&gt; &gt; than the ANN level.  In other words, the =\r\nkinds of hacks that you are\n&gt; &gt; talking about (&quot;building blocks&quot; provided a=\r\n priori) generally seem to\n&gt; &gt; be kind of neural &quot;modules&quot; that are built a=\r\n priori and just dumped\n&gt; &gt; into the network en masse.  Those are indeed a =\r\nbit cringe-inducing.\n&gt; &gt; However, my problem with them is not that they are=\r\n cringe-inducing.\n&gt; &gt; Rather, again, I doubt they will really be a big help=\r\n in the long run.\n&gt; &gt;  The reason I doubt their utility is because I believ=\r\ne that a massive\n&gt; &gt; brain needs to be also massively interwoven, such that=\r\n each internal\n&gt; &gt; area of each part is entirely accessible and &quot;speaks the=\r\n language&quot; of\n&gt; &gt; any other part.  Some ad hoc module thrown in the mix, wh=\r\nile perhaps\n&gt; &gt; helpful in the short run, will never be able to fill that r=\r\nole because\n&gt; &gt; it was not built along with the rest of the infrastructure.=\r\n  So that&#39;s\n&gt; &gt; why I&#39;m against it: Not because it&#39;s cheating, but because =\r\nit\nwon&#39;t work.\n&gt; &gt; \n&gt; &gt; So I think you have to distinguish between that typ=\r\ne of hack and the\n&gt; &gt; kind of thing where we provide sort of &quot;genetically-e=\r\nngineered&quot;\n&gt; &gt; information, i.e. at the genetic level.  That I do believe w=\r\nill be\n&gt; &gt; useful, and should be exploited, because those are knobs and\n&gt; &gt;=\r\n coordinate frames upon which a castle can be built.  So providing\n&gt; &gt; coor=\r\ndinate systems that are useful seems to me likely *long-run*\n&gt; &gt; useful.  I=\r\nt is not the same as telling it how to connect up, and the\n&gt; &gt; substrate th=\r\nat pops out in the end is going to be as pure as any:\n&gt; &gt; completely ANN th=\r\nrough and through and totally a product of the\n&gt; &gt; indirect encoding.\n&gt; &gt; \n=\r\n&gt; &gt; Finally, I think you are inferring too much about how much prior\n&gt; &gt; in=\r\nformation I am advocating based on just Multiagent HyperNEAT.  There\n&gt; &gt; is=\r\n no animal on earth that has to develop five disconnected brains on\n&gt; &gt; a s=\r\ningle sheet with five compartments.  Expecting evolution to just\n&gt; &gt; figure=\r\n out where one brain begins and the other ends seems to me very\n&gt; &gt; unnatur=\r\nal and bizarre, and also uninteresting.  Statistically\n&gt; &gt; speaking, it is =\r\nevident that *any* intelligence would take longer to\n&gt; &gt; figure that out an=\r\nd solve the problem on average than one that was\n&gt; &gt; provided such informat=\r\nion a priori.  So there&#39;s no surprise in that.\n&gt; &gt; \n&gt; &gt; So of course HyperN=\r\nEAT performs worse without knowing the divisions\n&gt; &gt; between separate brain=\r\ns on a substrate than when it knows them up\n&gt; &gt; front.  That doesn&#39;t imply =\r\n that HyperNEAT cannot figure it out on its\n&gt; &gt; own, or that I think it doe=\r\nsn&#39;t matter if HyperNEAT can find\n&gt; &gt; regularities on its own.  It&#39;s just, =\r\nit would take a while longer and\n&gt; &gt; would be less reliable, so why bother =\r\nwaiting?  The spatial divisions\n&gt; &gt; among the brains is ad hoc (something w=\r\ne simply decided a priori by\n&gt; &gt; fiat) and thus is not the interesting issu=\r\ne.\n&gt; &gt; \n&gt; &gt; I don&#39;t think it will be the same in a lot of non-multiagent ta=\r\nsks\n&gt; &gt; because this unnatural issue of multiple brains and their positions=\r\n\n&gt; &gt; does not come up, and I do believe that HyperNEAT often does discover\n=\r\n&gt; &gt; regularities on its own, and that&#39;s a good thing.\n&gt; &gt; \n&gt; &gt; Anyway, the =\r\nbroader point is that I will stick to my strong position:\n&gt; &gt; I do not beli=\r\neve that finding  middle ground or a sweet spot in terms\n&gt; &gt; of biases and =\r\nconstraints is the important issue in the long view of\n&gt; &gt; achieving genera=\r\nl AI through evolution *unless* you are only doing it\n&gt; &gt; to prove how cool=\r\n evolution is.  In contrast, I&#39;m using evolution\n&gt; &gt; because I think it is =\r\nthe best hope. The funny thing is that we will\n&gt; &gt; regardless end up agreei=\r\nng on a lot, because I too don&#39;t like to\n&gt; &gt; provide big building blocks.  =\r\nBut my reason is that they will end up\n&gt; &gt; being incapable of building a ge=\r\nneral AI.  So I think a lot of things\n&gt; &gt; that look bad also won&#39;t work, so=\r\n someone who is trying to make\n&gt; &gt; evolution look good will often see me as=\r\n sharing their assumptions.\n&gt; &gt; \n&gt; &gt; ken\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; --- In neat@yahoogro=\r\nups.com, Jeff Clune &lt;jclune@&gt; wrote:\n&gt; &gt;&gt; \n&gt; &gt;&gt; Ken-\n&gt; &gt;&gt; \n&gt; &gt;&gt; Thank you f=\r\nor explaining these arguments at length. I always\nlearn a lot\n&gt; &gt;&gt; when we =\r\nhave these sorts of discussions, and there is no exception\n&gt; &gt; in this\n&gt; &gt;&gt;=\r\n case. I agree with most of what you write below, especially that\n&gt; &gt;&gt; cons=\r\ntraining/biasing evolution is very important. I guess the only\n&gt; &gt; place we=\r\n\n&gt; &gt;&gt; disagree is that I believe there is a middle ground of constraint\n&gt; &gt;=\r\n that we\n&gt; &gt;&gt; should shoot for, whereas you seem to feel &#39;the more the bett=\r\ner&#39;.\n&gt; &gt;&gt; \n&gt; &gt;&gt; You write:\n&gt; &gt;&gt; \n&gt; &gt;&gt;&gt;  Therefore, progress is NE should in=\r\n part be measured with\nrespect to\n&gt; &gt;&gt;&gt; progress in constraining the proble=\r\nm to make such a discovery more\n&gt; &gt;&gt;&gt; likely.  When an NE algorithm is impr=\r\noved to allow us to tell it\nmore\n&gt; &gt;&gt;&gt; about the world in which its output =\r\nwill be situated, that is good\n&gt; &gt;&gt;&gt; news for the long view.  In short, we =\r\ndon&#39;t care at all how NE\n&gt; &gt;&gt;&gt; produced a brain as long as it really does.\n=\r\n&gt; &gt;&gt; \n&gt; &gt;&gt; This reminds me of something that Hod Lipson says repeatedly.\nWh=\r\nenever\n&gt; &gt;&gt; someone evolves something impressive the first question to ask =\r\nis,\n&gt; &gt; &quot;How big\n&gt; &gt;&gt; are your building blocks?&quot; I am going to provide a st=\r\nraw man of your\n&gt; &gt;&gt; argument. Hopefully the fact that I admit that up fron=\r\nt will make it\n&gt; &gt; less\n&gt; &gt;&gt; objectionable. Imagine that Kasparov and a neu=\r\nral net engineer\n&gt; &gt; teamed up and\n&gt; &gt;&gt; hand-created a neural  net (call it=\r\n &#39;DeepNet&#39;)  that played chess\nat a\n&gt; &gt;&gt; grandmaster level. Now imagine tha=\r\nt we create an NE algorithm for\n&gt; &gt; learning\n&gt; &gt;&gt; chess playing that was ot=\r\nherwise identical to NEAT, but had one extra\n&gt; &gt;&gt; mutation operator, which =\r\nwas &#39;clear out the current phenotype and\n&gt; &gt; replace it\n&gt; &gt;&gt; with DeepNet&#39;.=\r\n In this case we would have highly constrained the\n&gt; &gt; problem to\n&gt; &gt;&gt; find=\r\n a good chess playing solution. We would have also successfully\n&gt; &gt; injecte=\r\nd\n&gt; &gt;&gt; our a priori knowledge of the problem. However, it would be very\n&gt; &gt;=\r\n&gt; unimpressive as an accomplishment in the field of evolutionary\n&gt; &gt; comput=\r\nation.\n&gt; &gt;&gt; The credit goes to the humans that designed DeepNet, not for th=\r\ne\n&gt; &gt;&gt; evolutionary algorithm that recreated it.\n&gt; &gt;&gt; \n&gt; &gt;&gt; As I said, this=\r\n is an unfair caricature of your view. However, I\n&gt; &gt; think it\n&gt; &gt;&gt; might r=\r\neveal what I have been trying to say. In my mind, the goal\nis to\n&gt; &gt;&gt; provi=\r\nde smaller and smaller building blocks because then we know\nit is\n&gt; &gt;&gt; evol=\r\nution that is doing the work, and not us. There is a sweet spot\n&gt; &gt; in the\n=\r\n&gt; &gt;&gt; middle. If we humans don&#39;t do any work in biasing the search, then\n&gt; &gt;=\r\n evolution\n&gt; &gt;&gt; will perform terribly. But if we provide building blocks th=\r\nat are\n&gt; &gt; too large,\n&gt; &gt;&gt; then evolution did not really do the heavy lifti=\r\nng. So, as opposed\n&gt; &gt; to saying\n&gt; &gt;&gt; &#39;the more constraint the better,&#39; I t=\r\nhink it is interesting to try to\n&gt; &gt;&gt; provide smaller building blocks while=\r\n still gaining high levels of\n&gt; &gt;&gt; performance. As I have said before, I al=\r\nso think that if we make\n&gt; &gt; progress on\n&gt; &gt;&gt; this front, the evolutionary =\r\nalgorithm (not its product) will be\n&gt; &gt; more likely\n&gt; &gt;&gt; to generalize to s=\r\nolving other problems. The long term goal, of\n&gt; &gt; course, is\n&gt; &gt;&gt; to have o=\r\nur algorithms solve problems and create things where we\neither\n&gt; &gt;&gt; don&#39;t k=\r\nnow how to solve the problems, or can&#39;t be bothered to do\nso. For\n&gt; &gt;&gt; exam=\r\nple, the NE that produced DeepNet would not do very well at\nrace car\n&gt; &gt;&gt; d=\r\nriving. But an algorithm that was constrained in a more abstract\nway to\n&gt; &gt;=\r\n&gt; exploit regularities in its environment might do better on both car\n&gt; &gt; r=\r\nacing\n&gt; &gt;&gt; and chess. \n&gt; &gt;&gt; \n&gt; &gt;&gt; I guess I start from the recognition that=\r\n evolution produced humans\n&gt; &gt; without\n&gt; &gt;&gt; any bias from a conscious entit=\r\ny. How it did that is one of the most\n&gt; &gt;&gt; fascinating and open questions b=\r\noth in our field and in biology. We\n&gt; &gt; agree\n&gt; &gt;&gt; that trying to emulate w=\r\nays in which natural evolution did things\n&gt; &gt; like bias\n&gt; &gt;&gt; itself, and th=\r\nus allow the evolution of modularity, is the way\n&gt; &gt; forward for\n&gt; &gt;&gt; our f=\r\nield. HyperNEAT represents such amazing progress because it\n&gt; &gt; employed\n&gt; =\r\n&gt;&gt; this strategy. But it strikes me that nature was not told a priori\n&gt; &gt; h=\r\now many\n&gt; &gt;&gt; leg modules it should make or learn to control. Nor was it tol=\r\nd\nhow many\n&gt; &gt;&gt; neural modules it should create in the brain. It figured th=\r\nat stuff\n&gt; &gt; out on\n&gt; &gt;&gt; its own, and probably performed better as a result=\r\n because it could\n&gt; &gt; learn to\n&gt; &gt;&gt; tailor the number of modules it needed =\r\nto the regularity of the\n&gt; &gt; problems it\n&gt; &gt;&gt; faced. I guess I don&#39;t think =\r\nwe will make it very far towards\nevolving\n&gt; &gt;&gt; brains that are generally in=\r\ntelligent if our evolutionary algorithms\n&gt; &gt; cannot\n&gt; &gt;&gt; do likewise. It se=\r\nems that something is majorly lacking if we have\n&gt; &gt; to tell\n&gt; &gt;&gt; it each t=\r\nime what the regularities are in the environment, and\nhow to go\n&gt; &gt;&gt; about =\r\nexploiting them.\n&gt; &gt;&gt; \n&gt; &gt;&gt; Apologies for the straw man argument. I do thin=\r\nk there is a lot of\n&gt; &gt; merit to\n&gt; &gt;&gt; the general thrust of what you say. I=\r\n may be overreacting in\n&gt; &gt; focusing on the\n&gt; &gt;&gt; extremes\n&gt; &gt;&gt; \n&gt; &gt;&gt; \n&gt; &gt;&gt; =\r\n\n&gt; &gt;&gt; Cheers,\n&gt; &gt;&gt; Jeff Clune\n&gt; &gt;&gt; \n&gt; &gt;&gt; Digital Evolution Lab, Michigan St=\r\nate University\n&gt; &gt;&gt; \n&gt; &gt;&gt; jclune@\n&gt; &gt;&gt; \n&gt; &gt;&gt; \n&gt; &gt;&gt; \n&gt; &gt;&gt; \n&gt; &gt;&gt;&gt; From: Kenne=\r\nth Stanley &lt;kstanley@&gt;\n&gt; &gt;&gt;&gt; Reply-To: &quot;neat@yahoogroups.com&quot; &lt;neat@yahoogr=\r\noups.com&gt;\n&gt; &gt;&gt;&gt; Date: Sun, 27 Apr 2008 21:36:33 -0000\n&gt; &gt;&gt;&gt; To: &quot;neat@yahoo=\r\ngroups.com&quot; &lt;neat@yahoogroups.com&gt;\n&gt; &gt;&gt;&gt; Subject: [neat] Re: Machine Learni=\r\nng and the Long View of AI\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt; --- In neat@yahoogroups.com, &quot;Derek=\r\n James&quot; &lt;djames@&gt; wrote:\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt;&gt; \n&gt; &gt;&gt;&gt;&gt;&gt;  In RL, in contrast, the lo=\r\nng view is almost the opposite: They\n&gt; &gt;&gt;&gt; want to\n&gt; &gt;&gt;&gt;&gt;&gt;  remove all cons=\r\ntraints and still learn nevertheless.\n&gt; &gt;&gt;&gt;&gt; \n&gt; &gt;&gt;&gt;&gt; I&#39;m not sure what you =\r\nmean by this, Ken. Could you elaborate a\n&gt; &gt; little?\n&gt; &gt;&gt;&gt;&gt; \n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt; S=\r\nure.  I think the problem is that I can&#39;t find a way to explain my\n&gt; &gt;&gt;&gt; po=\r\nint concisely.  As I try to explain it, it starts taking up\ntoo much\n&gt; &gt;&gt;&gt; =\r\ntext so I shorten it and then it loses its meaning.  Let me give\nit a\n&gt; &gt;&gt;&gt;=\r\n try again...\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt; I think the difference between the goals of RL a=\r\nnd NE is an\n&gt; &gt;&gt;&gt; interesting topic because they are almost always conflate=\r\nd, as\nif they\n&gt; &gt;&gt;&gt; are trying to solve the same problem.\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt; The =\r\nRL community (e.g. value-function approaches) is trying to build\n&gt; &gt;&gt;&gt; some=\r\nthing that learns like a natural brain.  They are saying,\nthrough\n&gt; &gt;&gt;&gt; ana=\r\nlytic means we can deduce how a brain can learn from sparse\n&gt; &gt;&gt;&gt; reinforce=\r\nment and formalize that process in an algorithm.  The\nhope, I\n&gt; &gt;&gt;&gt; would t=\r\nhink, is to eventually build the &quot;general intelligence&quot; that\n&gt; &gt;&gt;&gt; aligns w=\r\nith the holy grail of AI.  So each step along the way is an\n&gt; &gt;&gt;&gt; improveme=\r\nnt in that general ability.\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt; So if that is your goal, then the =\r\nbenchmarks you choose have to be\n&gt; &gt;&gt;&gt; designed to measure progress to that=\r\n goal.  So what they need to\ndo is\n&gt; &gt;&gt;&gt; show that their designed intellige=\r\nnce can work largely independently\n&gt; &gt;&gt;&gt; of a priori &quot;cheats&quot; that provide =\r\nthe meat of the solution. \nBecause,\n&gt; &gt;&gt;&gt; after all, how can it be a genera=\r\nl intelligence if it needs you to\n&gt; &gt;&gt;&gt; tell it something that it is suppos=\r\ned to be able to figure out?\n This\n&gt; &gt;&gt;&gt; perspective, I believe, is aligned=\r\n with Jeff&#39;s view.\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt; However, NE as a long-term pursuit is invol=\r\nved in something\ndifferent,\n&gt; &gt;&gt;&gt; even though it can be applied to the same=\r\n problems.  NE is not an\n&gt; &gt;&gt;&gt; attempt to formalize how people learn with s=\r\nparse reinforcement.\n&gt; &gt;&gt;&gt; Rather, it is an attempt to formalize how evolut=\r\nion can build a\nbrain.\n&gt; &gt;&gt;&gt;  So RL is formalizing the brain itself and NE =\r\nis formalizing how\n&gt; &gt;&gt;&gt; evolution succeeds in creating a brain.  NE is the=\r\nrefore one step\n&gt; &gt; removed.\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt; This difference is ultimately a p=\r\nhilosophical difference on the best\n&gt; &gt;&gt;&gt; approach to creating a full-blown=\r\n AI.  The instrumental issue is\n&gt; &gt;&gt;&gt; whether you think it&#39;s easier to buil=\r\nd it yourself or to design an\n&gt; &gt;&gt;&gt; algorithm that can build it.  The confu=\r\nsion and hence conflation of\n&gt; &gt;&gt;&gt; the two approaches arises in part becaus=\r\ne they do indeed both aim at\n&gt; &gt;&gt;&gt; the same long view goal: a general AI.  =\r\nBut they are coming at\nit from\n&gt; &gt;&gt;&gt; very different angles.\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt; An=\r\nd because of this stark difference, the *metric* of progress\nshould\n&gt; &gt;&gt;&gt; b=\r\ne quite different.  We cannot measure our progress in building a\n&gt; &gt;&gt;&gt; gene=\r\nral intelligence directly in the same way that we measure our\n&gt; &gt;&gt;&gt; progres=\r\ns in creating an evolutionary algorithm that itself will\n&gt; &gt;&gt;&gt; someday outp=\r\nut one.\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt; This distinction is potentially subtle and confusing s=\r\no let me\ntry to\n&gt; &gt;&gt;&gt; make it clearer:  Human brains aren&#39;t designed to bui=\r\nld yet more\nhuman\n&gt; &gt;&gt;&gt; brains.  We are good at a lot of things, and we lea=\r\nrn generally, but\n&gt; &gt;&gt;&gt; we do not build 100-trillion part devices that are =\r\nmore complex than\n&gt; &gt;&gt;&gt; any known object in the universe.  I&#39;m not saying w=\r\ne won&#39;t ever be\n&gt; &gt;&gt;&gt; able to do it, but if you want to simulate a human br=\r\nain, your first\n&gt; &gt;&gt;&gt; thought would not be that it needs to be capable of d=\r\nesigning yet\n&gt; &gt;&gt;&gt; another brain by itself.  Your first thought is about th=\r\nings like\n&gt; &gt;&gt;&gt; object recognition or pursuit and evasion.\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt; In =\r\ncontrast, building brains is exactly what natural evolution did,\n&gt; &gt;&gt;&gt; and =\r\nit did it quite well.  Natural evolution does not perform object\n&gt; &gt;&gt;&gt; reco=\r\ngnition; it does not communicate with language; it does not run\n&gt; &gt;&gt;&gt; away =\r\nfrom predators or hunt for prey.  Yet it does build brains that\n&gt; &gt;&gt;&gt; thems=\r\nelves do those things.  And that is the aspect of it we wish to\n&gt; &gt;&gt;&gt; harne=\r\nss- a very specific niche kind of skill (though radically\n&gt; &gt;&gt;&gt; impressive)=\r\n- not a general skill.\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt; So the two pursuits are really quite di=\r\nfferent.  And therefore they\n&gt; &gt;&gt;&gt; deserve different metrics to judge their=\r\n progress with respect\nto the\n&gt; &gt;&gt;&gt; long term goal.  That is, unless we con=\r\nflate them to be the same\n&gt; &gt;&gt;&gt; thing, which we often do without thinking a=\r\nbout it.\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt; For example, we could just say, well, both NE and RL =\r\nare learning\n&gt; &gt;&gt;&gt; techniques, and after all, we can apply them to the same=\r\n\nproblems, so\n&gt; &gt;&gt;&gt; why make a big distinction in how we judge them?  Let&#39;s=\r\n just compare\n&gt; &gt;&gt;&gt; them directly on the same benchmarks and get on with it=\r\n.\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt; That&#39;s fine for the short-term view, i.e. let&#39;s just improve=\r\n our\n&gt; &gt;&gt;&gt; ability to tackle practical problems, but for the long view, the=\r\ny\n&gt; &gt;&gt;&gt; cannot be judged in the same way.  If I improve at my ability to\n&gt; =\r\n&gt;&gt;&gt; balance on one foot is that a sign that I will be able to build a\n&gt; &gt;&gt;&gt;=\r\n brain someday?  If evolution evolves a brain that plays checkers, is\n&gt; &gt;&gt;&gt;=\r\n that a sign that evolution *itself* is on the road to performing\n&gt; &gt;&gt;&gt; obj=\r\nect recognition?  These are totally different pursuits.\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt; So in =\r\nthat context, how should they be judged with respect to long\n&gt; &gt;&gt;&gt; term goa=\r\nls?  Well, I think RL deserves to be judged based on its\n&gt; &gt;&gt;&gt; increasing a=\r\nbility to learn more generally.  And in that sense,\n&gt; &gt;&gt;&gt; exactly Jeff&#39;s cr=\r\niteria should apply to it: We should be\ninterested in\n&gt; &gt;&gt;&gt; whether it &quot;nee=\r\nds&quot; a priori information to learn.  In other\nwords, the\n&gt; &gt;&gt;&gt; less we need =\r\nto constrain the problem for the learner, the more\n&gt; &gt;&gt;&gt; impressed we deser=\r\nve to be.  That shows progress towards more\nand more\n&gt; &gt;&gt;&gt; general AI and M=\r\nL.\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt; But if evolution is not *itself* supposed to be a general l=\r\nearner\n&gt; &gt;&gt;&gt; (rather, we just want it to concentrate on one very specific s=\r\nkill:\n&gt; &gt;&gt;&gt; brain building), then those considerations are orthogonal to it=\r\ns\n&gt; &gt;&gt;&gt; greatest promise.  Its promise is to evolve a brain itself, and as\n=\r\n&gt; &gt;&gt;&gt; such, neuroevolutionary algorithms deserve to be judged on our\nabilit=\r\ny\n&gt; &gt;&gt;&gt; to *constrain* the problem so that they can accomplish exactly that=\r\n.\n&gt; &gt;&gt;&gt; In other words, the problem NE *algorithms* face is leaps and bound=\r\ns\n&gt; &gt;&gt;&gt; beyond what RL algorithms face.  RL algorithms just need to be\nable=\r\n to\n&gt; &gt;&gt;&gt; do as well as brains; NE has to be able to discover brains\nthemse=\r\nlves.\n&gt; &gt;&gt;&gt;  Therefore, progress is NE should in part be measured with\nresp=\r\nect to\n&gt; &gt;&gt;&gt; progress in constraining the problem to make such a discovery =\r\nmore\n&gt; &gt;&gt;&gt; likely.  When an NE algorithm is improved to allow us to tell it=\r\n\nmore\n&gt; &gt;&gt;&gt; about the world in which its output will be situated, that is g=\r\nood\n&gt; &gt;&gt;&gt; news for the long view.  In short, we don&#39;t care at all how NE\n&gt; =\r\n&gt;&gt;&gt; produced a brain as long as it really does.  Will anyone\ncomplain if a\n=\r\n&gt; &gt;&gt;&gt; human brain pops out of a system that was a priori given the concept\n=\r\n&gt; &gt;&gt;&gt; of symmetry?  Rather, we should be glad that such a priori\ncontext wa=\r\ns\n&gt; &gt;&gt;&gt; possible to provide in the first place, because it may have\nsaved u=\r\ns a\n&gt; &gt;&gt;&gt; year of wasted computation in figuring it out needlessly.\n&gt; &gt;&gt;&gt; \n=\r\n&gt; &gt;&gt;&gt; This distinction is almost completely ignored when NE and RL are\n&gt; &gt;&gt;=\r\n&gt; compared directly.  Therefore, the implications of any such\ncomparison\n&gt; =\r\n&gt;&gt;&gt; are fuzzy and lacking context with respect to the long view.  I\nam not\n=\r\n&gt; &gt;&gt;&gt; sure if I should care or not if RL solves something better than\nNE, o=\r\nr\n&gt; &gt;&gt;&gt; vice versa, because the author doesn&#39;t explain how the result align=\r\ns\n&gt; &gt;&gt;&gt; with the long-term goals of the fields.  Long term goals seem like\n=\r\n&gt; &gt;&gt;&gt; unwelcome guests these days in AI, which is why I probably won&#39;t be\n&gt;=\r\n &gt;&gt;&gt; writing about any of this in a publication any time soon.\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt;=\r\n ...\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt; So Derek what you are saying about NE being good at &quot;hard=\r\n-wired&quot;\n&gt; &gt;&gt;&gt; solutions and RL being appropriate for ontogenetic lifetime\nl=\r\nearning,\n&gt; &gt;&gt;&gt; while true, is not what I think of as the primary long-view =\r\nissue.\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt; In the long view, NE will be used to evolve structures =\r\nthat do learn\n&gt; &gt;&gt;&gt; over their lifetime, i.e. not hardwired at all.  The on=\r\nly reason\nthat\n&gt; &gt;&gt;&gt; it tends to be used to evolve hardwired solutions toda=\r\ny is\nbecause we\n&gt; &gt;&gt;&gt; are trying to get a foothold on how to evolve certain=\r\n types of\ncomplex\n&gt; &gt;&gt;&gt; structures.   Once we get very good at it, focus wi=\r\nll naturally\nshift\n&gt; &gt;&gt;&gt; to evolving dynamic brains (and of course there is=\r\n already work\nalong\n&gt; &gt;&gt;&gt; these lines today, much from Floreano).  I do not=\r\n even think that we\n&gt; &gt;&gt;&gt; will need to include stock learning algorithms li=\r\nke Hebbian\nlearning.\n&gt; &gt;&gt;&gt;  When we achieve our long-term goals, those *the=\r\nmselves* will be\nleft\n&gt; &gt;&gt;&gt; up to evolution because after all there may be =\r\nsomething even\nbetter.\n&gt; &gt;&gt;&gt;  \n&gt; &gt;&gt;&gt;&gt;&gt; My aim is to design an\n&gt; &gt;&gt;&gt;&gt;&gt;  algo=\r\nrithm that will output a brain, not to design the brain\nitself.\n&gt; &gt;&gt;&gt;&gt; \n&gt; &gt;=\r\n&gt;&gt;&gt; But what kind of brain are you wanting to output?\n&gt; &gt;&gt;&gt;&gt; \n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt; =\r\nNote that I&#39;m speaking purely about the long view for these\ndifferent\n&gt; &gt;&gt;&gt;=\r\n fields here.  Of course on a day-to-day basis I am not solely\nfocused\n&gt; &gt;&gt;=\r\n&gt; on what will happen 100 years from now.  On a practical day-to-day\n&gt; &gt;&gt;&gt; =\r\nbasis, of course I want to make NE better capable to tackle problems\n&gt; &gt;&gt;&gt; =\r\nthat e.g. RL tackles.  So in the short-term context, I just want to\n&gt; &gt;&gt;&gt; o=\r\nutput something that works for the problem at hand.\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt; But in the=\r\n long view, which we were talking about, I think the\n&gt; &gt;&gt;&gt; ultimate goal wo=\r\nuld be to output a full-fledged adaptive system with\n&gt; &gt;&gt;&gt; astronomical com=\r\nplexity and the power and subtlety of human\nreasoning.\n&gt; &gt;&gt;&gt;  On that path,=\r\n constraint is the only hope, unless you want to wait\n&gt; &gt;&gt;&gt; three billion y=\r\nears and just hope in the meantime that the initial\n&gt; &gt;&gt;&gt; conditions were s=\r\net up correctly.  Therefore, demonstrations of the\n&gt; &gt;&gt;&gt; power of constrain=\r\nt deserve to be judged as evidence of the\npromise of\n&gt; &gt;&gt;&gt; and progress tow=\r\nards the long term goal in NE.\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt; ken\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt; \n&gt; &gt; \n&gt; =\r\n&gt; \n&gt; \n&gt; ------ End of Forwarded Message\n&gt;\n\n\n\n"}}