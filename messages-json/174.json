{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":91855544,"authorName":"Umer Iqbal","from":"&quot;Umer Iqbal&quot; &lt;umeriqbal81@...&gt;","profile":"umeriqbal81","replyTo":"LIST","senderId":"SS9y_iZzkpH2Lc2gLJz5_D9_bBFMrguUyjcUqukx4x6mFnQCF7TN-Bw7bcb-F_6dWg3FdJtl0CoyWWWnE0d5kFv-4iqtv6L3GJcxM8A","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] Re: Learning How to Learn","postDate":"1067506678","msgId":174,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PEJBWTctREFWMTM2T0pTTkVFeXowMDAxMDc5ZEBob3RtYWlsLmNvbT4=","referencesHeader":"PGJucGdyaysyZm5lQGVHcm91cHMuY29tPg=="},"prevInTopic":172,"nextInTopic":175,"prevInTime":173,"nextInTime":175,"topicId":170,"numMessagesInTopic":15,"msgSnippet":"Regarding the fact that the net that evolved had 44 neurons: There must be more than on solution to the given problem. And as I myself have come across some","rawEmail":"Return-Path: &lt;umeriqbal81@...&gt;\r\nX-Sender: umeriqbal81@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 44069 invoked from network); 30 Oct 2003 09:38:15 -0000\r\nReceived: from unknown (66.218.66.167)\n  by m16.grp.scd.yahoo.com with QMQP; 30 Oct 2003 09:38:15 -0000\r\nReceived: from unknown (HELO hotmail.com) (64.4.10.117)\n  by mta6.grp.scd.yahoo.com with SMTP; 30 Oct 2003 09:38:15 -0000\r\nReceived: from mail pickup service by hotmail.com with Microsoft SMTPSVC;\n\t Thu, 30 Oct 2003 01:38:15 -0800\r\nReceived: from 202.125.140.98 by bay7-dav13.bay7.hotmail.com with DAV;\n\tThu, 30 Oct 2003 09:38:15 +0000\r\nX-Originating-Email: [umeriqbal81@...]\r\nTo: &lt;neat@yahoogroups.com&gt;\r\nReferences: &lt;bnpgrk+2fne@...&gt;\r\nSubject: Re: [neat] Re: Learning How to Learn\r\nDate: Thu, 30 Oct 2003 14:37:58 +0500\r\nMIME-Version: 1.0\r\nContent-Type: multipart/alternative;\n\tboundary=&quot;----=_NextPart_000_0025_01C39EF3.69BDE500&quot;\r\nX-Priority: 3\r\nX-MSMail-Priority: Normal\r\nX-Mailer: Microsoft Outlook Express 5.50.4807.1700\r\nX-MimeOLE: Produced By Microsoft MimeOLE V5.50.4910.0300\r\nMessage-ID: &lt;BAY7-DAV136OJSNEEyz0001079d@...&gt;\r\nX-OriginalArrivalTime: 30 Oct 2003 09:38:15.0650 (UTC) FILETIME=[8B23CC20:01C39EC9]\r\nFrom: &quot;Umer Iqbal&quot; &lt;umeriqbal81@...&gt;\r\nX-Yahoo-Group-Post: member; u=91855544\r\nX-Yahoo-Profile: umeriqbal81\r\n\r\n\r\n------=_NextPart_000_0025_01C39EF3.69BDE500\r\nContent-Type: text/plain;\n\tcharset=&quot;iso-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\n\nRegarding the fact that the net that evolved had 44 neurons:\n\nThere must b=\r\ne more than on solution to the given problem. And as I myself have come acr=\r\noss some solutions (to another problem posted earlier) that are equally as =\r\ngood but vary in network complexity (number of neurons and connections vari=\r\ned in order of tens), I must say that 44 neurons might be just one possible=\r\n solution. And that NEAT might have missed a simpler solution earlier in th=\r\ne evolution process due to some reason. \n\nNow, If my above statements are c=\r\norrect, then what could be the possible reasons why NEAT missed the simpler=\r\n solution/s? Was it because that we complexified at a rate higher than we s=\r\nhould have? or was it just because of the nature of evolutionary process it=\r\nself? or was it due to some other reason? \nand what can we do about it?\n\nRe=\r\ngards,\nUmer..\n\n\n  ----- Original Message ----- \n  From: Kenneth Stanley \n  =\r\nTo: neat@yahoogroups.com \n  Sent: Thursday, October 30, 2003 4:01 AM\n  Subj=\r\nect: [neat] Re: Learning How to Learn\n\n\n  --- In neat@yahoogroups.com, Mitc=\r\nhell Timin &lt;zenguyuno@y...&gt; wrote:\n  &gt; A 44-neuron ANN has evolved which is=\r\n a perfect player\n  &gt; of the 4-card puzzle.\n  &gt; \n\n  ...\n\n  &gt; A friend of mi=\r\nne, Prof. Roderick Edwards here at U.\n  &gt; Vic., pointed out that the evolut=\r\nion software &quot;learned\n  &gt; how to learn&quot;.  This seems apt to me.  The progra=\r\nm had\n  &gt; to kill off a few millions ANNs that wouldn&#39;t quite\n  &gt; play perf=\r\nectly until finally producing some that\n  &gt; could.  That was the machine le=\r\narning phase, and the\n  &gt; results are fixed in the weights of the ANN.  Now=\r\n the\n  &gt; ANN can learn what it need to when it plays this game.\n  &gt; \n  &gt; I&#39;=\r\nm interested in finding out if this has been done\n  &gt; before; i.e., has som=\r\neone trained a recurrent ANN so\n  &gt; that it makes use of its memory while d=\r\noing its job. \n  &gt; I suspect some of your robots with NEAT ANN\n  &gt; controll=\r\ners qualify under this definition.\n  &gt; \n\n\n  Those are very nice results, sh=\r\nowing that a recurrent neural network\n  can indeed be evolved to be able to=\r\n memorize the necessary\n  information.  I still wonder why it needed 44 nod=\r\nes, but perhaps that\n  does make sense given all the possibilities.\n\n  Anyw=\r\nay, yes I&#39;ve been interested in &quot;learning how to learn&quot; for a long\n  time. =\r\n (Or, in other words, evolving networks that can learn.) \n  Interestingly, =\r\na lot of people think (and it makes sense) that\n  learning requires synapti=\r\nc plasticity, i.e. the ability to change\n  weights dynamically during the n=\r\network&#39;s lifetime.  Usually such\n  plasticity would be Hebbian learning or =\r\nsomething like it.  Your\n  experiment, and others, show that this is not ne=\r\ncessarily true;  in\n  fact, recurrent activation alone can store state info=\r\nrmation.\n\n  We actually do have a paper in which the two kinds of adaptatio=\r\nn were\n  tested in a very simple &quot;learning&quot; domain.  The domain involved\n  =\r\nforaging food.  A forager must collect items on a board that are\n  either f=\r\nood or poison, but it has to learn which is the case.  If they\n  are food, =\r\nit can keep eating, but if they are poisonous, it should\n  stop.  So it has=\r\n to make an adjustment in policy according to what it\n  discovers.  We foun=\r\nd that both recurrent networks and networks with\n  synaptic plasticity coul=\r\nd handle this task, although they tend to do\n  it differently.  Here is the=\r\n paper (yahoo may mangle the link, so try\n  copying it):\n\n  http://nn.cs.ut=\r\nexas.edu/pub-view.php?RECORD_KEY(Pu\n  bs)=3DPubID&PubID(Pubs)=3D131\n\n  (On =\r\na side note, for anyone wondering, yes this does mean there is\n  a version =\r\nof NEAT that can evolve Hebbian networks with plastic\n  synapses, but I hav=\r\nen&#39;t released it yet.) \n\n  Anyway, these are very interesting issues.  When=\r\n do you need Hebbian\n  connections to learn as opposed to only fixed-weight=\r\n recurrent\n  connections?  It isn&#39;t clear.  And what is the capacity of a\n =\r\n fixed-weight recurrent network to remember information?\n\n  ken\n\n\n        Y=\r\nahoo! Groups Sponsor \n              ADVERTISEMENT\n             \n       \n   =\r\n    \n\n  To unsubscribe from this group, send an email to:\n  neat-unsubscrib=\r\ne@yahoogroups.com\n\n\n\n  Your use of Yahoo! Groups is subject to the Yahoo! T=\r\nerms of Service. \n\n\r\n------=_NextPart_000_0025_01C39EF3.69BDE500\r\nContent-Type: text/html;\n\tcharset=&quot;iso-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\n&lt;!DOCTYPE HTML PUBLIC &quot;-//W3C//DTD HTML 4.0 Transitional//EN&quot;&gt;\n&lt;HTML&gt;&lt;HEAD&gt;=\r\n\n&lt;META content=3D&quot;text/html; charset=3Diso-8859-1&quot; http-equiv=3DContent-Typ=\r\ne&gt;\n&lt;META content=3D&quot;MSHTML 5.00.3502.5390&quot; name=3DGENERATOR&gt;\n&lt;STYLE&gt;&lt;/STYLE=\r\n&gt;\n&lt;/HEAD&gt;\n&lt;BODY bgColor=3D#ffffff&gt;\n&lt;DIV&gt;&lt;FONT face=3DArial size=3D2&gt;&lt;/FONT&gt;=\r\n&nbsp;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;FONT face=3DArial size=3D2&gt;Regarding the fact that the n=\r\net that evolved had 44 \nneurons:&lt;/FONT&gt;&lt;/DIV&gt;\n&lt;DIV&gt;&nbsp;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;FONT =\r\nface=3DArial size=3D2&gt;There must be more than on solution to the given \npro=\r\nblem. And as I myself have come across some solutions (to another problem \n=\r\nposted earlier) that are equally as good but vary in network complexity (nu=\r\nmber \nof neurons and connections varied in order of tens), I must say that =\r\n44 neurons \nmight be just one possible solution. And that NEAT might have m=\r\nissed a simpler \nsolution earlier in the evolution process due to some reas=\r\non. &lt;/FONT&gt;&lt;/DIV&gt;\n&lt;DIV&gt;&nbsp;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;FONT face=3DArial size=3D2&gt;Now, I=\r\nf my above statements are correct, then what \ncould be the possible reasons=\r\n why NEAT missed the simpler solution/s? \n&lt;/FONT&gt;&lt;FONT face=3DArial size=3D=\r\n2&gt;Was it because that we complexified at a rate \nhigher than we should have=\r\n? or was it just because of the nature of evolutionary \nprocess itself? or =\r\nwas it due to some other reason? &lt;/FONT&gt;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;FONT face=3DArial size=\r\n=3D2&gt;and what can we do about it?&lt;/FONT&gt;&lt;/DIV&gt;\n&lt;DIV&gt;&nbsp;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;FONT=\r\n face=3DArial size=3D2&gt;Regards,&lt;/FONT&gt;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;FONT face=3DArial size=\r\n=3D2&gt;Umer..&lt;/FONT&gt;&lt;/DIV&gt;\n&lt;DIV&gt;&nbsp;&lt;/DIV&gt;\n&lt;DIV&gt;&nbsp;&lt;/DIV&gt;\n&lt;BLOCKQUOTE \ns=\r\ntyle=3D&quot;BORDER-LEFT: #000000 2px solid; MARGIN-LEFT: 5px; MARGIN-RIGHT: 0px=\r\n; PADDING-LEFT: 5px; PADDING-RIGHT: 0px&quot;&gt;\n  &lt;DIV style=3D&quot;FONT: 10pt arial&quot;=\r\n&gt;----- Original Message ----- &lt;/DIV&gt;\n  &lt;DIV \n  style=3D&quot;BACKGROUND: #e4e4e4=\r\n; FONT: 10pt arial; font-color: black&quot;&gt;&lt;B&gt;From:&lt;/B&gt; \n  &lt;A href=3D&quot;mailto:ks=\r\ntanley@...&quot; title=3Dkstanley@...&gt;Kenneth \n  Stanley&lt;/A&gt;=\r\n &lt;/DIV&gt;\n  &lt;DIV style=3D&quot;FONT: 10pt arial&quot;&gt;&lt;B&gt;To:&lt;/B&gt; &lt;A href=3D&quot;mailto:neat=\r\n@yahoogroups.com&quot; \n  title=3Dneat@yahoogroups.com&gt;neat@yahoogroups.com&lt;/A&gt; =\r\n&lt;/DIV&gt;\n  &lt;DIV style=3D&quot;FONT: 10pt arial&quot;&gt;&lt;B&gt;Sent:&lt;/B&gt; Thursday, October 30,=\r\n 2003 4:01 \n  AM&lt;/DIV&gt;\n  &lt;DIV style=3D&quot;FONT: 10pt arial&quot;&gt;&lt;B&gt;Subject:&lt;/B&gt; [n=\r\neat] Re: Learning How to \n  Learn&lt;/DIV&gt;\n  &lt;DIV&gt;&lt;BR&gt;&lt;/DIV&gt;&lt;TT&gt;--- In &lt;A \n  h=\r\nref=3D&quot;mailto:neat@yahoogroups.com&quot;&gt;neat@yahoogroups.com&lt;/A&gt;, Mitchell Timi=\r\nn \n  &lt;&lt;A href=3D&quot;mailto:zenguyuno@y...&quot;&gt;zenguyuno@y...&lt;/A&gt;&gt; wrote:&lt;BR=\r\n&gt;&gt; A \n  44-neuron ANN has evolved which is a perfect player&lt;BR&gt;&gt; of t=\r\nhe 4-card \n  puzzle.&lt;BR&gt;&gt; &lt;BR&gt;&lt;BR&gt;...&lt;BR&gt;&lt;BR&gt;&gt; A friend of mine, Prof=\r\n. Roderick \n  Edwards here at U.&lt;BR&gt;&gt; Vic., pointed out that the evoluti=\r\non software \n  &quot;learned&lt;BR&gt;&gt; how to learn&quot;.&nbsp; This seems apt to me.&=\r\nnbsp; The program \n  had&lt;BR&gt;&gt; to kill off a few millions ANNs that would=\r\nn&#39;t quite&lt;BR&gt;&gt; play \n  perfectly until finally producing some that&lt;BR&gt;&g=\r\nt; could.&nbsp; That was the \n  machine learning phase, and the&lt;BR&gt;&gt; res=\r\nults are fixed in the weights of \n  the ANN.&nbsp; Now the&lt;BR&gt;&gt; ANN can =\r\nlearn what it need to when it plays \n  this game.&lt;BR&gt;&gt; &lt;BR&gt;&gt; I&#39;m inte=\r\nrested in finding out if this has been \n  done&lt;BR&gt;&gt; before; i.e., has so=\r\nmeone trained a recurrent ANN so&lt;BR&gt;&gt; that \n  it makes use of its memory=\r\n while doing its job. &lt;BR&gt;&gt; I suspect some of \n  your robots with NEAT A=\r\nNN&lt;BR&gt;&gt; controllers qualify under this \n  definition.&lt;BR&gt;&gt; &lt;BR&gt;&lt;BR&gt;&lt;B=\r\nR&gt;Those are very nice results, showing that a \n  recurrent neural network&lt;B=\r\nR&gt;can indeed be evolved to be able to memorize the \n  necessary&lt;BR&gt;informat=\r\nion.&nbsp; I still wonder why it needed 44 nodes, but \n  perhaps that&lt;BR&gt;do=\r\nes make sense given all the possibilities.&lt;BR&gt;&lt;BR&gt;Anyway, \n  yes I&#39;ve been =\r\ninterested in &quot;learning how to learn&quot; for a long&lt;BR&gt;time.&nbsp; \n  (Or, in =\r\nother words, evolving networks that can learn.) &lt;BR&gt;Interestingly, a \n  lot=\r\n of people think (and it makes sense) that&lt;BR&gt;learning requires synaptic \n =\r\n plasticity, i.e. the ability to change&lt;BR&gt;weights dynamically during the \n=\r\n  network&#39;s lifetime.&nbsp; Usually such&lt;BR&gt;plasticity would be Hebbian lea=\r\nrning \n  or something like it.&nbsp; Your&lt;BR&gt;experiment, and others, show t=\r\nhat this is \n  not necessarily true;&nbsp; in&lt;BR&gt;fact, recurrent activation=\r\n alone can store \n  state information.&lt;BR&gt;&lt;BR&gt;We actually do have a paper i=\r\nn which the two kinds \n  of adaptation were&lt;BR&gt;tested in a very simple &quot;lea=\r\nrning&quot; domain.&nbsp; The \n  domain involved&lt;BR&gt;foraging food.&nbsp; A forag=\r\ner must collect items on a \n  board that are&lt;BR&gt;either food or poison, but =\r\nit has to learn which is the \n  case.&nbsp; If they&lt;BR&gt;are food, it can kee=\r\np eating, but if they are \n  poisonous, it should&lt;BR&gt;stop.&nbsp; So it has =\r\nto make an adjustment in policy \n  according to what it&lt;BR&gt;discovers.&nbsp;=\r\n We found that both recurrent networks \n  and networks with&lt;BR&gt;synaptic pla=\r\nsticity could handle this task, although they \n  tend to do&lt;BR&gt;it different=\r\nly.&nbsp; Here is the paper (yahoo may mangle the \n  link, so try&lt;BR&gt;copyin=\r\ng it):&lt;BR&gt;&lt;BR&gt;&lt;A \n  href=3D&quot;http://nn.cs.utexas.edu/pub-view.php?RECORD_KEY=\r\n(Pu&quot;&gt;http://nn.cs.utexas.edu/pub-view.php?RECORD_KEY(Pu&lt;/A&gt;&lt;BR&gt;bs)=3DPubID&=\r\namp;PubID(Pubs)=3D131&lt;BR&gt;&lt;BR&gt;(On \n  a side note, for anyone wondering, yes =\r\nthis does mean there is&lt;BR&gt;a version of \n  NEAT that can evolve Hebbian net=\r\nworks with plastic&lt;BR&gt;synapses, but I haven&#39;t \n  released it yet.) &lt;BR&gt;&lt;BR&gt;=\r\nAnyway, these are very interesting issues.&nbsp; \n  When do you need Hebbia=\r\nn&lt;BR&gt;connections to learn as opposed to only \n  fixed-weight recurrent&lt;BR&gt;c=\r\nonnections?&nbsp; It isn&#39;t clear.&nbsp; And what is \n  the capacity of a&lt;BR=\r\n&gt;fixed-weight recurrent network to remember \n  information?&lt;BR&gt;&lt;BR&gt;ken&lt;BR&gt;&lt;=\r\nBR&gt;&lt;/TT&gt;&lt;BR&gt;&lt;BR&gt;&lt;TT&gt;To \n  unsubscribe from this group, send an email \n  to:=\r\n&lt;BR&gt;neat-unsubscribe@yahoogroups.com&lt;BR&gt;&lt;BR&gt;&lt;/TT&gt;&lt;BR&gt;&lt;BR&gt;&lt;TT&gt;Your use of \n =\r\n Yahoo! Groups is subject to the &lt;A \n  href=3D&quot;http://docs.yahoo.com/info/t=\r\nerms/&quot;&gt;Yahoo! Terms of Service&lt;/A&gt;.&lt;/TT&gt; \n&lt;BR&gt;&lt;/BLOCKQUOTE&gt;&lt;/BODY&gt;&lt;/HTML&gt;\n\r\n------=_NextPart_000_0025_01C39EF3.69BDE500--\r\n\n"}}