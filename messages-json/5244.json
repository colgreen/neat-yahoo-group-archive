{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":211599040,"authorName":"Jeff Clune","from":"Jeff Clune &lt;jclune@...&gt;","profile":"jeffreyclune","replyTo":"LIST","senderId":"IXYlXeMyrs2C37JFyqQFB6jDubeWZYPxGJDpwiB89tmn7dpCudTwJ3CHY9OgtlpC-p3fvJOfxhTj3xOvhMp1vGs3","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [neat] Re: &#39;Boxes&#39; Visual Discrimination Task","postDate":"1274907317","msgId":5244,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PEM4MjMwMkY1LjMyQzUxJWpjbHVuZUBtc3UuZWR1Pg==","inReplyToHeader":"PGh0ZnY3citjbmVvQGVHcm91cHMuY29tPg=="},"prevInTopic":5243,"nextInTopic":5245,"prevInTime":5243,"nextInTime":5245,"topicId":5237,"numMessagesInTopic":10,"msgSnippet":"... I agree that there is a lot of interesting things to be learned by changing the function set (in ANNs and CPPNs). In fact, one of the most frequent types","rawEmail":"Return-Path: &lt;jclune@...&gt;\r\nX-Sender: jclune@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 79840 invoked from network); 26 May 2010 20:56:23 -0000\r\nX-Received: from unknown (98.137.34.45)\n  by m14.grp.re1.yahoo.com with QMQP; 26 May 2010 20:56:23 -0000\r\nX-Received: from unknown (HELO mail-gy0-f174.google.com) (209.85.160.174)\n  by mta2.grp.sp2.yahoo.com with SMTP; 26 May 2010 20:56:23 -0000\r\nX-Received: by gyg13 with SMTP id 13so3317562gyg.19\n        for &lt;neat@yahoogroups.com&gt;; Wed, 26 May 2010 13:55:22 -0700 (PDT)\r\nX-Received: by 10.231.176.16 with SMTP id bc16mr1101498ibb.4.1274907322472;\n        Wed, 26 May 2010 13:55:22 -0700 (PDT)\r\nReturn-Path: &lt;jclune@...&gt;\r\nX-Received: from [10.0.1.7] (c-98-243-195-159.hsd1.mi.comcast.net [98.243.195.159])\n        by mx.google.com with ESMTPS id r12sm1957260ibi.14.2010.05.26.13.55.20\n        (version=TLSv1/SSLv3 cipher=RC4-MD5);\n        Wed, 26 May 2010 13:55:21 -0700 (PDT)\r\nUser-Agent: Microsoft-Entourage/12.13.0.080930\r\nDate: Wed, 26 May 2010 16:55:17 -0400\r\nTo: &quot;neat@yahoogroups.com&quot; &lt;neat@yahoogroups.com&gt;\r\nMessage-ID: &lt;C82302F5.32C51%jclune@...&gt;\r\nThread-Topic: [neat] Re: &#39;Boxes&#39; Visual Discrimination Task\r\nThread-Index: Acr9Fb7BnWexCyX3RkqxL/3DzeNt9Q==\r\nIn-Reply-To: &lt;htfv7r+cneo@...&gt;\r\nMime-version: 1.0\r\nContent-type: text/plain;\n\tcharset=&quot;US-ASCII&quot;\r\nContent-transfer-encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Jeff Clune &lt;jclune@...&gt;\r\nSubject: Re: [neat] Re: &#39;Boxes&#39; Visual Discrimination Task\r\nX-Yahoo-Group-Post: member; u=211599040; y=_nY5yvr0YIO7gvPlg7NVsKkeW9Lr-dYP-yBArYvTC0zvjE6YkYod\r\nX-Yahoo-Profile: jeffreyclune\r\n\r\n&gt; In general, there is definitely room for the neural model to be improved.  The\n&gt; main impediment to progress in this area is probably just that it is not very\n&gt; glorious work to be fiddling with different function sets and network models\n&gt; and hoping to see a breakthrough, but it is true there could be a payoff.\n\nI agree that there is a lot of interesting things to be learned by changing\nthe function set (in ANNs and CPPNs). In fact, one of the most frequent\ntypes of comments I get after each HyperNEAT talk I give relates to whether\nI have tested different sets of math functions in the CPPNs and what the\neffect of different function sets are.\n\nI also agree that it is not very glamorous work (despite it being relatively\nlow hanging fruit in terms of pubs).\n\nI think it would be really interesting if on Picbreeder some of the newly\nformed lineages were assigned different function sets at random (or other\nattributes, such as recurrence). That would allow us to develop some\nintuition for the types of patterns that could be evolved with different\nCPPNs. Just a thought. :-)\n\n\n\n\n\n\n&gt; \n&gt; Another problem is that it&#39;s difficult to tell if the model should be better\n&gt; or not.  For example, by providing distance to the CPPN in the boxes domain,\n&gt; you get an almost instant solution, but without that it takes maybe a few\n&gt; hundred generations.   So it still solves it, but it just takes longer.  But\n&gt; is that a good or bad thing?  It&#39;s hard to say.  A few hundred generations\n&gt; could be a drop in the bucket if we&#39;re worried about things that take a few\n&gt; million, so making an effort to whittle hundreds down to a couple generations\n&gt; under some improved model might not be worth whatever cost such whittling\n&gt; incurs (since there is usually some trade-off).\n&gt; \n&gt; But I&#39;m not saying there isn&#39;t something better. I&#39;m just saying it&#39;s hard to\n&gt; tell if there is a problem or not that needs fixing.  There may be and I would\n&gt; be very interested if something superior was demonstrated.  Will the new\n&gt; SharpNEAT provide an ability to play with such ideas?\n&gt; \n&gt; ken\n&gt; \n&gt; --- In neat@yahoogroups.com, Colin Green &lt;colin.green1@...&gt; wrote:\n&gt;&gt; \n&gt;&gt; On 24 May 2010 08:45, Ken &lt;kstanley@...&gt; wrote:\n&gt;&gt;&gt; \n&gt;&gt;&gt; \n&gt;&gt;&gt; Colin, yes, the idea of a powerful representation that can easily capture\n&gt;&gt;&gt; the most important\n&gt;&gt;&gt; operations (such as comparison or multiplication) is always lurking in the\n&gt;&gt;&gt; background. Yet\n&gt;&gt;&gt; trade-offs always seem to emerge when we try to bestow such capabilities\n&gt;&gt;&gt; explicitly. For\n&gt;&gt;&gt; example, if we added multiplication, it may theoretically allow certain key\n&gt;&gt;&gt; computations to be\n&gt;&gt;&gt; represented easily, yet it also gives evolution a new power to more easily\n&gt;&gt;&gt; destroy what it has\n&gt;&gt;&gt; built through trivial mutations (because multiplication is more powerful\n&gt;&gt;&gt; than summation).\n&gt;&gt; \n&gt;&gt; Point acknowledged.\n&gt;&gt; \n&gt;&gt;&gt; I am sure that CPPNs as a representational formalism can be improved, yet\n&gt;&gt;&gt; the presence of\n&gt;&gt;&gt; these nagging trade-offs makes me think that the idea of a super-powerful\n&gt;&gt;&gt; representation is a\n&gt;&gt;&gt; false ideal. It seems to me at this point that evolution in nature succeeds\n&gt;&gt;&gt; despite the limited\n&gt;&gt;&gt; nature of its representation rather than because DNA is somehow near\n&gt;&gt;&gt; perfect.\n&gt;&gt; \n&gt;&gt; I accept you chain of reasoning but I don&#39;t accept the\n&gt;&gt; foundation/axiom that natural neurons don&#39;t perform multiplication and\n&gt;&gt; comparison (and possibly quite a few other operations) at the neuronal\n&gt;&gt; level. Christoph Koch&#39;s work is a good starting point to learn more\n&gt;&gt; about what functionality is present in individual neurons (see\n&gt;&gt; Biophysics of Computation, section 21.1.1 Many Ways to Multiply). I&#39;ll\n&gt;&gt; pick one example which I think is the most significant -\n&gt;&gt; multiplication via addition and logarithms:\n&gt;&gt; \n&gt;&gt;     log(x*y) = log(x) + log(y)\n&gt;&gt; \n&gt;&gt; Thus:\n&gt;&gt; \n&gt;&gt;    x*y = e^(log(x) + log(y))\n&gt;&gt; \n&gt;&gt; Logarithms occur naturally by decay of some neurotransmitter\n&gt;&gt; concentration over the length of an axon or dendrite.\n&gt;&gt; \n&gt;&gt; Another compelling example is the non-linear interation of\n&gt;&gt; neurotransmitters in dendrites. Say we have incoming signals x and y\n&gt;&gt; which connect at sites X and Y on the dendritic tree. Those two sites\n&gt;&gt; produce two different neurotransmitters when excited and the resulting\n&gt;&gt; excitation of the neuron is the *product* of the two neurotransmitter\n&gt;&gt; concentrations. Why? Because one neurotransmitter acts chemically and\n&gt;&gt; electrically in improving the flow of the other (or more specifically,\n&gt;&gt; improving the flow of ions produced by the stimulating effects of the\n&gt;&gt; other). This setup is much like a transistor, and one use of\n&gt;&gt; transitors is of course amplification AKA multiplication.\n&gt;&gt; \n&gt;&gt; \n&gt;&gt;&gt; Of course, these thoughts don&#39;t contradict anything you said. The length\n&gt;&gt;&gt; input trick also\n&gt;&gt;&gt; shows that to some extent we can provide dramatic bias through a priori\n&gt;&gt;&gt; knowledge, which is\n&gt;&gt;&gt; a good thing (e.g. if it means skipping 10,000 years of evolution).\n&gt;&gt; \n&gt;&gt; My concern is that this was a simple test case where simple euclidean\n&gt;&gt; distance applied in an obvious way. Ideally I want HyperNEAT to learn\n&gt;&gt; what distance metrics apply to the problem rather than me suppling\n&gt;&gt; them precomputed.\n&gt;&gt; \n&gt;&gt; I think there&#39;s a strong case for taking a fresh look at what\n&gt;&gt; computations we perform at the neuronal level and whether traditional\n&gt;&gt; sigmoid based ANNs are deficient in some significant ways. I hope you\n&gt;&gt; can see where I&#39;m coming from here.\n&gt;&gt; \n&gt;&gt; Colin.\n&gt;&gt; \n&gt; \n&gt; \n\n\n\n"}}