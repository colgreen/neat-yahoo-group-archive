{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"4o5OrrGetboTik4KgMRt3wg8iC_JdZQtZNmBfrgyVKMDy3S0oWqOwQ1noYIoB55OWMeTSG38-4hI7SN2AtYYMoQH_fWTPbW6yllhGYpB2i-y","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Tile Coding and HyperNEAT","postDate":"1179296130","msgId":3287,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGYyZTdpMis5cmV0QGVHcm91cHMuY29tPg==","inReplyToHeader":"PEM2RDM1ODI1LTkwOTAtNDA5RS1CMjVELTkxNDVDMjA2QzYxRkBjcy51dGV4YXMuZWR1Pg=="},"prevInTopic":3277,"nextInTopic":3305,"prevInTime":3286,"nextInTime":3288,"topicId":3214,"numMessagesInTopic":27,"msgSnippet":"Joe, first, let me be clear that I am attacking tile coding and not RL in general.  As I already said, I am not certain that RL (aside from NEAT+Q-type stuff)","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 80636 invoked from network); 16 May 2007 06:15:35 -0000\r\nReceived: from unknown (66.218.67.34)\n  by m45.grp.scd.yahoo.com with QMQP; 16 May 2007 06:15:35 -0000\r\nReceived: from unknown (HELO n19c.bullet.sp1.yahoo.com) (69.147.64.132)\n  by mta8.grp.scd.yahoo.com with SMTP; 16 May 2007 06:15:35 -0000\r\nReceived: from [216.252.122.218] by n19.bullet.sp1.yahoo.com with NNFMP; 16 May 2007 06:15:31 -0000\r\nReceived: from [66.218.69.6] by t3.bullet.sp1.yahoo.com with NNFMP; 16 May 2007 06:15:31 -0000\r\nReceived: from [66.218.66.81] by t6.bullet.scd.yahoo.com with NNFMP; 16 May 2007 06:15:31 -0000\r\nDate: Wed, 16 May 2007 06:15:30 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;f2e7i2+9ret@...&gt;\r\nIn-Reply-To: &lt;C6D35825-9090-409E-B25D-9145C206C61F@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: Re: Tile Coding and HyperNEAT\r\nX-Yahoo-Group-Post: member; u=54567749; y=dKU3jsgj379d5k82sLZq2t29wdC9MMnfdXweDi7gK1JikK5sfUiV\r\nX-Yahoo-Profile: kenstanley01\r\n\r\nJoe, first, let me be clear that I am attacking tile coding and not \nRL in =\r\ngeneral.  As I already said, &quot;I am not certain that RL (aside \nfrom NEAT+Q-=\r\ntype stuff) is really incapable of optimizing the model \nbecause who knows =\r\nwhat we might realize how to do in the future.&quot;  \n\nIt&#39;s true that tile codi=\r\nng is a symptom of a problem, but that \ndoesn&#39;t mean that the problem can&#39;t=\r\n be avoided in some better way (or \nisn&#39;t already starting to be avoided by=\r\n people like Stone and \nLittman).  But that is no excuse for tile coding it=\r\nself.  It&#39;s a \nmisdirection on top of a misdirection, and two wrongs don&#39;t =\r\nmake a \nright.  So to say it&#39;s &quot;really good at what it does&quot; is misleading.=\r\n\n\nThe generality issue is more interesting.  It&#39;s interesting because \nnow =\r\nwe&#39;re veering back towards No Free Lunch (NFL).  Implicit in your \nlast par=\r\nagraph is the idea that we *want* to strive for complete \ngenerality in a l=\r\nearning method.  Why do you think that&#39;s a good \nidea?  My feeling is that =\r\nsuch a perspective is motivated by trying \nto spit out the bitter pill of N=\r\nFL,  or perhaps just an undue \nfixation on theoretical achievement.  But I =\r\nthink it&#39;s a bad goal, \nand once you let go of it, you can see that things =\r\nlike tile coding \nare obstacles to progress.\n\nNote that any method that mak=\r\nes the user choose inputs is not really \ngeneral because choosing inputs al=\r\none requires a priori assumptions, \njust as HyperNEAT does.  So requiring a=\r\n priori knowledge is nothing \nnew.  Well, you might then argue that HyperNE=\r\nAT requires more \ndifficult a priori knowledge because the right set of inp=\r\nuts is more \nstraightforward than the right geometry.\n\nYet that is not real=\r\nly true in practice.  In fact, in many important \nand common cases, the geo=\r\nmetry is *more* obvious than the inputs on \ntheir own.  In fact, the geomet=\r\nry is so obvious that we fail to \nnotice when we cut it out of the inputs, =\r\nwhich in fact we do all the \ntime.\n\nFor example, it is obvious that a chess=\r\n board has an 8x8 square \ngeometry.  Furthermore, it is obvious that such g=\r\neometry is essential \nto chess.  Who in their right mind would want to try =\r\nto learn chess \n*without* the information of where squares are on the board=\r\n?  Rather \nthan a laudable achievment in &quot;generality,&quot; it looks to me a wil=\r\nd \ngoose chase to try something like that.  \n\nOr, if you don&#39;t like that on=\r\ne, how about this:  Who in their right \nmind would want to try to learn any=\r\nthing off of a visual field that \nis scrambled into a random order?  Well, =\r\nif generality is your goal, \nthen apparently you do.\n\nAmazingly, even outsi=\r\nde tile coding, we have often (not always) been \ndoing exactly that routine=\r\nly as a field.  We take the most essential \nstructural information and gut =\r\nit from our representation before \nsending the rest in as input.  Look at h=\r\now we input a Go board, each \nsquare totally separate from all the others; =\r\n or how we input a \nvisual field as e.g. 100 separate inputs with no relati=\r\nonship between \nthem.  \n\nAnd you are suggesting that this is a good thing i=\r\nn anything other \nthan a pedantic theoretical view?  \n\nI am not so much att=\r\nacking prior work as I am pointing in a new \ndirection.  Of course one can =\r\nalways easily speak ill of what has \ncome before with the benefit of hindsi=\r\nght.  Yet I have seen that \nsomething has been wrong, something fairly deep=\r\n, and that is not so \nmuch a criticism of anything as it is an opportunity =\r\nto seize a new \ndirection.  \n\nTile coding, on the other hand, is surely a s=\r\ntep backward.  Not *all* \ncurrent input configurations throw away all essen=\r\ntial geometric \ninformation.  For example, something as simple as inputting=\r\n (x,y) as \nposition into a regular ANN does preserve some form of geometry.=\r\n  Yet \ntile code wants to destroy even that!  Tile coding says that what \nr=\r\nelationships you haven&#39;t destroyed on your own when you chose your \ninputs =\r\nit will be sure to destroy for you to finish the job.\n\nSo, if generality is=\r\n not the goal I&#39;m advocating, then what is it?  \nSuccess:  Evolve a world c=\r\nhess champion.  Evolve a vision-guided \ndriver.  Evolve the world Go champi=\r\non.  Learn to control an \nairplane.  Evolve a brain.  It matters little if =\r\nwe satisfy some \nesoteric definition of learner generality if we achieve su=\r\nch aims.  \nMore seriously, we never will achieve them if we try to be too \n=\r\ngeneral, because in the world of generality, we have to not only \nlearn to =\r\nplay chess but also learn that a chess board is arranged as \na square of sq=\r\nuares, and which square is which, problems that no \nchess player on earth h=\r\nas ever faced.\n\nTo ramble on a little more while I&#39;m at it :) ... You might=\r\n then \nraise natural evolution:  Wasn&#39;t natural evolution general enough to=\r\n \ndo exactly what I&#39;m saying is a lost cause?  In fact, no.  Natural \nevolu=\r\ntion happened in 3D physical space, where the geometric \nrelationships of a=\r\nll things, outside and inside the brain, are \nimplicit.   Photons from one =\r\nlocation in space hit a part of the eye \nnear photons from a directly adjac=\r\nent location.  That happens not \nbecause the eye was evolved to somehow dis=\r\ncover the structure of \nphysical space, but because of the geometry of spac=\r\ne itself.  So \nthere is no precedent for the generality you are seeking.\n\nk=\r\nen\n\n\n\n\n\n--- In neat@yahoogroups.com, Joseph Reisinger &lt;joeraii@...&gt; wrote:\n=\r\n&gt;\n&gt; Ok, now we&#39;re getting somewhere. The argument you&#39;re making is \nmuch  \n=\r\n&gt; larger, really. Not against tile-coding in specific but against \nall  \n&gt; =\r\nML methods that are incapable of taking into account variational  \n&gt; proper=\r\nties (evolvability). Thats fine. But then don&#39;t just attack  \n&gt; tile coding=\r\n, since its really a symptom, not the disease (to use \nyour  \n&gt; terminology=\r\n). In fact, tile-coding is really good at what it does, \nit  \n&gt; just happen=\r\ns to be part of a larger picture that you don&#39;t agree \nwith.\n&gt; \n&gt; Also, I w=\r\nould be a little wary of dismissing the entire field just  \n&gt; because it ca=\r\nn&#39;t take into account geometry the way you want it to. \nI  \n&gt; think the RL =\r\ncommunity knows that this is an issue: look at \nLittman&#39;s  \n&gt; work on Reloc=\r\natable Action Models (RAMs) which is based on the  \n&gt; Sherstov and Stone st=\r\nuff I pointed out earlier. Both of these are  \n&gt; moving towards some idea o=\r\nf building in &quot;geometry&quot; into the \nlearning  \n&gt; problem. Geometry in the RA=\r\nM case is very general, using only  \n&gt; similarities in function mappings.\n&gt;=\r\n \n&gt; But even that is beside the point. In a sense, the tile-coding  \n&gt; appr=\r\noach is more general than the HyperNEAT approach because it \ndoes  \n&gt; not r=\r\nequire the experimenter to set a priori some geometry. This  \n&gt; generality =\r\nis accomplished through being geometry-agnostic, i.e.  \n&gt; &quot;cutting the stat=\r\ne space up into little pieces and scattering them  \n&gt; around the room.&quot; To =\r\nhave a truly general learning method, you \nneed  \n&gt; one of two things: no a=\r\n priori assumptions on geometry (most of \nRL/ML  \n&gt; already does this), or =\r\nsome way of choosing the appropriate \ngeometry  \n&gt; during learning (which, =\r\nto my knowledge, not one has yet \nattacked).  \n&gt; Note that HyperNEAT does n=\r\not fulfill either of these criteria. \nThats  \n&gt; not to say HyperNEAT isn&#39;t =\r\na good first step towards the latter, \njust  \n&gt; that its not there yet.\n&gt; \n=\r\n&gt; &gt; Most of what you said is factually true.  But the spin you put on \nit\n&gt;=\r\n &gt; is wrong. It is correct that tile coding breaks up the \nstate/action\n&gt; &gt;=\r\n space into little pieces to make the right behavior for each \nlittle\n&gt; &gt; r=\r\negion easier to compute.  As you put it, &quot;subtiles can better fit\n&gt; &gt; the v=\r\nalue function being learned. Note that there is very little\n&gt; &gt; generalizat=\r\nion desired here.&quot;  You say that like it&#39;s a good \nthing.\n&gt; \n&gt; &gt; And that&#39;s=\r\n what tile coding is really indicating: Much of RL is \nDOA.\n&gt; &gt; Tile coding=\r\n is a symptom of a larger sickness.  You said it\n&gt; &gt; yourself: &quot;[most] RL i=\r\ns inherently incapable of performing model\n&gt; &gt; selection.&quot;  Well, if what t=\r\nhat means is that you can&#39;t exploit\n&gt; &gt; geometry, it&#39;s all a dead end.  I a=\r\nm not certain that RL (aside \nfrom\n&gt; &gt; NEAT+Q-type stuff) is really incapab=\r\nle of optimizing the model\n&gt; &gt; because who knows what we might realize how =\r\nto do in the future.\n&gt; &gt; However, for now, RL is falling back on tile codin=\r\ng because it is\n&gt; &gt; moving in the wrong direction.\n&gt; &gt;\n&gt; &gt; Here is what is =\r\nreally going on:  Each variable in the \nstate/action\n&gt; &gt; space is a dimensi=\r\non along which the value function varies.  A \ngood\n&gt; &gt; learning algorithm w=\r\nould represent how the value function varies \nwith\n&gt; &gt; respect to each stat=\r\ne variable.  However, such variation may be\n&gt; &gt; complex, i.e. the function =\r\ncould be pretty complicated.  The \nlearning\n&gt; &gt; methods (i.e. supervised fu=\r\nnction approximators) inside RL are\n&gt; &gt; sufficiently bad that they cannot h=\r\nandle approximating functions \nlike\n&gt; &gt; that.  So what do we do?  We break =\r\nthe whole space into chunks.  \nNow\n&gt; &gt; the appropriate action for each litt=\r\nle chunk requires a much \nsimpler\n&gt; &gt; function, so we have a chance with ou=\r\nr poor learning algorithm to\n&gt; &gt; maybe get all these little simple function=\r\ns right instead of only \na\n&gt; &gt; few big complicated functions.\n&gt; &gt;\n&gt; &gt; In ot=\r\nher words, we have a poor algorithm and the cure is to \ndestroy\n&gt; &gt; what va=\r\nriational structure there was to begin with so that we can\n&gt; &gt; look at ever=\r\ny little bit of the problem separately.   So we have \nnow\n&gt; &gt; lost the abil=\r\nity to exploit all the useful relationships that\n&gt; &gt; initially existed in t=\r\nhe space.  States that are related are now\n&gt; &gt; broken apart and must be lea=\r\nrned separately, that is, the geometry\n&gt; &gt; has been destroyed! The fact tha=\r\nt many see such an operation as a\n&gt; &gt; step in the right direction is sympto=\r\nmatic of serious \nmisdirection in\n&gt; &gt; the field.  If that&#39;s the best we can=\r\n do to make RL easier, than \nRL\n&gt; &gt; is in serious trouble!\n&gt; &gt;\n&gt; &gt; Think of=\r\n it like this:  Take a game like chess, which I learned \nas a\n&gt; &gt; little ki=\r\nd.  Now take the 64 squares and cut each piece out of the\n&gt; &gt; board individ=\r\nually.  Now sprinkle them all randomly all over your\n&gt; &gt; living room.  Each=\r\n square still represents the same location it \nwas\n&gt; &gt; originally taken fro=\r\nm in the board.  It&#39;s just you can&#39;t see where\n&gt; &gt; they were anymore.  Now =\r\nplace the chess pieces in the right \nstarting\n&gt; &gt; squares and teach a littl=\r\ne kid to play chess.  Think he or she \nwould\n&gt; &gt; learn anything at all?\n&gt; &gt;=\r\n\n&gt; &gt; Well, that&#39;s exactly what tile coding is!  A method that learns \nchess=\r\n\n&gt; &gt; (or anything else that has implicit or explicit geometry) needs to\n&gt; &gt;=\r\n know how the positions relate to each other geometrically because\n&gt; &gt; ther=\r\ne is massive regularity being lost without that information.\n&gt; &gt; What kind =\r\nof crazy algorithm would purposely put a chess board \ninto a\n&gt; &gt; meaningles=\r\ns order before learning begins?  A method \nthat &quot;benefits&quot;\n&gt; &gt; from such an=\r\n approach is clearly DOA.  RL researchers should be\n&gt; &gt; seriously concerned=\r\n about tile coding being necessary at all, not\n&gt; &gt; happy about it.\n&gt; &gt;\n&gt; &gt; =\r\nSo I stick to my position: Tile coding is anti-geometry and anti-\n&gt; &gt; repre=\r\nsentation.  It deserves no credit whatsoever for &quot;respecting&quot;\n&gt; &gt; anything.=\r\n\n&gt; &gt;\n&gt; &gt; ken\n&gt; &gt;\n&gt; &gt; --- In neat@yahoogroups.com, Joseph Reisinger &lt;joeraii=\r\n@&gt; wrote:\n&gt; &gt;&gt;\n&gt; &gt;&gt; I&#39;ve been aching to reply to this post for a while, and=\r\n I finally\n&gt; &gt;&gt; have enough free time to do so. I think we could have a rea=\r\nlly\n&gt; &gt;&gt; interesting discussion here, hopefully at least more interesting\n&gt;=\r\n &gt; than\n&gt; &gt;&gt; the NFL tangent.\n&gt; &gt;&gt;\n&gt; &gt;&gt;&gt;&gt; Sure, but tile-coding does respec=\r\nt at least one form of \ngeometry:\n&gt; &gt;&gt;&gt;&gt; Nearby elements in the state space=\r\n are known to be nearby, and\n&gt; &gt; thus\n&gt; &gt;&gt;&gt;&gt; are grouped in the same tile.\n=\r\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt; I have to dispute this characterization of tile coding\n&gt; &gt;&gt;&gt; as=\r\n &quot;respecting at least one form of geometry.&quot; I think you are\n&gt; &gt;&gt;&gt; being un=\r\nnecessarily equitable toward tile coding.\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt; What you are saying i=\r\ns that in effect taking a nice sculpture \nand\n&gt; &gt;&gt;&gt; cutting it into pieces =\r\n&quot;respects&quot; its geometry because those\n&gt; &gt; little\n&gt; &gt;&gt;&gt; pieces are not broke=\r\nn up any further than that. It&#39;s like saying\n&gt; &gt;&gt;&gt; that someone who cut you=\r\nr head off &quot;respected&quot; your head by\n&gt; &gt; keeping\n&gt; &gt;&gt;&gt; its internal integrit=\r\ny intact. In fact, tile coding is \npeforming a\n&gt; &gt;&gt;&gt; grievous violation aga=\r\ninst the existing geometry of the domain,\n&gt; &gt; and\n&gt; &gt;&gt;&gt; does not deserve to=\r\n be credited with respecting geometry\n&gt; &gt;&gt;&gt; whatsoever. I&#39;m hard pressed to=\r\n imagine how one could do worse\n&gt; &gt;&gt;&gt; beyond cutting things up into even ti=\r\nnier and tinier bits; but\n&gt; &gt; even\n&gt; &gt;&gt;&gt; then, those bits still contain &quot;ne=\r\narby elements in the state\n&gt; &gt;&gt;&gt; space.&quot; So that isn&#39;t saying much.\n&gt; &gt;&gt;\n&gt; =\r\n&gt;&gt; Yeah, from the way your framing this argument, e.g. tile-coding\n&gt; &gt; used=\r\n\n&gt; &gt;&gt; in the GA model-selection sense, you&#39;re absolutely right. I&#39;ll \nget\n&gt;=\r\n &gt;&gt; back to exactly what I mean by that in a bit. For now lets try to\n&gt; &gt;&gt; =\r\nreframe the issue from an RL perspective, which is where tile-\n&gt; &gt; codings\n=\r\n&gt; &gt;&gt; are predominantly used. In RL, the tile-coding is just a\n&gt; &gt;&gt; represen=\r\ntation for a function approximator (in a sense its sort \nof\n&gt; &gt;&gt; like a rea=\r\nlly simple spline cure) that learns in a supervised\n&gt; &gt; manner.\n&gt; &gt;&gt; Tile c=\r\noding makes a lot of sense in this domain because you can\n&gt; &gt;&gt; calculate wi=\r\nth a good deal of precision how much some particular\n&gt; &gt; tile\n&gt; &gt;&gt; differs =\r\nfrom the expected value of the function being \napproximated\n&gt; &gt;&gt; (in this c=\r\nase the Bellman error).\n&gt; &gt;&gt; Tiles hat is cover a broad area where the valu=\r\ne function changes \na\n&gt; &gt;&gt; lot (&quot;have bad fit&quot;, &quot;are too general&quot;, etc) are=\r\n then split so\n&gt; &gt; that\n&gt; &gt;&gt; the subtiles can better fit the value function=\r\n being learned. \nNote\n&gt; &gt;&gt; that there is very little generalization desired=\r\n here; the best\n&gt; &gt; thing\n&gt; &gt;&gt; given infinite computational resources would=\r\n be to have a whole\n&gt; &gt; ton\n&gt; &gt;&gt; of itty-bitty tiles that fit the value fun=\r\nction perfectly.\n&gt; &gt;&gt;\n&gt; &gt;&gt; Anyway, since we&#39;re in the standard RL framework=\r\n, there is really\n&gt; &gt; no\n&gt; &gt;&gt; way of learning the &quot;geometry&quot; of a value fun=\r\nction (well,\n&gt; &gt; technically\n&gt; &gt;&gt; there is, but thats a long tangent toward=\r\ns a really interesting\n&gt; &gt;&gt; research area). Maybe if the geometry was given=\r\n by the\n&gt; &gt; experimenter\n&gt; &gt;&gt; beforehand (this would also lead to an intere=\r\nsting extension of\n&gt; &gt; tile-\n&gt; &gt;&gt; codings that you might like a little bett=\r\ner). But in any case,\n&gt; &gt; since\n&gt; &gt;&gt; all we&#39;re trying to do in RL is superv=\r\nised function \napproximation,\n&gt; &gt;&gt; the lack of geometry isn&#39;t bad.\n&gt; &gt;&gt;\n&gt; &gt;=\r\n&gt;\n&gt; &gt;&gt;&gt; I&#39;m obviously not a big fan of tile coding :) I&#39;m not really\n&gt; &gt;&gt;&gt; =\r\nconcerned whether it might do better in some cases; the problem\n&gt; &gt; with\n&gt; =\r\n&gt;&gt;&gt; it is that it is a dead end for future progress because it is\n&gt; &gt; about=\r\n\n&gt; &gt;&gt;&gt; ruining our ability to exploit geometric relationships.\n&gt; &gt;&gt;\n&gt; &gt;&gt; Ok=\r\n, this is where the discussion gets really interesting. \nRemember\n&gt; &gt;&gt; when=\r\n I mentioned GA&#39;s &quot;performing model selection&quot; or something\n&gt; &gt; like\n&gt; &gt;&gt; t=\r\nhat before? Thats a fundamental difference in the GA approach \nand\n&gt; &gt;&gt; RL.=\r\n So what do I mean by model selection: roughly speaking, in\n&gt; &gt;&gt; Bayesian i=\r\nnference you have this idea of some separation of the\n&gt; &gt;&gt; parameters you a=\r\nre optimizing (e.g. the weights in an NN) and the\n&gt; &gt;&gt; model that generates=\r\n those parameters (e.g. the topology of the\n&gt; &gt; NN,\n&gt; &gt;&gt; or even whether yo=\r\nu use an NN or decision tree or something). RL\n&gt; &gt; is\n&gt; &gt;&gt; inherently incap=\r\nable of performing model selection (at least\n&gt; &gt; outside\n&gt; &gt;&gt; of NEAT+Q and=\r\n some others). Once you start learning with  a given\n&gt; &gt;&gt; value function re=\r\npresentation, you can no longer switch to a\n&gt; &gt;&gt; different representation w=\r\nithout throwing away everything you&#39;ve\n&gt; &gt; just\n&gt; &gt;&gt; learned.  GAs on the o=\r\nther hand learn one parameterized model per\n&gt; &gt;&gt; individual. This is an imp=\r\nortant distinction.\n&gt; &gt;&gt;\n&gt; &gt;&gt; Now, what does this have to do with tile codi=\r\nng and learning\n&gt; &gt;&gt; geometry?  When you talk about &quot;cutting up different v=\r\nariables&quot;\n&gt; &gt; you\n&gt; &gt;&gt; are inherently making an argument from the standpoin=\r\nt of model\n&gt; &gt;&gt; selection: i.e. what is the best representation for this le=\r\narning\n&gt; &gt;&gt; problem? This is a valid question in the GA world, and I agree\n=\r\n&gt; &gt; with\n&gt; &gt;&gt; you tile coding wouldn&#39;t work at all for learning good\n&gt; &gt;&gt; r=\r\nepresentations that allow good future learning. But from the RL\n&gt; &gt;&gt; standp=\r\noint, since all tile-coding is used for is function\n&gt; &gt;&gt; approximation, I d=\r\non&#39;t think they are as problematic as you \nimagine.\n&gt; &gt;&gt;\n&gt; &gt;&gt; -- Joe\n&gt; &gt;&gt;\n&gt;=\r\n &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; Yahoo! Groups Links\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt;\n\n\n\n"}}