{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":7192225,"authorName":"Ian Badcoe","from":"Ian Badcoe &lt;ian_badcoe@...&gt;","profile":"ian_badcoe","replyTo":"LIST","senderId":"VF85Np4Tdsyy4MkFq-mkaSJy5nkwxGuT5O_aG9_ECZJX1LeHra_Z-ZCzt1TYJVJHNLBhpB-0fTCVTfUWDSkHqugj-5SmcRlG4mc","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] Re: Symmetry, concepts and data buses in the brain","postDate":"1106328483","msgId":1834,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDYuMi4wLjE0LjAuMjAwNTAxMjExMTU1MzEuMDNjMTkwMDhAcG9wLm1haWwueWFob28uY28udWs+","inReplyToHeader":"PGNzb3QyNCtybTNlQGVHcm91cHMuY29tPg==","referencesHeader":"PDYuMi4wLjE0LjAuMjAwNTAxMjAxMjE0MTYuMDNjMTZjODhAcG9wLm1haWwueWFob28uY28udWs+IDxjc290MjQrcm0zZUBlR3JvdXBzLmNvbT4="},"prevInTopic":1833,"nextInTopic":1835,"prevInTime":1833,"nextInTime":1835,"topicId":1698,"numMessagesInTopic":40,"msgSnippet":"Hi, ... No problem.  I m really glad to have people to discuss with. ... I can see how a non-recurrent, simple sigmoidal neural net can be differentiable.  I m","rawEmail":"Return-Path: &lt;ian_badcoe@...&gt;\r\nX-Sender: ian_badcoe@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 22522 invoked from network); 21 Jan 2005 17:28:41 -0000\r\nReceived: from unknown (66.218.66.216)\n  by m24.grp.scd.yahoo.com with QMQP; 21 Jan 2005 17:28:41 -0000\r\nReceived: from unknown (HELO smtp004.mail.ukl.yahoo.com) (217.12.11.35)\n  by mta1.grp.scd.yahoo.com with SMTP; 21 Jan 2005 17:28:40 -0000\r\nReceived: from unknown (HELO ian2k.yahoo.co.uk) (ian?badcoe@212.159.73.108 with login)\n  by smtp004.mail.ukl.yahoo.com with SMTP; 21 Jan 2005 17:28:35 -0000\r\nMessage-Id: &lt;6.2.0.14.0.20050121115531.03c19008@...&gt;\r\nX-Mailer: QUALCOMM Windows Eudora Version 6.2.0.14\r\nDate: Fri, 21 Jan 2005 17:28:03 +0000\r\nTo: neat@yahoogroups.com\r\nIn-Reply-To: &lt;csot24+rm3e@...&gt;\r\nReferences: &lt;6.2.0.14.0.20050120121416.03c16c88@...&gt;\n &lt;csot24+rm3e@...&gt;\r\nMime-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;us-ascii&quot;; format=flowed\r\nX-eGroups-Remote-IP: 217.12.11.35\r\nFrom: Ian Badcoe &lt;ian_badcoe@...&gt;\r\nSubject: Re: [neat] Re: Symmetry, concepts and data buses in the brain\r\nX-Yahoo-Group-Post: member; u=7192225\r\nX-Yahoo-Profile: ian_badcoe\r\n\r\nHi,\n\nAt 18:25 20/01/2005, you wrote:\n\n\n&gt;Hi Ian:\n&gt;\n&gt;Apologizes for my English, I meant &quot;Differentiable Programs&quot;.\n\nNo problem.  I&#39;m really glad to have people to discuss with.\n\n&gt;  It means\n&gt;that a &quot;recurrent ANN&quot; with LSTM cells is able to create Loops,\n&gt;counters, variables, and conditional statements in a automatic way\n&gt;(through evolution) and besides, it is possible to perform a partial\n&gt;derivate over the parameter of the program (ANN Weights).\n\nI can see how a non-recurrent, simple sigmoidal neural net can be \ndifferentiable.  I&#39;m assuming that what was needed here was differentiation \nof the error with respect to a weight, for a given input -- so that one can \ndetermine the slope of the hypersurface defined by the error function on \nthe weight space.\n\nBut a recurrent neural net does not necessarily yield a single value for a \nsingle input.  e.g. it is quite possible to build an oscillator from a \nrecurrent net, so I don&#39;t see how they can remain differentiable.  Even if \nyou define the output as a sample taken on one particular evaluation frame, \nsome weight changes could switch the system from a period-two oscillator to \na period-four oscillator, which depending which frame you choose to \nexamine, can put a discontinuity into the output.\n\n&gt;That concept\n&gt;is useful if you want to perform a &quot;gradient based method&quot; to find a\n&gt;solution (See LSTM articles of Schmidhuber) as it is used in LSTM\n&gt;original learning algorithm.\n&gt;\n&gt;   I see your problem as Ken suggest, it is a &quot;sequential problem&quot; more\n&gt;than a &quot;Spatial Problem&quot;. So:\n\nI&#39;m sorry, I don&#39;t quite understand.  What are you referring to as &quot;my \nproblem&quot;, you mean the question:\n\n         How does a brain learn a concept once and then apply it as a \nmodule within multiple other learned behaviours?\n\nI won&#39;t reply to your exact arguments just yet, because I&#39;m not sure I \nfollow them, but maybe I haven&#39;t yet explained my question very clearly yet \n(I know it is very hard to put these things into words that can be clearly \nfollowed).\n\nI get the idea you are raising questions about how processing of different \ncases can be separated in time insteadof space, and how a concept might be \nstored &quot;holistically&quot; -- mixed in with all the other concepts -- rather \nthan in a spatially discrete module.\n\nI&#39;ll try to clarify by argument a bit in the area od these points.\n\n--\n\nI do not see how separation in time makes the problem any less difficult \nthan separation in space.  You still need to communicate information \ngenerated by the action of one &quot;module&quot; (which may be spatially remote or \n&quot;mixed in&quot; with the rest of the network) into another module.  To do that \nyou need to both physically move the information from one set of neurones \nto another, and also format the information in some manner that makes sense \nto both modules.\n\nIt is the second part of this, formatting data so that both modules \nunderstand, which is the hard part of the operation.  e.g. the activation \nlevel of a neurone is very much &quot;freeform&quot; -- just a single value with no \ninherent meaning (it could only be simpler if it were binary).  Activation \nlevels only acquire &quot;meaning&quot; as a result of (i) our mapping other data \nonto input and output neurones, (ii) evolution (or education) configuring a \nnetwork such that it performs particular functions.\n\nIt is this latter case of &quot;learnt meaning&quot; that matters, and note that it \nis seldom anything an outside observer can understand.  But this is the \ncrux of the question, because if the meaning of a single neurone cannot be \nunderstood by an outside observer, then how can it be understood by another \nmodule?\n\nThe obvious answer to that is that the same process (evolution/education) \nthat made the different neurones make sense to one-another can also adopt \none module to understand another.  And this is an interesting scenario, \nbecause it means if we have a general purpose module (such as a symmetry \ntransformer) then a later module (such as chess) can learn whatever to \nunderstand it.  This implies a sort of &quot;star configuration&quot; of modules, \nwhere the symmetry module outputs into a large number of later-learned \nmodules, each of which has learned its format.\n\nOn the other hand, does the chess module also need to understand &quot;raw&quot; \ninput (e.g. which has not been through the symmetry module)?  If it does, \nthen does that imply that the raw input and the symmetry module output must \nbe in the same format?  This might be the case and has deeper implications, \nbecause it implies that the &quot;raw&quot; input already has a format, and when the \nsymmetry module was created, one of the constraints imposed on it, besides \ndoing the transformation correctly, was to use the same format as already \nexisted.  This makes a lot of sense because if, for example, the symmetry \nmodule consisted of a reflection module and a rotation module, then general \nthought might want to say &quot;reflect it, then rotate it, then reflect it \nagain...&quot; which would require those two submodules to also input and output \nthe same format.\n\n--\n\nObviously this is all very vague and hypothetical, but I do feel that a \nlittle consideration of this type.  e.g. of the form &quot;given a brain, we do \nnot know the details, but what minimum things _must_ be possible&quot; might \nlead to insights useful for a theory of modularity.\n\n         Ian\n\n\n\nLiving@Home - Open Source Evolving Organisms - \nhttp://livingathome.sourceforge.net/\n\n\n\n\n"}}