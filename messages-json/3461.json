{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":204774783,"authorName":"Matt Simmerson","from":"&quot;Matt Simmerson&quot; &lt;m.simmerson@...&gt;","profile":"easablade","replyTo":"LIST","senderId":"LyFUikBNVWr0jfisJcVO9z0BPgp288yK5Cu1GikwiCe140yA8ANr0N8ol1yeZ1NwolSn1vC_4HgXpdq2yRkFifT0DtK8nVY_BhdKcK4aIWt6srsa3Q","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Different activation methods in NEAT4J","postDate":"1184249599","msgId":3461,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGY3NWN0ditrNnF0QGVHcm91cHMuY29tPg==","inReplyToHeader":"PGRkZjEwMDc4MDcwNzEwMTMxM2g1NmFmZjc1Ymk0ZDllM2M3MDM0NGZjMGM2QG1haWwuZ21haWwuY29tPg=="},"prevInTopic":3459,"nextInTopic":3462,"prevInTime":3460,"nextInTime":3462,"topicId":3459,"numMessagesInTopic":6,"msgSnippet":"Hi Cesar Your results are very interesting.  Is it possible you could send me your synchronous update code, and I will integrate it into NEAT4J, also, does it","rawEmail":"Return-Path: &lt;m.simmerson@...&gt;\r\nX-Sender: m.simmerson@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 25226 invoked from network); 12 Jul 2007 14:13:36 -0000\r\nReceived: from unknown (66.218.67.36)\n  by m57.grp.scd.yahoo.com with QMQP; 12 Jul 2007 14:13:36 -0000\r\nReceived: from unknown (HELO n29b.bullet.sp1.yahoo.com) (209.131.38.250)\n  by mta10.grp.scd.yahoo.com with SMTP; 12 Jul 2007 14:13:36 -0000\r\nReceived: from [216.252.122.216] by n29.bullet.sp1.yahoo.com with NNFMP; 12 Jul 2007 14:13:20 -0000\r\nReceived: from [66.218.69.6] by t1.bullet.sp1.yahoo.com with NNFMP; 12 Jul 2007 14:13:20 -0000\r\nReceived: from [66.218.66.80] by t6.bullet.scd.yahoo.com with NNFMP; 12 Jul 2007 14:13:19 -0000\r\nDate: Thu, 12 Jul 2007 14:13:19 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;f75ctv+k6qt@...&gt;\r\nIn-Reply-To: &lt;ddf100780707101313h56aff75bi4d9e3c70344fc0c6@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0\r\nFrom: &quot;Matt Simmerson&quot; &lt;m.simmerson@...&gt;\r\nSubject: Re: Different activation methods in NEAT4J\r\nX-Yahoo-Group-Post: member; u=204774783; y=ymYLP6O2k6sP9fHn-S431dTm0fnStbt8ldu6ovLo6lRdQEne5VBVsKxh4A-D5CRsaBsqbaHp\r\nX-Yahoo-Profile: easablade\r\n\r\nHi Cesar\n\nYour results are very interesting.  Is it possible you could send=\r\n me\nyour synchronous update code, and I will integrate it into NEAT4J,\nalso=\r\n, does it work for recurrent connections?\n\nI have been trying, in vain, to =\r\nsend you my updated code, but I keep\ngetting it rejected by your mail provi=\r\nder.\n\nCheers\n\nMatt\n\n--- In neat@yahoogroups.com, &quot;Cesar G. Miguel&quot; &lt;cesargm=\r\n@...&gt; wrote:\n&gt;\n&gt; Hi there,\n&gt; \n&gt; I&#39;ve been experimenting with two different =\r\nmethods for neural network\n&gt; update in NEAT4J.\n&gt; \n&gt; I&#39;ve used the same sett=\r\nings as Matt&#39;s: tahn(x) activation for hidden\n&gt; nodes and logistic(x) for t=\r\nhe output node.  There are two stop\n&gt; criteria: (1) when the error is below=\r\n 0.1 or (2) the number of\n&gt; generations is greater than 100.\n&gt; \n&gt; For a det=\r\nailed parameters list, please check:\n&gt; http://neat4j.sourceforge.net/docume=\r\nnts/config.html\n&gt; \n&gt; NEAT4J recursively activates the neurons linked to the=\r\n output layer.\n&gt; This is its performance for XOR:\n&gt; \n&gt; 20 runs:\n&gt; ---------=\r\n--------------------------------\n&gt;               Gen.      Hidden      Conn=\r\nections\n&gt; Avg.:      40.9      3.55          8.75\n&gt; Std.:       7.16      1=\r\n.54          2.45\n&gt; -----------------------------------------\n&gt; \n&gt; This is =\r\nmy modified version of NEAT4J (I removed some classes I don&#39;t\n&gt; need and im=\r\nplemented a synchronous updating method for the neural\n&gt; network). The resu=\r\nlts are:\n&gt; \n&gt; -----------------------------------------\n&gt;               Gen=\r\n.      Hidden      Connections\n&gt; Avg.:      18.95     1.6            4.2\n&gt; =\r\nStd.:       4.95      0.82          1.32\n&gt; --------------------------------=\r\n---------\n&gt; \n&gt; The only difference here is that the neural network is updat=\r\ned using\n&gt; information from the previous step only.\n&gt; \n&gt; Please note that f=\r\nollowing this methodology the network has to be\n&gt; activated in a &quot;dynamical=\r\n way&quot;, even for supervised training such as\n&gt; XOR. The input patters are pr=\r\nesented to the network as follows:\n&gt; \n&gt; (0,0) at time 1, (0,1) at time 2, (=\r\n1,0) at time 3 and (1,1) at time 4.\n&gt; \n&gt; At time 1 the output neuron is act=\r\nivated using the activation from the\n&gt; hidden neuron at time t0 (which is s=\r\net to zero) and the second input\n&gt; value, while the hidden neuron only uses=\r\n information from the inputs.\n&gt; \n&gt; At step 2 the output neuron uses the act=\r\nivation of the hidden neuron\n&gt; at time 1 and the second input.\n&gt; \n&gt; And so =\r\non..\n&gt; \n&gt; In ordinary activation methods for feed-forward networks the same=\r\n does\n&gt; not happen. We first need to activate the neuron from the first lay=\r\ner\n&gt; and then proceed to the next layer. To achieve the same behavior we&#39;d\n=\r\n&gt; need to activate the network as many times as the numbers of neurons\n&gt; it=\r\n has.\n&gt; \n&gt; It seems that evolution can take advantage of this method and th=\r\nus,\n&gt; achieve the expected error value in fewer steps exploring the\n&gt; seque=\r\nntial values presented to the inputs.\n&gt; \n&gt; The winner is attached :-)\n&gt; \n&gt; =\r\nCesar\n&gt;\n\n\n\n"}}