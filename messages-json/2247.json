{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":197999825,"authorName":"John Arrowwood","from":"John Arrowwood &lt;jarrowwx@...&gt;","profile":"jarrowwx","replyTo":"LIST","senderId":"m0ebB3ebTUFAtryIvCjRx8WFTY-5S9lISx6O9Pk5k1Kbs-CfZXIXmxCHq6y0d2YuFi-2kNIh4mj_sAGjkie0rMb5TlYcT1WnSAc","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [neat] Re: Introduction---recurrency question","postDate":"1125936597","msgId":2247,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDUxN2ZhNmYxMDUwOTA1MDkwOTNjNDlkZGMxQG1haWwuZ21haWwuY29tPg==","inReplyToHeader":"PDIwMDUwOTA1MTU1MjUyLk03NzM3MUBkZWFyZG9yZmYuY29tPg==","referencesHeader":"PDUxN2ZhNmYxMDUwODMxMTcyNzc1OWU1Y2RjQG1haWwuZ21haWwuY29tPgkgPDUxN2ZhNmYxMDUwOTAxMTUyNTIzYzU2NmJmQG1haWwuZ21haWwuY29tPgkgPDIwMDUwOTAxMjI1NDIxLk03NDg3MkBkZWFyZG9yZmYuY29tPgkgPDUxN2ZhNmYxMDUwOTAyMTYzMjMyOGFlOWE0QG1haWwuZ21haWwuY29tPgkgPDIwMDUwOTAyMjM0NDM0Lk05MTYxM0BkZWFyZG9yZmYuY29tPgkgPDUxN2ZhNmYxMDUwOTAyMTcwMDFmOTNmYTJlQG1haWwuZ21haWwuY29tPgkgPDQzMThGMDFFLjEwNzA2MDlAZHNsLnBpcGV4LmNvbT4gPDIwMDUwOTAzMDI1NjUyLk04OTU4N0BkZWFyZG9yZmYuY29tPgkgPDUxN2ZhNmYxMDUwOTAzMDczNGIzODMzYmRAbWFpbC5nbWFpbC5jb20+CSA8MjAwNTA5MDUxNTUyNTIuTTc3MzcxQGRlYXJkb3JmZi5jb20+"},"prevInTopic":2246,"nextInTopic":2248,"prevInTime":2246,"nextInTime":2248,"topicId":2209,"numMessagesInTopic":42,"msgSnippet":"... Interesting.  I d like to see that borne out.  It would be fantastic to see a neural network that can retain information without being subject to losing it","rawEmail":"Return-Path: &lt;jarrowwx@...&gt;\r\nX-Sender: jarrowwx@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 79834 invoked from network); 5 Sep 2005 16:09:58 -0000\r\nReceived: from unknown (66.218.66.167)\n  by m14.grp.scd.yahoo.com with QMQP; 5 Sep 2005 16:09:58 -0000\r\nReceived: from unknown (HELO wproxy.gmail.com) (64.233.184.206)\n  by mta6.grp.scd.yahoo.com with SMTP; 5 Sep 2005 16:09:58 -0000\r\nReceived: by wproxy.gmail.com with SMTP id i17so758737wra\n        for &lt;neat@yahoogroups.com&gt;; Mon, 05 Sep 2005 09:09:57 -0700 (PDT)\r\nReceived: by 10.54.56.8 with SMTP id e8mr4224201wra;\n        Mon, 05 Sep 2005 09:09:57 -0700 (PDT)\r\nReceived: by 10.54.81.13 with HTTP; Mon, 5 Sep 2005 09:09:57 -0700 (PDT)\r\nMessage-ID: &lt;517fa6f105090509093c49ddc1@...&gt;\r\nDate: Mon, 5 Sep 2005 09:09:57 -0700\r\nTo: neat@yahoogroups.com\r\nIn-Reply-To: &lt;20050905155252.M77371@...&gt;\r\nMime-Version: 1.0\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Transfer-Encoding: quoted-printable\r\nContent-Disposition: inline\r\nReferences: &lt;517fa6f10508311727759e5cdc@...&gt;\n\t &lt;517fa6f1050901152523c566bf@...&gt;\n\t &lt;20050901225421.M74872@...&gt;\n\t &lt;517fa6f10509021632328ae9a4@...&gt;\n\t &lt;20050902234434.M91613@...&gt;\n\t &lt;517fa6f105090217001f93fa2e@...&gt;\n\t &lt;4318F01E.1070609@...&gt; &lt;20050903025652.M89587@...&gt;\n\t &lt;517fa6f10509030734b3833bd@...&gt;\n\t &lt;20050905155252.M77371@...&gt;\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: John Arrowwood &lt;jarrowwx@...&gt;\r\nReply-To: John@...\r\nSubject: Re: [neat] Re: Introduction---recurrency question\r\nX-Yahoo-Group-Post: member; u=197999825; y=F53uiygunVuDRpNQQrDC3yiunYMqQXTi-tpgu4Kdiwczz0c\r\nX-Yahoo-Profile: jarrowwx\r\n\r\nOn 9/5/05, Jeff Haynes &lt;jeff@...&gt; wrote:\n&gt; Well, I&#39;m sure people =\r\nare getting weary of hearing us go on about this\n&gt; particular topic so I&#39;ll=\r\n just say one last thing on it.\n&gt; \n&gt; - One can recreate any arbitrary circu=\r\nit with a static neural network (in case\n&gt; there is any doubt about this, t=\r\nry creating a NAND gate).\n&gt; \n&gt; - Therefore, one can recreate a computer wit=\r\nh a neural network.\n&gt; - A computer can be programmed to learn.\n&gt; - Therefor=\r\ne, a static neural network can learn.\n\nInteresting.  I&#39;d like to see that b=\r\norne out.  It would be fantastic\nto see a neural network that can retain in=\r\nformation without being\nsubject to losing it in the event of a loss of inpu=\r\nts for an extended\nperiod.\n\nOf course, the question of whether the computer=\r\n can be programmed to\n&#39;learn&#39; is debatable.  Kind of like the question of w=\r\nhether or not a\nsubmarine can swim... :)\n\n&gt; Unless there is some error in t=\r\nhat logic, I&#39;m not sure what the real issue is.\n&gt;  If we could refrain from=\r\n talking about nebulous concepts like constructive\n&gt; signal reinforcement i=\r\nn the brain, that would really make this easier for me\n&gt; too.  Thanks, rega=\r\nrds,\n\nSorry if that&#39;s over your head.  It&#39;s an interest of mine.  If you\nha=\r\nve no interest in learning more about it, then we won&#39;t talk about\nit.  But=\r\n if you do, look up &#39;Hebbian Learning&#39; (if memory serves) to\nsee the basic =\r\nprinciple.\n\n"}}