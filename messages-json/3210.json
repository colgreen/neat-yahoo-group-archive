{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":281645563,"authorName":"afcarl2","from":"&quot;afcarl2&quot; &lt;a.carl@...&gt;","profile":"afcarl2","replyTo":"LIST","senderId":"IciQjzLw6l5TjedGXsfMXvWo8Zam1QY_safFZS9Po83AcCdBV2svdpkSf_0_e0p9P_6XaqZmOpcgijYCcXYU_Xs","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Question Re: HyperNEAT Speciate","postDate":"1177800838","msgId":3210,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGYxMGphNis1Y2Z2QGVHcm91cHMuY29tPg==","inReplyToHeader":"PDQyMUZFQjU1LTQwMzItNEQ0Qi1BODk1LTc1N0FBMjQxNjQ4Q0B3YWl0cy5uZXQ+"},"prevInTopic":3209,"nextInTopic":3211,"prevInTime":3209,"nextInTime":3211,"topicId":3197,"numMessagesInTopic":14,"msgSnippet":"Therein lies the possible rub associated w/ the HypercubeNEAT approach. Does the economy of evolving a smaller more complex network more than offset the","rawEmail":"Return-Path: &lt;a.carl@...&gt;\r\nX-Sender: a.carl@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 39450 invoked from network); 28 Apr 2007 22:54:01 -0000\r\nReceived: from unknown (66.218.66.71)\n  by m50.grp.scd.yahoo.com with QMQP; 28 Apr 2007 22:54:01 -0000\r\nReceived: from unknown (HELO n18a.bullet.scd.yahoo.com) (66.94.237.47)\n  by mta13.grp.scd.yahoo.com with SMTP; 28 Apr 2007 22:54:00 -0000\r\nReceived: from [209.73.164.86] by n18.bullet.scd.yahoo.com with NNFMP; 28 Apr 2007 22:53:59 -0000\r\nReceived: from [66.218.66.82] by t8.bullet.scd.yahoo.com with NNFMP; 28 Apr 2007 22:53:59 -0000\r\nDate: Sat, 28 Apr 2007 22:53:58 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;f10ja6+5cfv@...&gt;\r\nIn-Reply-To: &lt;421FEB55-4032-4D4B-A895-757AA241648C@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0\r\nFrom: &quot;afcarl2&quot; &lt;a.carl@...&gt;\r\nSubject: Question Re: HyperNEAT Speciate\r\nX-Yahoo-Group-Post: member; u=281645563; y=u8hn4vTlZ--EX2WecnjQkxa8CWfBRNhBdffEg-lu5mcsHQ\r\nX-Yahoo-Profile: afcarl2\r\n\r\n   Therein lies the possible rub associated w/ the HypercubeNEAT \napproach.=\r\n Does the economy of evolving a smaller more complex network \nmore than off=\r\nset the computational cost of a larger and more \nsimplistic network? It wou=\r\nld seem that the utility of the Hypercube \nnetwork would be undermined with=\r\n changes in the substrate topology \n(i.e. the addition of hidden nodes). Th=\r\ne attractiveness of evolving \nglobal relationships in the Hypercube network=\r\n is self-apparent, but \nthe set of useful problems in which the substrate n=\r\network made up of \nonly input and output nodes seems limiting. Intuition wo=\r\nuld suggest \nthat a evolution genome that comprises both the substrate and =\r\n\nHypercube topology would be appropriate, since until the solution of \nweig=\r\nhts for the substrate network is obtained, they both only have \nmeaning in =\r\nthe context of the other. Presently, the substrate \ntopology is defined a p=\r\nriori, and thus not tracked. This seems to \nharken back to the problems ass=\r\nociated w/ neural networks which NEAT \ncomplexification was designed to add=\r\nress.\n\n\n--- In neat@yahoogroups.com, Stephen Waits &lt;steve@...&gt; wrote:\n&gt;\n&gt; \n=\r\n&gt; On Apr 28, 2007, at 12:28 PM, afcarl2 wrote:\n&gt; \n&gt; &gt; (c) I&#39;m not sure I co=\r\nmmunicated my question adequately. To point,\n&gt; &gt; activation functions w/ mo=\r\nre than one parameter, not just an\n&gt; &gt; operation on the summation of the in=\r\nputs to the node. I believe \nthis\n&gt; &gt; would require either evolved paramete=\r\nrs associated with the\n&gt; &gt; implemented activation function for a given node=\r\n, or the ability \nto\n&gt; &gt; maintain unique multiple inputs, each of which per=\r\nform a \nsummation of\n&gt; &gt; the conntected inputs to a given unique input. Is =\r\nthis stated any\n&gt; &gt; better?\n&gt; \n&gt; This is simply implemented, such that when=\r\never you add a new node,  \n&gt; it&#39;s given some random parameters.\n&gt; \n&gt; Howeve=\r\nr, I believe your adding unnecessary complexity in doing \nthis  \n&gt; (or even=\r\n in supporting multiple activation functions) -- \neffectively  \n&gt; increasin=\r\ng the size of your search space.  But, I haven&#39;t \nresearched  \n&gt; the learni=\r\nng performance difference.\n&gt; \n&gt; --Steve\n&gt;\n\n\n\n"}}