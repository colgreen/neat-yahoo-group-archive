{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":234577593,"authorName":"Oliver Coleman","from":"Oliver Coleman &lt;oliver.coleman@...&gt;","profile":"olivercoleman04","replyTo":"LIST","senderId":"TDUx-vu9tLQaDx3b3C9lU5rwYcp3FL31fagr-us-RCXAJ2J6pZnI-3gpRtHAZcyS9iB6i20ksAR_Wqp_iPibqh__qXk1aWbrhTI9O1lABfI","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [neat] Relevant Paper - Re: New (review) paper on evolving plastic...","postDate":"1347512517","msgId":5864,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PENBK2R1aW1OVEstcStCdzF6RjRBX1FwZHA3am81S0ZiZXVNPXBleUg1Vz12RTBhQkxwd0BtYWlsLmdtYWlsLmNvbT4=","inReplyToHeader":"PGsycXBobSt0dDZxQGVHcm91cHMuY29tPg==","referencesHeader":"PENBK2R1aW1NeUVPaWR3cFpxUTZLZHZlVHM1ZEp1a29oK3c5SG0zRnJIQ1IrYWgxd1BpUUBtYWlsLmdtYWlsLmNvbT4JPGsycXBobSt0dDZxQGVHcm91cHMuY29tPg=="},"prevInTopic":5857,"nextInTopic":5869,"prevInTime":5863,"nextInTime":5865,"topicId":5853,"numMessagesInTopic":7,"msgSnippet":"Hi Ken, Thanks for your kind words. I (humbly) hope the paper inspires more work in the area (though it s already picking up speed). The new paper is very","rawEmail":"Return-Path: &lt;oliver.coleman@...&gt;\r\nX-Sender: oliver.coleman@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 16650 invoked from network); 13 Sep 2012 05:01:59 -0000\r\nX-Received: from unknown (98.137.34.45)\n  by m15.grp.sp2.yahoo.com with QMQP; 13 Sep 2012 05:01:59 -0000\r\nX-Received: from unknown (HELO mail-lpp01m010-f45.google.com) (209.85.215.45)\n  by mta2.grp.sp2.yahoo.com with SMTP; 13 Sep 2012 05:01:58 -0000\r\nX-Received: by lagz14 with SMTP id z14so1972906lag.32\n        for &lt;neat@yahoogroups.com&gt;; Wed, 12 Sep 2012 22:01:57 -0700 (PDT)\r\nMIME-Version: 1.0\r\nX-Received: by 10.112.25.99 with SMTP id b3mr396405lbg.114.1347512517541; Wed,\n 12 Sep 2012 22:01:57 -0700 (PDT)\r\nX-Received: by 10.112.110.72 with HTTP; Wed, 12 Sep 2012 22:01:57 -0700 (PDT)\r\nIn-Reply-To: &lt;k2qphm+tt6q@...&gt;\r\nReferences: &lt;CA+duimMyEOidwpZqQ6KdveTs5dJukoh+w9Hm3FrHCR+ah1wPiQ@...&gt;\n\t&lt;k2qphm+tt6q@...&gt;\r\nDate: Thu, 13 Sep 2012 15:01:57 +1000\r\nMessage-ID: &lt;CA+duimNTK-q+Bw1zF4A_Qpdp7jo5KFbeuM=peyH5W=vE0aBLpw@...&gt;\r\nTo: neat@yahoogroups.com\r\nContent-Type: multipart/alternative; boundary=bcaec554de10f0409604c98e3484\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Oliver Coleman &lt;oliver.coleman@...&gt;\r\nSubject: Re: [neat] Relevant Paper - Re: New (review) paper on evolving plastic...\r\nX-Yahoo-Group-Post: member; u=234577593; y=tfsdGVjmWslNHMlU_cWhaP01ynf2exU2oTlPPhZIqYuZ0jQx1Fv2L76b2qne_xh-bK7OeUcT3A\r\nX-Yahoo-Profile: olivercoleman04\r\n\r\n\r\n--bcaec554de10f0409604c98e3484\r\nContent-Type: text/plain; charset=windows-1252\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\nHi Ken,\n\nThanks for your kind words. I (humbly) hope the paper inspires mor=\r\ne work in\nthe area (though it&#39;s already picking up speed).\n\nThe new paper i=\r\ns very interesting, thanks for mentioning it. I think\nself-organisation pro=\r\ncesses can be a powerful tool, and will probably be\nrequired in some form o=\r\nr other to create an AI. Perhaps we could benefit\nfrom explicitly studying =\r\nit more in the context of neuroevolution (eg how\ncan we evolve neural netwo=\r\nrks that (must) use self-organisation mechanisms\nto form the required netwo=\r\nrk structure or weight pattern; perhaps it\nrepresents a deceptive fitness l=\r\nandscape like (online) learning). Some of\nKhan and Miller&#39;s work evolves ne=\r\nural networks that must self-organise due\nto starting out with random struc=\r\ntures at the start of every life-time. So\nit&#39;s great to have a paper that d=\r\nescribes a neural network model that can\nlearn via an explicit self-organis=\r\ning process.\n\nCheers,\nOliver\n\nOn 13 September 2012 05:58, Ken &lt;kstanley@cs.=\r\nutexas.edu&gt; wrote:\n\n&gt; **\n&gt;\n&gt;\n&gt;\n&gt;\n&gt; Hi Oliver, congrats on your first public=\r\nation. It&#39;s great to have such a\n&gt; comprehensive review of the subject. Of =\r\ncourse the area also has a lot of\n&gt; potential and your paper helps to point=\r\n the way forward.\n&gt;\n&gt; I also wanted to mention a paper relevant to this sub=\r\nject that I published\n&gt; recently with Andrea Soltoggio, based on work he di=\r\nd in part when he was\n&gt; visiting our lab here at UCF. The paper, titled &quot;Fr=\r\nom Modulated Hebbian\n&gt; Plasticity to Simple Behavior Learning through Noise=\r\n and Weight\n&gt; Saturation,&quot; is available here:\n&gt;\n&gt; http://eplex.cs.ucf.edu/p=\r\nublications/2012/soltoggio-nn12\n&gt;\n&gt; It&#39;s important to note that the paper h=\r\nas no evolution - all the\n&gt; experiments are done with small networks that l=\r\nearn entirely from their\n&gt; plasticity rules from scratch. However, I think =\r\nthe unique new plasticity\n&gt; mechanisms that are introduced can probably pro=\r\nvoke a lot of interesting\n&gt; new directions for neuroevolved networks. Part =\r\nof its story also goes\n&gt; against conventional wisdom: It shows how neural n=\r\noise can be an asset and\n&gt; that weight saturation, which is often seen as a=\r\n problem in Hebbian\n&gt; learning, can be turned into an advantage.\n&gt;\n&gt; ken\n&gt;\n=\r\n&gt; --- In neat@yahoogroups.com, Oliver Coleman &lt;oliver.coleman@...&gt; wrote:\n&gt;=\r\n &gt;\n&gt; &gt; I&#39;m proud to announce my first paper produced as part of my PhD will=\r\n\n&gt; &gt; be presented at the 25th Australian Joint Conference on Artificial\n&gt; &gt;=\r\n Intelligence, 4-7 December 2012.\n&gt; &gt;\n&gt; &gt; Title: Evolving Plastic Neural Ne=\r\ntworks for Online Learning: Review\n&gt; &gt; and Future Directions\n&gt; &gt; Authors: O=\r\nliver J. Coleman, Alan D. Blair\n&gt; &gt;\n&gt; &gt; Abstract:\n&gt; &gt; Recent years have see=\r\nn a resurgence of interest in evolving plastic\n&gt; &gt; neural networks for onli=\r\nne learning. These approaches have an\n&gt; &gt; intrinsic appeal =96 since, to da=\r\nte, the only working example of general\n&gt; &gt; intelligence is the human brain=\r\n, which has developed through\n&gt; &gt; evolution, and exhibits a great capacity =\r\nto adapt to unfamiliar\n&gt; &gt; environments. In this paper we review prior work=\r\n in this area =96\n&gt; &gt; including problem domains and tasks, fitness function=\r\ns, synaptic\n&gt; &gt; plasticity models and neural network encoding schemes. We c=\r\nonclude\n&gt; &gt; with a discussion of current findings and promising future dire=\r\nctions,\n&gt; &gt; including incorporation of functional properties observed in\n&gt; =\r\n&gt; biological neural networks which appear to play a role in learning\n&gt; &gt; pr=\r\nocesses, and addressing the &quot;general&quot; in general intelligence by the\n&gt; &gt; in=\r\ntroduction of previously unseen tasks during the evolution process.\n&gt; &gt;\n&gt; &gt;=\r\n The full paper is available at\n&gt; &gt;\n&gt; http://ojcoleman.com/sites/default/fi=\r\nles/files/ojcoleman_ajcai2012_evolving_plastic_neural_networks_for_online_l=\r\nearning_review_and_future_directions.pdf\n&gt; &gt;\n&gt;\n&gt;  \n&gt;\n\r\n--bcaec554de10f0409604c98e3484\r\nContent-Type: text/html; charset=windows-1252\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\nHi Ken,&lt;br&gt;&lt;br&gt;Thanks for your kind words. I (humbly) hope the paper inspir=\r\nes more work in the area (though it&#39;s already picking up speed).&lt;br&gt;&lt;br=\r\n&gt;The new paper is very interesting, thanks for mentioning it. I think self-=\r\norganisation processes can be a powerful tool, and will probably be require=\r\nd in some form or other to create an AI. Perhaps we could benefit from expl=\r\nicitly studying it more in the context of neuroevolution (eg how can we evo=\r\nlve neural networks that (must) use self-organisation mechanisms to form th=\r\ne required network structure or weight pattern; perhaps it represents a dec=\r\neptive fitness landscape like (online) learning). Some of Khan and Miller&#=\r\n39;s work evolves neural networks that must self-organise due to starting o=\r\nut with random structures at the start of every life-time. So it&#39;s grea=\r\nt to have a paper that describes a neural network model that can learn via =\r\nan explicit self-organising process.&lt;br&gt;\n&lt;br&gt;Cheers,&lt;br&gt;Oliver&lt;br&gt;&lt;br&gt;&lt;div =\r\nclass=3D&quot;gmail_quote&quot;&gt;On 13 September 2012 05:58, Ken &lt;span dir=3D&quot;ltr&quot;&gt;&lt=\r\n;&lt;a href=3D&quot;mailto:kstanley@...&quot; target=3D&quot;_blank&quot;&gt;kstanley@...=\r\nexas.edu&lt;/a&gt;&gt;&lt;/span&gt; wrote:&lt;br&gt;&lt;blockquote class=3D&quot;gmail_quote&quot; style=\r\n=3D&quot;margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex&quot;&gt;\n\n\n\n&lt;u&gt;&lt;=\r\n/u&gt;\n\n\n\n\n\n\n\n\n\n\n&lt;div style&gt;\n&lt;span&gt;=A0&lt;/span&gt;\n\n\n&lt;div&gt;\n  &lt;div&gt;\n\n\n    &lt;div&gt;\n    =\r\n  \n      \n      &lt;p&gt;&lt;br&gt;\n&lt;br&gt;\nHi Oliver, congrats on your first publication.=\r\n  It&#39;s great to have such a comprehensive review of the subject.  Of co=\r\nurse the area also has a lot of potential and your paper helps to point the=\r\n way forward.&lt;br&gt;\n&lt;br&gt;\nI also wanted to mention a paper relevant to this su=\r\nbject that I published recently with Andrea Soltoggio, based on work he did=\r\n in part when he was visiting our lab here at UCF.  The paper, titled &quot=\r\n;From Modulated Hebbian Plasticity to Simple Behavior Learning through Nois=\r\ne and Weight Saturation,&quot; is available here:&lt;br&gt;\n\n&lt;br&gt;\n&lt;a href=3D&quot;http=\r\n://eplex.cs.ucf.edu/publications/2012/soltoggio-nn12&quot; target=3D&quot;_blank&quot;&gt;htt=\r\np://eplex.cs.ucf.edu/publications/2012/soltoggio-nn12&lt;/a&gt;&lt;br&gt;\n&lt;br&gt;\nIt&#39;s=\r\n important to note that the paper has no evolution - all the experiments ar=\r\ne done with small networks that learn entirely from their plasticity rules =\r\nfrom scratch.  However, I think the unique new plasticity mechanisms that a=\r\nre introduced can probably provoke a lot of interesting new directions for =\r\nneuroevolved networks.  Part of its story also goes against conventional wi=\r\nsdom:  It shows how neural noise can be an asset and that weight saturation=\r\n, which is often seen as a problem in Hebbian learning, can be turned into =\r\nan advantage.  &lt;br&gt;\n\n&lt;br&gt;\nken&lt;br&gt;\n&lt;br&gt;\n--- In &lt;a href=3D&quot;mailto:neat%40yaho=\r\nogroups.com&quot; target=3D&quot;_blank&quot;&gt;neat@yahoogroups.com&lt;/a&gt;, Oliver Coleman &lt=\r\n;oliver.coleman@...&gt; wrote:&lt;br&gt;\n&gt;&lt;br&gt;\n&gt; I&#39;m proud to announce =\r\nmy first paper produced as part of my PhD will&lt;br&gt;\n&gt; be presented at the=\r\n 25th Australian Joint Conference on Artificial&lt;br&gt;\n&gt; Intelligence, 4-7 =\r\nDecember 2012.&lt;br&gt;\n&gt; &lt;br&gt;\n&gt; Title: Evolving Plastic Neural Networks f=\r\nor Online Learning: Review&lt;br&gt;\n&gt; and Future Directions&lt;br&gt;\n&gt; Authors:=\r\n Oliver J. Coleman, Alan D. Blair&lt;br&gt;\n&gt; &lt;br&gt;\n&gt; Abstract:&lt;br&gt;\n&gt; Rec=\r\nent years have seen a resurgence of interest in evolving plastic&lt;br&gt;\n&gt; n=\r\neural networks for online learning. These approaches have an&lt;br&gt;\n&gt; intri=\r\nnsic appeal =96 since, to date, the only working example of general&lt;br&gt;\n&gt=\r\n; intelligence is the human brain, which has developed through&lt;br&gt;\n&gt; evo=\r\nlution, and exhibits a great capacity to adapt to unfamiliar&lt;br&gt;\n&gt; envir=\r\nonments. In this paper we review prior work in this area =96&lt;br&gt;\n&gt; inclu=\r\nding problem domains and tasks, fitness functions, synaptic&lt;br&gt;\n&gt; plasti=\r\ncity models and neural network encoding schemes. We conclude&lt;br&gt;\n&gt; with =\r\na discussion of current findings and promising future directions,&lt;br&gt;\n&gt; =\r\nincluding incorporation of functional properties observed in&lt;br&gt;\n&gt; biolo=\r\ngical neural networks which appear to play a role in learning&lt;br&gt;\n&gt; proc=\r\nesses, and addressing the &quot;general&quot; in general intelligence by th=\r\ne&lt;br&gt;\n&gt; introduction of previously unseen tasks during the evolution pro=\r\ncess.&lt;br&gt;\n&gt; &lt;br&gt;\n&gt; The full paper is available at&lt;br&gt;\n&gt; &lt;a href=3D=\r\n&quot;http://ojcoleman.com/sites/default/files/files/ojcoleman_ajcai2012_evolvin=\r\ng_plastic_neural_networks_for_online_learning_review_and_future_directions.=\r\npdf&quot; target=3D&quot;_blank&quot;&gt;http://ojcoleman.com/sites/default/files/files/ojcol=\r\neman_ajcai2012_evolving_plastic_neural_networks_for_online_learning_review_=\r\nand_future_directions.pdf&lt;/a&gt;&lt;br&gt;\n\n&gt;&lt;br&gt;\n&lt;br&gt;\n&lt;/p&gt;\n\n    &lt;/div&gt;\n     \n\n  =\r\n  \n    &lt;div style=3D&quot;color:#fff;min-height:0&quot;&gt;&lt;/div&gt;\n\n\n&lt;/div&gt;\n\n\n\n  \n\n\n\n\n\n\n&lt;=\r\n/blockquote&gt;&lt;/div&gt;&lt;br&gt;\n\r\n--bcaec554de10f0409604c98e3484--\r\n\n"}}