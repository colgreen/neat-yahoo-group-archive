{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":7192225,"authorName":"Ian Badcoe","from":"Ian Badcoe &lt;ian_badcoe@...&gt;","profile":"ian_badcoe","replyTo":"LIST","senderId":"wE6-Yz5SgLI3iN-Db7SFnEzv4ICn61808C4PsBVAttKLm0lDkIXuUFjs0wJljDXsqLxwoaJdqw0v7RFLwUbBSd3EBdEWQ1XFUgg","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [neat] Re: Bloat defeated!  (Maybe...)","postDate":"1141823947","msgId":2562,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDIwMDYwMzA4MTMxOTA3LjkzMDkyLnFtYWlsQHdlYjI1MTAyLm1haWwudWtsLnlhaG9vLmNvbT4=","inReplyToHeader":"PGR1amdmbythMjZiQGVHcm91cHMuY29tPg=="},"prevInTopic":2561,"nextInTopic":2578,"prevInTime":2561,"nextInTime":2563,"topicId":2532,"numMessagesInTopic":24,"msgSnippet":"Hi Ken, I m not sure I can reply to all the detailed points in this right now, because I m not sure my ideas are so developed (call my current status informed","rawEmail":"Return-Path: &lt;ian_badcoe@...&gt;\r\nX-Sender: ian_badcoe@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 29424 invoked from network); 8 Mar 2006 13:19:44 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m35.grp.scd.yahoo.com with QMQP; 8 Mar 2006 13:19:44 -0000\r\nReceived: from unknown (HELO web25102.mail.ukl.yahoo.com) (217.12.10.50)\n  by mta5.grp.scd.yahoo.com with SMTP; 8 Mar 2006 13:19:43 -0000\r\nReceived: (qmail 93094 invoked by uid 60001); 8 Mar 2006 13:19:07 -0000\r\nMessage-ID: &lt;20060308131907.93092.qmail@...&gt;\r\nReceived: from [195.152.206.139] by web25102.mail.ukl.yahoo.com via HTTP; Wed, 08 Mar 2006 13:19:07 GMT\r\nDate: Wed, 8 Mar 2006 13:19:07 +0000 (GMT)\r\nTo: neat@yahoogroups.com\r\nIn-Reply-To: &lt;dujgfo+a26b@...&gt;\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=iso-8859-1\r\nContent-Transfer-Encoding: 8bit\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: Ian Badcoe &lt;ian_badcoe@...&gt;\r\nSubject: Re: [neat] Re: Bloat defeated!  (Maybe...)\r\nX-Yahoo-Group-Post: member; u=7192225; y=KQA-9MZhNI-6Dl9tZEeEyjrEcXDZbOd1-fGghdsMH76-sEBA8Q\r\nX-Yahoo-Profile: ian_badcoe\r\n\r\nHi Ken,\n\n    I&#39;m not sure I can reply to all the detailed\npoints in\nthis right now, because I&#39;m not sure my ideas are so\ndeveloped (call my current status &quot;informed\nphenomenology&quot;).  But my generally:\n\n1) &quot;Bloat&quot; and &quot;increasing complexity&quot; are not\n   the same thing.  In our (my) terminology &quot;bloat&quot;\nmeans\n   out-of-control growth of genome size. E.g. which\n   happens whether or not there is a need for it in\nthe\n   search.  I feel that my result when I\n investigated\n   keeping all unmatched genes vs. 50% chance\n of keeping\n   an unmatched gene very clearly illustrated\n that the\n   former was causing an unconditional drive to\n bigger\n   genomes.  A drive which people have had to address\nin\n   various ways to keep their systems \nreasonable.  My\n   &quot;trimming&quot; and Colin&#39;s &quot;phased\n searching&quot;\n both\n   addressed this directly and I believe your\n original\n   speciation\n helps keep this under control\n as a side\n   effect.  Also many people may have\n generally set their\n   add mutations lower to try and\n counter this, when a\n   higher rate might have given a\n better search...\n\n\n2) The dynamic equilibrium approach to add/del\nmutations\n   does give a range of sizes in a population,\nobviously\n   that range will only be moderate unless the\npopulation\n   is huge.  This approach also gives a random\n drift in\n   genome size verses time (see those graphs\n I posted\n   into the groups file area) this constitutes\n an on-going\n   search\n over genome size.\n\n3) everything I discussed in this thread was pretty\n   much without regard to speciation.  So any\nmechanism\n   for keeping (or creating) a range of sizes and\nother\n   diversity via speciation would be something layered\non\n   top of what I did here.  I would like to regard\n   the mechanism I discribed as something that can\napply\n   to each species independently irrespective of what\n   speciation mechanism is in place.  Thus each\nspecies\n   would maintain a different dynamic equilibrium, and\n   focus in a different part of the search space.\n\n   I&#39;m not sure how this would apply with the\ntraditional\n\n   speciation\n, e.g. with individuals being assigned to\n   species rather than the species defined by the\n   individuals in it, but it might work out...\n\n&gt; This is an interesting discussion and I wanted to\nthink\n&gt; about it for a minute at a more general level.\n\n&gt; Basically I&#39;m wondering, how would you guys\narticulate\n&gt; the ideal situation with respect to the distribution\n&gt; and direction of change in complexities in a\npopulation\n&gt; when it is far from a solution?  This seems to be\nharder\n&gt; to pin down than it initially seems.\n\nThere are two cases.  If you are far from the solution\nbut have a clear direction of progress (and for our\npurposes &quot;clear progress&quot; might mean &quot;happening more\noften than once in N generations&quot;) then I wouldn&#39;t say\nwe particularly needed a wide range of complexities.\n\n((or to put it another way, since it is making\nprogress,\n  then the current range of complexities must be OK))\n\nIn the other case, if you are not making progress then\nthere could be a case for fostering a wide range of\ncomplexities _BUT_:\n\ni) just because you haven&#39;t made progress with a\ncertain\n   size, doesn&#39;t mean that you cannot make progress\nwith\n   it.  So would you choose to go immediately for some\n   bigger (expensive) genomes?  I would think that if\n   there is any progress to be made with a simpler\n   genome, then you would want to do that first.\n\n   [[there&#39;s a whole load of reasoning here about the\n     shape of the fitness landscape and the minimum\nsize\n     of a genome required to make _any_ inroad on the\n     problem]]\n\nii) diversity in complexity != diversity in strategy\n   (they intersect, but neither is a subset of the\nother)\n\n\n\niii) how much are we really worried about junk genes?\n   There are two reasons for avoiding them: (a) if\nthey\n   reduce fitness, or (b) if they slow the search.\n   Now these two are different cases for case (a) I\n   think we need not worry, because things which\nreduce\n   fitness are good (e.g. over neutral things which\ntell\n   us\n nothing about the solution space) and will be\n   eliminated by simple selection.\n\n   For case (b), how much should some junk genes slow\n   the search?  There&#39;s two sides to this:  (1)\nslowing\n   of the fitness function, obviously this is a real\n   problem in extreme cases but not for any of the\n   junk genes that can be detected as NOPs and\neliminated\n   from the phenotype.\n\n   The other side is that of slowing the actual search\n   and whilst this is a core consideration of the\n   design of NEAT, and one that I am sure still\napplies,\n   it need not be as severe as it sometimes is.  Junk\n   genes can slow the search by moving us into\n   a more-complex are of the fitness landscape - this\n   is unavoidable but should be self-limiting as long\n   as some less-complex individuals\n remain around\n   (either they can progress, in whcih case they will\n   do it faster than the complex ones, or else we\n_need_\n   to be complex and pay this cost).  The other way\n   that junk genes can slow the search is if they soak\n   up mutations (by definition mostly neutral) which\n   could be usefully expended on functional genes.\n\n   This last point brings us back to the same issue\n   that came up in my original explanation of the\n   kinetics of adding and removing genes.  e.g. as\nlong\n   as mutation is a &quot;per gene&quot; activity and not a \n   &quot;per genome&quot; activity, junk genes will not dilute\n   the mutation of any functional genes, because every\n   gene will have its own mutation allocation (its\n   a mutation allocation situation, in fact, this is\nthe\n   mutation allocation situation explanation).\n\n&gt; I think we&#39;d all agree at first that ideally, you\nwant\n&gt; to find the minimum necessary complexity for a task.\n \n&gt; But the truth is that the general principle of\nfinding\n&gt; the &quot;smallest possible&quot; doesn&#39;t necessarily inform\n&gt; practice to a great extent since we don&#39;t know what\n&gt; the smallest possible is anyway.  It&#39;s rather just a\n\n&gt; theoretical goal.\n\nI&#39;d not even say that _minimum_ complexity was the\ngoal,\nI&#39;d say the real goal is &quot;sufficient simplicity&quot;.\ne.g. as long as the system stays simple enough that it\ncan complete the search, I would not mind if it had\nsome % junk dna in it when the search ended.  It is\nusually very easy to remove junk genes from a genome\n(that&#39;s what my &quot;trimming&quot; approach does).\n \n&gt; A more practical perspective is to ask what is the\n&gt; overall ideal complexity distribution in a situation\n&gt; where you don&#39;t have a solution and don&#39;t know the\n&gt; complexity of a solution?  And it seems to me the\ngoal\n&gt; should be to increase the *range* of complexity in\nthe\n&gt; population.  That is, I would not articulate the\n&gt; objective in terms of average complexity, or max\n&gt; complexity, but in terms of range and distribution\n&gt; (hopefully nicely distributed across that range).\n&gt;\n&gt; The reason is that barring any notion of the &quot;right\n&gt; size,&quot; the right approach is to not put all your\neggs\n&gt; in any one basket.  You have to assume that the\nlonger\n&gt; you search without success, the less you really know\n&gt; about where you should be.  You want to probe the\nwaters\n&gt; of deeper complexity, but you don&#39;t want to give up\non\n&gt; your simple spaces yet either, since any time they\ncould\n&gt; yield gold, especially if they show improvement\n&gt; (although not a solution).\n\nSure but variation over time has to also be crucial\n.\nThese fitness\n landscapes are (by definition or you\nwouldn&#39;t\n use a genetic techique) too complex to sample\nwith a single population at one point in time.  Given\nthat,\none must then take into account the reverse\nconsideration: given that you are localised to a part\n(or several parts with speciation) of the search\nspace,\nyou must sample densely enough within that part to\nreally\nreveal its potential.\n\ne.g. spreading your population too thinly is also bad.\n\n&gt; Or would you disagree with this perspective?  Would\nyou\n&gt; perhaps say that the whole population should move\n&gt; together in a certain direction?\n\nA whole _species_ should move together on a random\nwalk.\nThat is what I dislike about trad speciation, the\nsearch\nfor the right species for each new individuals binds\nthe\nspecies (somewhat) together, so it is the whole\npopulation\nthat searches, not each individual species (obviously\nthat&#39;s a relative thing, it&#39;s not 100% one way or the\nother).\n\n&gt;  It becomes an\n&gt; interesting question with NEAT since NEAT can\nsupport a\n&gt; robust range of complexities, which is different\nfrom \n&gt; GP where the population tends to bump itself up\ntogether\n&gt; because of the free-for-all with no speciation.  \nAnd if\n&gt; the complexity range is really a key factor, then\n&gt; methods that attempt to limit bloat may be taking an\n&gt; overly simplified view because they look at the goal\n\n&gt; with respect to a single value (i.e. max or average\n&gt; complexity) as opposed to a vector of values (the\nwhole\n&gt; set of complexities).   Therefore they be may\nignoring\n&gt; the more significant methodological question of how\nto\n&gt; keep a healthy range, or how rapidly to increase\nthat\n&gt; range in the face of stagnation.\n\nAgain I think you are talking about a &quot;reasonable\ncomplexity range&quot; which we need, I completely agree\nwith\nyou on that.\n\n&quot;Bloat&quot; is different, bloat is the &quot;equilibrium at\ninfinity&quot; which drives to larger and larger genomes.\n\nYou can maintain a reasonable complexity range, even\nin\nthe presence of bloat, but it is harder work and you\nhave\nless control over the fine-turning.\n\n&gt; So what is the right idea?  If you aren&#39;t close to a\n&gt; solution, where do you want complexities to go?  \n\n   I think that in that situation you don&#39;e, and\ncannot\nknow, so you have to try a range of things, but you\ncannot\ntry them all at once.\n\n&gt; ken\n\n\tIan\n\np.s. you can tell it&#39;s a good discussion when it takes\ntwo days to type the reply.\n\n\n\n\t\t\n___________________________________________________________ \nWin a BlackBerry device from O2 with Yahoo!. Enter now. http://www.yahoo.co.uk/blackberry\n\n"}}