{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":151231063,"authorName":"Joseph Reisinger","from":"Joseph Reisinger &lt;joeraii@...&gt;","profile":"joeraii","replyTo":"LIST","senderId":"BVNK6oBKANhk4i_corqVD3pIV7mSAuCn68X5-2IUqUfKItQ2KXvmnZqtGiQ1adUYdA6CteE3-R0Np_SUYNhYagLXVUzO6Fa1jGDlG9s1JQ","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] Paper on evolving modular neural networks","postDate":"1081192961","msgId":613,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PFBpbmUuTE5YLjQuNTguMDQwNDA1MTQwNzE1MC4zMjU2MkBvcmFuZ2UtcGVrb2UuY3MudXRleGFzLmVkdT4=","inReplyToHeader":"PDUuMi4xLjEuMC4yMDA0MDQwNTExMTkxMy4wMTgzNmVhOEBwb3AubWFpbC55YWhvby5jby51az4=","referencesHeader":"PDUuMi4xLjEuMC4yMDA0MDQwNTExMTkxMy4wMTgzNmVhOEBwb3AubWFpbC55YWhvby5jby51az4="},"prevInTopic":611,"nextInTopic":626,"prevInTime":612,"nextInTime":614,"topicId":535,"numMessagesInTopic":47,"msgSnippet":"Ian, ... Yes the network is essentially reset for each move. I did not allow recurrent connections in order to simplify the process of propagating activation,","rawEmail":"Return-Path: &lt;joeraii@...&gt;\r\nX-Sender: joeraii@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 68417 invoked from network); 5 Apr 2004 19:23:08 -0000\r\nReceived: from unknown (66.218.66.216)\n  by m20.grp.scd.yahoo.com with QMQP; 5 Apr 2004 19:23:08 -0000\r\nReceived: from unknown (HELO mail.cs.utexas.edu) (128.83.139.10)\n  by mta1.grp.scd.yahoo.com with SMTP; 5 Apr 2004 19:23:07 -0000\r\nReceived: from orange-pekoe.cs.utexas.edu (joeraii@... [128.83.120.122])\n\tby mail.cs.utexas.edu (8.12.11/8.12.11) with ESMTP id i35JMg74004108\n\tfor &lt;neat@yahoogroups.com&gt;; Mon, 5 Apr 2004 14:22:42 -0500 (CDT)\r\nReceived: (from joeraii@localhost)\n\tby orange-pekoe.cs.utexas.edu (8.12.11/8.12.11/Submit) id i35JMg0o032716;\n\tMon, 5 Apr 2004 14:22:42 -0500\r\nDate: Mon, 5 Apr 2004 14:22:41 -0500 (CDT)\r\nTo: neat@yahoogroups.com\r\nIn-Reply-To: &lt;5.2.1.1.0.20040405111913.01836ea8@...&gt;\r\nMessage-ID: &lt;Pine.LNX.4.58.0404051407150.32562@...&gt;\r\nReferences: &lt;5.2.1.1.0.20040405111913.01836ea8@...&gt;\r\nMIME-Version: 1.0\r\nContent-Type: TEXT/PLAIN; charset=US-ASCII\r\nX-eGroups-Remote-IP: 128.83.139.10\r\nFrom: Joseph Reisinger &lt;joeraii@...&gt;\r\nSubject: Re: [neat] Paper on evolving modular neural networks\r\nX-Yahoo-Group-Post: member; u=151231063\r\nX-Yahoo-Profile: joeraii\r\n\r\nIan,\n\n&gt; \tI&#39;m a bit puzzled about how the networks placed stones.  If the\n&gt; game consisted of placing 5 stones, was the network reset between\n&gt; placing one stone and the next?  Or were the 5 placed stones just the 5\n&gt; highest outputs taken from a single evaluation of the framework?\n\nYes the network is essentially reset for each move. I did not allow\nrecurrent connections in order to simplify the process of propagating\nactivation, so really there is no concept of &quot;reset,&quot; instead one input\nvector yields one output vector.\n\nI also tried the method you describe &quot;5 highest outputs&quot; which tends to\nspeed up evaluation quite a bit, although it is a more difficult problem\nto learn.\n\n&gt; \tPresumably placed stones appeared as 1&#39;s or -1&#39;s in the input just\n&gt; like the setup ones?  Since the latter can be used for scoring, but the\n&gt; former not, how did the networks distinguish between them?  (obviously\n&gt; this would only matter if the stones were placed one at a time)\n\nYes, part of the task is the network learning to distinguish its own\nstones from other stones. This is compounded in the modular version of the\ngame, since this depends on locality.\n\n&gt; \tIf a stone could be placed touching two of the originals, how did\n&gt; that score?\n\nYou essentially iterate over all of the original stones and add a point if\nit is checked. So, therefore you can never have a score higher than the\nnumber of original stones.\n\n&gt; \tI assume that the mapping of board-locations to inputs is entirely\n&gt; anonymous.  e.g. there&#39;s nothing in the system that provides any hint\n&gt; that (1,2) is close to (2,2) or that (1,2) and (2,2) have the same\n&gt; relationship as (2,4) and (3,4)...\n\nThe mutation heuristics are given some sense of locality, i.e. if the\nproblem has a square phenotype, then mutations moving (1,1)-&gt;(1,2) should\noccur equally as likely as (1,1)-&gt;(2,1). One area of research I think\nwould be interesting is to see how this kind of knowledge could be learned\nby the system, instead of added a priori. This would be analogous to\nlearning the symmetries inherent in the problem, however, which turns out\nto be a difficult task.\n\n&gt; Presumably with the framework&lt;-&gt;module mapping, you hope to see reuse of\n&gt; the same module on different areas of the board.  If I am right in my\n&gt; assumption just here, then this would require multiple coordinated\n&gt; mutations (to map the inputs to another area of the same shape).  Isn&#39;t\n&gt; that putting rather a low-probability obstacle in the way?  Did you do\n&gt; any analysis to reveal reuse where the same module is mapped in the same\n&gt; way to different locations?  e.g. Fixed width font required, numbers\n&gt; represent the position of a network&#39;s inputs on the board:\n\nYes, there is no such mutation &quot;move module A right two spaces&quot; ... that\nseems to be the primary critique of the method, and I agree that it would\nbe a promising research venue. However I&#39;m more interested in how this\ncould be accomplished more generally, using a developmental system, then\nhand coded by the programmer.\n\nMy main defense for allowing modules to be bound in different internal\nconfigurations is that it allows for the /possibility/ of discovering\nsymmetries that aren&#39;t obvious to humans. I agree that this lack of\nconstraint makes the search space enormous.\n\n&gt; \tDid you consider letting the framework specify that one network&#39;s\n&gt; input be taken from another&#39;s outputs?\n\nYeah. Actually my original design was much more flexible, allowing for\nmodule hierarchies, and more complex configurations. It seemed too\narbitrary though, having to specify exactly how module interactions had to\ntake place.\n\n\n&gt; I can&#39;t remember now whether you were permitting recurrence in the\n&gt; networks.  Even if you were, you would probably want to forbid it\n&gt; _between_ networks, but feeding one networks output into another would\n&gt; allow one network to encapsulate a low-level concept (like &quot;stone is\n&gt; isolated&quot;) and be used by a higher-level network.  Actually, this is\n&gt; probably more advanced than required by your game.\n\nIf this method were applied to a more complex domain, I think hierarchical\nmodularity would be required. As it is, it is useful for performing some\nkind of repeated computation over a large space, as long as that\ncomputation is relatively simple.\n\n\nJoe\n\n-- \n\nJoseph Reisinger\nhttp://www.cs.utexas.edu/users/joeraii\n\n"}}