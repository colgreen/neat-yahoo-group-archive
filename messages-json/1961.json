{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":183620859,"authorName":"Philip Tucker","from":"Philip Tucker &lt;ptucker@...&gt;","profile":"tucker0171","replyTo":"LIST","senderId":"aRwvTPVjYUi7oN-VfK4ttu0VaUSMnoKsaWwQzpcYqsXuK1Ja_SPHh8OyqtPW3nwsxgP-sWlm1LN16N_Z7uu2hV8vECLA5NVM","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [neat] Digest Number 398","postDate":"1115834656","msgId":1961,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGFiNWU2ZmY3MDUwNTExMTEwNDQ2NzMyYThmQG1haWwuZ21haWwuY29tPg==","inReplyToHeader":"PDExMTU4MzA0ODIuMTYyOC4zOTU4Ni5tMjBAeWFob29ncm91cHMuY29tPg==","referencesHeader":"PDExMTU4MzA0ODIuMTYyOC4zOTU4Ni5tMjBAeWFob29ncm91cHMuY29tPg=="},"prevInTopic":0,"nextInTopic":1963,"prevInTime":1960,"nextInTime":1962,"topicId":1961,"numMessagesInTopic":3,"msgSnippet":"... I think we re dealing with ambiguous terminology here.  The first layer of neurons in ANJI are responsible for nothing more than passing the input data on","rawEmail":"Return-Path: &lt;ptucker@...&gt;\r\nX-Sender: ptucker@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 14598 invoked from network); 11 May 2005 18:04:18 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m25.grp.scd.yahoo.com with QMQP; 11 May 2005 18:04:18 -0000\r\nReceived: from unknown (HELO rproxy.gmail.com) (64.233.170.200)\n  by mta5.grp.scd.yahoo.com with SMTP; 11 May 2005 18:04:17 -0000\r\nReceived: by rproxy.gmail.com with SMTP id r35so91756rna\n        for &lt;neat@yahoogroups.com&gt;; Wed, 11 May 2005 11:04:17 -0700 (PDT)\r\nReceived: by 10.38.73.53 with SMTP id v53mr199028rna;\n        Wed, 11 May 2005 11:04:16 -0700 (PDT)\r\nReceived: by 10.39.1.26 with HTTP; Wed, 11 May 2005 11:04:16 -0700 (PDT)\r\nMessage-ID: &lt;ab5e6ff7050511110446732a8f@...&gt;\r\nDate: Wed, 11 May 2005 13:04:16 -0500\r\nTo: neat@yahoogroups.com\r\nIn-Reply-To: &lt;1115830482.1628.39586.m20@yahoogroups.com&gt;\r\nMime-Version: 1.0\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Transfer-Encoding: quoted-printable\r\nContent-Disposition: inline\r\nReferences: &lt;1115830482.1628.39586.m20@yahoogroups.com&gt;\r\nX-eGroups-Msg-Info: 1:12:0\r\nFrom: Philip Tucker &lt;ptucker@...&gt;\r\nReply-To: Philip Tucker &lt;ptucker@...&gt;\r\nSubject: Re: [neat] Digest Number 398\r\nX-Yahoo-Group-Post: member; u=183620859\r\nX-Yahoo-Profile: tucker0171\r\n\r\n&gt; &gt; If, by input connections, you mean connections\n&gt; &gt; incoming to the inpu=\r\nt\n&gt; &gt; layer,  those don&#39;t have weights, they serve only to\n&gt; &gt; pass the inp=\r\nut\n&gt; &gt; values to the input neurons.\n&gt; &lt;snip&gt;\n&gt; \n&gt; Whoa!  In my experience m=\r\nost ANNs have weights here,\n&gt; lots of them.  It&#39;s often the case that the m=\r\najority\n&gt; of the weights are between the inputs and the first\n&gt; layer of ne=\r\nurons.\n\nI think we&#39;re dealing with ambiguous terminology here.  The first\nl=\r\nayer of neurons in ANJI are responsible for nothing more than passing\nthe i=\r\nnput data on to the rest of the network.  They do have an\nactivation functi=\r\non, but typically this is just linear (i.e., it\neffectively does not have a=\r\nn activation function).\n\nSo, if you do not view these neurons as a layer, t=\r\nhen what I would\ncall the first hidden layer is what you would call the inp=\r\nut layer. \nAnd yes, the inputs to this layer do have weights.\n\nANJI does al=\r\nlow the first layer of neurons to be connected to each\nother if recurrency =\r\nis enabled.  And, in NEAT there really is no\nconcept of layers, just neuron=\r\ns and connections.\n\n- Philip\n\n"}}