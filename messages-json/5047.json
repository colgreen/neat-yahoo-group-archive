{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":211599040,"authorName":"Jeff Clune","from":"Jeff Clune &lt;jclune@...&gt;","profile":"jeffreyclune","replyTo":"LIST","senderId":"5rHcbU3cbOX8FFJbEAgegE10dTVpOQP-QDNUfpwUofrQzsgfMI9iNUoo7sZmSwXwMeeWbmVp_2hpsxGHPP-mKKxE","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Does evolving mutation rates work?","postDate":"1263257758","msgId":5047,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PEM3NzEzMkNFLjJGNzFFJWpjbHVuZUBtc3UuZWR1Pg==","inReplyToHeader":"PDU2YzJmY2UwMDkxMjA4MTQyMnM1ZjM1NDc4MHM2NDYyNmVlMWE2NjE1MzE3QG1haWwuZ21haWwuY29tPg=="},"prevInTopic":5032,"nextInTopic":5048,"prevInTime":5046,"nextInTime":5048,"topicId":4984,"numMessagesInTopic":16,"msgSnippet":"Hello Wesley- Sorry for the delayed reply, but I am catching up on this list and saw your comment (below). Out of curiosity, do you have any evidence that","rawEmail":"Return-Path: &lt;jclune@...&gt;\r\nX-Sender: jclune@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 30012 invoked from network); 12 Jan 2010 00:56:03 -0000\r\nX-Received: from unknown (98.137.34.45)\n  by m12.grp.re1.yahoo.com with QMQP; 12 Jan 2010 00:56:03 -0000\r\nX-Received: from unknown (HELO mail-yw0-f174.google.com) (209.85.211.174)\n  by mta2.grp.sp2.yahoo.com with SMTP; 12 Jan 2010 00:56:03 -0000\r\nX-Received: by mail-yw0-f174.google.com with SMTP id 4so19709530ywh.10\n        for &lt;neat@yahoogroups.com&gt;; Mon, 11 Jan 2010 16:56:03 -0800 (PST)\r\nX-Received: by 10.150.118.20 with SMTP id q20mr5631537ybc.112.1263257763249;\n        Mon, 11 Jan 2010 16:56:03 -0800 (PST)\r\nReturn-Path: &lt;jclune@...&gt;\r\nX-Received: from ?10.0.1.2? (c-76-20-138-10.hsd1.mi.comcast.net [76.20.138.10])\n        by mx.google.com with ESMTPS id 22sm3205269iwn.8.2010.01.11.16.56.01\n        (version=TLSv1/SSLv3 cipher=RC4-MD5);\n        Mon, 11 Jan 2010 16:56:02 -0800 (PST)\r\nUser-Agent: Microsoft-Entourage/12.13.0.080930\r\nDate: Mon, 11 Jan 2010 19:55:58 -0500\r\nTo: &quot;neat@yahoogroups.com&quot; &lt;neat@yahoogroups.com&gt;\r\nMessage-ID: &lt;C77132CE.2F71E%jclune@...&gt;\r\nThread-Topic: Does evolving mutation rates work?\r\nThread-Index: AcqTIgB/0PHtcBaatkeqDutIkbKbow==\r\nIn-Reply-To: &lt;56c2fce00912081422s5f354780s64626ee1a6615317@...&gt;\r\nMime-version: 1.0\r\nContent-type: text/plain;\n\tcharset=&quot;ISO-8859-1&quot;\r\nContent-transfer-encoding: quoted-printable\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Jeff Clune &lt;jclune@...&gt;\r\nSubject: Does evolving mutation rates work?\r\nX-Yahoo-Group-Post: member; u=211599040; y=ety7hyd8rVOSmG7j_hXXs0lLAKkviER2rXRouoB-mIL5RQUvlQp5\r\nX-Yahoo-Profile: jeffreyclune\r\n\r\nHello Wesley-\n\nSorry for the delayed reply, but I am catching up on this li=\r\nst and saw your\ncomment (below). \n\nOut of curiosity, do you have any eviden=\r\nce that self-adaptation of (i.e.,\nevolving) mutation rates works? In the be=\r\nlow paper we found that evolution\nis not able to optimize its mutation rate=\r\n for long term adaptation. Instead,\nit just tries to lower the mutation rat=\r\ne as much as possible.\n\nThat may help evolution home-in on precise answers,=\r\n in the sense of getting\nto the very top of a current (and usually suboptim=\r\nal) fitness peak, but it\ngreatly harms the discovery of other, better fitne=\r\nss peaks. In other words,\nself-adaptation was found in our paper to be extr=\r\nemely greedy and\neffectively eliminate meaningful adaptation.\n\nHere is the =\r\ncite and link:\n\nClune J, Misevic D, Ofria C, Lenski RE, Elena SF, and Sanju=\r\n=E1n, R (2008)\nNatural selection fails to optimize mutation rates for long-=\r\nterm adaptation\non rugged fitness landscapes.\n\nPLoS Computational Biology 4=\r\n(9): e1000187.\n\nhttps://www.msu.edu/~jclune/webfiles/publications/Clune-Evo=\r\nlvingMutationRate\ns-PLoSCB-2008.pdf\n\nI know the Evolutionary Strategies peo=\r\nple like self-adaptation, but I have\nnever seen evidence that it works when=\r\n evolution is controlling the mutation\nrate. Note that evolution is not in =\r\ncontrol in schemes like the 1/5th rule,\nCMA, EDA, etc. \n\n\nCheers,\nJeff\n\n&gt; T=\r\nhis is particularly true if networks are self-adaptive, though I don&#39;t\n&gt; th=\r\nink any NEAT framework supports self-adaptation. Not to hijack the thread,\n=\r\n&gt; but I don&#39;t see any reason why it couldn&#39;t be incorporated. All that&#39;s\n&gt; =\r\nnecessary is for every gene to have its own sigma to be used in the mutatio=\r\nn\n&gt; operator; the custom sigma is mutated as (C# code):\n&gt; \n&gt; Sigma =3D Math=\r\n.Max(0, Sigma * Math.Exp(GaussianMutation(0,1)));\n&gt; //GaussianMutation(mean=\r\n, stdev)\n&gt; \n&gt; Self-adaptation helps evolution home-in on precise answers be=\r\ntter than using\n&gt; a fixed sigma for mutation.\n&gt; \n&gt; Wesley\n&gt; \n&gt; On Tue, Dec =\r\n8, 2009 at 1:58 PM, Colin Green\n&gt; &lt;colin.green1@...&gt;wrote:\n&gt; \n&gt;&gt;=\r\n \n&gt;&gt; \n&gt;&gt; 2009/12/8 snapmedown &lt;snapmedown@... &lt;snapmedown%40yahoo.com=\r\n&gt;&gt;\n&gt;&gt; \n&gt;&gt;&gt; \n&gt;&gt;&gt; What are the thoughts on a neural network that has inputs a=\r\nnd outputs\n&gt;&gt; digitized? Perhaps 8\n&gt;&gt;&gt; bits or even less? This would reduce=\r\n the search space, but encourage\n&gt;&gt; larger structures.\n&gt;&gt; \n&gt;&gt; Hi,\n&gt;&gt; \n&gt;&gt; St=\r\nrictly speaking the signals and weights and already digitized in\n&gt;&gt; that th=\r\ney&#39;re represented by a 32 bit floating point binary\n&gt;&gt; representation (or 6=\r\n4bit for double precision). However your question\n&gt;&gt; essentially then becom=\r\nes how much precision is necessary for a given\n&gt;&gt; problem domain? and that&#39;=\r\ns certainly an interesting question both from\n&gt;&gt; the perspective of the mat=\r\nhs of neural networks but also at the\n&gt;&gt; implementation level where an 8 bi=\r\nt based ANN is is a lot less\n&gt;&gt; computing resource hungry compared to a 64b=\r\nit one of the same\n&gt;&gt; structural size/complexity.\n&gt;&gt; \n&gt;&gt; You might be inter=\r\nested in the short article I wrote abotu\n&gt;&gt; implementing neural nets with i=\r\nnteger maths and /fixed/ point\n&gt;&gt; arithmetic:\n&gt;&gt; \n&gt;&gt; http://sharpneat.sourc=\r\neforge.net/integer_network.html\n&gt;&gt; \n&gt;&gt; In terms of pushing NEAT to it&#39;s lim=\r\nits I think something like 16bit\n&gt;&gt; maths running on CUDA or equivalent pla=\r\ntforms is a logical goal in the\n&gt;&gt; near term. There&#39;s potentially a three o=\r\nrders of magnitude speedup to\n&gt;&gt; be achieved there.\n&gt;&gt; \n&gt;&gt; Colin.\n&gt;&gt;  \n&gt;&gt; \n=\r\n\n\n\n"}}