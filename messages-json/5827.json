{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":234577593,"authorName":"Oliver Coleman","from":"Oliver Coleman &lt;oliver.coleman@...&gt;","profile":"olivercoleman04","replyTo":"LIST","senderId":"nTcHEAEZcCAJ5M8x7I37F47GdTi9kQKBSJ1jY7UW18P7Ewr43Z0vwwZKRTQzXL7YA_jkeQLppOMAUJI6RNORdwYar3LvNNo7F9GryAIKC6Q","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [neat] Re: Models of brains, what should we borrow from biology?","postDate":"1342093717","msgId":5827,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PENBK2R1aW1OMXQyek5MT1QyY0FFMUhzZmNtOFF2ZllrTU53Q2dVX2sxNUpTNU1ZTS1nQUBtYWlsLmdtYWlsLmNvbT4=","inReplyToHeader":"PEMyRTE3MjVELTNEMUMtNDE1QS05QkNGLUIyOEVDQjAxQzRBRkBjb3JuZWxsLmVkdT4=","referencesHeader":"PENBK2R1aW1QWlp3MEJXbjg2Z3hXM2FoSmNVY18yVVhlUkNULWFxMjlpcHpXOWhrUExZQUBtYWlsLmdtYWlsLmNvbT4JPEMyRTE3MjVELTNEMUMtNDE1QS05QkNGLUIyOEVDQjAxQzRBRkBjb3JuZWxsLmVkdT4="},"prevInTopic":5826,"nextInTopic":5828,"prevInTime":5826,"nextInTime":5828,"topicId":5801,"numMessagesInTopic":16,"msgSnippet":"Hi Jeff, Thanks for taking the time to write your thoughts. :) Perhaps I did not describe my intentions clearly: I don t intend to evolve the parameters of the","rawEmail":"Return-Path: &lt;oliver.coleman@...&gt;\r\nX-Sender: oliver.coleman@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 93549 invoked from network); 12 Jul 2012 11:48:38 -0000\r\nX-Received: from unknown (98.137.34.44)\n  by m16.grp.sp2.yahoo.com with QMQP; 12 Jul 2012 11:48:38 -0000\r\nX-Received: from unknown (HELO mail-gh0-f180.google.com) (209.85.160.180)\n  by mta1.grp.sp2.yahoo.com with SMTP; 12 Jul 2012 11:48:38 -0000\r\nX-Received: by ghbz12 with SMTP id z12so2465820ghb.11\n        for &lt;neat@yahoogroups.com&gt;; Thu, 12 Jul 2012 04:48:38 -0700 (PDT)\r\nMIME-Version: 1.0\r\nX-Received: by 10.50.222.134 with SMTP id qm6mr16755130igc.49.1342093717771;\n Thu, 12 Jul 2012 04:48:37 -0700 (PDT)\r\nX-Received: by 10.231.145.147 with HTTP; Thu, 12 Jul 2012 04:48:37 -0700 (PDT)\r\nIn-Reply-To: &lt;C2E1725D-3D1C-415A-9BCF-B28ECB01C4AF@...&gt;\r\nReferences: &lt;CA+duimPZZw0BWn86gxW3ahJcUc_2UXeRCT-aq29ipzW9hkPLYA@...&gt;\n\t&lt;C2E1725D-3D1C-415A-9BCF-B28ECB01C4AF@...&gt;\r\nDate: Thu, 12 Jul 2012 21:48:37 +1000\r\nMessage-ID: &lt;CA+duimN1t2zNLOT2cAE1Hsfcm8QvfYkMNwCgU_k15JS5MYM-gA@...&gt;\r\nTo: neat@yahoogroups.com\r\nContent-Type: multipart/alternative; boundary=14dae93409374d970c04c4a08b4e\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Oliver Coleman &lt;oliver.coleman@...&gt;\r\nSubject: Re: [neat] Re: Models of brains, what should we borrow from biology?\r\nX-Yahoo-Group-Post: member; u=234577593; y=111XrW4PVurzm6Wqh4rqbd2UaW0zW15Da6HxYiyUWw8ROpNNoK1h0HUJ0sTrdRja8lZbj5QnGw\r\nX-Yahoo-Profile: olivercoleman04\r\n\r\n\r\n--14dae93409374d970c04c4a08b4e\r\nContent-Type: text/plain; charset=ISO-8859-1\r\n\r\nHi Jeff,\n\nThanks for taking the time to write your thoughts. :)\n\nPerhaps I did not describe my intentions clearly: I don&#39;t intend to evolve\nthe parameters of the EA, but rather the parameters of some neural network\ncomponents or functions (eq the parameters for synaptic plasticity update\nrules).\n\nYes, I&#39;ve been wondering how best to go about testing the usefulness of\nmultiple functional properties in a model. I&#39;m aware of Miller and Khan&#39;s\nwork. They certainly did throw just about everything into the pot. And it\nwas hard to draw many solid conclusions as a result. I think the best\napproach I&#39;ve come up with so far (and I&#39;m open to criticism or other\nideas) is to create a model with (computationally tractable abstractions\nof) most or all of the biological phenomena/properties for which there is\nevidence of a role in learning, and/or which are computationally cheap, and\nthen perform an ablative study where each property is disabled in turn: if\ndisabling a property reduces the efficacy of the model in some learning\ntask or other (in terms of evolvability, quality of solutions, or some\nother metric), then that property is useful (I believe Stanley and\nMiikkulainen used this approach with their NEAT algorithm). This way one\ndoesn&#39;t have to try every combination of properties, and avoids the problem\nwhere one property might only be useful in combination with one or more\nother properties (which would be a problem if only one property is enabled\nat a time).\n\nComparing the performance of the new model against other existing models on\nthe same learning tasks would help demonstrate whether the new model as a\nwhole is an improvement or not, although like you say this doesn&#39;t avoid\nthe issue of &quot;interaction effect[s] with one of the features in your\nbackdrop&quot;. Perhaps testing against many kinds of learning tasks would help\nalleviate this effect?\n\n[I&#39;m wondering if all of this is a little off-topic for this list...]\n\nCheers,\nOliver\n\nOn 11 July 2012 16:12, Jeff Clune &lt;jeffclune@...&gt; wrote:\n\n&gt; Hello Oliver,\n&gt;\n&gt; I&#39;m much delayed in reading all of this as I have been insanely busy\n&gt; lately, but I have a few thoughts that might help you out:\n&gt;\n&gt; 1) Be careful with meta-evolution (evolving the parameters of evolutionary\n&gt; algorithms). It sounds good in theory, but can be tricky in practice\n&gt; because evolution is short-sighted and conservative, preferring\n&gt; exploitation over exploration, which can be very harmful vis a vis\n&gt; long-term adaptation. Check out my PLoS Computational Biology paper for a\n&gt; smoking gun on this front (the evolution of mutation rates). You may face\n&gt; the exact same problem if you go down this road. [Note, however, that using\n&gt; a divergent search algorithm like novelty search may allow you to take\n&gt; better advantage of meta-evolution: see Joel and Ken&#39;s 2012 alife review\n&gt; article on that subject.]\n\n\n&gt; 2) Another reason people do not throw all of the biology into the soup to\n&gt; see what happens is because scientifically you end up in an impenetrable\n&gt; quagmire where you can&#39;t figure out what is going on and you end up not\n&gt; learning much/anything. The scientific method demands keeping all else\n&gt; equal, and if you have a lot of variables you don&#39;t perfectly understand,\n&gt; it takes years to figure out what is going on if you are lucky! Even if you\n&gt; keep all else equal, if you are doing so against a backdrop that involves a\n&gt; lot of complexity you don&#39;t understand, any difference you see may be due\n&gt; to an interaction effect with one of the features in your backdrop...and\n&gt; that may invalidate generalizing your result to other backdrops of\n&gt; interest. In my limited experience, I have found that most new scientists\n&gt; want to throw a million things into their model--especially biologically\n&gt; motivated phenomena--to see what happens, and as they grow older/more\n&gt; jaded/wiser/more experienced/gun shy/etc. they increasingly keep things as\n&gt; simple as possible. In fact, a pretty good heuristic for good\n&gt; hypothesis-testing science is to keep things absolutely as simple as\n&gt; possible while allowing the question to be asked. However, that may not be\n&gt; a good heuristic for more exploratory science where you just set out and\n&gt; see what you discover.\n\n\n&gt; 3) You should check out Julian Miller&#39;s papers on evolving a checkers\n&gt; player (with his student M. Khan, I believe). Or, better, email/Skype him\n&gt; (he&#39;s an extremely nice guy and I&#39;m sure he would be happy to talk to you).\n&gt; He decided in the last few years that he is running out of time as a\n&gt; scientist and has tenure and he has spent years keeping things as simple as\n&gt; possible, and he now just wants to do what he originally wanted to do when\n&gt; he started: throw as much biology in the soup as possible and see if a\n&gt; golem crawls out. He has incorporated a ton of biologically inspired\n&gt; low-level mechanisms in evolving neural networks. From what I recall,\n&gt; however, it did become very difficult to figure out which ingredients were\n&gt; essential and exactly what was going on because of all the involved\n&gt; complexity. He may have updated results since I last checked in, however.\n&gt; So, you may benefit the actual work that he has done on this front and,\n&gt; more generally, from his opinions on the general scientific approach you\n&gt; are proposing.\n&gt;\n&gt; I hope that helps. Best of luck, and I look forward to hearing what you\n&gt; learn!\n&gt;\n&gt; Best regards,\n&gt; Jeff Clune\n&gt;\n&gt; Postdoctoral Fellow\n&gt; Cornell University\n&gt; jeffclune@...\n&gt; jeffclune.com\n&gt;\n&gt; On May 11, 2012, at 6:34 AM, Oliver Coleman wrote:\n&gt;\n&gt; &gt; Hi Ken,\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; Yes, I&#39;m pretty sure that not all of the phenomena I listed are\n&gt; important; and that a good starting point in general is to assume that they\n&gt; are not. I also agree with your argument that a lot of the low-level\n&gt; phenomena we see may be a result of implementation with particular physical\n&gt; systems (and I would add perhaps as a result of evolutionary happenstance).\n&gt; The CPPN is a particularly compelling example of significant abstraction of\n&gt; developmental processes, producing many of the same features of the end\n&gt; result of developmental processes. One thing it does abstract away, in the\n&gt; context of plastic networks, is the effect of external input on the\n&gt; developmental process (which may or may not be an issue depending on\n&gt; details of implementation, problem domain, etc...).\n&gt; &gt;\n&gt; &gt; Perhaps we could also assume that, rather than some specific set of\n&gt; functions being the only workable set, what matters is having a workable\n&gt; combination of functions, and that there are many possible combinations\n&gt; that would work equally well. In this framework we could assume that\n&gt; biological neural networks represent at least a reasonably good combination\n&gt; of low-level functions, and so we could use this combination as a guide\n&gt; (but of course this doesn&#39;t answer what functions in this combination are\n&gt; actually important, or what things can be abstracted away). Also, some\n&gt; combinations may be workable, but are far harder to evolve solutions with,\n&gt; or require much larger networks, etc (eg evolving networks incorporating\n&gt; neuromodulation of synaptic plasticity can be much easier for some tasks\n&gt; than for those without this type of neuromodulation).\n&gt; &gt;\n&gt; &gt; I&#39;m intending to run some experiments to explore these questions (which\n&gt; phenomena are important, acceptable level of abstraction, etc), but of\n&gt; course to try and thoroughly explore all of these functions in many\n&gt; combinations would be a massive undertaking, and is not my main interest,\n&gt; so at some point I will have to pick a model and run with it after only a\n&gt; few, hopefully well chosen, experiments... Perhaps one approach is to\n&gt; create flexible parameterised versions of these functions, and let\n&gt; evolution determine what combination is right (like your approach described\n&gt; in &quot;Evolving adaptive neural networks with and without adaptive synapses&quot;,\n&gt; but perhaps more flexible and applied to more functions).\n&gt; &gt;\n&gt; &gt; Do you mind if I post/quote some/all of this discussion in the comments\n&gt; of my blog post?\n&gt; &gt;\n&gt; &gt; Cheers,\n&gt; &gt; Oliver\n&gt; &gt;\n&gt; &gt;\n&gt;\n&gt;\n&gt;\n&gt; ------------------------------------\n&gt;\n&gt; Yahoo! Groups Links\n&gt;\n&gt;\n&gt;\n&gt;\n\r\n--14dae93409374d970c04c4a08b4e\r\nContent-Type: text/html; charset=ISO-8859-1\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\nHi Jeff,&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Thanks for taking the time to write your though=\r\nts. :)&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;&lt;div&gt;Perhaps I did not describe my intentio=\r\nns clearly: I don&#39;t intend to evolve the parameters of the EA, but rath=\r\ner the parameters of some neural network components or functions (eq the pa=\r\nrameters for synaptic plasticity update rules).&lt;/div&gt;\n&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;=\r\n&lt;div&gt;Yes, I&#39;ve been wondering how best to go about testing the usefulne=\r\nss of multiple functional properties in a model. I&#39;m aware of Miller an=\r\nd Khan&#39;s work. They certainly did throw just about everything into the =\r\npot. And it was hard to draw many solid conclusions as a result. I think th=\r\ne best approach I&#39;ve come up with so far (and I&#39;m open to criticism=\r\n or other ideas) is to create a model with (computationally tractable abstr=\r\nactions of) most or all of the biological phenomena/properties for which th=\r\nere is evidence of a role in learning, and/or which are computationally che=\r\nap, and then perform an ablative study where each property is disabled in t=\r\nurn: if disabling a property reduces the efficacy of the model in some lear=\r\nning task or other (in terms of evolvability, quality of solutions, or some=\r\n other metric), then that property is useful (I believe Stanley and Miikkul=\r\nainen used this approach with their NEAT algorithm). This way one doesn&#39=\r\n;t have to try every combination of properties, and avoids the problem wher=\r\ne one property might only be useful in combination with one or more other p=\r\nroperties (which would be a problem if only one property is enabled at a ti=\r\nme).&lt;/div&gt;\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Comparing the performance of the new model a=\r\ngainst other existing models on the same learning tasks would help demonstr=\r\nate whether the new model as a whole is an improvement or not, although lik=\r\ne you say this doesn&#39;t avoid the issue of &quot;interaction effect[s] w=\r\nith one of the features in your backdrop&quot;. Perhaps testing against man=\r\ny kinds of learning tasks would help alleviate this effect?&lt;/div&gt;\n&lt;div&gt;&lt;br&gt;=\r\n&lt;/div&gt;&lt;div&gt;[I&#39;m wondering if all of this is a little off-topic for this=\r\n list...]&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Cheers,&lt;/div&gt;&lt;div&gt;Oliver&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;=\r\ndiv class=3D&quot;gmail_quote&quot;&gt;On 11 July 2012 16:12, Jeff Clune &lt;span dir=3D&quot;lt=\r\nr&quot;&gt;&lt;&lt;a href=3D&quot;mailto:jeffclune@...&quot; target=3D&quot;_blank&quot;&gt;jeffclune=\r\n@...&lt;/a&gt;&gt;&lt;/span&gt; wrote:&lt;br&gt;\n&lt;blockquote class=3D&quot;gmail_quote&quot; st=\r\nyle=3D&quot;margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex&quot;&gt;Hello=\r\n Oliver,&lt;br&gt;\n&lt;br&gt;\nI&#39;m much delayed in reading all of this as I have bee=\r\nn insanely busy lately, but I have a few thoughts that might help you out:&lt;=\r\nbr&gt;\n&lt;br&gt;\n1) Be careful with meta-evolution (evolving the parameters of evol=\r\nutionary algorithms). It sounds good in theory, but can be tricky in practi=\r\nce because evolution is short-sighted and conservative, preferring exploita=\r\ntion over exploration, which can be very harmful vis a vis long-term adapta=\r\ntion. Check out my PLoS Computational Biology paper for a smoking gun on th=\r\nis front (the evolution of mutation rates). You may face the exact same pro=\r\nblem if you go down this road. [Note, however, that using a divergent searc=\r\nh algorithm like novelty search may allow you to take better advantage of m=\r\neta-evolution: see Joel and Ken&#39;s 2012 alife review article on that sub=\r\nject.]&lt;/blockquote&gt;\n&lt;blockquote class=3D&quot;gmail_quote&quot; style=3D&quot;margin:0 0 0=\r\n .8ex;border-left:1px #ccc solid;padding-left:1ex&quot;&gt;\n&lt;br&gt;\n2) Another reason =\r\npeople do not throw all of the biology into the soup to see what happens is=\r\n because scientifically you end up in an impenetrable quagmire where you ca=\r\nn&#39;t figure out what is going on and you end up not learning much/anythi=\r\nng. The scientific method demands keeping all else equal, and if you have a=\r\n lot of variables you don&#39;t perfectly understand, it takes years to fig=\r\nure out what is going on if you are lucky! Even if you keep all else equal,=\r\n if you are doing so against a backdrop that involves a lot of complexity y=\r\nou don&#39;t understand, any difference you see may be due to an interactio=\r\nn effect with one of the features in your backdrop...and that may invalidat=\r\ne generalizing your result to other backdrops of interest. In my limited ex=\r\nperience, I have found that most new scientists want to throw a million thi=\r\nngs into their model--especially biologically motivated phenomena--to see w=\r\nhat happens, and as they grow older/more jaded/wiser/more experienced/gun s=\r\nhy/etc. they increasingly keep things as simple as possible. In fact, a pre=\r\ntty good heuristic for good hypothesis-testing science is to keep things ab=\r\nsolutely as simple as possible while allowing the question to be asked. How=\r\never, that may not be a good heuristic for more exploratory science where y=\r\nou just set out and see what you discover.&lt;/blockquote&gt;\n&lt;blockquote class=\r\n=3D&quot;gmail_quote&quot; style=3D&quot;margin:0 0 0 .8ex;border-left:1px #ccc solid;padd=\r\ning-left:1ex&quot;&gt;\n&lt;br&gt;\n3) You should check out Julian Miller&#39;s papers on e=\r\nvolving a checkers player (with his student M. Khan, I believe). Or, better=\r\n, email/Skype him (he&#39;s an extremely nice guy and I&#39;m sure he would=\r\n be happy to talk to you). He decided in the last few years that he is runn=\r\ning out of time as a scientist and has tenure and he has spent years keepin=\r\ng things as simple as possible, and he now just wants to do what he origina=\r\nlly wanted to do when he started: throw as much biology in the soup as poss=\r\nible and see if a golem crawls out. He has incorporated a ton of biological=\r\nly inspired low-level mechanisms in evolving neural networks. From what I r=\r\necall, however, it did become very difficult to figure out which ingredient=\r\ns were essential and exactly what was going on because of all the involved =\r\ncomplexity. He may have updated results since I last checked in, however. S=\r\no, you may benefit the actual work that he has done on this front and, more=\r\n generally, from his opinions on the general scientific approach you are pr=\r\noposing.&lt;br&gt;\n\n&lt;br&gt;\nI hope that helps. Best of luck, and I look forward to h=\r\nearing what you learn!&lt;br&gt;\n&lt;br&gt;\nBest regards,&lt;br&gt;\nJeff Clune&lt;br&gt;\n&lt;br&gt;\nPostd=\r\noctoral Fellow&lt;br&gt;\nCornell University&lt;br&gt;\n&lt;a href=3D&quot;mailto:jeffclune@corne=\r\nll.edu&quot;&gt;jeffclune@...&lt;/a&gt;&lt;br&gt;\n&lt;a href=3D&quot;http://jeffclune.com&quot; targ=\r\net=3D&quot;_blank&quot;&gt;jeffclune.com&lt;/a&gt;&lt;br&gt;\n&lt;div&gt;&lt;div class=3D&quot;h5&quot;&gt;&lt;br&gt;\nOn May 11, =\r\n2012, at 6:34 AM, Oliver Coleman wrote:&lt;br&gt;\n&lt;br&gt;\n&gt; Hi Ken,&lt;br&gt;\n&gt;&lt;br&gt;\n=\r\n&gt;&lt;br&gt;\n&gt; Yes, I&#39;m pretty sure that not all of the phenomena I list=\r\ned are important; and that a good starting point in general is to assume th=\r\nat they are not. I also agree with your argument that a lot of the low-leve=\r\nl phenomena we see may be a result of implementation with particular physic=\r\nal systems (and I would add perhaps as a result of evolutionary happenstanc=\r\ne). The CPPN is a particularly compelling example of significant abstractio=\r\nn of developmental processes, producing many of the same features of the en=\r\nd result of developmental processes. One thing it does abstract away, in th=\r\ne context of plastic networks, is the effect of external input on the devel=\r\nopmental process (which may or may not be an issue depending on details of =\r\nimplementation, problem domain, etc...).&lt;br&gt;\n\n&gt;&lt;br&gt;\n&gt; Perhaps we coul=\r\nd also assume that, rather than some specific set of functions being the on=\r\nly workable set, what matters is having a workable combination of functions=\r\n, and that there are many possible combinations that would work equally wel=\r\nl. In this framework we could assume that biological neural networks repres=\r\nent at least a reasonably good combination of low-level functions, and so w=\r\ne could use this combination as a guide (but of course this doesn&#39;t ans=\r\nwer what functions in this combination are actually important, or what thin=\r\ngs can be abstracted away). Also, some combinations may be workable, but ar=\r\ne far harder to evolve solutions with, or require much larger networks, etc=\r\n (eg evolving networks incorporating neuromodulation of synaptic plasticity=\r\n can be much easier for some tasks than for those without this type of neur=\r\nomodulation).&lt;br&gt;\n\n&gt;&lt;br&gt;\n&gt; I&#39;m intending to run some experiments =\r\nto explore these questions (which phenomena are important, acceptable level=\r\n of abstraction, etc), but of course to try and thoroughly explore all of t=\r\nhese functions in many combinations would be a massive undertaking, and is =\r\nnot my main interest, so at some point I will have to pick a model and run =\r\nwith it after only a few, hopefully well chosen, experiments... Perhaps one=\r\n approach is to create flexible parameterised versions of these functions, =\r\nand let evolution determine what combination is right (like your approach d=\r\nescribed in &quot;Evolving adaptive neural networks with and without adapti=\r\nve synapses&quot;, but perhaps more flexible and applied to more functions)=\r\n.&lt;br&gt;\n\n&gt;&lt;br&gt;\n&gt; Do you mind if I post/quote some/all of this discussio=\r\nn in the comments of my blog post?&lt;br&gt;\n&gt;&lt;br&gt;\n&gt; Cheers,&lt;br&gt;\n&gt; Olive=\r\nr&lt;br&gt;\n&gt;&lt;br&gt;\n&gt;&lt;br&gt;\n&lt;br&gt;\n&lt;br&gt;\n&lt;br&gt;\n&lt;/div&gt;&lt;/div&gt;------------------------=\r\n------------&lt;br&gt;\n&lt;br&gt;\nYahoo! Groups Links&lt;br&gt;\n&lt;br&gt;\n&lt;*&gt; To visit your =\r\ngroup on the web, go to:&lt;br&gt;\n=A0 =A0 &lt;a href=3D&quot;http://groups.yahoo.com/gro=\r\nup/neat/&quot; target=3D&quot;_blank&quot;&gt;http://groups.yahoo.com/group/neat/&lt;/a&gt;&lt;br&gt;\n&lt;br=\r\n&gt;\n&lt;*&gt; Your email settings:&lt;br&gt;\n=A0 =A0 Individual Email | Traditional=\r\n&lt;br&gt;\n&lt;br&gt;\n&lt;*&gt; To change settings online go to:&lt;br&gt;\n=A0 =A0 &lt;a href=3D=\r\n&quot;http://groups.yahoo.com/group/neat/join&quot; target=3D&quot;_blank&quot;&gt;http://groups.y=\r\nahoo.com/group/neat/join&lt;/a&gt;&lt;br&gt;\n=A0 =A0 (Yahoo! ID required)&lt;br&gt;\n&lt;br&gt;\n&lt;=\r\n*&gt; To change settings via email:&lt;br&gt;\n=A0 =A0 &lt;a href=3D&quot;mailto:neat-dige=\r\nst@yahoogroups.com&quot;&gt;neat-digest@yahoogroups.com&lt;/a&gt;&lt;br&gt;\n=A0 =A0 &lt;a href=3D&quot;=\r\nmailto:neat-fullfeatured@yahoogroups.com&quot;&gt;neat-fullfeatured@yahoogroups.com=\r\n&lt;/a&gt;&lt;br&gt;\n&lt;br&gt;\n&lt;*&gt; To unsubscribe from this group, send an email to:&lt;b=\r\nr&gt;\n=A0 =A0 &lt;a href=3D&quot;mailto:neat-unsubscribe@yahoogroups.com&quot;&gt;neat-unsubsc=\r\nribe@yahoogroups.com&lt;/a&gt;&lt;br&gt;\n&lt;br&gt;\n&lt;*&gt; Your use of Yahoo! Groups is su=\r\nbject to:&lt;br&gt;\n=A0 =A0 &lt;a href=3D&quot;http://docs.yahoo.com/info/terms/&quot; target=\r\n=3D&quot;_blank&quot;&gt;http://docs.yahoo.com/info/terms/&lt;/a&gt;&lt;br&gt;\n&lt;br&gt;\n&lt;/blockquote&gt;&lt;/d=\r\niv&gt;&lt;br&gt;&lt;/div&gt;\n\r\n--14dae93409374d970c04c4a08b4e--\r\n\n"}}