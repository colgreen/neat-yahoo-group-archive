{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"rfiGGZa6PaSFmqTkR7-ATwZ98YyN6MnlOYugCEyiheKuNuu0KVQ9-8CSZWKNCT0gmeInfUJ1YCAbP3OsAS9OAMqicelyFm9mM661iJUkRR9x","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: leaky integrator neuron","postDate":"1099082076","msgId":1661,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGNsdTlncytldTVqQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDUuMS4wLjE0LjAuMjAwNDEwMjkyMTIyMzYuMDI0N2FkNjhAcG9wLm1haWwueWFob28uY28udWs+"},"prevInTopic":1660,"nextInTopic":1663,"prevInTime":1660,"nextInTime":1662,"topicId":1656,"numMessagesInTopic":9,"msgSnippet":"Guys, it should be noted that Ian s equation is not the actual leaky integrator update equation.  Leaky integrators multiply both the activation level from the","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 5416 invoked from network); 29 Oct 2004 20:35:27 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m6.grp.scd.yahoo.com with QMQP; 29 Oct 2004 20:35:27 -0000\r\nReceived: from unknown (HELO n6a.bulk.scd.yahoo.com) (66.94.237.40)\n  by mta5.grp.scd.yahoo.com with SMTP; 29 Oct 2004 20:35:27 -0000\r\nReceived: from [66.218.69.3] by n6.bulk.scd.yahoo.com with NNFMP; 29 Oct 2004 20:34:37 -0000\r\nReceived: from [66.218.67.162] by mailer3.bulk.scd.yahoo.com with NNFMP; 29 Oct 2004 20:34:37 -0000\r\nDate: Fri, 29 Oct 2004 20:34:36 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;clu9gs+eu5j@...&gt;\r\nIn-Reply-To: &lt;5.1.0.14.0.20041029212236.0247ad68@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Length: 3081\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Remote-IP: 66.94.237.40\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: Re: leaky integrator neuron\r\nX-Yahoo-Group-Post: member; u=54567749\r\nX-Yahoo-Profile: kenstanley01\r\n\r\n\nGuys, it should be noted that Ian&#39;s equation is not the actual leaky \nintegrator update equation.  Leaky integrators multiply both the \nactivation level from the previous timestep *and* the incoming \nactivation by a time constant that control how fast activation leaks \nout and comes in.\n\nActually, (I had forgotten to mention..) my dissertation has a \nuseful introductory section on leaky integrator neurons in its \ndiscussion section:\n\nhttp://nn.cs.utexas.edu/keyword?stanley:phd04\n\nSee section 10.4.2.\n\nken\n\n--- In neat@yahoogroups.com, Ian Badcoe &lt;ian_badcoe@y...&gt; wrote:\n&gt; At 01:59 29/10/2004 +0100, you wrote:\n&gt; \n&gt; &gt;Ian Badcoe wrote:\n&gt; &gt;\n&gt; &gt; &gt;Hi,\n&gt; &gt; &gt;         I&#39;ve never heard of it, but an obvious interpretation \nsprings from\n&gt; &gt; &gt;the name...\n&gt; &gt; &gt;\n&gt; &gt; &gt;         What it sounds like, is a neurone with some &quot;memory&quot; \nin that it\n&gt; &gt; &gt;keeps a variable from update to update, and its update function \nwould look\n&gt; &gt; &gt;something like:\n&gt; &gt; &gt;\n&gt; &gt; &gt;         var&#39; = var * f + weighted_sum(input);           f &lt; 1.0\n&gt; &gt; &gt;\n&gt; &gt; &gt;         So var integrates input across updates, and f acts to \nslowly leak\n&gt; &gt; &gt;away the old contents...\n&gt; &gt; &gt;\n&gt; &gt; &gt;         whether the output is var itself or f(var) is, I \nguess, your \n&gt; &gt; choice...\n&gt; &gt; &gt;\n&gt; &gt; &gt;         Ian\n&gt; &gt; &gt;\n&gt; &gt; &gt;p.s. presumably you can built one of these from a few standard \nneurones?\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt;Absolutely. From your explanation I would say that this same \neffect\n&gt; &gt;could be achieved with a recursive connection from a neuron&#39;s \noutput\n&gt; &gt;back into its input. So from that perspective you could say that \nNEAT as\n&gt; &gt;it stands can represent this type of functionality through its \ntopology\n&gt; &gt;searching propoperties. However, relying on topology searching to \nfind\n&gt; &gt;solutions might not be as effective as actually defining a leaky\n&gt; &gt;integrater neuron directly. This comes back to our previous \n[brief]\n&gt; &gt;discussion regarding implementing different types of neuron and\n&gt; &gt;activation function - which I hope to look into soon.\n&gt; \n&gt; Except after I suggest this, it occurred to me that completely \nstandard \n&gt; neurones would put unwanted sigmoids onto the definition.  e.g. \ntaking your \n&gt; idea of a single feedback connection within a single node we would \nhave:\n&gt; \n&gt; \n&gt;           var&#39; = sigmoid(var * f + weighted_sum(input));\n&gt; \n&gt; Rather than:\n&gt; \n&gt;           var&#39; = var * f + weighted_sum(input);\n&gt; \n&gt; [[var * f is the effect of the feedback connection, obviously one \nwould \n&gt; normally just regard this as part of the weighted sum over inputs \nbut I&#39;m \n&gt; writting it separately so we can see it :) ]]\n&gt; \n&gt; Which brings us back to the point we had briefly some months ago, \nwhere \n&gt; some equations fit reasonably nicely into an ANN framework, but \nbecause \n&gt; they depend so heavily on the linear portion of of the sigmoids, \ntheir \n&gt; weights are constrained so much that it is almost impossible to \nevolve \n&gt; them.  Remember when we discusses whether one could evolve \nnetworks for A + \n&gt; B or A * B using sigmoidal neurones?\n&gt; \n&gt;          Ian\n&gt; \n&gt; \n&gt; \n&gt; http://livingathome.sourceforge.net/ - evolution for the desktop\n\n\n\n\n"}}