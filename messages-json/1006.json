{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"u1uKvhNV2OfGM9WgPi4i00rUVn0XFr8rJiTMxSeD2pHE3KRXHiSF9UwpbckDrBr6BcuRnJNGGbSx7P5vp9t0rzaTQb6SsWDudx_GKnBdhI5V","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: Bloat","postDate":"1086554289","msgId":1006,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGM5dnZiaCt1MmNxQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGdpbGxhbS0wUlYrZUFha0hybW9PSjR6SEMwdXA2Skd2eC9nZVJyQG1haWxibG9ja3MuY29tPg=="},"prevInTopic":1005,"nextInTopic":1007,"prevInTime":1005,"nextInTime":1007,"topicId":904,"numMessagesInTopic":68,"msgSnippet":"Michael, I agree that one of the next big steps will be towards finding better translations between genotype and phenotype (in GA terms this is called","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 35219 invoked from network); 6 Jun 2004 20:40:18 -0000\r\nReceived: from unknown (66.218.66.167)\n  by m22.grp.scd.yahoo.com with QMQP; 6 Jun 2004 20:40:18 -0000\r\nReceived: from unknown (HELO n1.grp.scd.yahoo.com) (66.218.66.64)\n  by mta6.grp.scd.yahoo.com with SMTP; 6 Jun 2004 20:40:18 -0000\r\nReceived: from [66.218.67.250] by n1.grp.scd.yahoo.com with NNFMP; 06 Jun 2004 20:38:11 -0000\r\nDate: Sun, 06 Jun 2004 20:38:09 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;c9vvbh+u2cq@...&gt;\r\nIn-Reply-To: &lt;gillam-0RV+eAakHrmoOJ4zHC0up6JGvx/geRr@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Length: 17010\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-eGroups-Remote-IP: 66.218.66.64\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: Re: Bloat\r\nX-Yahoo-Group-Post: member; u=54567749\r\nX-Yahoo-Profile: kenstanley01\r\n\r\nMichael, I agree that one of the next big steps will be towards \nfinding better translations between genotype and phenotype (in GA \nterms this is called &quot;encoding&quot;, and &quot;indirect encoding&quot; refers to \nencodings that are more complex mappings).  There are two ways to go \nabout this.  One way is to try to let the GA search for the encoding \nitself, trying out different forms of translation early in \nevolution.  I know Joe Reisinger, who is on this group, has been \nthinking about this approach.\n\nThe other way is for us to try lots of different encoding \nmechanisms, some biologically-motivated and some not, and compare \nthem systematically, so we can begin to develop a theory of what \nworks well and what doesn&#39;t.  That&#39;s generally what our paper, &quot;A \nTaxonomy for Artificial Embryogeny&quot; is about, retrievable here:\n\nhttp://nn.cs.utexas.edu/keyword?stanley:alife03\n\nI&#39;m not sure if I may have pointed you to this paper previously in \nemail, but if you haven&#39;t seen it before you may find it \ninteresting.  We also have a more recent position statement coming \nout at GECCO on this topic:\n\nhttp://nn.cs.utexas.edu/keyword?stanley:gecco04ws\n\nIt is my hope that NEAT will eventually be combined with an \neffective indirect encoding to produce a complexifying developmental \nsystem, a class of systems which so far have almost never been \nexplored.\n\nken\n\n\n--- In neat@yahoogroups.com, &quot;Michael Gillam&quot; &lt;gillam@m...&gt; wrote:\n&gt; \n&gt; Excellent post Ken - and I agree completely (from a biologic and \nintuitively\n&gt; computational perspective.)\n&gt; \n&gt; Your statements are what attracted my interest originally to \nNEAT.  I built\n&gt; a significantly simplified but ideological similar system to NEAT \nback in\n&gt; medical school almost 10 1/2 years ago(!)  (An old computer bag \nwith my\n&gt; Apple Powerbook 170 recently yielded the disks on which it was \nwritten!)\n&gt; \n&gt; It seemed clear at that time that neural networks were mired in \nrigid\n&gt; pre-defined structures that did not adequately imitate the neural\n&gt; complexification we see in the fossil and evolutionary record..  I\n&gt; affectionately called the project Prevolve to represent this idea \nof taking\n&gt; a step-backwards (away fro pre-defined rigid structures) to move \nforwards.\n&gt; Unfortunately, Prevolve took a back seat to other strenuous \ndemands for\n&gt; time... : ( ...but I am glad to see that an amazingly active group \nof\n&gt; programmers have independently created their own torch and are \nleading the\n&gt; way into this Great Frontier.  Ironically, a striking example of \nconvergent\n&gt; cognitive evolution, if I ever saw one!\n&gt; \n&gt; This leads me to my second point.\n&gt; \n&gt; The next great leap forward just might be taking another big step \nbackwards\n&gt; from what we rigidly define a priori.    Many systems today \n*rigidly* define\n&gt; the translation steps from genome to neural phenome.  How do we \nknow that\n&gt; our translation of the genetic code of our organisms into neural \nstructure\n&gt; is *the* optimal translation algorithm?  Perhaps other translation \nmethods\n&gt; that use structures like homeotic boxes, replicative units, \noptimally\n&gt; balanced &#39;exxons&#39; or others might more optimally step through \ntopologies.\n&gt; Early work in Artificial Life strongly suggests that life was \ncreated and\n&gt; destroyed millions perhaps trillions of times.  Early life \ncompeted in a\n&gt; palette of likely vastly different translation methods of their \ngenotypes to\n&gt; neural phenotypes.  Ultimately, the most rapid system for \ntopological and\n&gt; phenotypical evolution likely triumphed and is the system we see \ntoday.\n&gt; \n&gt; Call this process Genotypic Evolutionary Translation (GET).  \nThough many\n&gt; might dismiss GET as unnecessary, strong evidence points to its \nneed.\n&gt; Perhaps most strongly is the fact that the only evolutionary \nsystem we know\n&gt; of that has successfully evolved sentience, had GET in its \ntoolbox.  : )\n&gt; \n&gt; To me, creating GET is one of the pre-eminent intellectual \nchallenges of\n&gt; neural-evolution today.  Kudos to the individual who solves this \nconundrum\n&gt; and is able to still obtain the evolutionary simulation speeds we \nsee today.\n&gt; \n&gt; -Mike\n&gt; ------------------------------------------------------------\n&gt; Michael Gillam, MD\n&gt; Director, Medical Media Lab (http://imedi.org)\n&gt; Research Director, National Center for Emergency Medicine \nInformatics\n&gt; (http://ncemi.org)\n&gt; Washington D.C.\n&gt; \n&gt; Informatics Director, Division of Emergency Medicine\n&gt; Evanston Northwestern Healthcare\n&gt; \n&gt; Chair, Section on Web Education\n&gt; Society of Academic Emergency Medicine\n&gt; \n&gt; \n&gt; \n&gt; \n&gt; \n&gt; \n&gt; ----- Original Message ----- \n&gt; From: &quot;Kenneth Stanley&quot; &lt;kstanley@c...&gt;\n&gt; To: &lt;neat@yahoogroups.com&gt;\n&gt; Sent: Saturday, June 05, 2004 8:06 PM\n&gt; Subject: [neat] Re: Bloat\n&gt; \n&gt; \n&gt; &gt; Jim, I hope you will allow me a rather long and detailed \nresponse to\n&gt; &gt; your point.  I feel this is the right time for me to respond\n&gt; &gt; broadly, since you have touched on the central theme behind much\n&gt; &gt; discussion on this group, and ultimately behind my own \nmotivations\n&gt; &gt; for introducing NEAT.  Therefore, forgive me for a long-winded\n&gt; &gt; response, but one I would to get on the record.\n&gt; &gt;\n&gt; &gt; I doubt that the importance of topology can be overstated.  That\n&gt; &gt; said, I want to concede up front that there is no question that \nmost\n&gt; &gt; of the key steps in exploration are through weight mutation, and\n&gt; &gt; that weight mutation will get you far.  In fact, there are very\n&gt; &gt; sophisticated methods for altering the weight mutation \ndistribution\n&gt; &gt; to point it in more promising directions, and these methods can \nbe\n&gt; &gt; quite powerful.\n&gt; &gt;\n&gt; &gt; Nevertheless, weight mutation is no more than exploring a fixed\n&gt; &gt; space, and exploring a fixed space is well understood and tried \nand\n&gt; &gt; tested.  In fact, it is proven that there  is only so much a \nblack\n&gt; &gt; box method can do to explore a space.  No method can promise \nalways\n&gt; &gt; to escape local optima, and no method ever will make such a \npromise\n&gt; &gt; (so says the No Free Lunch Theorem).\n&gt; &gt;\n&gt; &gt; There are fundamental questions at the core of AI that fixed-\nspace\n&gt; &gt; exploration can never address.  Most perplexing and fundamental \nis\n&gt; &gt; the question of what space should we be exploring in the first\n&gt; &gt; place?  Fogel tried 3 topologies (and probably more, off the\n&gt; &gt; ecord).  But where did those topologies come from?  What was the\n&gt; &gt; basis of the decisions to use them?  Isn&#39;t our mission, as\n&gt; &gt; researchers in AI, to make *that* decision automatic?  After all,\n&gt; &gt; *that* decision- the decision of what topology to search, i.e. \nwhat\n&gt; &gt; space to search in- is really the only hard decision, the one \nthat\n&gt; &gt; requires &quot;intelligence&quot;.  It is a relatively trivial matter, once\n&gt; &gt; you know what to search, just to go searching.  The fact that \nweight\n&gt; &gt; mutation alone (once the correct topology has been identified) is\n&gt; &gt; sufficient to solve checkers says more about checkers and human\n&gt; &gt; intelligence (intelligence for choosing the right space to \nsearch)\n&gt; &gt; than about the prowess of weight mutation.\n&gt; &gt;\n&gt; &gt; Yet this is not only a philosophical argument about what AI \nshould\n&gt; &gt; be able to do automatically.  It is also a critical practical\n&gt; &gt; matter.  Contrary to your reasoning, the real danger is not in\n&gt; &gt; adding a single dimension to a search space, but in beginning \nsearch\n&gt; &gt; in a bad space in the first place.  If you are concerned that\n&gt; &gt; addition of a single dimension has some exponential expense \n(which I\n&gt; &gt; believe is not correct anyway), what cost then must there be in\n&gt; &gt; searching in a topology with dozens or even hundreds of \nunnecessary\n&gt; &gt; dimensions?  The effect on search could be catastrophic.\n&gt; &gt;\n&gt; &gt; And yet for most difficult problems we have not the slightest \nidea\n&gt; &gt; what the right space is to search, other than that it is large.  \nHow\n&gt; &gt; many dimensions are in the brain of a robotic maid?  Surely at \nleast\n&gt; &gt; thousands; maybe millions.  Should we begin search then in a \nnetwork\n&gt; &gt; of a million connections?  Weight space exploration offers no\n&gt; &gt; comfort: The search is intractable in million dimensional space,\n&gt; &gt; even if the solution is somewhere within.\n&gt; &gt;\n&gt; &gt; Yet even as there is danger from above in the form of too-high\n&gt; &gt; dimensional space, there is danger from below in spaces of too-\nfew\n&gt; &gt; dimensions, where a solution may not even exist.  What if Fogel \nhad\n&gt; &gt; chosen to search in networks with 2 fewer neurons?  5 fewer? At \nsome\n&gt; &gt; point, the good player just doesn&#39;t exist in that space anymore.\n&gt; &gt; But how could we know this in advance?  There is no analysis that\n&gt; &gt; can tell us a priori how many dimensions we need.  And if we try \nto\n&gt; &gt; go lean and get just the right amount, we might miss the boat\n&gt; &gt; entirely, even by a single connection, and end up searching \nforever\n&gt; &gt; in futility in a space without a solution.\n&gt; &gt;\n&gt; &gt; Worse, even if we knew *exactly* the minimal number of \nconnections\n&gt; &gt; necessary to solve a problem *and* the perfect topology, even \nthen,\n&gt; &gt; if the space is too large, weight mutation alone is likely to \nfail.\n&gt; &gt; The problem is, where in a large space do you *begin* to search? \nAnd\n&gt; &gt; that problem is impossible to address since by definition you \ndon&#39;t\n&gt; &gt; know anything about the space before you begin searching!\n&gt; &gt; Therefore, in a high-dimensional space, you are highly likely to\n&gt; &gt; begin in an unpromising part of the space; it&#39;s simply too large.\n&gt; &gt;\n&gt; &gt; Therefore, to begin minimally and complexify into the the proper\n&gt; &gt; space is addressing a fundamental issue and I believe is \nultimately\n&gt; &gt; unavoidable as a critical component of any black box search for\n&gt; &gt; complex behaviors.  Rather than adding expense as you imply, it \nis\n&gt; &gt; reducing expense by spending most of search in lower-dimensional\n&gt; &gt; space than the final solution.  A complexifying method only may \nbe\n&gt; &gt; searching in the space of the final solution for 10% of the run.\n&gt; &gt; Fixed-topology search spends 100% of the run in the high \ndimensional\n&gt; &gt; space of the final solution, which, according to your formulation\n&gt; &gt; should incur an incomprehensibly vast exponential penalty.\n&gt; &gt;\n&gt; &gt; I think ultimately what you are misunderstanding is that NEAT is \nnot\n&gt; &gt; an attempt to search in high-dimensional space.  It is a method \nfor\n&gt; &gt; spending most of your search in *lower-dimensional space* than \nthe\n&gt; &gt; final solution, and complexifying up to the complexity of the \nfinal\n&gt; &gt; solution.  The goal is to be able to find solutions that *exist* \nin\n&gt; &gt; high-dimensional space.  That&#39;s not the same as a goal of \nsearching\n&gt; &gt; directly in high-dimensional space no matter the problem.  The\n&gt; &gt; latter goal is the antithesis of what NEAT is about.  NEAT is\n&gt; &gt; designed to avoid searching in unnecessarily high-dimensional \nspace.\n&gt; &gt;\n&gt; &gt; Thus, I feel strongly that the idea of searching through \ntopologies\n&gt; &gt; must be taken seriously, and should not be viewed as merely\n&gt; &gt; a &quot;fun&quot;, &quot;sexy,&quot; or &quot;somwhat spatially interesting&quot;  \nrecreation.  It\n&gt; &gt; is not mere intellectual exercise.  Prior topology-evolving \nsystems\n&gt; &gt; before NEAT were perhaps better targets for your criticism, since\n&gt; &gt; they were essentially aimed at flipping through random topologies\n&gt; &gt; unsystematically for its own sake.  However, NEAT is designed to \nto\n&gt; &gt; use topology as a way of minimizing dimensionality in search, and\n&gt; &gt; ultimately to automatically address that fundamental question of\n&gt; &gt; what space to be searching in, a completely different endeavor.\n&gt; &gt;\n&gt; &gt; (\n&gt; &gt;\n&gt; &gt; A couple side notes:\n&gt; &gt;\n&gt; &gt; I agree that structural mutation needs to be relatively rare.  In\n&gt; &gt; NEAT, it is generally 5% or lower.  Years of experimentation with\n&gt; &gt; NEAT have gone into testing different rates of structure-adding.\n&gt; &gt;\n&gt; &gt; Finally, I believe your mathematical formulation is incorrect.\n&gt; &gt; Adding a dimensions to an already-partially-optimized structure\n&gt; &gt; certainly does not incur exponential expense in the search \nprocess.\n&gt; &gt; In fact, the effect can be quite the opposite, adding new routes \noff\n&gt; &gt; the top of a local optimum.\n&gt; &gt;\n&gt; &gt; Not to be picky, and this isn&#39;t really important, but here&#39;s what\n&gt; &gt; doesn&#39;t make sense to me about your formal argument:\n&gt; &gt;\n&gt; &gt; -&quot;What is occurring to me is that just doing weight mutation is a\n&gt; &gt; search at a rate X in a huge space.&quot;  How do you define &quot;search \nat\n&gt; &gt; rate X?&quot;  This does not seem to mean anything formally speaking.\n&gt; &gt; What are the units of search rate?  How is it derived?\n&gt; &gt;\n&gt; &gt; -&quot;And it seems to me adding topological variation is not just a\n&gt; &gt; multiplier, but an exponent increasing X.&quot;  If X is a rate (as \nyou\n&gt; &gt; defined it), then increasing X means the rate becomes faster.  \nSo I\n&gt; &gt; assume X is not a rate.  But then what is it?\n&gt; &gt;\n&gt; &gt; -&quot;topological complexity curve for having a more fit player is\n&gt; &gt; exponetial&quot;  What is a topological complexity curve?  Is it based\n&gt; &gt; somehow on rate X?  Complexity is usually defined as the size of \nthe\n&gt; &gt; space or number of connections in a network.  Under that usual\n&gt; &gt; definition, complexity goes up linearly with the addition of new\n&gt; &gt;   structure, not exponentially.\n&gt; &gt;\n&gt; &gt; -&quot;X^T, where T = Y^Z and Z is the complexity curve&quot;  I still am \nnot\n&gt; &gt; sure what X really means formally, but you haven&#39;t given a\n&gt; &gt; definition for T or Y or Z either.  What are these variables?\n&gt; &gt;\n&gt; &gt; Ultimately I think you are arguing from intuition rather than\n&gt; &gt; formally, and intuitions can be misleading.\n&gt; &gt; )\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; Sorry to all for the long-windedness of this response!  I hope \nit is\n&gt; &gt; still useful!\n&gt; &gt;\n&gt; &gt; --- In neat@yahoogroups.com, &quot;Jim O&#39;Flaherty, Jr.&quot;\n&gt; &gt; &lt;jim_oflaherty_jr@y...&gt; wrote:\n&gt; &gt; &gt; John, Colin, Ken and Derek,\n&gt; &gt; &gt;\n&gt; &gt; &gt; I am wondering if there is not a wee bit too much focus on\n&gt; &gt; topological vairation and insignificant focus on just weight\n&gt; &gt; mutation.  I get that NEAT is unique in the fact that it has a \nvery\n&gt; &gt; effective search mechanism for topoligical variation, with the\n&gt; &gt; ability to stress both additive and subtractive aspects of \nchange.\n&gt; &gt; And I get that it is fun to focus on the topological variation \nas it\n&gt; &gt; is somewhat spatially interesting.\n&gt; &gt; &gt;\n&gt; &gt; &gt; However, I am realizing that just doing effective weight\n&gt; &gt; mutations, sans topological changes, can end up producing \nsolutions\n&gt; &gt; that are very &quot;fit&quot;.  I have been focused on reproducing the\n&gt; &gt; experiments Fogel and Kumar produced which are covered in their \nbook\n&gt; &gt; Blondie24.  In that, they had only 3 topologies they experimented\n&gt; &gt; with.  All of the GA searching was just done with weight mutation\n&gt; &gt; within a step size that was both a GA parameter and nudged \ntowards\n&gt; &gt; smaller values.  In the Fogel experiments, they arrived at an \nexpert\n&gt; &gt; player (well, at least against human opponents) using just co-\n&gt; &gt; evolution and a static topology.  And in my own replication of \nthe\n&gt; &gt; experiments, something as simple as turn on/off biases had a\n&gt; &gt; substantial effect in how long it took to arrive at a specimen of\n&gt; &gt; similar fitness.\n&gt; &gt; &gt;\n&gt; &gt; &gt; Now, I realize that mutating weights only is not near as sexy\n&gt; &gt; sounding as both weight mutation and topological variation.\n&gt; &gt; However, what I am wondering and hope to be able to evaluate with\n&gt; &gt; experimentation is whether the topological mutation rates ought \nnot\n&gt; &gt; be very small with the focus more on trying out many weight\n&gt; &gt; mutations within a given topology?  What is occurring to me is \nthat\n&gt; &gt; just doing weight mutation is a search at a rate X in a huge \nspace.\n&gt; &gt; And it seems to me adding topological variation is not just a\n&gt; &gt; multiplier, but an exponent increasing X.  Perhaps the space is\n&gt; &gt; being made too large too quickly, before a search just in the \nweight\n&gt; &gt; mutation space might demonstrate a uniquely fit individual.\n&gt; &gt; &gt;\n&gt; &gt; &gt; My understanding is that the &quot;search in higher dimensional \nspace&quot;\n&gt; &gt; might produce more robust and fit players.  At what computational\n&gt; &gt; cost?  If the topological complexity curve for having a &quot;more fit\n&gt; &gt; player&quot; is exponetial, then doesn&#39;t that mean that there is a\n&gt; &gt; threshold of diminishing returns somewhere?  Granted, it may not\n&gt; &gt; be.  However, when it is exponential (and my intuition says it is\n&gt; &gt; more of the time), we now have an exponent on an exponent of\n&gt; &gt; complexification which massively enlarges the search space, X^T,\n&gt; &gt; where T = Y^Z and Z is the complexity curve.  If this is true, \nthen\n&gt; &gt; we are massive amounts of processing power away from achieving\n&gt; &gt; result in anything but the simplest of domains, like XOR and Tic-\nTac-\n&gt; &gt; Toe.\n&gt; &gt; &gt;\n&gt; &gt; &gt; Am I missing something here?  Perhaps I need to do more direct\n&gt; &gt; experimentation and examine the results before jumping to this \nkind\n&gt; &gt; of conclusion.  I just get the sense that simple weight mutation\n&gt; &gt; achieved quite a bit in Checkers, a domain more complex than Tic-\nTac-\n&gt; &gt; Toe.  It would be interesting to see how Checkers might do with \nNEAT\n&gt; &gt; and see what kinds of mutation rates might be more/less effective\n&gt; &gt; and why.\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt; Jim\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; Yahoo! Groups Links\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n\n\n"}}