{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"vSAjRov8fIK9rKTnXjVJ3_jELKxPBTXWT2vxqGgrk-Ht9Q5vhauTPn3-gcPGmrCov-YfU0DWpew0MBqqPToFYNh9bYUrG5SrFhQnTUILzXLC","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: High rez input (i.e. Video) generalization","postDate":"1102311560","msgId":1758,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGNwMHJhOCtnbjE5QGVHcm91cHMuY29tPg==","inReplyToHeader":"PDE5YjEwZDUxMDQxMjAyMDcyMDE2YjRhNjM5QG1haWwuZ21haWwuY29tPg=="},"prevInTopic":1749,"nextInTopic":1760,"prevInTime":1757,"nextInTime":1759,"topicId":1743,"numMessagesInTopic":9,"msgSnippet":"... the ... Just front end, non-active vision.  Basically, we feed the view out the front window. ... visually ... and ... seem ... would ... True, the richer","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 44622 invoked from network); 6 Dec 2004 05:39:23 -0000\r\nReceived: from unknown (66.218.66.172)\n  by m13.grp.scd.yahoo.com with QMQP; 6 Dec 2004 05:39:23 -0000\r\nReceived: from unknown (HELO n2a.bulk.scd.yahoo.com) (66.94.237.36)\n  by mta4.grp.scd.yahoo.com with SMTP; 6 Dec 2004 05:39:22 -0000\r\nReceived: from [66.218.66.58] by n2.bulk.scd.yahoo.com with NNFMP; 06 Dec 2004 05:39:22 -0000\r\nReceived: from [66.218.67.153] by mailer7.bulk.scd.yahoo.com with NNFMP; 06 Dec 2004 05:39:22 -0000\r\nDate: Mon, 06 Dec 2004 05:39:20 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;cp0ra8+gn19@...&gt;\r\nIn-Reply-To: &lt;19b10d51041202072016b4a639@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Length: 2953\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Remote-IP: 66.94.237.36\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: Re: High rez input (i.e. Video) generalization\r\nX-Yahoo-Group-Post: member; u=54567749\r\nX-Yahoo-Profile: kenstanley01\r\n\r\n\n--- In neat@yahoogroups.com, Derek James &lt;djames@g...&gt; wrote:\n&gt; I guess my next question is how you are feeding pixel input from \nthe\n&gt; RARS model into the neural network.  Are you using an active vision\n&gt; approach?  Is there just a front-end view?  Or also a rear view?\n&gt; \n\nJust front end, non-active vision.  Basically, we feed the view out \nthe front window.\n\n&gt; Are you inputting grayscale values, or including color?  How \nvisually\n&gt; complex is the RARS model (i.e., Are walls always the same color \nand\n&gt; brightness?  Is there shading in this environement?) ?  It would \nseem\n&gt; like more richness and variation in the simulation environment \nwould\n&gt; lead to more robustness.\n&gt; \n\nTrue, the richer the better.  RARS gives a decent, though a bit \nvideo-gamish, first-person driver view.  It&#39;s not photorealistic by \nany stretch.\n\n&gt; &gt; In general, it has been shown that for real world transfer &quot;model\n&gt; &gt; noise&quot; is quite effective, meaning that the actions requested by \nthe\n&gt; &gt; outputs result in slightly unpredictable results.  Faustino Gomez\n&gt; &gt; showed this here at UT.\n&gt; &gt; \n&gt; &gt; However, in the particular case of raw visual input, we are \ntalking\n&gt; &gt; about a specific kind of problem- learning pixel correlations to\n&gt; &gt; states - and that might be best served by some noise in the\n&gt; &gt; display.\n&gt; \n&gt; Well, for car driving, if what you&#39;re saying is true, then it would\n&gt; make sense to add noise to the control outputs.  But for the case \nof\n&gt; just correlating pixel information with particular states or\n&gt; situations, that makes sense.\n&gt; \n&gt; For the fingerprint domain, as we did with the basic shapes, we\n&gt; randomize the images in the evaluation set by slightly moving them \na\n&gt; random number of pixels up/down, right/left, scaling them up/down \nby\n&gt; 10-20%, and rotating them in either direction 10-20 degrees.  We\n&gt; thought about adding random pixel noise, as Floreano did, and also\n&gt; considered randomizing other visual features, such as brightness, \nbut\n&gt; in the end decided that the noise we were adding was probably\n&gt; sufficient.\n&gt; \n&gt; Again, I don&#39;t know how sophisticated the RARS visual modeling is, \nbut\n&gt; that would seem to be one of the big challenges in such an accident\n&gt; warning system...the complexity of real-world visual scenes,\n&gt; complicated by the fact that people drive all hours of the day and\n&gt; night, so there&#39;s a huge amount of variance in lighting \nconditions. \n&gt; Does RARS, for example, have night driving?\n&gt; \n\nNo I don&#39;t believe it does.  We aren&#39;t really trying to solve the \nvisual recognition task though.  What we are doing is trying various \nlevels of input from processed radars to raw visual input to see \nwhat NEAT can figure out from different kinds of input.  A truly \nindustrial strength warning system would include some visual \npreprocessing algorithms at the front end before things get to NEAT.\n\nWe haven&#39;t really waded into action vision territory yet, and \nwhether we will has not yet been determined.\n\nken\n\n\n\n\n"}}