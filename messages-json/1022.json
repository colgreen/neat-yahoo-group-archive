{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":82117382,"authorName":"Jim O&#39;Flaherty, Jr.","from":"&quot;Jim O&#39;Flaherty, Jr.&quot; &lt;jim_oflaherty_jr@...&gt;","profile":"jim_oflaherty_jr","replyTo":"LIST","senderId":"Pc3DzD2PbGKZHU_1vVylEHnU50AxEiWbQpHaug42vjGILJAUxl_3WtY9UQY09rPN0A4mxDucYKDFi2sbeH2LNA6CS1Hd_DhVjkqoda3zF6thTUAo8VrPb6E","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] Re: Bloat","postDate":"1086649169","msgId":1022,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDAwOTEwMWM0NGNlMyQxNzFjOGExMCQzMjAxYThjMEBORVdBR0U+","referencesHeader":"PGM5dG41dSt1djByQGVHcm91cHMuY29tPg=="},"prevInTopic":1021,"nextInTopic":1023,"prevInTime":1021,"nextInTime":1023,"topicId":904,"numMessagesInTopic":68,"msgSnippet":"Ken, I meant no disrespect in my original post.  I was honestly confused why huge variations in topology were desirable when finding the proper weight values","rawEmail":"Return-Path: &lt;jim_oflaherty_jr@...&gt;\r\nX-Sender: jim_oflaherty_jr@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 54630 invoked from network); 7 Jun 2004 22:59:32 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m23.grp.scd.yahoo.com with QMQP; 7 Jun 2004 22:59:32 -0000\r\nReceived: from unknown (HELO smtp100.mail.sc5.yahoo.com) (216.136.174.138)\n  by mta5.grp.scd.yahoo.com with SMTP; 7 Jun 2004 22:59:31 -0000\r\nReceived: from unknown (HELO NEWAGE) (jim?oflaherty?jr@24.1.159.151 with login)\n  by smtp100.mail.sc5.yahoo.com with SMTP; 7 Jun 2004 22:59:30 -0000\r\nMessage-ID: &lt;009101c44ce3$171c8a10$3201a8c0@NEWAGE&gt;\r\nTo: &lt;neat@yahoogroups.com&gt;\r\nReferences: &lt;c9tn5u+uv0r@...&gt;\r\nDate: Mon, 7 Jun 2004 17:59:29 -0500\r\nMIME-Version: 1.0\r\nContent-Type: multipart/alternative;\n\tboundary=&quot;----=_NextPart_000_008E_01C44CB9.2DF7ECE0&quot;\r\nX-Priority: 3\r\nX-MSMail-Priority: Normal\r\nX-Mailer: Microsoft Outlook Express 6.00.2720.3000\r\nX-MimeOLE: Produced By Microsoft MimeOLE V6.00.2739.300\r\nX-eGroups-Remote-IP: 216.136.174.138\r\nFrom: &quot;Jim O&#39;Flaherty, Jr.&quot; &lt;jim_oflaherty_jr@...&gt;\r\nSubject: Re: [neat] Re: Bloat\r\nX-Yahoo-Group-Post: member; u=82117382\r\nX-Yahoo-Profile: jim_oflaherty_jr\r\n\r\n\r\n------=_NextPart_000_008E_01C44CB9.2DF7ECE0\r\nContent-Type: text/plain;\n\tcharset=&quot;iso-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\nKen,\n\nI meant no disrespect in my original post.  I was honestly confused w=\r\nhy huge variations in topology were desirable when finding the proper weigh=\r\nt values is just as important.  That whole argument is pretty much moot now=\r\n.  ;^)\n\nNow that I have re-read this and I understand the &quot;mutation rate&quot; i=\r\nssues, a huge part of my concern is now alleviated.  And what little I have=\r\n remaining is insignificant.  So until I have done some more experiments wi=\r\nth NEAT, I will wait to address any other issues.\n\n\nJim\n\n\n  ----- Original =\r\nMessage ----- \n  From: Kenneth Stanley \n  To: neat@yahoogroups.com \n  Sent:=\r\n Saturday, June 05, 2004 7:06 PM\n  Subject: [neat] Re: Bloat\n\n\n  Jim, I hop=\r\ne you will allow me a rather long and detailed response to \n  your point.  =\r\nI feel this is the right time for me to respond \n  broadly, since you have =\r\ntouched on the central theme behind much \n  discussion on this group, and u=\r\nltimately behind my own motivations \n  for introducing NEAT.  Therefore, fo=\r\nrgive me for a long-winded \n  response, but one I would to get on the recor=\r\nd.\n\n  I doubt that the importance of topology can be overstated.  That \n  s=\r\naid, I want to concede up front that there is no question that most \n  of t=\r\nhe key steps in exploration are through weight mutation, and \n  that weight=\r\n mutation will get you far.  In fact, there are very \n  sophisticated metho=\r\nds for altering the weight mutation distribution \n  to point it in more pro=\r\nmising directions, and these methods can be \n  quite powerful.\n\n  Neverthel=\r\ness, weight mutation is no more than exploring a fixed \n  space, and explor=\r\ning a fixed space is well understood and tried and \n  tested.  In fact, it =\r\nis proven that there  is only so much a black \n  box method can do to explo=\r\nre a space.  No method can promise always \n  to escape local optima, and no=\r\n method ever will make such a promise \n  (so says the No Free Lunch Theorem=\r\n).  \n\n  There are fundamental questions at the core of AI that fixed-space =\r\n\n  exploration can never address.  Most perplexing and fundamental is \n  th=\r\ne question of what space should we be exploring in the first \n  place?  Fog=\r\nel tried 3 topologies (and probably more, off the \n  ecord).  But where did=\r\n those topologies come from?  What was the \n  basis of the decisions to use=\r\n them?  Isn&#39;t our mission, as \n  researchers in AI, to make *that* decision=\r\n automatic?  After all, \n  *that* decision- the decision of what topology t=\r\no search, i.e. what \n  space to search in- is really the only hard decision=\r\n, the one that \n  requires &quot;intelligence&quot;.  It is a relatively trivial matt=\r\ner, once \n  you know what to search, just to go searching.  The fact that w=\r\neight \n  mutation alone (once the correct topology has been identified) is =\r\n\n  sufficient to solve checkers says more about checkers and human \n  intel=\r\nligence (intelligence for choosing the right space to search) \n  than about=\r\n the prowess of weight mutation.\n\n  Yet this is not only a philosophical ar=\r\ngument about what AI should \n  be able to do automatically.  It is also a c=\r\nritical practical \n  matter.  Contrary to your reasoning, the real danger i=\r\ns not in \n  adding a single dimension to a search space, but in beginning s=\r\nearch \n  in a bad space in the first place.  If you are concerned that \n  a=\r\nddition of a single dimension has some exponential expense (which I \n  beli=\r\neve is not correct anyway), what cost then must there be in \n  searching in=\r\n a topology with dozens or even hundreds of unnecessary \n  dimensions?  The=\r\n effect on search could be catastrophic.\n\n  And yet for most difficult prob=\r\nlems we have not the slightest idea \n  what the right space is to search, o=\r\nther than that it is large.  How \n  many dimensions are in the brain of a r=\r\nobotic maid?  Surely at least \n  thousands; maybe millions.  Should we begi=\r\nn search then in a network \n  of a million connections?  Weight space explo=\r\nration offers no \n  comfort: The search is intractable in million dimension=\r\nal space, \n  even if the solution is somewhere within.  \n\n  Yet even as the=\r\nre is danger from above in the form of too-high \n  dimensional space, there=\r\n is danger from below in spaces of too-few \n  dimensions, where a solution =\r\nmay not even exist.  What if Fogel had \n  chosen to search in networks with=\r\n 2 fewer neurons?  5 fewer? At some \n  point, the good player just doesn&#39;t =\r\nexist in that space anymore.  \n  But how could we know this in advance?  Th=\r\nere is no analysis that \n  can tell us a priori how many dimensions we need=\r\n.  And if we try to \n  go lean and get just the right amount, we might miss=\r\n the boat \n  entirely, even by a single connection, and end up searching fo=\r\nrever \n  in futility in a space without a solution.\n\n  Worse, even if we kn=\r\new *exactly* the minimal number of connections \n  necessary to solve a prob=\r\nlem *and* the perfect topology, even then, \n  if the space is too large, we=\r\night mutation alone is likely to fail.  \n  The problem is, where in a large=\r\n space do you *begin* to search? And \n  that problem is impossible to addre=\r\nss since by definition you don&#39;t \n  know anything about the space before yo=\r\nu begin searching!  \n  Therefore, in a high-dimensional space, you are high=\r\nly likely to \n  begin in an unpromising part of the space; it&#39;s simply too =\r\nlarge.\n\n  Therefore, to begin minimally and complexify into the the proper =\r\n\n  space is addressing a fundamental issue and I believe is ultimately \n  u=\r\nnavoidable as a critical component of any black box search for \n  complex b=\r\nehaviors.  Rather than adding expense as you imply, it is \n  reducing expen=\r\nse by spending most of search in lower-dimensional \n  space than the final =\r\nsolution.  A complexifying method only may be \n  searching in the space of =\r\nthe final solution for 10% of the run.  \n  Fixed-topology search spends 100=\r\n% of the run in the high dimensional \n  space of the final solution, which,=\r\n according to your formulation \n  should incur an incomprehensibly vast exp=\r\nonential penalty.  \n\n  I think ultimately what you are misunderstanding is =\r\nthat NEAT is not\n  an attempt to search in high-dimensional space.  It is a=\r\n method for \n  spending most of your search in *lower-dimensional space* th=\r\nan the \n  final solution, and complexifying up to the complexity of the fin=\r\nal \n  solution.  The goal is to be able to find solutions that *exist* in \n=\r\n  high-dimensional space.  That&#39;s not the same as a goal of searching \n  di=\r\nrectly in high-dimensional space no matter the problem.  The \n  latter goal=\r\n is the antithesis of what NEAT is about.  NEAT is \n  designed to avoid sea=\r\nrching in unnecessarily high-dimensional space.\n\n  Thus, I feel strongly th=\r\nat the idea of searching through topologies \n  must be taken seriously, and=\r\n should not be viewed as merely \n  a &quot;fun&quot;, &quot;sexy,&quot; or &quot;somwhat spatially i=\r\nnteresting&quot;  recreation.  It \n  is not mere intellectual exercise.  Prior t=\r\nopology-evolving systems \n  before NEAT were perhaps better targets for you=\r\nr criticism, since \n  they were essentially aimed at flipping through rando=\r\nm topologies \n  unsystematically for its own sake.  However, NEAT is design=\r\ned to to \n  use topology as a way of minimizing dimensionality in search, a=\r\nnd \n  ultimately to automatically address that fundamental question of \n  w=\r\nhat space to be searching in, a completely different endeavor. \n\n  (\n\n  A c=\r\nouple side notes:\n\n  I agree that structural mutation needs to be relativel=\r\ny rare.  In \n  NEAT, it is generally 5% or lower.  Years of experimentation=\r\n with \n  NEAT have gone into testing different rates of structure-adding.\n\n=\r\n  Finally, I believe your mathematical formulation is incorrect. \n  Adding =\r\na dimensions to an already-partially-optimized structure \n  certainly does =\r\nnot incur exponential expense in the search process.  \n  In fact, the effec=\r\nt can be quite the opposite, adding new routes off \n  the top of a local op=\r\ntimum.\n\n  Not to be picky, and this isn&#39;t really important, but here&#39;s what=\r\n \n  doesn&#39;t make sense to me about your formal argument:\n\n  -&quot;What is occur=\r\nring to me is that just doing weight mutation is a \n  search at a rate X in=\r\n a huge space.&quot;  How do you define &quot;search at \n  rate X?&quot;  This does not se=\r\nem to mean anything formally speaking.  \n  What are the units of search rat=\r\ne?  How is it derived?\n\n  -&quot;And it seems to me adding topological variation=\r\n is not just a \n  multiplier, but an exponent increasing X.&quot;  If X is a rat=\r\ne (as you \n  defined it), then increasing X means the rate becomes faster. =\r\n So I \n  assume X is not a rate.  But then what is it?\n\n  -&quot;topological com=\r\nplexity curve for having a more fit player is \n  exponetial&quot;  What is a top=\r\nological complexity curve?  Is it based \n  somehow on rate X?  Complexity i=\r\ns usually defined as the size of the \n  space or number of connections in a=\r\n network.  Under that usual \n  definition, complexity goes up linearly with=\r\n the addition of new\n    structure, not exponentially.\n\n  -&quot;X^T, where T =\r\n=3D Y^Z and Z is the complexity curve&quot;  I still am not \n  sure what X reall=\r\ny means formally, but you haven&#39;t given a \n  definition for T or Y or Z eit=\r\nher.  What are these variables?\n\n  Ultimately I think you are arguing from =\r\nintuition rather than \n  formally, and intuitions can be misleading.\n  )\n\n\n=\r\n  Sorry to all for the long-windedness of this response!  I hope it is \n  s=\r\ntill useful!\n\n  --- In neat@yahoogroups.com, &quot;Jim O&#39;Flaherty, Jr.&quot; \n  &lt;jim_=\r\noflaherty_jr@y...&gt; wrote:\n  &gt; John, Colin, Ken and Derek,\n  &gt; \n  &gt; I am won=\r\ndering if there is not a wee bit too much focus on \n  topological vairation=\r\n and insignificant focus on just weight \n  mutation.  I get that NEAT is un=\r\nique in the fact that it has a very \n  effective search mechanism for topol=\r\nigical variation, with the \n  ability to stress both additive and subtracti=\r\nve aspects of change.  \n  And I get that it is fun to focus on the topologi=\r\ncal variation as it \n  is somewhat spatially interesting.\n  &gt; \n  &gt; However,=\r\n I am realizing that just doing effective weight \n  mutations, sans topolog=\r\nical changes, can end up producing solutions \n  that are very &quot;fit&quot;.  I hav=\r\ne been focused on reproducing the \n  experiments Fogel and Kumar produced w=\r\nhich are covered in their book \n  Blondie24.  In that, they had only 3 topo=\r\nlogies they experimented \n  with.  All of the GA searching was just done wi=\r\nth weight mutation \n  within a step size that was both a GA parameter and n=\r\nudged towards \n  smaller values.  In the Fogel experiments, they arrived at=\r\n an expert \n  player (well, at least against human opponents) using just co=\r\n-\n  evolution and a static topology.  And in my own replication of the \n  e=\r\nxperiments, something as simple as turn on/off biases had a \n  substantial =\r\neffect in how long it took to arrive at a specimen of \n  similar fitness.\n =\r\n &gt; \n  &gt; Now, I realize that mutating weights only is not near as sexy \n  so=\r\nunding as both weight mutation and topological variation.  \n  However, what=\r\n I am wondering and hope to be able to evaluate with \n  experimentation is =\r\nwhether the topological mutation rates ought not \n  be very small with the =\r\nfocus more on trying out many weight \n  mutations within a given topology? =\r\n What is occurring to me is that \n  just doing weight mutation is a search =\r\nat a rate X in a huge space.  \n  And it seems to me adding topological vari=\r\nation is not just a \n  multiplier, but an exponent increasing X.  Perhaps t=\r\nhe space is \n  being made too large too quickly, before a search just in th=\r\ne weight \n  mutation space might demonstrate a uniquely fit individual.\n  &gt;=\r\n \n  &gt; My understanding is that the &quot;search in higher dimensional space&quot; \n  =\r\nmight produce more robust and fit players.  At what computational \n  cost? =\r\n If the topological complexity curve for having a &quot;more fit \n  player&quot; is e=\r\nxponetial, then doesn&#39;t that mean that there is a \n  threshold of diminishi=\r\nng returns somewhere?  Granted, it may not \n  be.  However, when it is expo=\r\nnential (and my intuition says it is \n  more of the time), we now have an e=\r\nxponent on an exponent of \n  complexification which massively enlarges the =\r\nsearch space, X^T, \n  where T =3D Y^Z and Z is the complexity curve.  If th=\r\nis is true, then \n  we are massive amounts of processing power away from ac=\r\nhieving \n  result in anything but the simplest of domains, like XOR and Tic=\r\n-Tac-\n  Toe.\n  &gt; \n  &gt; Am I missing something here?  Perhaps I need to do mo=\r\nre direct \n  experimentation and examine the results before jumping to this=\r\n kind \n  of conclusion.  I just get the sense that simple weight mutation \n=\r\n  achieved quite a bit in Checkers, a domain more complex than Tic-Tac-\n  T=\r\noe.  It would be interesting to see how Checkers might do with NEAT \n  and =\r\nsee what kinds of mutation rates might be more/less effective \n  and why.\n =\r\n &gt; \n  &gt; \n  &gt; Jim\n  &gt; \n  &gt; \n\n\n\n        Yahoo! Groups Sponsor \n              =\r\nADVERTISEMENT\n             \n       \n       \n\n\n-----------------------------=\r\n-------------------------------------------------\n  Yahoo! Groups Links\n\n  =\r\n  a.. To visit your group on the web, go to:\n    http://groups.yahoo.com/gr=\r\noup/neat/\n      \n    b.. To unsubscribe from this group, send an email to:\n=\r\n    neat-unsubscribe@yahoogroups.com\n      \n    c.. Your use of Yahoo! Grou=\r\nps is subject to the Yahoo! Terms of Service. \n\n\n\r\n------=_NextPart_000_008E_01C44CB9.2DF7ECE0\r\nContent-Type: text/html;\n\tcharset=&quot;iso-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\n&lt;!DOCTYPE HTML PUBLIC &quot;-//W3C//DTD HTML 4.0 Transitional//EN&quot;&gt;\n&lt;HTML&gt;&lt;HEAD&gt;=\r\n\n&lt;META http-equiv=3DContent-Type content=3D&quot;text/html; charset=3Diso-8859-1=\r\n&quot;&gt;\n&lt;META content=3D&quot;MSHTML 6.00.2737.800&quot; name=3DGENERATOR&gt;\n&lt;STYLE&gt;&lt;/STYLE&gt;=\r\n\n&lt;/HEAD&gt;\n&lt;BODY bgColor=3D#ffffff&gt;\n&lt;DIV&gt;&lt;FONT face=3DArial size=3D2&gt;Ken,&lt;/FO=\r\nNT&gt;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;FONT face=3DArial size=3D2&gt;&lt;/FONT&gt;&nbsp;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;FONT f=\r\nace=3DArial size=3D2&gt;\n&lt;DIV&gt;&lt;FONT face=3DArial size=3D2&gt;I meant no disrespec=\r\nt in my original post.&nbsp; I \nwas honestly confused why huge variations i=\r\nn topology were desirable when \nfinding the proper weight values is just as=\r\n important.&nbsp; That whole argument \nis pretty much moot now.&nbsp; ;^)&lt;/=\r\nFONT&gt;&lt;/DIV&gt;\n&lt;DIV&gt;&nbsp;&lt;/DIV&gt;Now that I have re-read this and I understand =\r\nthe &quot;mutation \nrate&quot; issues, a huge part of my concern is now alleviated.&n=\r\nbsp; And what little \nI have remaining is insignificant.&nbsp; So until I h=\r\nave done some more \nexperiments with NEAT, I will wait to address any other=\r\n issues.&lt;/FONT&gt;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;FONT face=3DArial size=3D2&gt;&lt;/FONT&gt;&nbsp;&lt;/DIV&gt;\n=\r\n&lt;DIV&gt;&lt;FONT face=3DArial size=3D2&gt;&lt;/FONT&gt;&nbsp;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;FONT face=3DAria=\r\nl size=3D2&gt;Jim&lt;/FONT&gt;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;FONT face=3DArial size=3D2&gt;&lt;/FONT&gt;&nbsp;&lt;=\r\n/DIV&gt;\n&lt;DIV&gt;&lt;FONT face=3DArial size=3D2&gt;&lt;/FONT&gt;&nbsp;&lt;/DIV&gt;\n&lt;BLOCKQUOTE \nsty=\r\nle=3D&quot;PADDING-RIGHT: 0px; PADDING-LEFT: 5px; MARGIN-LEFT: 5px; BORDER-LEFT:=\r\n #000000 2px solid; MARGIN-RIGHT: 0px&quot;&gt;\n  &lt;DIV style=3D&quot;FONT: 10pt arial&quot;&gt;-=\r\n---- Original Message ----- &lt;/DIV&gt;\n  &lt;DIV \n  style=3D&quot;BACKGROUND: #e4e4e4; =\r\nFONT: 10pt arial; font-color: black&quot;&gt;&lt;B&gt;From:&lt;/B&gt; \n  &lt;A title=3Dkstanley@cs=\r\n.utexas.edu href=3D&quot;mailto:kstanley@...&quot;&gt;Kenneth \n  Stanley&lt;/A&gt; &lt;=\r\n/DIV&gt;\n  &lt;DIV style=3D&quot;FONT: 10pt arial&quot;&gt;&lt;B&gt;To:&lt;/B&gt; &lt;A title=3Dneat@yahoogro=\r\nups.com \n  href=3D&quot;mailto:neat@yahoogroups.com&quot;&gt;neat@yahoogroups.com&lt;/A&gt; &lt;/=\r\nDIV&gt;\n  &lt;DIV style=3D&quot;FONT: 10pt arial&quot;&gt;&lt;B&gt;Sent:&lt;/B&gt; Saturday, June 05, 2004=\r\n 7:06 \n  PM&lt;/DIV&gt;\n  &lt;DIV style=3D&quot;FONT: 10pt arial&quot;&gt;&lt;B&gt;Subject:&lt;/B&gt; [neat] =\r\nRe: Bloat&lt;/DIV&gt;\n  &lt;DIV&gt;&lt;BR&gt;&lt;/DIV&gt;&lt;TT&gt;Jim, I hope you will allow me a rather=\r\n long and detailed \n  response to &lt;BR&gt;your point.&nbsp; I feel this is the =\r\nright time for me to \n  respond &lt;BR&gt;broadly, since you have touched on the =\r\ncentral theme behind much \n  &lt;BR&gt;discussion on this group, and ultimately b=\r\nehind my own motivations &lt;BR&gt;for \n  introducing NEAT.&nbsp; Therefore, forg=\r\nive me for a long-winded &lt;BR&gt;response, \n  but one I would to get on the rec=\r\nord.&lt;BR&gt;&lt;BR&gt;I doubt that the importance of \n  topology can be overstated.&n=\r\nbsp; That &lt;BR&gt;said, I want to concede up front \n  that there is no question=\r\n that most &lt;BR&gt;of the key steps in exploration are \n  through weight mutati=\r\non, and &lt;BR&gt;that weight mutation will get you far.&nbsp; \n  In fact, there =\r\nare very &lt;BR&gt;sophisticated methods for altering the weight \n  mutation dist=\r\nribution &lt;BR&gt;to point it in more promising directions, and these \n  methods=\r\n can be &lt;BR&gt;quite powerful.&lt;BR&gt;&lt;BR&gt;Nevertheless, weight mutation is no \n  m=\r\nore than exploring a fixed &lt;BR&gt;space, and exploring a fixed space is well \n=\r\n  understood and tried and &lt;BR&gt;tested.&nbsp; In fact, it is proven that \n  =\r\nthere&nbsp; is only so much a black &lt;BR&gt;box method can do to explore a \n  s=\r\npace.&nbsp; No method can promise always &lt;BR&gt;to escape local optima, and no=\r\n \n  method ever will make such a promise &lt;BR&gt;(so says the No Free Lunch \n  =\r\nTheorem).&nbsp; &lt;BR&gt;&lt;BR&gt;There are fundamental questions at the core of AI t=\r\nhat \n  fixed-space &lt;BR&gt;exploration can never address.&nbsp; Most perplexing=\r\n and \n  fundamental is &lt;BR&gt;the question of what space should we be explorin=\r\ng in the \n  first &lt;BR&gt;place?&nbsp; Fogel tried 3 topologies (and probably m=\r\nore, off the \n  &lt;BR&gt;ecord).&nbsp; But where did those topologies come from?=\r\n&nbsp; What was the \n  &lt;BR&gt;basis of the decisions to use them?&nbsp; Isn&#39;t =\r\nour mission, as \n  &lt;BR&gt;researchers in AI, to make *that* decision automatic=\r\n?&nbsp; After all, \n  &lt;BR&gt;*that* decision- the decision of what topology to=\r\n search, i.e. what \n  &lt;BR&gt;space to search in- is really the only hard decis=\r\nion, the one that \n  &lt;BR&gt;requires &quot;intelligence&quot;.&nbsp; It is a relatively =\r\ntrivial matter, once \n  &lt;BR&gt;you know what to search, just to go searching.&=\r\nnbsp; The fact that weight \n  &lt;BR&gt;mutation alone (once the correct topology=\r\n has been identified) is \n  &lt;BR&gt;sufficient to solve checkers says more abou=\r\nt checkers and human \n  &lt;BR&gt;intelligence (intelligence for choosing the rig=\r\nht space to search) \n  &lt;BR&gt;than about the prowess of weight mutation.&lt;BR&gt;&lt;B=\r\nR&gt;Yet this is not only a \n  philosophical argument about what AI should &lt;BR=\r\n&gt;be able to do \n  automatically.&nbsp; It is also a critical practical &lt;BR&gt;=\r\nmatter.&nbsp; \n  Contrary to your reasoning, the real danger is not in &lt;BR&gt;=\r\nadding a single \n  dimension to a search space, but in beginning search &lt;BR=\r\n&gt;in a bad space in the \n  first place.&nbsp; If you are concerned that &lt;BR&gt;=\r\naddition of a single \n  dimension has some exponential expense (which I &lt;BR=\r\n&gt;believe is not correct \n  anyway), what cost then must there be in &lt;BR&gt;sea=\r\nrching in a topology with \n  dozens or even hundreds of unnecessary &lt;BR&gt;dim=\r\nensions?&nbsp; The effect on \n  search could be catastrophic.&lt;BR&gt;&lt;BR&gt;And ye=\r\nt for most difficult problems we \n  have not the slightest idea &lt;BR&gt;what th=\r\ne right space is to search, other than \n  that it is large.&nbsp; How &lt;BR&gt;m=\r\nany dimensions are in the brain of a robotic \n  maid?&nbsp; Surely at least=\r\n &lt;BR&gt;thousands; maybe millions.&nbsp; Should we \n  begin search then in a n=\r\network &lt;BR&gt;of a million connections?&nbsp; Weight \n  space exploration offe=\r\nrs no &lt;BR&gt;comfort: The search is intractable in million \n  dimensional spac=\r\ne, &lt;BR&gt;even if the solution is somewhere within.&nbsp; \n  &lt;BR&gt;&lt;BR&gt;Yet even =\r\nas there is danger from above in the form of too-high \n  &lt;BR&gt;dimensional sp=\r\nace, there is danger from below in spaces of too-few \n  &lt;BR&gt;dimensions, whe=\r\nre a solution may not even exist.&nbsp; What if Fogel had \n  &lt;BR&gt;chosen to =\r\nsearch in networks with 2 fewer neurons?&nbsp; 5 fewer? At some \n  &lt;BR&gt;poin=\r\nt, the good player just doesn&#39;t exist in that space anymore.&nbsp; \n  &lt;BR&gt;B=\r\nut how could we know this in advance?&nbsp; There is no analysis that \n  &lt;B=\r\nR&gt;can tell us a priori how many dimensions we need.&nbsp; And if we try to =\r\n\n  &lt;BR&gt;go lean and get just the right amount, we might miss the boat \n  &lt;BR=\r\n&gt;entirely, even by a single connection, and end up searching forever &lt;BR&gt;in=\r\n \n  futility in a space without a solution.&lt;BR&gt;&lt;BR&gt;Worse, even if we knew \n=\r\n  *exactly* the minimal number of connections &lt;BR&gt;necessary to solve a prob=\r\nlem \n  *and* the perfect topology, even then, &lt;BR&gt;if the space is too large=\r\n, weight \n  mutation alone is likely to fail.&nbsp; &lt;BR&gt;The problem is, whe=\r\nre in a large \n  space do you *begin* to search? And &lt;BR&gt;that problem is im=\r\npossible to address \n  since by definition you don&#39;t &lt;BR&gt;know anything abou=\r\nt the space before you \n  begin searching!&nbsp; &lt;BR&gt;Therefore, in a high-d=\r\nimensional space, you are \n  highly likely to &lt;BR&gt;begin in an unpromising p=\r\nart of the space; it&#39;s simply \n  too large.&lt;BR&gt;&lt;BR&gt;Therefore, to begin mini=\r\nmally and complexify into the the \n  proper &lt;BR&gt;space is addressing a funda=\r\nmental issue and I believe is ultimately \n  &lt;BR&gt;unavoidable as a critical c=\r\nomponent of any black box search for \n  &lt;BR&gt;complex behaviors.&nbsp; Rather=\r\n than adding expense as you imply, it is \n  &lt;BR&gt;reducing expense by spendin=\r\ng most of search in lower-dimensional &lt;BR&gt;space \n  than the final solution.=\r\n&nbsp; A complexifying method only may be \n  &lt;BR&gt;searching in the space of =\r\nthe final solution for 10% of the run.&nbsp; \n  &lt;BR&gt;Fixed-topology search s=\r\npends 100% of the run in the high dimensional \n  &lt;BR&gt;space of the final sol=\r\nution, which, according to your formulation \n  &lt;BR&gt;should incur an incompre=\r\nhensibly vast exponential penalty.&nbsp; &lt;BR&gt;&lt;BR&gt;I \n  think ultimately what=\r\n you are misunderstanding is that NEAT is not&lt;BR&gt;an \n  attempt to search in=\r\n high-dimensional space.&nbsp; It is a method for \n  &lt;BR&gt;spending most of y=\r\nour search in *lower-dimensional space* than the \n  &lt;BR&gt;final solution, and=\r\n complexifying up to the complexity of the final \n  &lt;BR&gt;solution.&nbsp; The=\r\n goal is to be able to find solutions that *exist* in \n  &lt;BR&gt;high-dimension=\r\nal space.&nbsp; That&#39;s not the same as a goal of searching \n  &lt;BR&gt;directly =\r\nin high-dimensional space no matter the problem.&nbsp; The \n  &lt;BR&gt;latter go=\r\nal is the antithesis of what NEAT is about.&nbsp; NEAT is \n  &lt;BR&gt;designed t=\r\no avoid searching in unnecessarily high-dimensional \n  space.&lt;BR&gt;&lt;BR&gt;Thus, =\r\nI feel strongly that the idea of searching through \n  topologies &lt;BR&gt;must b=\r\ne taken seriously, and should not be viewed as merely \n  &lt;BR&gt;a &quot;fun&quot;, &quot;sexy=\r\n,&quot; or &quot;somwhat spatially interesting&quot;&nbsp; \n  recreation.&nbsp; It &lt;BR&gt;is =\r\nnot mere intellectual exercise.&nbsp; Prior \n  topology-evolving systems &lt;B=\r\nR&gt;before NEAT were perhaps better targets for your \n  criticism, since &lt;BR&gt;=\r\nthey were essentially aimed at flipping through random \n  topologies &lt;BR&gt;un=\r\nsystematically for its own sake.&nbsp; However, NEAT is \n  designed to to &lt;=\r\nBR&gt;use topology as a way of minimizing dimensionality in \n  search, and &lt;BR=\r\n&gt;ultimately to automatically address that fundamental question \n  of &lt;BR&gt;wh=\r\nat space to be searching in, a completely different endeavor. \n  &lt;BR&gt;&lt;BR&gt;(&lt;=\r\nBR&gt;&lt;BR&gt;A couple side notes:&lt;BR&gt;&lt;BR&gt;I agree that structural mutation \n  need=\r\ns to be relatively rare.&nbsp; In &lt;BR&gt;NEAT, it is generally 5% or \n  lower.=\r\n&nbsp; Years of experimentation with &lt;BR&gt;NEAT have gone into testing \n  dif=\r\nferent rates of structure-adding.&lt;BR&gt;&lt;BR&gt;Finally, I believe your \n  mathema=\r\ntical formulation is incorrect. &lt;BR&gt;Adding a dimensions to an \n  already-pa=\r\nrtially-optimized structure &lt;BR&gt;certainly does not incur exponential \n  exp=\r\nense in the search process.&nbsp; &lt;BR&gt;In fact, the effect can be quite the =\r\n\n  opposite, adding new routes off &lt;BR&gt;the top of a local optimum.&lt;BR&gt;&lt;BR&gt;N=\r\not to \n  be picky, and this isn&#39;t really important, but here&#39;s what &lt;BR&gt;doe=\r\nsn&#39;t make \n  sense to me about your formal argument:&lt;BR&gt;&lt;BR&gt;-&quot;What is occur=\r\nring to me is \n  that just doing weight mutation is a &lt;BR&gt;search at a rate =\r\nX in a huge \n  space.&quot;&nbsp; How do you define &quot;search at &lt;BR&gt;rate X?&quot;&nbsp=\r\n; This does not \n  seem to mean anything formally speaking.&nbsp; &lt;BR&gt;What =\r\nare the units of \n  search rate?&nbsp; How is it derived?&lt;BR&gt;&lt;BR&gt;-&quot;And it s=\r\neems to me adding \n  topological variation is not just a &lt;BR&gt;multiplier, bu=\r\nt an exponent increasing \n  X.&quot;&nbsp; If X is a rate (as you &lt;BR&gt;defined it=\r\n), then increasing X means the \n  rate becomes faster.&nbsp; So I &lt;BR&gt;assum=\r\ne X is not a rate.&nbsp; But then \n  what is it?&lt;BR&gt;&lt;BR&gt;-&quot;topological compl=\r\nexity curve for having a more fit player \n  is &lt;BR&gt;exponetial&quot;&nbsp; What i=\r\ns a topological complexity curve?&nbsp; Is it \n  based &lt;BR&gt;somehow on rate =\r\nX?&nbsp; Complexity is usually defined as the size \n  of the &lt;BR&gt;space or n=\r\number of connections in a network.&nbsp; Under that usual \n  &lt;BR&gt;definition=\r\n, complexity goes up linearly with the addition of new&lt;BR&gt;&nbsp; \n  structu=\r\nre, not exponentially.&lt;BR&gt;&lt;BR&gt;-&quot;X^T, where T =3D Y^Z and Z is the \n  comple=\r\nxity curve&quot;&nbsp; I still am not &lt;BR&gt;sure what X really means formally, \n  =\r\nbut you haven&#39;t given a &lt;BR&gt;definition for T or Y or Z either.&nbsp; What a=\r\nre \n  these variables?&lt;BR&gt;&lt;BR&gt;Ultimately I think you are arguing from intui=\r\ntion \n  rather than &lt;BR&gt;formally, and intuitions can be \n  misleading.&lt;BR&gt;)=\r\n&lt;BR&gt;&lt;BR&gt;&lt;BR&gt;Sorry to all for the long-windedness of this \n  response!&nbsp;=\r\n I hope it is &lt;BR&gt;still useful!&lt;BR&gt;&lt;BR&gt;--- In \n  neat@yahoogroups.com, &quot;Jim=\r\n O&#39;Flaherty, Jr.&quot; &lt;BR&gt;&lt;jim_oflaherty_jr@y...&gt; \n  wrote:&lt;BR&gt;&gt; John,=\r\n Colin, Ken and Derek,&lt;BR&gt;&gt; &lt;BR&gt;&gt; I am wondering if \n  there is not a=\r\n wee bit too much focus on &lt;BR&gt;topological vairation and \n  insignificant f=\r\nocus on just weight &lt;BR&gt;mutation.&nbsp; I get that NEAT is \n  unique in the=\r\n fact that it has a very &lt;BR&gt;effective search mechanism for \n  topoligical =\r\nvariation, with the &lt;BR&gt;ability to stress both additive and \n  subtractive =\r\naspects of change.&nbsp; &lt;BR&gt;And I get that it is fun to focus on \n  the to=\r\npological variation as it &lt;BR&gt;is somewhat spatially interesting.&lt;BR&gt;&gt; \n =\r\n &lt;BR&gt;&gt; However, I am realizing that just doing effective weight \n  &lt;BR&gt;m=\r\nutations, sans topological changes, can end up producing solutions \n  &lt;BR&gt;t=\r\nhat are very &quot;fit&quot;.&nbsp; I have been focused on reproducing the \n  &lt;BR&gt;exp=\r\neriments Fogel and Kumar produced which are covered in their book \n  &lt;BR&gt;Bl=\r\nondie24.&nbsp; In that, they had only 3 topologies they experimented \n  &lt;BR=\r\n&gt;with.&nbsp; All of the GA searching was just done with weight mutation \n  =\r\n&lt;BR&gt;within a step size that was both a GA parameter and nudged towards \n  &lt;=\r\nBR&gt;smaller values.&nbsp; In the Fogel experiments, they arrived at an exper=\r\nt \n  &lt;BR&gt;player (well, at least against human opponents) using just \n  co-&lt;=\r\nBR&gt;evolution and a static topology.&nbsp; And in my own replication of the =\r\n\n  &lt;BR&gt;experiments, something as simple as turn on/off biases had a \n  &lt;BR&gt;=\r\nsubstantial effect in how long it took to arrive at a specimen of \n  &lt;BR&gt;si=\r\nmilar fitness.&lt;BR&gt;&gt; &lt;BR&gt;&gt; Now, I realize that mutating weights \n  onl=\r\ny is not near as sexy &lt;BR&gt;sounding as both weight mutation and topological =\r\n\n  variation.&nbsp; &lt;BR&gt;However, what I am wondering and hope to be able to=\r\n \n  evaluate with &lt;BR&gt;experimentation is whether the topological mutation r=\r\nates \n  ought not &lt;BR&gt;be very small with the focus more on trying out many =\r\nweight \n  &lt;BR&gt;mutations within a given topology?&nbsp; What is occurring to=\r\n me is that \n  &lt;BR&gt;just doing weight mutation is a search at a rate X in a =\r\nhuge space.&nbsp; \n  &lt;BR&gt;And it seems to me adding topological variation is=\r\n not just a \n  &lt;BR&gt;multiplier, but an exponent increasing X.&nbsp; Perhaps =\r\nthe space is \n  &lt;BR&gt;being made too large too quickly, before a search just =\r\nin the weight \n  &lt;BR&gt;mutation space might demonstrate a uniquely fit indivi=\r\ndual.&lt;BR&gt;&gt; \n  &lt;BR&gt;&gt; My understanding is that the &quot;search in higher di=\r\nmensional space&quot; \n  &lt;BR&gt;might produce more robust and fit players.&nbsp; At=\r\n what computational \n  &lt;BR&gt;cost?&nbsp; If the topological complexity curve =\r\nfor having a &quot;more fit \n  &lt;BR&gt;player&quot; is exponetial, then doesn&#39;t that mean=\r\n that there is a \n  &lt;BR&gt;threshold of diminishing returns somewhere?&nbsp; G=\r\nranted, it may not \n  &lt;BR&gt;be.&nbsp; However, when it is exponential (and my=\r\n intuition says it is \n  &lt;BR&gt;more of the time), we now have an exponent on =\r\nan exponent of \n  &lt;BR&gt;complexification which massively enlarges the search =\r\nspace, X^T, &lt;BR&gt;where \n  T =3D Y^Z and Z is the complexity curve.&nbsp; If =\r\nthis is true, then &lt;BR&gt;we are \n  massive amounts of processing power away f=\r\nrom achieving &lt;BR&gt;result in anything \n  but the simplest of domains, like X=\r\nOR and Tic-Tac-&lt;BR&gt;Toe.&lt;BR&gt;&gt; &lt;BR&gt;&gt; Am \n  I missing something here?&nb=\r\nsp; Perhaps I need to do more direct \n  &lt;BR&gt;experimentation and examine the=\r\n results before jumping to this kind &lt;BR&gt;of \n  conclusion.&nbsp; I just get=\r\n the sense that simple weight mutation \n  &lt;BR&gt;achieved quite a bit in Check=\r\ners, a domain more complex than \n  Tic-Tac-&lt;BR&gt;Toe.&nbsp; It would be inter=\r\nesting to see how Checkers might do \n  with NEAT &lt;BR&gt;and see what kinds of =\r\nmutation rates might be more/less \n  effective &lt;BR&gt;and why.&lt;BR&gt;&gt; &lt;BR&gt;&gt=\r\n; &lt;BR&gt;&gt; Jim&lt;BR&gt;&gt; &lt;BR&gt;&gt; \n  &lt;BR&gt;&lt;BR&gt;&lt;BR&gt;&lt;/TT&gt;&lt;BR&gt;&lt;/BODY&gt;&lt;/HTML&gt;\n\r\n------=_NextPart_000_008E_01C44CB9.2DF7ECE0--\r\n\n"}}