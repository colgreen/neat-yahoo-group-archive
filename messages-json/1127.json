{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":7192225,"authorName":"Ian Badcoe","from":"Ian Badcoe &lt;ian_badcoe@...&gt;","profile":"ian_badcoe","replyTo":"LIST","senderId":"3yikQ07KvR-6dC0b894NpjT9_M5Cy9vOp0DWVf9-sTGMPcBw_6KDEtBB2gk35yiciZIbZUN-FoMUjOVU2slpeT9qD1Qc_hO9qlM","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] Computation Time","postDate":"1087985850","msgId":1127,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDYuMS4wLjYuMC4yMDA0MDYyMzEwNTkzNy4wMjRjMTk0MEBwb3AubWFpbC55YWhvby5jby51az4=","inReplyToHeader":"PEJBWTItRjE0MFVTZ2ZjUExLekUwMDA0NTg4N0Bob3RtYWlsLmNvbT4=","referencesHeader":"PEJBWTItRjE0MFVTZ2ZjUExLekUwMDA0NTg4N0Bob3RtYWlsLmNvbT4="},"prevInTopic":1121,"nextInTopic":1129,"prevInTime":1126,"nextInTime":1128,"topicId":845,"numMessagesInTopic":99,"msgSnippet":"... As efficient as a direct call, but less efficient than an inline call (for those calls that do inline well, a _vast_ function will not). ... Erm, sort of,","rawEmail":"Return-Path: &lt;ian_badcoe@...&gt;\r\nX-Sender: ian_badcoe@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 71900 invoked from network); 23 Jun 2004 10:12:18 -0000\r\nReceived: from unknown (66.218.66.167)\n  by m10.grp.scd.yahoo.com with QMQP; 23 Jun 2004 10:12:18 -0000\r\nReceived: from unknown (HELO smtp005.mail.ukl.yahoo.com) (217.12.11.36)\n  by mta6.grp.scd.yahoo.com with SMTP; 23 Jun 2004 10:12:18 -0000\r\nReceived: from unknown (HELO ian2k.yahoo.co.uk) (ian?badcoe@212.159.73.108 with login)\n  by smtp005.mail.ukl.yahoo.com with SMTP; 23 Jun 2004 10:12:15 -0000\r\nMessage-Id: &lt;6.1.0.6.0.20040623105937.024c1940@...&gt;\r\nX-Sender: ian_badcoe@...\r\nX-Mailer: QUALCOMM Windows Eudora Version 6.1.0.6\r\nDate: Wed, 23 Jun 2004 11:17:30 +0100\r\nTo: neat@yahoogroups.com\r\nIn-Reply-To: &lt;BAY2-F140USgfcPLKzE00045887@...&gt;\r\nReferences: &lt;BAY2-F140USgfcPLKzE00045887@...&gt;\r\nMime-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;us-ascii&quot;; format=flowed\r\nX-eGroups-Remote-IP: 217.12.11.36\r\nFrom: Ian Badcoe &lt;ian_badcoe@...&gt;\r\nSubject: Re: [neat] Computation Time\r\nX-Yahoo-Group-Post: member; u=7192225\r\nX-Yahoo-Profile: ian_badcoe\r\n\r\nAt 17:34 22/06/2004, you wrote:\n&gt; &gt;From: Ian Badcoe &lt;ian_badcoe@...&gt;\n&gt; &gt; &gt;Which brings up an important question:  Suppose I have something like\n&gt; &gt;this:\n&gt; &gt; &gt;\n&gt; &gt; &gt;an array of function pointers to custom activation functions\n&gt; &gt; &gt;an array of inputs and expected outputs\n&gt; &gt;\n&gt; &gt;&lt;aside&gt;\n&gt; &gt;\n&gt; &gt;Before we start, you have an array of function pointers...\n&gt; &gt;\n&gt; &gt;This is potentially inefficient (at the instruction level) because calls\n&gt; &gt;through pointers are a type of conditional branch with the same costs as\n&gt; &gt;any conditional.\n&gt;\n&gt;So, if I set the pointer to the value of the first function, and then call\n&gt;that function a million times in a loop, then for those million calls, it is\n&gt;as efficient as a direct call, yes?  But then when I exit the inner loop and\n&gt;then move the pointer to the next function in the list, then the first time\n&gt;I call it I will likely get a pipeline stall.\n\nAs efficient as a direct call, but less efficient than an inline call (for \nthose calls that do inline well, a _vast_ function will not).\n\n&gt;So, I have a choice.  I can either generate code that has one loop per\n&gt;network with a hard-coded function call, or I can have a double loop and\n&gt;call through the function pointer, where the pointer is set by the outer\n&gt;loop.  The double loop has smaller code, and thus fits entirely in the\n&gt;cache.  But once per network it will encounter a pipeline stall.  The many\n&gt;loops will avoid the stall, but will not all fit in cache memory.  It will\n&gt;be a linear memory access pattern, but it is hard to be certain if the CPU\n&gt;will pre-fetch the instructions in time to prevent a stall.  And a memory\n&gt;stall is more expensive than a pipeline stall, is it not?\n\nErm, sort of, I do wonder whether even these functions will really stress \nthe code cache, e.g. the L1 code cache is going to be 64K on even the the \nmeanest CPU, so that&#39;s about 11000 assembly commands and 4000 double \nconstants (assuming they do end up in the code cache, which I think not).\n\nAlso, bear in mind that cache loading is on the basis of 32-byte memory \nlines, not whole functions, so if your loop count is high, it would be good \njust to fit one loop in the code cache at a time.\n\nWhy not reserve judgment until you actually see the size of the function.\n\nIf the function is in one file then you can compile it to an object file \n(.obj or .o, were you on linux or windows?) and as long as you omit any \ndebug or map information from that file, it&#39;s size is pretty close to the \ncode size (a bit larger).\n\nAlso, you can get most compilers (gcc and VC certainly) to compiler to \nreadable assembly (-S on gcc, some tick-box on VC) and it&#39; can be \ninformative to examine the code actually generated.  You can even sometimes \nget the source-lines included as comments (although with optimisation flags \non -- you _DO_ want them on -- the instructions from one line can be \nscattered throughout the function).\n\n&gt;In this case, I don&#39;t think unrolling the outer loop is worthwhile.  For one\n&gt;thing, I will have to transmit the generated code over the internet.  And\n&gt;I&#39;ll bet that if I unroll that loop, the added cost of transmitting the\n&gt;source will exceed the calculation speedup.  And there is a risk that the\n&gt;unrolled loop would be slower, anyway.\n\nI&#39;d guess the opposite.  It depends how many loops you are splitting it \ninto (and whether your order of processing will allow it) but if it is just \na couple of loops then I would try it, and I&#39;d then be able to inline the \nactivation, which I think would also be worthwhile.\n\nYou&#39;ll only know when you get the real functions, however.\n\nMaybe we should stop theorizing and wait for the real code?  I&#39;m perfectly \nhappy to help you dissect one of your real, big generated functions when \nthe you have it.\n\n         Ian Badcoe\n\n\nLiving@Home - Open Source Evolving Organisms - \nhttp://livingathome.sourceforge.net/\n\n\n\n\n"}}