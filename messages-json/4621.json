{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"6F--7G3tOt5VS49Jt_bc894pFXN9iCaAJWm2-iux67whtZ8rBgiLgA5lMledoT8h4G4K09FlPzkOVtfONJ6YdrF8vQFE5pK8IFagurpJQdII","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: The Next Generation of Neural Networks - Geoff Hinton TechTalk","postDate":"1239572177","msgId":4621,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGdydG1zaCtlYjBmQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGdycXN0ZitqbWozQGVHcm91cHMuY29tPg=="},"prevInTopic":4620,"nextInTopic":4622,"prevInTime":4620,"nextInTime":4622,"topicId":4620,"numMessagesInTopic":8,"msgSnippet":"Colin, thanks for the link.  Actually, I d seen this before and it is interesting.  However, I still think something significant is missing:  Like almost any","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 48823 invoked from network); 12 Apr 2009 21:37:03 -0000\r\nX-Received: from unknown (69.147.108.200)\n  by m3.grp.sp2.yahoo.com with QMQP; 12 Apr 2009 21:37:03 -0000\r\nX-Received: from unknown (HELO n15c.bullet.sp1.yahoo.com) (69.147.64.120)\n  by mta1.grp.re1.yahoo.com with SMTP; 12 Apr 2009 21:37:03 -0000\r\nX-Received: from [69.147.65.171] by n15.bullet.sp1.yahoo.com with NNFMP; 12 Apr 2009 21:36:19 -0000\r\nX-Received: from [98.137.34.33] by t13.bullet.mail.sp1.yahoo.com with NNFMP; 12 Apr 2009 21:36:19 -0000\r\nDate: Sun, 12 Apr 2009 21:36:17 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;grtmsh+eb0f@...&gt;\r\nIn-Reply-To: &lt;grqstf+jmj3@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: Re: The Next Generation of Neural Networks - Geoff Hinton TechTalk\r\nX-Yahoo-Group-Post: member; u=54567749; y=jSGtJJOgSJVeQHAr_AmyCXwZ3oU_iOXoYgO-YLe0eU556lJSA0HB\r\nX-Yahoo-Profile: kenstanley01\r\n\r\nColin, thanks for the link.  Actually, I&#39;d seen this before and it is inter=\r\nesting.  However, I still think something significant is missing:  Like alm=\r\nost any other neural network learning algorithm, geometry is irrelevant to =\r\nthe learner in this model.  For example, Hinton shows a bunch of alphanumer=\r\nic characters that the algorithm can learn from their pixel patterns.  As a=\r\n thought experiment, let&#39;s imagine that we scrambled the order of the pixel=\r\n array that represents these characters.  For example, we might map pixel (=\r\n0,0) to (4,2) (arbitrarily).  Every other pixel is also mapped to a unique =\r\nrandom location.  Then assume that we use this same scrambled mapping on ev=\r\nery example, so they are all still consistent.  Unrealistically, assuming I=\r\n understand the model, it will *still* learn the patterns just as effective=\r\nly as before, irrespective of the fact that they are now completely incompr=\r\nehensible scrambles of dots from a human perspective.  \n\nThis thought exper=\r\niment shows that whatever this algorithm is doing, it is missing a major pa=\r\nrt of what helps humans learn patterns: We perceive their geometric relatio=\r\nnships.  One reason that alphanumeric characters are almost always connecte=\r\nd (i.e. they usually don&#39;t have numerous disconnected pixels; the &quot;i&quot; is an=\r\n minor exception) is because we understand adjacency, a key geometric relat=\r\nionship, and exploit it in the way we learn.  We also appreciate symmetry (=\r\nsuch as in &quot;A&quot; and &quot;8&quot;) and we know things like the fact that &quot;O&quot; is round =\r\nin the same way all at all points.  These perceptions are part of what allo=\r\nw us to recognize characters, and also part of why we designed characters t=\r\no be the way they are in the first place.  An algorithm that cannot exploit=\r\n such relationship is missing something big.\n\nI think an interesting questi=\r\non is whether Hinton&#39;s type of algorithm could be combined with an encoding=\r\n that in fact can exploit such relationships.  While HyperNEAT shows that a=\r\n computer can learn from geometric relationships through an indirect encodi=\r\nng, it only works in an evolutionary context.  A big question for the futur=\r\ne is whether indirect encoding can be exploited outside the evolutionary co=\r\nntext.  \n\nken\n\n \n\n--- In neat@yahoogroups.com, &quot;Colin Green&quot; &lt;colin.green1@=\r\n...&gt; wrote:\n&gt;\n&gt; Hi all,\n&gt; \n&gt; I&#39;m posting this link because although it&#39;s no=\r\nt about NEAT I think it&#39;s sufficiently interesting to anyone doing work wit=\r\nh neural nets to warrant a post. Basically Hinton demonstrates how to categ=\r\norise input patterns (he uses handwritten digits) without labels - unsuperv=\r\nised learning or self-supervised if you like.\n&gt; \n&gt; http://www.youtube.com/w=\r\natch?v=3DAyzOUbkUf3M\n&gt; \n&gt; Colin.\n&gt;\n\n\n\n"}}