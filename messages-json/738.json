{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"-dJ_PQOeGq5KhPM3oRdP7TB2OttmC8j3OEId2r2rJWYSkVoJYi50DRZBll1FMK6GjIXV2-bF05FDZ5ioBencehSYkoMSQPhejy75zl4QMI-Y","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: simple question","postDate":"1083628029","msgId":738,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGM3NmxsdCs4dmFmQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDIwMDQwNTAzMjExODI4LjEwNzA0LnFtYWlsQHdlYjE0MzA0Lm1haWwueWFob28uY29tPg=="},"prevInTopic":737,"nextInTopic":739,"prevInTime":737,"nextInTime":739,"topicId":730,"numMessagesInTopic":13,"msgSnippet":"Jim and Derek (I am responding to your posting here too), I probably should have put a bit more time into my short little explanation because obviously I","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 86019 invoked from network); 3 May 2004 23:47:20 -0000\r\nReceived: from unknown (66.218.66.218)\n  by m4.grp.scd.yahoo.com with QMQP; 3 May 2004 23:47:20 -0000\r\nReceived: from unknown (HELO n14.grp.scd.yahoo.com) (66.218.66.69)\n  by mta3.grp.scd.yahoo.com with SMTP; 3 May 2004 23:47:19 -0000\r\nReceived: from [66.218.67.136] by n14.grp.scd.yahoo.com with NNFMP; 03 May 2004 23:47:11 -0000\r\nDate: Mon, 03 May 2004 23:47:09 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;c76llt+8vaf@...&gt;\r\nIn-Reply-To: &lt;20040503211828.10704.qmail@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Length: 7686\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-eGroups-Remote-IP: 66.218.66.69\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: Re: simple question\r\nX-Yahoo-Group-Post: member; u=54567749\r\nX-Yahoo-Profile: kenstanley01\r\n\r\nJim and Derek (I am responding to your posting here too),\n\nI probably should have put a bit more time into my short little\nexplanation because obviously I caused a lot of confusion, and I think\nprobably it&#39;s unwarranted since we basically agree.\n\nMy point was not that the final topology doesn&#39;t matter (it does), but\nthat it is not the *only* thing that matters.  Prior algorithms took\nthe perspective that the *only* goal is to pop out with something\noptimal (whether it be optimal in performance or in minimizing\ntopology).  \n\nWhat I am saying is that, in contrast, it is important that *every*\nnetwork you test over all of evolution should be as minimal as\npossible.  The reason for this is that it is easier to search in small\nspaces.  Jim, there seems to be some confusion in your post about what\nI meant by &quot;space to search through.&quot;  While you interpret this as\n&quot;number of topologies,&quot; I didn&#39;t mean that.  I meant the number of\nconnections *in* the topologies you search through.  The number of\nconnections (and the connections themselves) comprise the space you\nare searching through.  The point is to minimize that space.\n\nI hope it&#39;s clear that the main point of NEAT is to keep candidate\nsolutions as small as possible, so I hope it doesn&#39;t seem like I am\nimplying that topology doesn&#39;t matter.  On the contrary, it matters a\nlot, that&#39;s the whole idea.  The thing is, in old topology-and-weight\nevolving systems, papers usually didn&#39;t pay attention to their\nintermediate solutions- they only looked at the final solution- which\nignores the  topologies you had to go through to get there.\n\nSearch is the big deal here, i.e. can you find what you need in a\npractical amount of time?   For that, topology matters *throughout the\nrun*.  And not because a large topology takes more time to execute,\nbut rather because a large topology means a high-dimensional search\nspace, which means more search will be necessary. \n\nLet me know if it&#39;s still unclear.\n\nken\n\n--- In neat@yahoogroups.com, &quot;Jim O&#39;Flaherty, Jr.&quot;\n&lt;jim_oflaherty_jr@y...&gt; wrote:\n&gt; Ken,\n&gt; \n&gt; I think I may be missing your point somewhere.  I am interested in\nunderstanding.  As such, this\n&gt; is how what your wrote occurred to me.\n&gt; \n&gt; You wrote:\n&gt; &gt; ...The question is not whether it could perform as well with 25%\nless structure, but why would\n&gt; we care if it did?\n&gt; \n&gt; I think this may be a classic example of the theoretical\n(mathmatically) not effectively dealing\n&gt; with some of the tradeoffs necessary in practical application\n(actual computation effort spent\n&gt; combined with obtaining better, not best, results).\n&gt; \n&gt; For example, please imagine the scenerio where a specimen is\nproduced which required 30% less\n&gt; &quot;computational effort&quot; (machine instructions, not network\nactivations) to find a solution 50% the\n&gt; size that of another method.  Theoretically, as long as the\nresultant specimens behave at a\n&gt; similar performance level in the target domain *and* this\nperformance level is essentially the\n&gt; sole measurement, then I would agree with you.\n&gt; \n&gt; However, if you have finite computational resources, intend to use\nNEAT to generate sub-components\n&gt; to a larger structure *and* you intend to do runs in the ten\nthousand to million generation range,\n&gt; then doesn&#39;t the &quot;efficiency&quot; of the specimens becomes highly\ninfluential in the quantity of\n&gt; experiments one can run and evaluate over-all in limited time? \n*Computational* efficiencies can\n&gt; make a slightly less efficient search (with smaller specimens)\nsubstantially outperform that of\n&gt; another method (with larger specimens).  At least that is what sure\nseems to be the case as far as\n&gt; I can see, empirically.\n&gt; \n&gt; Concretely and in the domain of my personal interest thus far,\nCheckers, I am VERY interested in\n&gt; ensuring I am maximizing computing resources. I am NOT interested in\nfinding the perfect checker\n&gt; player.  I am very interested in a resulting phenotype that has the\nminimal number of genes to\n&gt; produce an effective player.  There are local optima &quot;solutions&quot;\nthat are VERY DESIRABLE and are\n&gt; &quot;imperfect&quot; in that they did not find the &quot;optima&quot;.  The found\nsolution given the computation\n&gt; effort is good enough.  In other words, I will put up with an\n&quot;incomplete&quot; or &quot;imperfect&quot; search\n&gt; to arrive at a good enough solution.\n&gt; \n&gt; I get that my &quot;low gene count phenotypes&quot; may lose some robustness\nthe larger gene count\n&gt; phenotypes might be able to offer.  However, in *my* world, if I\ngain only a 0.05% advantage with\n&gt; the more complex type at a cost of more than double the gene count\nin the phenotype, I would need\n&gt; to justify that huge tradeoff, right?  I am not saying it is\nunjustifiable.  I am just saying the\n&gt; tradeoff exists and it would be useful to me to be able to have that\nchoice, right?\n&gt; \n&gt; \n&gt; You wrote:\n&gt; &gt; ...the point of NEAT is that *every* networks you evaluate over\nthe course of all of evolution\n&gt; has a cost.  What&#39;s important is that on average over all of\nevolution we are minimizing the space\n&gt; we have to search through.\n&gt; \n&gt; If I am reading this correctly, your definition of &quot;space to search\nthrough&quot; is to minimize the\n&gt; number of topologies (but not topological complexity) needed to\nsearch the space.  Without the\n&gt; complexity being bounded, won&#39;t this will inevitably lead to an\nexponential growth of\n&gt; computational effort without a downward pressure forcing topological\nefficiency (number of\n&gt; nodes/number of connections kept minimized for fittest specimen)?\n&gt; \n&gt; If that is true, then given anything but very very simple domains,\nXOR and Tic-Tac-Toe, I then\n&gt; think your above assertion might not be as useful for practical\napplication.  This is where I\n&gt; think I may be missing something very subtle.  If you have time and\npatience, please elaborate\n&gt; with concrete examples here.  For whatever reason, I am not getting\nthe theory.  :^)\n&gt; \n&gt; As I consider both the topological generation via its genotype of\nthe specimen *and* the\n&gt; computational effort uniquely required by the resulting phenotype to\nbe the &quot;cost&quot; of a specimen,\n&gt; what use is there in my minimizing the &quot;search space&quot; (reducing the\ncount of genotypes but not\n&gt; complexity) only to have enormous specimens (phenotypes with many\ngenes) which have a very high\n&gt; cost, as I define cost?\n&gt; \n&gt; Help!  What is it I am missing here?\n&gt; \n&gt; \n&gt; Jim O&#39;Flaherty\n&gt; \n&gt; \n&gt; --- Kenneth Stanley &lt;kstanley@c...&gt; wrote:\n&gt; &gt; --- In neat@yahoogroups.com, Derek James &lt;djames@g...&gt; wrote:\n&gt; &gt; &gt; The only way to know if you&#39;ve got redundant features would be\nto \n&gt; &gt; use\n&gt; &gt; &gt; one of the above approaches, correct?  Has any of this type of\n&gt; &gt; &gt; analysis been done?  That is, how do we know that the champions \n&gt; &gt; from\n&gt; &gt; &gt; experiments such as double pole-balancing or the robot duels\ncould \n&gt; &gt; not\n&gt; &gt; &gt; perform as well with 25% or more of their topology removed?\n&gt; &gt; &gt; \n&gt; &gt; \n&gt; &gt; I think this is missing the point, though.  This is exactly the \n&gt; &gt; perspective I was trying to argue we should change.  The question\nis \n&gt; &gt; not whether it could perform as well with 25% less structure, but \n&gt; &gt; why would we care if it did?  The goal of systems like that, that \n&gt; &gt; try to prune down at the end of evolution to a small &quot;good&quot; \n&gt; &gt; solution, is just to find a final solution that looks good.  But\nthe \n&gt; &gt; point of NEAT is that *every* networks you evaluate over the\ncourse \n&gt; &gt; of all of evolution has a cost.  What&#39;s important is that on\naverage \n&gt; &gt; over all of evolution we are minimizing the space we have to\nsearch \n&gt; &gt; through.  \n&gt; &gt; \n&gt; &gt; ken\n&gt; &gt; \n&gt; &gt; \n&gt; \n&gt; \n&gt; \n&gt; \t\n&gt; \t\t\n&gt; __________________________________\n&gt; Do you Yahoo!?\n&gt; Win a $20,000 Career Makeover at Yahoo! HotJobs  \n&gt; http://hotjobs.sweepstakes.yahoo.com/careermakeover\n\n\n"}}