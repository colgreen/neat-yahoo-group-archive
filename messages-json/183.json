{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":104426122,"authorName":"Mitchell Timin","from":"Mitchell Timin &lt;zenguyuno@...&gt;","profile":"zenguyuno","replyTo":"LIST","senderId":"X1nrnFw6yNuD3glxXGbjj9vrLxbqF9DwjJf5FycaE9varkDIqArMpGVkPpBUQPLfsYKeQfjvQDF4XhyjWZTYy-EbawkxBU4Wu81u","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] Re: Learning How to Learn","postDate":"1067630467","msgId":183,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDIwMDMxMDMxMjAwMTA3LjQ4NTY1LnFtYWlsQHdlYjIxNDA3Lm1haWwueWFob28uY29tPg==","inReplyToHeader":"PGJudWNraythcXVhQGVHcm91cHMuY29tPg=="},"prevInTopic":182,"nextInTopic":187,"prevInTime":182,"nextInTime":185,"topicId":170,"numMessagesInTopic":15,"msgSnippet":"... Yes.  In that case the state consists of N floating point numbers rather than N bits, so theoretically the RAM is greatly increased.  However, it may or","rawEmail":"Return-Path: &lt;zenguyuno@...&gt;\r\nX-Sender: zenguyuno@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 84132 invoked from network); 31 Oct 2003 20:01:08 -0000\r\nReceived: from unknown (66.218.66.172)\n  by m16.grp.scd.yahoo.com with QMQP; 31 Oct 2003 20:01:08 -0000\r\nReceived: from unknown (HELO web21407.mail.yahoo.com) (216.136.232.77)\n  by mta4.grp.scd.yahoo.com with SMTP; 31 Oct 2003 20:01:07 -0000\r\nMessage-ID: &lt;20031031200107.48565.qmail@...&gt;\r\nReceived: from [64.180.115.78] by web21407.mail.yahoo.com via HTTP; Fri, 31 Oct 2003 12:01:07 PST\r\nDate: Fri, 31 Oct 2003 12:01:07 -0800 (PST)\r\nSubject: Re: [neat] Re: Learning How to Learn\r\nTo: neat@yahoogroups.com\r\nIn-Reply-To: &lt;bnuckk+aqua@...&gt;\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=us-ascii\r\nFrom: Mitchell Timin &lt;zenguyuno@...&gt;\r\nX-Yahoo-Group-Post: member; u=104426122\r\nX-Yahoo-Profile: zenguyuno\r\n\r\n--- Kenneth Stanley &lt;kstanley@...&gt; wrote:\n&gt; --- In neat@yahoogroups.com, Mitchell Timin\n&gt; &lt;zenguyuno@y...&gt; wrote:\n&gt; &gt; Ken wrote:\n&gt; &gt; &lt;snip&gt;\n&gt; &gt; &gt; And what is the\n&gt; &gt; &gt; capacity of a\n&gt; &gt; &gt; fixed-weight recurrent network to remember\n&gt; &gt; &gt; information?\n&gt; &gt; \n&gt; &gt; The total amount of RAM is quite clearly defined: \n&gt; N\n&gt; &gt; bits in the case of N binary neurons.   So N bits\n&gt; is\n&gt; &gt; clearly an upper limit on the memory capacity of a\n&gt; &gt; fully recurrent binary ANN with N neurons.  Since\n&gt; this\n&gt; &gt; ANN has more than two thousand weights, the memory\n&gt; &gt; capacity seems tiny, in proportion.  Furthermore,\n&gt; some\n&gt; &gt; of it is probably required to support the\n&gt; computation,\n&gt; &gt; and hence not available for the requirements of\n&gt; the\n&gt; &gt; problem domain.\n&gt; \n&gt; What about neurons with continuous activation\n&gt; functions,\n&gt; like sigmoid.  Do you think that increases the\n&gt; effective\n&gt; storage capacity of the network?\n\nYes.  In that case the state consists of N floating\npoint numbers rather than N bits, so theoretically the\nRAM is greatly increased.  However, it may or may not\nbe usable in a particular problem domain.  In the case\nof my 4-card puzzle we are using binary inputs and\noutputs.  If we continue to do that, this theoretical\nextra memory may not be useful, but it might.  The\nmain reason I&#39;m not eager to try that is because it\nwould slow the execution substantially.  This is\nbecause with my binary network I don&#39;t have to\nactually multiply the weights by anything.  Every\ninput is either 0 or 1.  For the 0&#39;s I just skip that\nsignal; for the 1&#39;s I just add that weight to the\ntotal.  Hence my ANN code is extremely rapid; it has\nto be to deal with 2000 weights!\n\n=====\nMitchell Timin\nhttp://annevolve.sourceforge.net\n\n__________________________________\nDo you Yahoo!?\nExclusive Video Premiere - Britney Spears\nhttp://launch.yahoo.com/promos/britneyspears/\n\n"}}