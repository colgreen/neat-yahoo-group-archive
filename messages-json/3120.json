{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":283334584,"authorName":"petar_chervenski","from":"&quot;petar_chervenski&quot; &lt;petar_chervenski@...&gt;","profile":"petar_chervenski","replyTo":"LIST","senderId":"_dYyENC9qGC-vB5w_8XyXFYvksgBejvRlI70qVoYZ862wd_ye4LpyaHUtabF7tRctha-d21ntrpzSGt9Zz8ktkObGD5bsHZcRYKxUrdKaKnBd9RMShg","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Mix of Identity and Sigmoid activation functions","postDate":"1176312334","msgId":3120,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGV2ajVtZSt1N284QGVHcm91cHMuY29tPg==","inReplyToHeader":"PGQyNzhlM2FkMDcwNDExMDk0OW00ZDYzZDllZXRlZDA5Zjg5NzJjNzZlMzFhQG1haWwuZ21haWwuY29tPg=="},"prevInTopic":3119,"nextInTopic":3123,"prevInTime":3119,"nextInTime":3121,"topicId":3114,"numMessagesInTopic":8,"msgSnippet":"Well, in some point of view, Neuro-Evolution is a genetic programming scheme, composing many many 1/(1+exp(-(weighted_inputs * x))) in such a way, that the","rawEmail":"Return-Path: &lt;petar_chervenski@...&gt;\r\nX-Sender: petar_chervenski@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 20490 invoked from network); 11 Apr 2007 17:27:58 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m48.grp.scd.yahoo.com with QMQP; 11 Apr 2007 17:27:58 -0000\r\nReceived: from unknown (HELO n10c.bullet.sp1.yahoo.com) (69.147.64.105)\n  by mta5.grp.scd.yahoo.com with SMTP; 11 Apr 2007 17:27:55 -0000\r\nReceived: from [216.252.122.217] by n10.bullet.sp1.yahoo.com with NNFMP; 11 Apr 2007 17:25:37 -0000\r\nReceived: from [66.218.69.3] by t2.bullet.sp1.yahoo.com with NNFMP; 11 Apr 2007 17:25:36 -0000\r\nReceived: from [66.218.66.82] by t3.bullet.scd.yahoo.com with NNFMP; 11 Apr 2007 17:25:36 -0000\r\nDate: Wed, 11 Apr 2007 17:25:34 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;evj5me+u7o8@...&gt;\r\nIn-Reply-To: &lt;d278e3ad0704110949m4d63d9eeted09f8972c76e31a@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0\r\nFrom: &quot;petar_chervenski&quot; &lt;petar_chervenski@...&gt;\r\nSubject: Re: Mix of Identity and Sigmoid activation functions\r\nX-Yahoo-Group-Post: member; u=283334584; y=ra03-W0OyK1PUv9e5CUqbufzAV7SJP02qk3GZ_SUW-ceQrN3BTjCZDIGlg\r\nX-Yahoo-Profile: petar_chervenski\r\n\r\nWell, in some point of view, Neuro-Evolution is a genetic programming \nsche=\r\nme, composing many many &quot;1/(1+exp(-(weighted_inputs * x)))&quot; in \nsuch a way,=\r\n that the composition will compute something at all :) \nIn fact, the basic =\r\nprinciples of NEAT are important, not the thing \nthey are applied on.\n\n--- =\r\nIn neat@yahoogroups.com, &quot;Rafael C.P.&quot; &lt;kurama.youko.br@...&gt; \nwrote:\n&gt;\n&gt; He=\r\ny, all that things will simply take us to genetic programming at \nthe\n&gt; end=\r\n...\n&gt; \n&gt; On 4/11/07, Rafael C.P. &lt;kurama.youko.br@...&gt; wrote:\n&gt; &gt;\n&gt; &gt; Total=\r\nly agreed! Your solution seems good for the problem. About \nthe 4\n&gt; &gt; funct=\r\nions (MIN, MAX, ADD (default), MUL), do you mean apply them \nbefore the\n&gt; &gt;=\r\n activation function? It seems good too, one more generalization \nto neural=\r\n\n&gt; &gt; nets. AVG (average), SD (standard deviation) and other \nstatistical fu=\r\nnctions\n&gt; &gt; would be fine too.\n&gt; &gt;\n&gt; &gt; And something must be clear about it=\r\n: adding all that things \ndoesn&#39;t\n&gt; &gt; increases the computational power of =\r\na NN since every operation \nhere can be\n&gt; &gt; simulated. It could be better i=\r\nn other ways (like using less \nneurons).\n&gt; &gt;\n&gt; &gt; On 4/11/07, petar_chervens=\r\nki &lt;petar_chervenski@...&gt; wrote:\n&gt; &gt; &gt;\n&gt; &gt; &gt;   This is the Identity functio=\r\nn? Heh, I call it linear.. So I \nhave this\n&gt; &gt; &gt; already :)\n&gt; &gt; &gt; You&#39;re ri=\r\nght that a simple mutation will have a significant \nimpact,\n&gt; &gt; &gt; but what =\r\nif we adjust the importance factors for speciation? I \nmean\n&gt; &gt; &gt; that if a=\r\nctivation function types are so important to \nperformace, we\n&gt; &gt; &gt; can incr=\r\nease that multiplier and every time such a mutation \noccurs, a\n&gt; &gt; &gt; new sp=\r\necies will be created that will further optimize its \nweights\n&gt; &gt; &gt; and con=\r\nnections, not changing the activation functions..\n&gt; &gt; &gt; I think the best th=\r\ning to do is to supply the system with \neverything,\n&gt; &gt; &gt; MIN,MAX,ADD and M=\r\nUL summing functions, and let evolution decide \nwhat\n&gt; &gt; &gt; is best. There m=\r\nay be some heuristics, of course.\n&gt; &gt; &gt;\n&gt; &gt; &gt; --- In neat@yahoogroups.com &lt;=\r\nneat%40yahoogroups.com&gt;, &quot;Rafael \nC.P.&quot; &lt;\n&gt; &gt; &gt; kurama.youko.br@&gt;\n&gt; &gt; &gt; wro=\r\nte:\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; Identity function is f(x) =3D x, simple =3D)\n&gt; &gt; &gt; &gt; I&#39;=\r\nve thought about multi-activation-functions populations as \nyou 2\n&gt; &gt; &gt; pro=\r\nposed.\n&gt; &gt; &gt; &gt; My contribution here is to add the sine function too, allowi=\r\nng\n&gt; &gt; &gt; recognition\n&gt; &gt; &gt; &gt; of periodic patterns.\n&gt; &gt; &gt; &gt; The problem I se=\r\ne with that approach is that a simple mutation\n&gt; &gt; &gt; &gt; including/modifying =\r\nan activation function would cause a big \nchange\n&gt; &gt; &gt; (with\n&gt; &gt; &gt; &gt; undefi=\r\nned direction) to the outputs. There must be a way to \ncluster\n&gt; &gt; &gt; simila=\r\nr\n&gt; &gt; &gt; &gt; functions in order to make always the smallest change \npossible a=\r\nnd\n&gt; &gt; &gt; to the\n&gt; &gt; &gt; &gt; right direction.\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; PS: you could use =\r\nthe MIN function instead of AND (and MAX \nfor OR),\n&gt; &gt; &gt; turning\n&gt; &gt; &gt; &gt; it=\r\n into fuzzy logic.\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; On 4/11/07, petar_chervenski &lt;petar_cher=\r\nvenski@&gt; wrote:\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; Hi Shane,\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; I don&#39;t k=\r\nnow what the identity function is exactly.. I need \nto\n&gt; &gt; &gt; learn\n&gt; &gt; &gt; &gt; =\r\n&gt; that.\n&gt; &gt; &gt; &gt; &gt; But I suppose you can also try to split the neuron \nactiv=\r\nation into\n&gt; &gt; &gt; &gt; &gt; stages, introduce another function that sums the weigh=\r\nted \ninputs\n&gt; &gt; &gt; in\n&gt; &gt; &gt; &gt; &gt; different ways, you know that this is usuall=\r\ny addition, but\n&gt; &gt; &gt; &gt; &gt; multiplication is also promising, since it is lik=\r\ne the AND \nlogical\n&gt; &gt; &gt; &gt; &gt; operation on those inputs. When this is combin=\r\ned with \nlinear and\n&gt; &gt; &gt; &gt; &gt; other activation functions, it turns NEAT int=\r\no actually a ..\n&gt; &gt; &gt; &gt; &gt; something like a math equation evolving system.. =\r\nI don&#39;t \nthink\n&gt; &gt; &gt; this\n&gt; &gt; &gt; &gt; &gt; will enhance performace in most tasks, =\r\nbut interesting \nresults\n&gt; &gt; &gt; may be\n&gt; &gt; &gt; &gt; &gt; obtained.\n&gt; &gt; &gt; &gt; &gt; There m=\r\nay be also a MAX summing function, that returns the \nmaximum\n&gt; &gt; &gt; &gt; &gt; inpu=\r\nt of all. This way the neurons can have winner-take-all\n&gt; &gt; &gt; &gt; &gt; behaviour=\r\n, I guess. I have implemented this and I see it \ngives\n&gt; &gt; &gt; &gt; &gt; results.\n&gt;=\r\n &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; Peter\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; --- In neat@yahoogroups.com &lt;n=\r\neat%40yahoogroups.com&gt; &lt;neat%\n&gt; &gt; &gt; 40yahoogroups.com&gt;, &quot;shanemcdonaldryan&quot;=\r\n\n&gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &lt;shanemcdonaldryan@&gt; wrote:\n&gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; =\r\n&gt; &gt; I am sure this must have been done but I can&#39;t find any \npapers\n&gt; &gt; &gt; o=\r\nn\n&gt; &gt; &gt; &gt; &gt; it.\n&gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; Has anyone tried starting out with =\r\nan initial population \nthat\n&gt; &gt; &gt; &gt; &gt; connects\n&gt; &gt; &gt; &gt; &gt; &gt; all the inputs t=\r\no the outputs with a parallel set of \nneurons\n&gt; &gt; &gt; using\n&gt; &gt; &gt; &gt; &gt; &gt; ident=\r\nity as the activation function? Then add a new type \nof\n&gt; &gt; &gt; mutation\n&gt; &gt; =\r\n&gt; &gt; &gt; &gt; that can add new neurons with the identity activation \nfunction.\n&gt; =\r\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; Basically what I am proposing is starting with the sa=\r\nme \ninitial\n&gt; &gt; &gt; NEAT\n&gt; &gt; &gt; &gt; &gt; &gt; configuration of all the inputs connecte=\r\nd to the outputs \nvia\n&gt; &gt; &gt; &gt; &gt; sigmoids.\n&gt; &gt; &gt; &gt; &gt; &gt; But doubling up this =\r\nconfiguration with an identical net\n&gt; &gt; &gt; connected\n&gt; &gt; &gt; &gt; &gt; &gt; via the ide=\r\nntity activation function.\n&gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; The whole point of this =\r\nwould be to create a hybrid \nnetwork\n&gt; &gt; &gt; that is\n&gt; &gt; &gt; &gt; &gt; &gt; good at capt=\r\nuring the linear aspects (A+B, cross product) \nof the\n&gt; &gt; &gt; &gt; &gt; data\n&gt; &gt; &gt; =\r\n&gt; &gt; &gt; and composing it with the non-linear aspects.\n&gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt;=\r\n But now that I think about it. I guess a good starting \nquestion\n&gt; &gt; &gt; &gt; &gt;=\r\n would\n&gt; &gt; &gt; &gt; &gt; &gt; be is a Neural Net using only the Identity activation \nf=\r\nunction\n&gt; &gt; &gt; good\n&gt; &gt; &gt; &gt; &gt; &gt; at approximating linear functions?\n&gt; &gt; &gt; &gt; &gt;=\r\n &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; Something like this might be good at approximating time \nser=\r\nies\n&gt; &gt; &gt; with\n&gt; &gt; &gt; &gt; &gt; a\n&gt; &gt; &gt; &gt; &gt; &gt; simple linear trend. So you don&#39;t ha=\r\nve to detrend the \ndata.\n&gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; &gt; Thanks,\n&gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; =\r\n&gt; &gt; &gt; Shane\n&gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt;=\r\n &gt;\n&gt; &gt; &gt; &gt; --\n&gt; &gt; &gt; &gt; =3D=3D=3D=3D=3D=3D=3D=3D=3D\n&gt; &gt; &gt; &gt; Rafael C.P.\n&gt; &gt; &gt;=\r\n &gt; =3D=3D=3D=3D=3D=3D=3D=3D=3D\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;  \n&gt; &gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; =\r\n--\n&gt; &gt; =3D=3D=3D=3D=3D=3D=3D=3D=3D\n&gt; &gt; Rafael C.P.\n&gt; &gt; =3D=3D=3D=3D=3D=3D=\r\n=3D=3D=3D\n&gt; \n&gt; \n&gt; \n&gt; \n&gt; -- \n&gt; =3D=3D=3D=3D=3D=3D=3D=3D=3D\n&gt; Rafael C.P.\n&gt; =\r\n=3D=3D=3D=3D=3D=3D=3D=3D=3D\n&gt;\n\n\n\n"}}