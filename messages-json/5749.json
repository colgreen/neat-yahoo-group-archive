{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":234577593,"authorName":"olivercoleman04","from":"&quot;olivercoleman04&quot; &lt;oliver.coleman@...&gt;","profile":"olivercoleman04","replyTo":"LIST","senderId":"E6rxTa3p8LVX8QANLhGa-FCimjYrw5BGxLiIjCyDnmdWUMIdOih5vV26i0QVUOQm57ieTUkNFlw3IRS1Atm4AxNfSgWL19wVsyqUWFgoZQM4FK0","spamInfo":{"isSpam":false,"reason":"6"},"subject":"CPPN IO encoding for fully connected/recurrent networks using HyperNEAT","postDate":"1325049546","msgId":5749,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGpkZThzYStoMWtzQGVHcm91cHMuY29tPg=="},"prevInTopic":0,"nextInTopic":5750,"prevInTime":5748,"nextInTime":5750,"topicId":5749,"numMessagesInTopic":2,"msgSnippet":"I ve been thinking about methods for evolving recurrent networks or network modules using HyperNEAT, and have been wondering about the input and output","rawEmail":"Return-Path: &lt;oliver.coleman@...&gt;\r\nX-Sender: oliver.coleman@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 40467 invoked from network); 28 Dec 2011 05:19:07 -0000\r\nX-Received: from unknown (98.137.35.161)\n  by m7.grp.sp2.yahoo.com with QMQP; 28 Dec 2011 05:19:07 -0000\r\nX-Received: from unknown (HELO ng15-ip1.bullet.mail.ne1.yahoo.com) (98.138.215.234)\n  by mta5.grp.sp2.yahoo.com with SMTP; 28 Dec 2011 05:19:07 -0000\r\nX-Received: from [98.138.217.179] by ng15.bullet.mail.ne1.yahoo.com with NNFMP; 28 Dec 2011 05:19:06 -0000\r\nX-Received: from [69.147.65.148] by tg4.bullet.mail.ne1.yahoo.com with NNFMP; 28 Dec 2011 05:19:06 -0000\r\nX-Received: from [98.137.34.36] by t11.bullet.mail.sp1.yahoo.com with NNFMP; 28 Dec 2011 05:19:06 -0000\r\nDate: Wed, 28 Dec 2011 05:19:06 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;jde8sa+h1ks@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;olivercoleman04&quot; &lt;oliver.coleman@...&gt;\r\nSubject: CPPN IO encoding for fully connected/recurrent networks using HyperNEAT\r\nX-Yahoo-Group-Post: member; u=234577593; y=afuUODHWEJsGn5rszlPppPxP9kF9aNKQSwSYnI-GJorJTow1EnYeE1V8d6W_fQe1xqd6YUYYTQ\r\nX-Yahoo-Profile: olivercoleman04\r\n\r\nI&#39;ve been thinking about methods for evolving recurrent networks or network=\r\n modules using HyperNEAT, and have been wondering about the input and outpu=\r\nt representation used by the CPPN.\n\nMost papers that evolve networks with m=\r\nultiple layers use 2D layers (and so 2 inputs for the CPPN) and an output i=\r\nn the CPPN per every pair of connected layers (call it a weight layer). An =\r\nalternative is to use an additional input to specify the connection layer, =\r\nor z-axis coordinates; it&#39;s just another dimension orthogonal to the two di=\r\nmensions of the layers. Or if you&#39;re allowing connections between arbitrary=\r\n layers (rather than just the next layer), then you would use two additiona=\r\nl inputs to specify the z-axis coordinates for source and target neurons.\n\n=\r\nAn output for each weight layer, as is done in most/all published experimen=\r\nts to date, seems to make sense because we generally want quite different p=\r\natterns of weights between different pairs of layers (even if different con=\r\nnection layer patterns are geometrically correlated they are still typicall=\r\ny quite different), and it seems to be easier to generate CPPNs that produc=\r\ne very different (but correlated) patterns from different outputs than it i=\r\ns to generate CPPNs that produce very different patterns depending on the v=\r\nalue of an input (at least, I tried both encoding schemes and this is what =\r\nI found).\n\nGetting back to recurrent networks; what does the above mean for=\r\n a collection of neurons arranged in some N-dimensional space that are pote=\r\nntially fully recurrent/connected? Let&#39;s say the substrate neurons are arra=\r\nnged in a 3-dimensional space. We would create CPPNS with 6 inputs (2 for e=\r\nach dimension for source and target neuron coordinates) and one output. How=\r\never, allowing recurrent connections essentially means that we have many di=\r\nfferent 2D (and 1D) layers connecting to many other layers (both parallel a=\r\nnd orthogonal in two ways), so according to the above discussion perhaps we=\r\n need an output for each possible combination of layers? This doesn&#39;t seem =\r\nlike a very good solution as we would quickly end up with vast numbers of o=\r\nutputs.\n\nSo how could we make it easier to generate CPPNs that have an inpu=\r\nt (or some constant number of inputs) for each dimension and one output for=\r\n encoding fully connected/recurrent substrate networks? If we assume the re=\r\nason having multiple outputs works is because the CPPN can easily encode la=\r\nrgely disparate (but correlated) functions for each output, the challenge f=\r\nor a single output is to enable &quot;piping&quot; or multiplexing largely disparate =\r\nfunctions through it. Perhaps one method is to introduce a transfer functio=\r\nn available to the CPPN that produces a high/on output only for a specific =\r\nrange of input value, and low/off output for everything else. Seems plausib=\r\nle, except we already commonly use tthe Gaussian function with a bias, whic=\r\nh can already perform this function (especially when combined serially with=\r\n a step function). Perhaps introducing a specialised transfer function (eg =\r\nakin to a Gaussian and step function in series, or something more like an e=\r\nntire multiplexer in a node) for this task would help things along?\n\nThough=\r\nts and/or experimental results, anyone? :)\n\n\n"}}