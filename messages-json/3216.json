{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"XVlLkFD_b2d4oplGWfGUmh4dTJKbe1v5cImDW4HyiWf7gWxMwP4XOXDyNIvHBR20jcET0WJ-0X4qY0RC6dhQBbVhDQYmujTc100KbiABqsOy","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: A Few Thoughts on HyperNEAT","postDate":"1177894055","msgId":3216,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGYxM2ViOCtsbTB1QGVHcm91cHMuY29tPg==","inReplyToHeader":"PFBpbmUuTE5YLjQuNjQuMDcwNDI5MTcxODQ3MC4xODEyNkBjaGllZi13aWdndW0uY3MudXRleGFzLmVkdT4="},"prevInTopic":3215,"nextInTopic":3217,"prevInTime":3215,"nextInTime":3217,"topicId":3214,"numMessagesInTopic":27,"msgSnippet":"Oh well, I was trying to avoid getting too technical about No Free Lunch because I figured that would be veering too far off the point that people were","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 2591 invoked from network); 30 Apr 2007 00:48:11 -0000\r\nReceived: from unknown (66.218.66.70)\n  by m45.grp.scd.yahoo.com with QMQP; 30 Apr 2007 00:48:11 -0000\r\nReceived: from unknown (HELO n13c.bullet.sp1.yahoo.com) (69.147.64.114)\n  by mta12.grp.scd.yahoo.com with SMTP; 30 Apr 2007 00:48:11 -0000\r\nReceived: from [216.252.122.219] by n13.bullet.sp1.yahoo.com with NNFMP; 30 Apr 2007 00:47:36 -0000\r\nReceived: from [66.218.69.4] by t4.bullet.sp1.yahoo.com with NNFMP; 30 Apr 2007 00:47:36 -0000\r\nReceived: from [66.218.66.89] by t4.bullet.scd.yahoo.com with NNFMP; 30 Apr 2007 00:47:36 -0000\r\nDate: Mon, 30 Apr 2007 00:47:35 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;f13eb8+lm0u@...&gt;\r\nIn-Reply-To: &lt;Pine.LNX.4.64.0704291718470.18126@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: Re: A Few Thoughts on HyperNEAT\r\nX-Yahoo-Group-Post: member; u=54567749; y=PvOePQAys4zFO4KcC0vshtgw7oeuBKa2ZU3GZ-el_WO3Cj7dmxtv\r\nX-Yahoo-Profile: kenstanley01\r\n\r\nOh well, I was trying to avoid getting too technical about No Free \nLunch b=\r\necause I figured that would be veering too far off the point \nthat people w=\r\nere discussing.  But yes if you want to get into the \ndetails of course the=\r\nre are a number of considerations.  That&#39;s why I \nsaid it &quot;may&quot; be better f=\r\nor evolving very large scale brains.  \nThe &quot;may&quot; hinges on the issues you b=\r\nring up and others.  However, \neven the opportunity to be better is an adva=\r\nntage over having no such \nopportunity.  The opportunity of course can be s=\r\nquandered with the \nwrong a priori knowledge.   Yet part of my point is tha=\r\nt it will \noften be the case that the natural geometry of a task is all you=\r\n need \nto provide a powerful bias (or at least a bias that is better than \n=\r\nnothing), so it will often be possible to seize the opportunity \nwithout a =\r\ngreat deal of effort.\n\nIt is true too that NEAT and other neural network al=\r\ngorithms do \nindeed allow for some inclusion of prior knowledge through the=\r\nir \ninput/output encoding.  However, note how I phrased my \nclaim: &quot;HyperNE=\r\nAT is not subject to the No Free Lunch theorem when \ncomparing to algorithm=\r\ns that do not allow injecting such a priori \nknowledge.&quot;  That is, among al=\r\ngorithms that allow you to decide on an \ninput encoding in the traditional =\r\nway, the provision of such encoding \ndoes not give one algorithm a leg up o=\r\nver another since they all \nallow for such knowledge to be included.  Hyper=\r\nNEAT, on the other \nhand, allows a new kind of knowledge (i.e. geometry) to=\r\n be included \nand therefore does have a potential leg up on that class of \n=\r\nalgorithms.  Of course it depends on how well the user takes \nadvantage of =\r\nthe opportunity, but the opportunity is now there.  This \nfact does indeed =\r\nmean that statements about HyperNEAT vs. other \nneuroevolution (or even mac=\r\nhine learning) algorithms can cite an \nopportunity to genuinely be better o=\r\nn average, which in effect brings \nit outside NFL in one particular sense. =\r\n \n\nIn a way, this claim is nothing surprising, because it only says that \nt=\r\nhe algorithm is biased to a certain type of problem, which is \nexactly what=\r\n Wolpert said algorithms need to be to have any real \nadvantage.  However, =\r\nwhen you consider what type of problem we are \ntalking about, it&#39;s a pretty=\r\n significant observation.\n\nken\n\n\n--- In neat@yahoogroups.com, Joseph Reisin=\r\nger &lt;joeraii@...&gt; wrote:\n&gt;\n&gt; &gt; While it may be viewed as a weakness that th=\r\ne user must decide the\n&gt; &gt; node layout, my view is that it is actually quit=\r\ne a bonus, \nbecause it\n&gt; &gt; means we have an opportuntiy to inject intuitive=\r\n relationships \ninto\n&gt; &gt; the learning process from the get-go.  The most in=\r\nteresting\n&gt; &gt; consequence of this ability is that HyperNEAT is not subject =\r\nto \nthe\n&gt; &gt; No Free Lunch theorem when comparing to algorithms that do not =\r\n\nallow\n&gt; &gt; injecting such a priori knowledge, which justifies the expectati=\r\non\n&gt; &gt; that HyperNEAT actually may be &quot;better&quot; for evolving very-large-\nsca=\r\nle\n&gt; &gt; brains.\n&gt; &gt;\n&gt; &gt; No Free Lunch is a theorem showing that no single bl=\r\nack-box search\n&gt; &gt; method can be better than any other when averaged over a=\r\nll \npossible\n&gt; &gt; problems.  However, HyperNEAT escapes this trap because it=\r\n is no\n&gt; &gt; longer a black box algorithm, thanks to the ability to inject a\n=\r\n&gt; &gt; priori relationships at the start.  In many cases this a priori\n&gt; &gt; kno=\r\nwledge is very simple to include because it follows directly \nfrom\n&gt; &gt; the =\r\nobvious geometry of the task (such as a visual field or game\n&gt; &gt; board bein=\r\ng Cartesian in an self-evident arrangement).  Yet the\n&gt; &gt; significance of s=\r\nuch knowledge (as opposed to not having it) is\n&gt; &gt; priceless.  So having th=\r\ne capacity to arrange sensors and outputs \nin\n&gt; &gt; the way you want is quite=\r\n a powerful new capability.\n&gt; \n&gt; Hi Ken,\n&gt; \n&gt; I&#39;d be really careful in how =\r\nyou word this statement. The point you \nare \n&gt; trying to make, I think, is =\r\nthat for any specific problem you&#39;d \nwant to \n&gt; solve with HyperNEAT, the e=\r\nxperimenter will always inject the \n&gt; /appropriate/ prior knowledge for tha=\r\nt problem, and thus HyperNEAT \nwill \n&gt; /never/ be used in a black-box setti=\r\nng (e.g. in situations where no \nprior \n&gt; knowledge is available). In this =\r\ncase, yes, NFL no longer holds \nbecause, \n&gt; in a sense, you are not applyin=\r\ng the same algorithm to each \nproblem. \n&gt; Rather, you are peeking at the pr=\r\noblem, and then selecting the most \n&gt; appropriate settings for HyperNEAT to=\r\n solve that problem.\n&gt; \n&gt; Since your argument relies on the use of an &quot;orac=\r\nle&quot; experimenter \nto know \n&gt; the correct properties of the problem to use a=\r\ns prior knowledge, I \nthink \n&gt; you are being a little disingenuous. What if=\r\n the experimenter \nmesses up \n&gt; and puts in the wrong prior knowledge? Or, =\r\neven worse, puts in \nalmost \n&gt; correct prior knowledge that s/he assumes ge=\r\nneralizes from other \nproblems. \n&gt; In this case, without any other limitati=\r\nons placed on the space of \n&gt; problems we may be trying, we are back to NFL=\r\n land. Based on this I \ndon&#39;t \n&gt; think you are justified in saying HyperNEA=\r\nT is not a black box \nalgorithm.\n&gt; \n&gt; Furthermore, I would argue that Hyper=\r\nNEAT&#39;s ability to inject prior \n&gt; knowledge is not something special to tha=\r\nt algorithm. You can do \nthis in \n&gt; NEAT as well, albeit to a less spectacu=\r\nlar extent, by manipulating \nthe \n&gt; input coding. The same problem can be m=\r\nade arbitrarily difficult by \nusing \n&gt; &quot;dumb&quot; input codings, i.e. input cod=\r\nings that you might use if you \ndidn&#39;t \n&gt; have any prior knowledge of the p=\r\nroblem.  Nate has done some \ninteresting \n&gt; work in this area.\n&gt; \n&gt; In any =\r\ncase, I do agree with your original point: HyperNEAT \nprobably has a \n&gt; bia=\r\ns which makes it better for large scale problems, given the \nappropriate \n&gt;=\r\n prior knowledge, and NEAT probably has a bias towards simpler \nproblems. I=\r\n \n&gt; would leave out the appeal to &quot;injecting prior knowledge&quot; and just \nsay=\r\n: \n&gt; &quot;HyperNEAT is expected to perform better on the class of large-\nscale =\r\n\n&gt; problems with regular structure.&quot; This statement can be made NFL-\nproof =\r\nin \n&gt; and of itsel0f, simply by ensuring that that class of problems is \nno=\r\nt \n&gt; closed-under-permutation, which it probably isn&#39;t: See, for \ninstance,=\r\n Marc \n&gt; Toussaint&#39;s work on compressible search landscapes (i.e. problems =\r\n\nwith \n&gt; regular structure) and how the class of such landscapes is not CUP=\r\n \nand \n&gt; thus NFL does not apply.\n&gt; \n&gt; \n&gt; Ok, phew. Let me recap my argumen=\r\nt just in case I lost anyone: Ken, \nI \n&gt; think your intuition here about Hy=\r\nperNEAT is perfectly correct, but \nyour \n&gt; justification and statement that=\r\n HyperNEAT does not fall under the \npurview \n&gt; of NFL is tenuous at best.\n&gt;=\r\n \n&gt; \n&gt; -- Joe\n&gt; \n&gt; -- \n&gt; \n&gt; Joseph Reisinger\n&gt; http://www.cs.utexas.edu/~jo=\r\neraii\n&gt;\n\n\n\n"}}