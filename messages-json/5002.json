{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":413981748,"authorName":"openmind767","from":"&quot;openmind767&quot; &lt;openmind767@...&gt;","profile":"openmind767","replyTo":"LIST","senderId":"fhhplKUKJAdhWDSCaVgD8LvbdJTyTNjAdbORZV5orbP-QAlXDKbikWYUkCGWJL0B-9tTxqrWovXqUdnGmkNyMPpW7xbSGwO3ey6PPg","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: solution for NEAT on CUDA","postDate":"1260467807","msgId":5002,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGhmcmNvditvYXJyQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGhmcjIzNitkdXFpQGVHcm91cHMuY29tPg=="},"prevInTopic":5001,"nextInTopic":5003,"prevInTime":5001,"nextInTime":5003,"topicId":4995,"numMessagesInTopic":8,"msgSnippet":"The whole NEAT lib and experiment are both full of branch. Making the whole things running on CUDA won t speed up, and it is not my target. public void","rawEmail":"Return-Path: &lt;openmind767@...&gt;\r\nX-Sender: openmind767@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 26738 invoked from network); 10 Dec 2009 17:56:50 -0000\r\nX-Received: from unknown (66.196.94.106)\n  by m1.grp.sp2.yahoo.com with QMQP; 10 Dec 2009 17:56:50 -0000\r\nX-Received: from unknown (HELO n46b.bullet.mail.sp1.yahoo.com) (66.163.168.160)\n  by mta2.grp.re1.yahoo.com with SMTP; 10 Dec 2009 17:56:49 -0000\r\nX-Received: from [69.147.65.147] by n46.bullet.mail.sp1.yahoo.com with NNFMP; 10 Dec 2009 17:56:49 -0000\r\nX-Received: from [98.137.34.35] by t10.bullet.mail.sp1.yahoo.com with NNFMP; 10 Dec 2009 17:56:49 -0000\r\nDate: Thu, 10 Dec 2009 17:56:47 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;hfrcov+oarr@...&gt;\r\nIn-Reply-To: &lt;hfr236+duqi@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;openmind767&quot; &lt;openmind767@...&gt;\r\nSubject: Re: solution for NEAT on CUDA\r\nX-Yahoo-Group-Post: member; u=413981748; y=s8g1i9HCpv7irLP0jc8H4oLCw1hZaaMtkUra3ke6l3027xBMiH0\r\nX-Yahoo-Profile: openmind767\r\n\r\n\n\n\nThe whole NEAT lib and experiment are both full of branch. Making the wh=\r\nole things running on CUDA won&#39;t speed up, and it is not my target. \npublic=\r\n void SingleStep()\n{\n  for(int i=3D0; i&lt;connectionArray.Length; i++)\n  conn=\r\nectionArray[i].signal =3D neuronSignalArray[connectionArray[i].sourceNeuron=\r\nIdx] * connectionArray[i].weight;\n  for(int i=3D0; i&lt;connectionArray.Length=\r\n; i++)\n    _neuronSignalArray[connectionArray[i].targetNeuronIdx] +=3D conn=\r\nectionArray[i].signal;\n  for(int i=3DtotalInputNeuronCount; i&lt;_neuronSignal=\r\nArray.Length; i++)\n  {\n     neuronSignalArray[i] =3D activationFn.Calculate=\r\n(_neuronSignalArray[i]);\t\t\t\n     _neuronSignalArray[i]=3D0.0F;\n  }\n}\nI only=\r\n want move above code (Matrix-Vector Multiplication and sigmoid, from sharp=\r\nNEAT) to CUDA.\n\n\n\nSingle network is not too large, CUDA memory latency will=\r\n not be hidden when call CUDA do this network. So I think it need do some c=\r\nhange. My idea is join all networks of population in same gen.\nThe big netw=\r\nork will hide memory latency well.\nfor on gen:\nfor (int i =3D 0; i &lt; popula=\r\ntion.count; i++)\n{\n   IGenome g =3D pop.GenomeList[i];\n   networks.Add(g.De=\r\ncode);\n}\nloop\n{\n  for (int i =3D 0; i &lt; population.count; i++)\n  {\n     sta=\r\ntes[i]=3Dget_state(i);\n     networks.set_signals(i, states);\n  }\n  call_CUD=\r\nA_do_SingleStep(networks.get_single_large_network);\n  for (int i =3D 0; i &lt;=\r\n population.count; i++)\n  {\n     outputs[i] =3D networks.get_outputs(i);\n  =\r\n   do_action(outputs[i]);\n  }\n}\n\n\n--- In neat@yahoogroups.com, &quot;Andrei&quot; &lt;an=\r\ndrei.rusu@...&gt; wrote:\n&gt;\n&gt; CUDA is extremely appropriate for a numerous, but=\r\n very specific set of problems, namely highly parallel, vector computation =\r\ndriven problems. Whenever the treads have to do very different work, CUDA p=\r\nrograms don&#39;t provide much speed-up, because the hardware scheduler has to =\r\nserialise operations. Many times though, you can implement serial programs =\r\nin CUDA and still get impressive speed-ups. \n\n\n\n&gt; \n&gt; I implemented an adapt=\r\ned version of the Conjugate Gradient sparse system solver you can find on W=\r\nikipedia in CUDA, and it was still 10x faster than the GMM++ version. This =\r\nis an iterative algorithm, but some subroutines can be done very efficientl=\r\ny on a GPU, hence the speed-up. I am talking FLOAT operations, not double, =\r\nthe latter are much slower than the former on current GPUs.  \n&gt; \n&gt; The key =\r\nis to implement very large portions of your program in CUDA, things that wo=\r\nuld take the CPU minutes, to give a reference. Just calling a GPU matrix mu=\r\nltiplication will make you program slower many times, unless we are talking=\r\n huge matrices. \n\n&gt; I have to warn you, getting performance out of CUDA doe=\r\ns take considerably longer, and it&#39;s painfully more difficult to program an=\r\nd debug WELL, compared to CPU programming. I would also recommend a GTX200 =\r\nto have enough memory and speed. \n&gt; \n&gt; That being said, I am looking forwar=\r\nd to CUDA projects for the summer break!\n&gt; \n&gt; Cheers!\n&gt; Andrei\n&gt; \nSorry for=\r\n my bad English, hope code will explain my idea well.\n\nThanks,\nBaihi\n\n\n&gt; --=\r\n- In neat@yahoogroups.com, &quot;openmind767&quot; &lt;openmind767@&gt; wrote:\n&gt; &gt;\n&gt; &gt; Hi, =\r\nI use NEAT these days. Although I have add parallel for \n&gt; &gt; EvaluateNetwor=\r\nk and SSE for sigmoid, the performance\n&gt; &gt; is still not well. Maybe perform=\r\nance will never be satisfied.\n&gt; &gt; The performance profile show 90% cpu time=\r\n is used in \n&gt; &gt; Matrix-Vector Multiplication and sigmoid.\n&gt; &gt; \n&gt; &gt; CUDA ma=\r\nybe is the best solution for the performance now. But\n&gt; &gt; CUDA program is n=\r\not like normal program. I don&#39;t have any \n&gt; &gt; experience with CUDA. As I th=\r\nink, in most case single network \n&gt; &gt; structure is not too big. When Callin=\r\ng CUDA do Matrix-Vector \n&gt; &gt; Multiplication and sigmoid for one network, CU=\r\nDA memory latency \n&gt; &gt; will not be hidden. so it wont gain too much perform=\r\nance for \n&gt; &gt; single network. Join all networks of population into one big =\r\n\n&gt; &gt; network, and call CUDA to do this big network, CUDA memory latency\n&gt; &gt;=\r\n will be hidden well. Maybe this is good solution for NEAT on CUDA.\n&gt; &gt; Any=\r\n suggestion and experience is welcome.\n&gt; &gt; \n&gt; &gt; Thanks,\n&gt; &gt; Baihi\n&gt; &gt;\n&gt;\n\n\n\n"}}