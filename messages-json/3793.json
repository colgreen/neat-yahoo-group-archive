{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":142377161,"authorName":"zymeth02","from":"&quot;zymeth02&quot; &lt;zymeth02@...&gt;","profile":"zymeth02","replyTo":"LIST","senderId":"pocPfbxdYZwntif-SpEdIUBo5SfXFp4ieT4uiCoEYLfh1QwWcPQkzAxJ1wiQshFIF05_pyoCYdAL5oWXV7INJF5g_g0pvw","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: How to choose parameter values when comparing 2 different evolutionary algor","postDate":"1200967764","msgId":3793,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGZuM2o4ays4YnZzQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGZuMzhsOStsaDFlQGVHcm91cHMuY29tPg=="},"prevInTopic":3792,"nextInTopic":0,"prevInTime":3792,"nextInTime":3794,"topicId":3791,"numMessagesInTopic":3,"msgSnippet":"I see. Thanks for answering my concern Ken!","rawEmail":"Return-Path: &lt;zymeth02@...&gt;\r\nX-Sender: zymeth02@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 10109 invoked from network); 22 Jan 2008 02:09:25 -0000\r\nX-Received: from unknown (66.218.67.95)\n  by m49.grp.scd.yahoo.com with QMQP; 22 Jan 2008 02:09:25 -0000\r\nX-Received: from unknown (HELO n39a.bullet.mail.sp1.yahoo.com) (66.163.168.133)\n  by mta16.grp.scd.yahoo.com with SMTP; 22 Jan 2008 02:09:25 -0000\r\nX-Received: from [216.252.122.216] by n39.bullet.mail.sp1.yahoo.com with NNFMP; 22 Jan 2008 02:09:25 -0000\r\nX-Received: from [66.218.69.1] by t1.bullet.sp1.yahoo.com with NNFMP; 22 Jan 2008 02:09:25 -0000\r\nX-Received: from [66.218.66.78] by t1.bullet.scd.yahoo.com with NNFMP; 22 Jan 2008 02:09:25 -0000\r\nDate: Tue, 22 Jan 2008 02:09:24 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;fn3j8k+8bvs@...&gt;\r\nIn-Reply-To: &lt;fn38l9+lh1e@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;zymeth02&quot; &lt;zymeth02@...&gt;\r\nSubject: Re: How to choose parameter values when comparing 2 different evolutionary algor\r\nX-Yahoo-Group-Post: member; u=142377161; y=Kif5_ahV7985rKwsz2-A60Dpe4Qhx8nLqnL0S9Avf-b0vHc\r\nX-Yahoo-Profile: zymeth02\r\n\r\nI see. Thanks for answering my concern Ken!\n\n--- In neat@yahoogroups.com, &quot;=\r\nKenneth Stanley&quot; &lt;kstanley@...&gt; wrote:\n&gt;\n&gt; Parameter setting is a nasty iss=\r\nue in machine learning in general \n&gt; for which there is no perfect approach=\r\n.  The reason most papers say \n&gt; they found their parameters through &quot;preli=\r\nminary experimentation&quot; is \n&gt; because there is generally no unversally-acce=\r\npted approach to \n&gt; finding the best parameters.  You just have to trust th=\r\ne authors.\n&gt; \n&gt; However, when you are comparing two variants of the same me=\r\nthod \n&gt; (such as NEAT), you do have an advantage because they can both \n&gt; u=\r\ntilize the same parameter settings to make the comparison fair.  So \n&gt; in t=\r\nhat case, that is what is important:  Find settings that are the \n&gt; same fo=\r\nr both and work well for both.  That&#39;s what I would suggest. \n&gt; \n&gt; It is no=\r\nt important whether the settings are &quot;perfect.&quot;   The truth \n&gt; is that what=\r\n&#39;s perfect in one domain is not in another; and besides, \n&gt; no one can be s=\r\nure what the optimal values are without an exhaustive \n&gt; meta-search, which=\r\n isn&#39;t generally worth the effort.  What is \n&gt; important is just that they =\r\nare reasonable.  If there is precedent, \n&gt; i.e. if people have found good p=\r\narameter settings in the past, \n&gt; loosely following that precedent also len=\r\nds some credibility, so you \n&gt; can say that your settings are based on some=\r\n prior published work.\n&gt; \n&gt; I suggest not getting hung up on parameter sett=\r\nings.  They are the \n&gt; kind of thing you can fiddle with forever without re=\r\nally learning \n&gt; anything deep.  What you want to do is isolate the parts o=\r\nf your \n&gt; methods are are interesting and keep the parameters from being th=\r\ne \n&gt; main issue.\n&gt; \n&gt; ken \n&gt; \n&gt; --- In neat@yahoogroups.com, &quot;zymeth02&quot; &lt;zy=\r\nmeth02@&gt; wrote:\n&gt; &gt;\n&gt; &gt; Hi, I&#39;m having some doubts in doing my research and=\r\n was hoping \n&gt; that I\n&gt; &gt; could seek some help here.\n&gt; &gt; \n&gt; &gt; Basically, I =\r\nhave two algorithms that I want to compare (both use \n&gt; NEAT\n&gt; &gt; btw), but =\r\nhave to first choose the parameter values for each\n&gt; &gt; algorithm. I suppose=\r\n that to have a fair comparison, each algorithm\n&gt; &gt; should be tested with d=\r\nifferent parameter settings to determine its\n&gt; &gt; ideal setting. My question=\r\n is, how comprehensive should it be? (eg.\n&gt; &gt; for 1 variable, should we tes=\r\nt 10 different values? should we have \n&gt; 10\n&gt; &gt; trials for each run?) This =\r\nhas become my concern because I don&#39;t \n&gt; want\n&gt; &gt; to waste too much time in=\r\n finding the right parameters.\n&gt; &gt; \n&gt; &gt; I am mostly concerned with finding =\r\nthe appropriate mutation\n&gt; &gt; probabilities (mut add link, mut add node, mut=\r\n link weight, mut \n&gt; power).\n&gt; &gt; \n&gt; &gt; I looked through some papers on how t=\r\nhey chose their parameters \n&gt; when\n&gt; &gt; comparing, and noticed that research=\r\ners usually say that the \n&gt; settings\n&gt; &gt; are found experimentally, without =\r\nmention of how comprehensive it \n&gt; is.\n&gt; &gt; Also, most don&#39;t show experiment=\r\nal statistics of these parameter\n&gt; &gt; testing. Is it because such thing is t=\r\nrivial? \n&gt; &gt; \n&gt; &gt; I appreciate your answers. Thanks!\n&gt; &gt;\n&gt;\n\n\n\n"}}