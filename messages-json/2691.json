{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"pnYZEw78gpwSHNQoWn-3DQy7MD7Yf8PCrLwZHXM7uteHhDSN5XrS6ET3YcbjjhPaPfLjamsq4c6LAKzWf-Jm2F6gH9-5BGiWDtiGumv_ViY0","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: NEAT enhancements","postDate":"1154570496","msgId":2691,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGVhcmxlMCtva3UzQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGVhbDlqNCtjNTNmQGVHcm91cHMuY29tPg=="},"prevInTopic":2687,"nextInTopic":2692,"prevInTime":2690,"nextInTime":2692,"topicId":2684,"numMessagesInTopic":17,"msgSnippet":"There has been significant work showing that a combination of NEAT to optimize topology and initial weights, and an addition learning algorithm to further","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 51839 invoked from network); 3 Aug 2006 02:01:40 -0000\r\nReceived: from unknown (66.218.67.35)\n  by m26.grp.scd.yahoo.com with QMQP; 3 Aug 2006 02:01:40 -0000\r\nReceived: from unknown (HELO n27.bullet.scd.yahoo.com) (66.94.237.56)\n  by mta9.grp.scd.yahoo.com with SMTP; 3 Aug 2006 02:01:39 -0000\r\nReceived: from [66.218.69.6] by n27.bullet.scd.yahoo.com with NNFMP; 03 Aug 2006 02:01:36 -0000\r\nReceived: from [66.218.66.71] by t6.bullet.scd.yahoo.com with NNFMP; 03 Aug 2006 02:01:36 -0000\r\nDate: Thu, 03 Aug 2006 02:01:36 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;earle0+oku3@...&gt;\r\nIn-Reply-To: &lt;eal9j4+c53f@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: Re: NEAT enhancements\r\nX-Yahoo-Group-Post: member; u=54567749; y=U5TMSIa7GRsHdMJRqu8oMYH-g0psi1Wu6u66tDhXyvarc3zS30af\r\nX-Yahoo-Profile: kenstanley01\r\n\r\nThere has been significant work showing that a combination of NEAT \nto opti=\r\nmize topology and initial weights, and an addition learning \nalgorithm to f=\r\nurther optimize in the domain, can work well:\n\nhttp://www.cs.utexas.edu/use=\r\nrs/shimon/pubs/whitesonjmlr06.pdf\n\nThe above paper uses Q-learning on indiv=\r\nidual networks evolve with \nNEAT.  (which internally involved backpropagati=\r\non during the update \nphase)\n\nChris Christenson also published a paper rece=\r\nntly showing success \nusing NEAT + simple backprop alone in a supervised do=\r\nmain.  I&#39;m not \nsure if Chris is still reading this group, but if he is may=\r\nbe he can \npost the paper to the files section or provide more info.\n\nIn an=\r\ny case, there is reason to believe that NEAT can work in \nconcert with othe=\r\nr methods effectively in the right context, \nalthough it is probably often =\r\nnot necessary.\n\nTime delay networks are similar to leaky integrator network=\r\ns, also \nknown as CTRNNs.  NEAT has been combined with leaky integrator \nne=\r\nurons with good results.  The files section of this group includes \na versi=\r\non of this by Tyler Streeter:\n\nleakyNEAT.zip \n\nken\n\n--- In neat@yahoogroups=\r\n.com, &quot;aklein07&quot; &lt;a.klein@...&gt; wrote:\n&gt;\n&gt; All,\n&gt; \n&gt; Ken&#39;s papers mention (a=\r\nt least I think that they do) that \n(dynamic) \n&gt; annealing of mutation rate=\r\ns might be advantageous in order to \nconfine \n&gt; global searching after a wh=\r\nile to a promising region in search \nspace. \n&gt; Having read some papers, I a=\r\nlso found indications that a hybrid \n&gt; weight training algorithm (global se=\r\narch by means of genetic \n&gt; algorithms and local search using some gradient=\r\n descent algorithm) \n&gt; might be able to produce better results than genetic=\r\n algorithm \nsearch \n&gt; alone. The reason is, that each type of algorithm per=\r\nforms well \n&gt; mostly only in its specific field. Has anyone further insight=\r\ns \ninto \n&gt; this topic ? Has anyone tried to implement any of the approaches=\r\n \n&gt; mentioned ? \n&gt; \n&gt; Also time delay network connections might provide imp=\r\nrovements for \n&gt; some model estimation tasks. Has anyone tested / implement=\r\ned yet \nany \n&gt; of these ideas ?\n&gt; \n&gt; Thanks for any comments,\n&gt; Achim\n&gt;\n\n\n\n=\r\n\n\n"}}