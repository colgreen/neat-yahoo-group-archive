{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"7cCp7dv5LkZHNaWRjUybAzWnQlApFaHzebZmlJF2T2_lixgga8BARkQ50wFeW1iwbXPn2ApHo24FdjvDklfavLqc-oLI5k4dQyDoRwuv0ahD","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: Bloat","postDate":"1086480382","msgId":1003,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGM5dG41dSt1djByQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDAwZGEwMWM0NGE5NCRjMDQ0OGRmMCQzMjAxYThjMEBORVdBR0U+"},"prevInTopic":1002,"nextInTopic":1004,"prevInTime":1002,"nextInTime":1004,"topicId":904,"numMessagesInTopic":68,"msgSnippet":"Jim, I hope you will allow me a rather long and detailed response to your point.  I feel this is the right time for me to respond broadly, since you have","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 19893 invoked from network); 6 Jun 2004 00:06:23 -0000\r\nReceived: from unknown (66.218.66.216)\n  by m25.grp.scd.yahoo.com with QMQP; 6 Jun 2004 00:06:23 -0000\r\nReceived: from unknown (HELO n3.grp.scd.yahoo.com) (66.218.66.86)\n  by mta1.grp.scd.yahoo.com with SMTP; 6 Jun 2004 00:06:23 -0000\r\nReceived: from [66.218.67.186] by n3.grp.scd.yahoo.com with NNFMP; 06 Jun 2004 00:06:22 -0000\r\nDate: Sun, 06 Jun 2004 00:06:22 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;c9tn5u+uv0r@...&gt;\r\nIn-Reply-To: &lt;00da01c44a94$c0448df0$3201a8c0@NEWAGE&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Length: 11496\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-eGroups-Remote-IP: 66.218.66.86\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: Re: Bloat\r\nX-Yahoo-Group-Post: member; u=54567749\r\nX-Yahoo-Profile: kenstanley01\r\n\r\nJim, I hope you will allow me a rather long and detailed response to \nyour point.  I feel this is the right time for me to respond \nbroadly, since you have touched on the central theme behind much \ndiscussion on this group, and ultimately behind my own motivations \nfor introducing NEAT.  Therefore, forgive me for a long-winded \nresponse, but one I would to get on the record.\n\nI doubt that the importance of topology can be overstated.  That \nsaid, I want to concede up front that there is no question that most \nof the key steps in exploration are through weight mutation, and \nthat weight mutation will get you far.  In fact, there are very \nsophisticated methods for altering the weight mutation distribution \nto point it in more promising directions, and these methods can be \nquite powerful.\n\nNevertheless, weight mutation is no more than exploring a fixed \nspace, and exploring a fixed space is well understood and tried and \ntested.  In fact, it is proven that there  is only so much a black \nbox method can do to explore a space.  No method can promise always \nto escape local optima, and no method ever will make such a promise \n(so says the No Free Lunch Theorem).  \n\nThere are fundamental questions at the core of AI that fixed-space \nexploration can never address.  Most perplexing and fundamental is \nthe question of what space should we be exploring in the first \nplace?  Fogel tried 3 topologies (and probably more, off the \necord).  But where did those topologies come from?  What was the \nbasis of the decisions to use them?  Isn&#39;t our mission, as \nresearchers in AI, to make *that* decision automatic?  After all, \n*that* decision- the decision of what topology to search, i.e. what \nspace to search in- is really the only hard decision, the one that \nrequires &quot;intelligence&quot;.  It is a relatively trivial matter, once \nyou know what to search, just to go searching.  The fact that weight \nmutation alone (once the correct topology has been identified) is \nsufficient to solve checkers says more about checkers and human \nintelligence (intelligence for choosing the right space to search) \nthan about the prowess of weight mutation.\n\nYet this is not only a philosophical argument about what AI should \nbe able to do automatically.  It is also a critical practical \nmatter.  Contrary to your reasoning, the real danger is not in \nadding a single dimension to a search space, but in beginning search \nin a bad space in the first place.  If you are concerned that \naddition of a single dimension has some exponential expense (which I \nbelieve is not correct anyway), what cost then must there be in \nsearching in a topology with dozens or even hundreds of unnecessary \ndimensions?  The effect on search could be catastrophic.\n\nAnd yet for most difficult problems we have not the slightest idea \nwhat the right space is to search, other than that it is large.  How \nmany dimensions are in the brain of a robotic maid?  Surely at least \nthousands; maybe millions.  Should we begin search then in a network \nof a million connections?  Weight space exploration offers no \ncomfort: The search is intractable in million dimensional space, \neven if the solution is somewhere within.  \n\nYet even as there is danger from above in the form of too-high \ndimensional space, there is danger from below in spaces of too-few \ndimensions, where a solution may not even exist.  What if Fogel had \nchosen to search in networks with 2 fewer neurons?  5 fewer? At some \npoint, the good player just doesn&#39;t exist in that space anymore.  \nBut how could we know this in advance?  There is no analysis that \ncan tell us a priori how many dimensions we need.  And if we try to \ngo lean and get just the right amount, we might miss the boat \nentirely, even by a single connection, and end up searching forever \nin futility in a space without a solution.\n\nWorse, even if we knew *exactly* the minimal number of connections \nnecessary to solve a problem *and* the perfect topology, even then, \nif the space is too large, weight mutation alone is likely to fail.  \nThe problem is, where in a large space do you *begin* to search? And \nthat problem is impossible to address since by definition you don&#39;t \nknow anything about the space before you begin searching!  \nTherefore, in a high-dimensional space, you are highly likely to \nbegin in an unpromising part of the space; it&#39;s simply too large.\n\nTherefore, to begin minimally and complexify into the the proper \nspace is addressing a fundamental issue and I believe is ultimately \nunavoidable as a critical component of any black box search for \ncomplex behaviors.  Rather than adding expense as you imply, it is \nreducing expense by spending most of search in lower-dimensional \nspace than the final solution.  A complexifying method only may be \nsearching in the space of the final solution for 10% of the run.  \nFixed-topology search spends 100% of the run in the high dimensional \nspace of the final solution, which, according to your formulation \nshould incur an incomprehensibly vast exponential penalty.  \n\nI think ultimately what you are misunderstanding is that NEAT is not\nan attempt to search in high-dimensional space.  It is a method for \nspending most of your search in *lower-dimensional space* than the \nfinal solution, and complexifying up to the complexity of the final \nsolution.  The goal is to be able to find solutions that *exist* in \nhigh-dimensional space.  That&#39;s not the same as a goal of searching \ndirectly in high-dimensional space no matter the problem.  The \nlatter goal is the antithesis of what NEAT is about.  NEAT is \ndesigned to avoid searching in unnecessarily high-dimensional space.\n\nThus, I feel strongly that the idea of searching through topologies \nmust be taken seriously, and should not be viewed as merely \na &quot;fun&quot;, &quot;sexy,&quot; or &quot;somwhat spatially interesting&quot;  recreation.  It \nis not mere intellectual exercise.  Prior topology-evolving systems \nbefore NEAT were perhaps better targets for your criticism, since \nthey were essentially aimed at flipping through random topologies \nunsystematically for its own sake.  However, NEAT is designed to to \nuse topology as a way of minimizing dimensionality in search, and \nultimately to automatically address that fundamental question of \nwhat space to be searching in, a completely different endeavor. \n\n(\n\nA couple side notes:\n\nI agree that structural mutation needs to be relatively rare.  In \nNEAT, it is generally 5% or lower.  Years of experimentation with \nNEAT have gone into testing different rates of structure-adding.\n\nFinally, I believe your mathematical formulation is incorrect. \nAdding a dimensions to an already-partially-optimized structure \ncertainly does not incur exponential expense in the search process.  \nIn fact, the effect can be quite the opposite, adding new routes off \nthe top of a local optimum.\n\nNot to be picky, and this isn&#39;t really important, but here&#39;s what \ndoesn&#39;t make sense to me about your formal argument:\n\n-&quot;What is occurring to me is that just doing weight mutation is a \nsearch at a rate X in a huge space.&quot;  How do you define &quot;search at \nrate X?&quot;  This does not seem to mean anything formally speaking.  \nWhat are the units of search rate?  How is it derived?\n\n-&quot;And it seems to me adding topological variation is not just a \nmultiplier, but an exponent increasing X.&quot;  If X is a rate (as you \ndefined it), then increasing X means the rate becomes faster.  So I \nassume X is not a rate.  But then what is it?\n\n-&quot;topological complexity curve for having a more fit player is \nexponetial&quot;  What is a topological complexity curve?  Is it based \nsomehow on rate X?  Complexity is usually defined as the size of the \nspace or number of connections in a network.  Under that usual \ndefinition, complexity goes up linearly with the addition of new\n  structure, not exponentially.\n\n-&quot;X^T, where T = Y^Z and Z is the complexity curve&quot;  I still am not \nsure what X really means formally, but you haven&#39;t given a \ndefinition for T or Y or Z either.  What are these variables?\n\nUltimately I think you are arguing from intuition rather than \nformally, and intuitions can be misleading.\n)\n\n\nSorry to all for the long-windedness of this response!  I hope it is \nstill useful!\n\n--- In neat@yahoogroups.com, &quot;Jim O&#39;Flaherty, Jr.&quot; \n&lt;jim_oflaherty_jr@y...&gt; wrote:\n&gt; John, Colin, Ken and Derek,\n&gt; \n&gt; I am wondering if there is not a wee bit too much focus on \ntopological vairation and insignificant focus on just weight \nmutation.  I get that NEAT is unique in the fact that it has a very \neffective search mechanism for topoligical variation, with the \nability to stress both additive and subtractive aspects of change.  \nAnd I get that it is fun to focus on the topological variation as it \nis somewhat spatially interesting.\n&gt; \n&gt; However, I am realizing that just doing effective weight \nmutations, sans topological changes, can end up producing solutions \nthat are very &quot;fit&quot;.  I have been focused on reproducing the \nexperiments Fogel and Kumar produced which are covered in their book \nBlondie24.  In that, they had only 3 topologies they experimented \nwith.  All of the GA searching was just done with weight mutation \nwithin a step size that was both a GA parameter and nudged towards \nsmaller values.  In the Fogel experiments, they arrived at an expert \nplayer (well, at least against human opponents) using just co-\nevolution and a static topology.  And in my own replication of the \nexperiments, something as simple as turn on/off biases had a \nsubstantial effect in how long it took to arrive at a specimen of \nsimilar fitness.\n&gt; \n&gt; Now, I realize that mutating weights only is not near as sexy \nsounding as both weight mutation and topological variation.  \nHowever, what I am wondering and hope to be able to evaluate with \nexperimentation is whether the topological mutation rates ought not \nbe very small with the focus more on trying out many weight \nmutations within a given topology?  What is occurring to me is that \njust doing weight mutation is a search at a rate X in a huge space.  \nAnd it seems to me adding topological variation is not just a \nmultiplier, but an exponent increasing X.  Perhaps the space is \nbeing made too large too quickly, before a search just in the weight \nmutation space might demonstrate a uniquely fit individual.\n&gt; \n&gt; My understanding is that the &quot;search in higher dimensional space&quot; \nmight produce more robust and fit players.  At what computational \ncost?  If the topological complexity curve for having a &quot;more fit \nplayer&quot; is exponetial, then doesn&#39;t that mean that there is a \nthreshold of diminishing returns somewhere?  Granted, it may not \nbe.  However, when it is exponential (and my intuition says it is \nmore of the time), we now have an exponent on an exponent of \ncomplexification which massively enlarges the search space, X^T, \nwhere T = Y^Z and Z is the complexity curve.  If this is true, then \nwe are massive amounts of processing power away from achieving \nresult in anything but the simplest of domains, like XOR and Tic-Tac-\nToe.\n&gt; \n&gt; Am I missing something here?  Perhaps I need to do more direct \nexperimentation and examine the results before jumping to this kind \nof conclusion.  I just get the sense that simple weight mutation \nachieved quite a bit in Checkers, a domain more complex than Tic-Tac-\nToe.  It would be interesting to see how Checkers might do with NEAT \nand see what kinds of mutation rates might be more/less effective \nand why.\n&gt; \n&gt; \n&gt; Jim\n&gt; \n&gt; \n\n\n\n"}}