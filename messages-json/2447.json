{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":181688725,"authorName":"yurifromtomsk","from":"&quot;yurifromtomsk&quot; &lt;neuroevolution@...&gt;","profile":"yurifromtomsk","replyTo":"LIST","senderId":"Jws0szfpToGdLlcP3M-hBpYvtLPW0GcdrRiGZ51-jUtwqDtSmpQbuv49gSx3cp3aDeM8O7dQ8gigIoUhE3dFKPTSx8GGX8up6rBAWpsrWA","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: question about the topology of ANN","postDate":"1133315762","msgId":2447,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGRtajByaSs2MHU3QGVHcm91cHMuY29tPg==","inReplyToHeader":"PGRtaGluMitvcDQzQGVHcm91cHMuY29tPg=="},"prevInTopic":2446,"nextInTopic":0,"prevInTime":2446,"nextInTime":2448,"topicId":2441,"numMessagesInTopic":7,"msgSnippet":"... The application of the, for example, BP to the arbitrary-topology ANN does not differ much from the BP for multy-layer nets, because you can rearrange","rawEmail":"Return-Path: &lt;neuroevolution@...&gt;\r\nX-Sender: neuroevolution@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 92148 invoked from network); 30 Nov 2005 01:57:10 -0000\r\nReceived: from unknown (66.218.66.216)\n  by m24.grp.scd.yahoo.com with QMQP; 30 Nov 2005 01:57:10 -0000\r\nReceived: from unknown (HELO n24.bullet.scd.yahoo.com) (66.94.237.53)\n  by mta1.grp.scd.yahoo.com with SMTP; 30 Nov 2005 01:57:10 -0000\r\nComment: DomainKeys? See http://antispam.yahoo.com/domainkeys\r\nReceived: from [66.218.69.5] by n24.bullet.scd.yahoo.com with NNFMP; 30 Nov 2005 01:56:04 -0000\r\nReceived: from [66.218.66.78] by mailer5.bulk.scd.yahoo.com with NNFMP; 30 Nov 2005 01:56:04 -0000\r\nDate: Wed, 30 Nov 2005 01:56:02 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;dmj0ri+60u7@...&gt;\r\nIn-Reply-To: &lt;dmhin2+op43@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: &quot;yurifromtomsk&quot; &lt;neuroevolution@...&gt;\r\nSubject: Re: question about the topology of ANN\r\nX-Yahoo-Group-Post: member; u=181688725; y=t4_yzqvAlkf-G1AWy5l6X8fkPnUZePhqh3A0n6dityypVci8iZjsOA\r\nX-Yahoo-Profile: yurifromtomsk\r\n\r\n&gt; I&#39;m excited to know that gradient-based algorithms such as BP can be \n&gt; a=\r\npplied to train ANN with an arbitrary topology (Colin and Yuri). \n&gt; But I d=\r\non&#39;t know how to implement the application :(. Could someone \n&gt; teach me or=\r\n recommend some paper or books to me?\n\nThe application of the, for example,=\r\n BP to the arbitrary-topology ANN \ndoes not differ much from the BP for mul=\r\nty-layer nets, because you can \nrearrange nodes in ANN in any way you like.=\r\n I.e when you correcting \nconnections weights for some node you can think o=\r\nf all the input \nconnections as they come from the previous-layer-nodes. Th=\r\nis lets you \nto apply BP or any other gradient training algorithm although =\r\nthis \n&quot;rearranging&quot; in some way complexify things. Of course, the activatio=\r\nn \nfunctions of the nodes should differentiable. Here is one more \nadvantag=\r\ne of the evolutionary training of ANNs: it can be easily \napplied for the n=\r\nets with an arbitrary activation functions without \nrearranging and so on.\n=\r\n\nAbout the books and the papers. Try to search for the &quot;reccurent \nmulti-la=\r\nyer perceptrons training&quot; or &quot;RMLP training&quot; and so on. The \nliterature I&#39;v=\r\ne read is in Russian so the titles of the books won&#39;t be \nmuch helpful ;).\n=\r\n\n\n\n\n\n"}}