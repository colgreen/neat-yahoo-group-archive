{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":203001720,"authorName":"Wesley Tansey","from":"Wesley Tansey &lt;tansey@...&gt;","profile":"tansey4","replyTo":"LIST","senderId":"ceZggBZfkZb8qGM_Ou7amKOnvkoN8MEICrid6Lm5GtA89738gKr64AFkZLGIJVZCJYJJrOFDxiD7uTHyvdAUI0y0RG4","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [neat] Does evolving mutation rates work?","postDate":"1263261728","msgId":5048,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRCNEJEODIwLjgwOTAzMDFAdnQuZWR1Pg==","inReplyToHeader":"PEM3NzEzMkNFLjJGNzFFJWpjbHVuZUBtc3UuZWR1Pg==","referencesHeader":"PEM3NzEzMkNFLjJGNzFFJWpjbHVuZUBtc3UuZWR1Pg=="},"prevInTopic":5047,"nextInTopic":5049,"prevInTime":5047,"nextInTime":5049,"topicId":4984,"numMessagesInTopic":16,"msgSnippet":"Hi Jeff, Below are a few references on self-adaptation: Fogel DB, Fogel LJ, and Atmar JW (1991) Meta-Evolutionary Programming, Proc. of 25th Asilomar","rawEmail":"Return-Path: &lt;tansey@...&gt;\r\nX-Sender: tansey@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 54421 invoked from network); 12 Jan 2010 02:02:09 -0000\r\nX-Received: from unknown (98.137.34.45)\n  by m4.grp.sp2.yahoo.com with QMQP; 12 Jan 2010 02:02:09 -0000\r\nX-Received: from unknown (HELO cdptpa-omtalb.mail.rr.com) (75.180.132.120)\n  by mta2.grp.sp2.yahoo.com with SMTP; 12 Jan 2010 02:02:09 -0000\r\nReturn-Path: &lt;tansey@...&gt;\r\nX-Authority-Analysis: v=1.0 c=1 a=Wa6ed2RCwswA:10 a=UFTA8MH_AAAA:8 a=G1H9XeTJAAAA:8 a=mK_AVkanAAAA:8 a=n0uWIUZOAAAA:8 a=CjxXgO3LAAAA:8 a=FP58Ms26AAAA:8 a=qV0mfD4mAAAA:8 a=Vv3G4n63AAAA:20 a=50KvngZD2gQ8gVB8VYQA:9 a=V6BWVL9Biney3onjOTUA:7 a=asns1xPT0XQBdgJ10bjCNUh_C1gA:4 a=dHPXWuXgQVsA:10 a=eZpROafALsgA:10 a=vwxvUJPQ250A:10 a=GHtaKrYbcl0A:10 a=9xyTavCNlvEA:10 a=89VfvvnUmSoA:10 a=rC2wZJ5BpNYA:10 a=rnjSDB93LHIA:10 a=ao4RW71Vw8YA:10\r\nX-Cloudmark-Score: 0\r\nX-Received: from [76.88.116.144] ([76.88.116.144:2052] helo=[192.168.1.2])\n\tby cdptpa-oedge01.mail.rr.com (envelope-from &lt;tansey@...&gt;)\n\t(ecelerity 2.2.2.39 r()) with ESMTP\n\tid F9/CB-19578-028DB4B4; Tue, 12 Jan 2010 02:02:08 +0000\r\nMessage-ID: &lt;4B4BD820.8090301@...&gt;\r\nDate: Mon, 11 Jan 2010 18:02:08 -0800\r\nUser-Agent: Thunderbird 2.0.0.23 (Windows/20090812)\r\nMIME-Version: 1.0\r\nTo: neat@yahoogroups.com\r\nReferences: &lt;C77132CE.2F71E%jclune@...&gt;\r\nIn-Reply-To: &lt;C77132CE.2F71E%jclune@...&gt;\r\nContent-Type: text/plain; charset=windows-1252; format=flowed\r\nContent-Transfer-Encoding: 8bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Wesley Tansey &lt;tansey@...&gt;\r\nSubject: Re: [neat] Does evolving mutation rates work?\r\nX-Yahoo-Group-Post: member; u=203001720; y=cyevh4GzSWZdFxq9KonZCfUBNELLpx4CxZOQ4hb_ui-6zw\r\nX-Yahoo-Profile: tansey4\r\n\r\nHi Jeff,\n\nBelow are a few references on self-adaptation:\n\nFogel DB, Fogel LJ, and Atmar JW (1991) &quot;Meta-Evolutionary Programming,&quot; \nProc. of 25th Asilomar Conference on Signals, Systems & Computers, R.R. \nChen (ed.), Pacific Grove, CA, pp. 540-545.\n\nFogel DB, Fogel GB, and Ohkura K (2001) &quot;Multiple-Vector Self-Adaptation \nin Evolutionary Algorithms,&quot; BioSystems, Vol. 61:2-3, pp. 155-162.\n\nSaravanan N and Fogel DB (1996) &quot;An Empirical Comparison of Methods for \nCorrelated Mutations under Self-Adaptation,&quot; Evolutionary Programming V, \nL.J. Fogel, P.J. Angeline, T. B�ck (eds.), MIT Press, Cambridge, MA, pp. \n479-485.\n\nAngeline PJ, Fogel DB, Fogel LJ (1996) &quot;A Comparison of Self-Adaptation \nMethods for Finite State Machines in a Dynamic Environment,&quot; \nEvolutionary Programming V, L.J. Fogel, P.J. Angeline, T. B�ck (eds.), \nMIT Press, Cambridge, MA, pp. 441-449.\n\nChellapilla K and Fogel DB (1997) &quot;Exploring Self-Adaptive Methods to \nImprove the Efficiency of Generating Approximate Solutions to Traveling \nSalesman Problems Using Evolutionary Programming,&quot; Evolutionary \nProgramming VI, P.J. Angeline, R.G. Reynolds, J.R. McDonnell, and R. \nEberhart (eds.), Springer, Berlin, pp. 361-371.\n\nPersonally, I&#39;ve found that self-adaptation works better on just about \nevery fitness landscape where I&#39;ve explicitly compared the two.\n\nRegarding your paper, I think you&#39;re doing a different kind of SA than \nmy version. If I understand correctly (and please correct me if I missed \nsomething), you&#39;re taking bit strings and evolving the probability that \na child will be mutated. This seems like it would converge the \npopulation too quickly, and thus your results make sense.\n\nThe type of SA that I typically work with is evolving the standard \ndeviation for Gaussian mutation. All child parameters are mutated. In \npseudo code, I create children as follows:\n\nFor each parent, p\nCreate a copy of p and store it in child c\nFor each gene, g in c,\ng.Sigma = Max(0, g.Sigma * Math.Exp(GaussianMutation(0,1)))\ng.Value = GaussianMutation(g.Value, g.Sigma)\n\nI think if you try this approach on some of the fitness functions in Yao \net al&#39;s &quot;Evolutionary programming made faster&quot; ( \nhttp://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.45.1830&rep=rep1&type=pdf \n), you&#39;ll be pleasantly surprised. :)\n\nSincerely,\nWesley\n\nJeff Clune wrote:\n&gt;\n&gt; Hello Wesley-\n&gt;\n&gt; Sorry for the delayed reply, but I am catching up on this list and saw \n&gt; your\n&gt; comment (below).\n&gt;\n&gt; Out of curiosity, do you have any evidence that self-adaptation of (i.e.,\n&gt; evolving) mutation rates works? In the below paper we found that evolution\n&gt; is not able to optimize its mutation rate for long term adaptation. \n&gt; Instead,\n&gt; it just tries to lower the mutation rate as much as possible.\n&gt;\n&gt; That may help evolution home-in on precise answers, in the sense of \n&gt; getting\n&gt; to the very top of a current (and usually suboptimal) fitness peak, but it\n&gt; greatly harms the discovery of other, better fitness peaks. In other \n&gt; words,\n&gt; self-adaptation was found in our paper to be extremely greedy and\n&gt; effectively eliminate meaningful adaptation.\n&gt;\n&gt; Here is the cite and link:\n&gt;\n&gt; Clune J, Misevic D, Ofria C, Lenski RE, Elena SF, and Sanju�n, R (2008)\n&gt; Natural selection fails to optimize mutation rates for long-term \n&gt; adaptation\n&gt; on rugged fitness landscapes.\n&gt;\n&gt; PLoS Computational Biology 4(9): e1000187.\n&gt;\n&gt; https://www.msu.edu/~jclune/webfiles/publications/Clune-EvolvingMutationRate \n&gt; &lt;https://www.msu.edu/%7Ejclune/webfiles/publications/Clune-EvolvingMutationRate&gt;\n&gt; s-PLoSCB-2008.pdf\n&gt;\n&gt; I know the Evolutionary Strategies people like self-adaptation, but I have\n&gt; never seen evidence that it works when evolution is controlling the \n&gt; mutation\n&gt; rate. Note that evolution is not in control in schemes like the 1/5th \n&gt; rule,\n&gt; CMA, EDA, etc.\n&gt;\n&gt; Cheers,\n&gt; Jeff\n&gt;\n&gt; &gt; This is particularly true if networks are self-adaptive, though I don&#39;t\n&gt; &gt; think any NEAT framework supports self-adaptation. Not to hijack the \n&gt; thread,\n&gt; &gt; but I don&#39;t see any reason why it couldn&#39;t be incorporated. All that&#39;s\n&gt; &gt; necessary is for every gene to have its own sigma to be used in the \n&gt; mutation\n&gt; &gt; operator; the custom sigma is mutated as (C# code):\n&gt; &gt;\n&gt; &gt; Sigma = Math.Max(0, Sigma * Math.Exp(GaussianMutation(0,1)));\n&gt; &gt; //GaussianMutation(mean, stdev)\n&gt; &gt;\n&gt; &gt; Self-adaptation helps evolution home-in on precise answers better \n&gt; than using\n&gt; &gt; a fixed sigma for mutation.\n&gt; &gt;\n&gt; &gt; Wesley\n&gt; &gt;\n&gt; &gt; On Tue, Dec 8, 2009 at 1:58 PM, Colin Green\n&gt; &gt; &lt;colin.green1@... \n&gt; &lt;mailto:colin.green1%40googlemail.com&gt;&gt;wrote:\n&gt; &gt;\n&gt; &gt;&gt;\n&gt; &gt;&gt;\n&gt; &gt;&gt; 2009/12/8 snapmedown &lt;snapmedown@... \n&gt; &lt;mailto:snapmedown%40yahoo.com&gt; &lt;snapmedown%40yahoo.com&gt;&gt;\n&gt; &gt;&gt;\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt; What are the thoughts on a neural network that has inputs and outputs\n&gt; &gt;&gt; digitized? Perhaps 8\n&gt; &gt;&gt;&gt; bits or even less? This would reduce the search space, but encourage\n&gt; &gt;&gt; larger structures.\n&gt; &gt;&gt;\n&gt; &gt;&gt; Hi,\n&gt; &gt;&gt;\n&gt; &gt;&gt; Strictly speaking the signals and weights and already digitized in\n&gt; &gt;&gt; that they&#39;re represented by a 32 bit floating point binary\n&gt; &gt;&gt; representation (or 64bit for double precision). However your question\n&gt; &gt;&gt; essentially then becomes how much precision is necessary for a given\n&gt; &gt;&gt; problem domain? and that&#39;s certainly an interesting question both from\n&gt; &gt;&gt; the perspective of the maths of neural networks but also at the\n&gt; &gt;&gt; implementation level where an 8 bit based ANN is is a lot less\n&gt; &gt;&gt; computing resource hungry compared to a 64bit one of the same\n&gt; &gt;&gt; structural size/complexity.\n&gt; &gt;&gt;\n&gt; &gt;&gt; You might be interested in the short article I wrote abotu\n&gt; &gt;&gt; implementing neural nets with integer maths and /fixed/ point\n&gt; &gt;&gt; arithmetic:\n&gt; &gt;&gt;\n&gt; &gt;&gt; http://sharpneat.sourceforge.net/integer_network.html \n&gt; &lt;http://sharpneat.sourceforge.net/integer_network.html&gt;\n&gt; &gt;&gt;\n&gt; &gt;&gt; In terms of pushing NEAT to it&#39;s limits I think something like 16bit\n&gt; &gt;&gt; maths running on CUDA or equivalent platforms is a logical goal in the\n&gt; &gt;&gt; near term. There&#39;s potentially a three orders of magnitude speedup to\n&gt; &gt;&gt; be achieved there.\n&gt; &gt;&gt;\n&gt; &gt;&gt; Colin.\n&gt; &gt;&gt;\n&gt; &gt;&gt;\n&gt;\n&gt; \n\n"}}