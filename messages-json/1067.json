{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":7192225,"authorName":"Ian Badcoe","from":"Ian Badcoe &lt;ian_badcoe@...&gt;","profile":"ian_badcoe","replyTo":"LIST","senderId":"ZEVtWbh00XwMSqhVdhYYPrfC8CKpvfDO8YY2Rk2O4LaKzg27LBhh7DLWkC-EUwcQsf0XgOcH3zA_JQD4GHH5gYzvcKulv4f_ZGg","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] Re: Computation Time","postDate":"1087299417","msgId":1067,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDYuMS4wLjYuMC4yMDA0MDYxNTEwMTA1MS4wMjUwMDlkMEBwb3AubWFpbC55YWhvby5jby51az4=","inReplyToHeader":"PDQwQzYzOEQwLjEwNjA0MDJAZHNsLnBpcGV4LmNvbT4=","referencesHeader":"PGNhMzl2ays0djFhQGVHcm91cHMuY29tPiA8NDBDNjM4RDAuMTA2MDQwMkBkc2wucGlwZXguY29tPg=="},"prevInTopic":1040,"nextInTopic":1074,"prevInTime":1066,"nextInTime":1068,"topicId":845,"numMessagesInTopic":99,"msgSnippet":"... By my calculations, this makes just over 3 cycles per complete calculation.  That s not impossible.  e.g. ISRT on the K7 (Athlon predecessor) a","rawEmail":"Return-Path: &lt;ian_badcoe@...&gt;\r\nX-Sender: ian_badcoe@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 9116 invoked from network); 15 Jun 2004 17:27:37 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m13.grp.scd.yahoo.com with QMQP; 15 Jun 2004 17:27:37 -0000\r\nReceived: from unknown (HELO smtp004.mail.ukl.yahoo.com) (217.12.11.35)\n  by mta5.grp.scd.yahoo.com with SMTP; 15 Jun 2004 17:27:35 -0000\r\nReceived: from unknown (HELO ian2k.yahoo.co.uk) (ian?badcoe@212.159.73.108 with login)\n  by smtp004.mail.ukl.yahoo.com with SMTP; 15 Jun 2004 12:52:41 -0000\r\nMessage-Id: &lt;6.1.0.6.0.20040615101051.025009d0@...&gt;\r\nX-Sender: ian_badcoe@...\r\nX-Mailer: QUALCOMM Windows Eudora Version 6.1.0.6\r\nDate: Tue, 15 Jun 2004 12:36:57 +0100\r\nTo: neat@yahoogroups.com\r\nIn-Reply-To: &lt;40C638D0.1060402@...&gt;\r\nReferences: &lt;ca39vk+4v1a@...&gt;\n &lt;40C638D0.1060402@...&gt;\r\nMime-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;us-ascii&quot;; format=flowed\r\nX-eGroups-Remote-IP: 217.12.11.35\r\nFrom: Ian Badcoe &lt;ian_badcoe@...&gt;\r\nSubject: Re: [neat] Re: Computation Time\r\nX-Yahoo-Group-Post: member; u=7192225\r\nX-Yahoo-Profile: ian_badcoe\r\n\r\n\n&gt;Hi Philip,\n&gt;\n&gt;My curiosity got the better of me :) I tried the above functions using\n&gt;optimized C# on an AMD Athlon 2400+ (actually 2.17Ghz). The results are\n&gt;slightly bizarre,\n&gt;  oh BTW I think you quoted the tanh function wrong, so I used y =\n&gt;tanh(0.9*x) which gives a nice sigmoid. Firstly I had to use 100 million\n&gt;(10^8) loops to get readable results, the approx. 50x difference is\n&gt;partly due to the CPU (obviously!) but maybe the rest is due to my\n&gt;oversimplistic implementation whereby I used the same value for x every\n&gt;time - did you generate random numbers perhaps? Also I know that Java\n&gt;has JIT compilers but sometime only optimize in code hot-spots during\n&gt;code execution, they can also run in interpreter mode - my run was with\n&gt;JITed code.\n&gt;\n&gt;Here are the figures:\n&gt;\n&gt;sigmoid:  3625ms\n&gt;evsail:    2359ms\n&gt;inv-abs:  188ms\n\nBy my calculations, this makes just over 3 cycles per complete \ncalculation.  That&#39;s not impossible.  e.g. ISRT on the K7 (Athlon \npredecessor) a floating-divide took 3 cycles but that the chip was able to \nhave 2 fdivs and 2fadds and some integer instructions running simulatneously.\n\nIt does sound suspiciously good, however.  I don&#39;t know much about C# but \npresumably it&#39;s inlining the function, and maybe unrolling the loop a \nlittle.  OTOH, if it did all that, then it should be able to see that you \nare making the same call every time and that the function has no side \neffects, so did it need to run the function at all?\n\nCompilers can be blind to that sort of thing, however, like I mentioned before.\n\n&gt;tanh:     12,400ms\n&gt;\n&gt;weird huh.  The tanh loop took 66x longer then the ins-abs one!\n\nAll trig, exp and log are very expensive.\nSqrt is expensive but maybe not so bad.\nDivide is releatively cheap nowadays.\n\nThe thing about the more exotic instructions, like tan, is that not only do \nthey take a lot of cycles, but the chip only has one processor for \nthem.  Also slow instructions have a disproportionate effect on throughput \nbecause all the shorter instructions, which could run in parallel, can only \ngo so far before they hit a dependency on the result of the long \ninstruction and have to stop.  Thus effectively the whole chip hangs on the \nresult of the tan.\n\n&gt;Now to put all this into perspective 10^8 activations is a LOT. I just\n&gt;ran a 53 node network with 413 connections (7.8 a node on avg.) and it\n&gt;took 4700ms to do100,000 epochs. If you then add in the extra time for\n&gt;activation functions this will be 53*100,000 = 5,300,000 activations.\n&gt;Which in turn should take between 657ms (slowest fn) and 10ms(fastest)\n&gt;extra in total - for 100,000 epochs of what is quite a big network. In\n&gt;other words the activation fn is small beer! (well, in my code at least)\n\nIt probably will be for most people.  It is usually best to study some \nprofile information before devoting time to optimisation.\n\nAMD do a tool: \nhttp://www.amd.com/us-en/Processors/DevelopWithAMD/0,,30_2252_3604,00.html\nWhich I believe is still free and which helps optimise code.  It&#39;s got a \nprofiler and it will simulate the CPU so show where the bottle-necks \nare.  These tools are not for the faint-hearted, however.\n\n&gt;So then I tried to eek out a liitle more performance from my Network\n&gt;code using some of the hints Ian has been posting - result... hardly any\n&gt;improvement at all!\n\nBut it didn&#39;t slow you down?  I am surprised, because I was specifically \nthinking about John&#39;s case where his memory load will be very \nsubstantial.  As you rightly say, if the whole model fits in the cache then \nyou won&#39;t have that problem and breaking into multiple loops will increase \nyour overheads.\n\n&gt;   I wonder though if the technqiue of trying to do\n&gt;many sequentail ops in order wll only become beneficial when the\n&gt;networks get *really* big, simply because the memory caches in modern\n&gt;CPU&#39;s are so large. So there may be some network size at which we would\n&gt;see a dramatic slow down of our code if it&#39;s not optimized in such a way.\n\nThat&#39;s what I would expect, not just the network size, however, also the \ntotal size of the data you want to pass through.  If you run many copies of \nthe same small network on different data then memory-access may be your \nbottleneck.\n\n&gt;Another way of estimating how efficient my code is is to caclulate the\n&gt;average number of clock cycles that it requires per neuron and\n&gt;connection. So e.g. My 53 neuron / 413 connection network performs 413\n&gt;additions and 53 activations per epoch. So that&#39;s 466 necessary\n&gt;operations in all, this is an absolute minimum. ok, plus a couple\n&gt;because the activation fn is several operations (but this is just a\n&gt;rough bit of maths). Using a simple bit of maths I can then determine:\n&gt;\n&gt;ops per epoch = 466\n&gt;ops per test run = 466 * 100,000 (loops) = 46,600,000\n&gt;ops per second = 46,600,000 / 5000ms(approx) = 9,320,000\n&gt;CPU clock cycles per op = 2.17Ghz / 9,320,000 = 232.\n&gt;\n&gt;\n&gt;Now 232 isn&#39;t all that bad when you consider this doesn&#39;t take into\n&gt;account the extra code that is required to do the looping/indexing\n&gt;through all of the neurons and connections. So perhaps hand optimized\n&gt;assembler could get this down to 100 cylcles or maybe 50, but this is in\n&gt;the same ball park as optimum - and therefore I wouldn&#39;t expect any\n&gt;massive improvements. Well, not unless you start using SIMD\n&gt;instructions, which I&#39;m definitely NOT! :)\n\nYou easily can do a better analysis than that.  Run the timing a few times \nwith different sizes of network (number of Ops) then plot the line of \nnumber of ops (x) vs time (y).  You should get an +ve intercept on the \ny-axis which is the constant cost of your program and a +ve sloping line, \nwhich is the cost per op...\n\n         Ian Badcoe\n\n\n\n\nLiving@Home - Open Source Evolving Organisms - \nhttp://livingathome.sourceforge.net/\n\n\n\n"}}