{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":464818732,"authorName":"Jeff Clune","from":"Jeff Clune &lt;jclune@...&gt;","profile":"jeffreyclune","replyTo":"LIST","senderId":"frJpRTLFWAtU0qOfEHmE3uESEvR_NDL2g9J5l0sWqSLA1nLYVUT0QVkv8TbN72e69g4zH2UrFO6qCGCmJuTNmV0tcsY","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [neat] New paper on why modules evolve, and how to evolve modular artificial neural networks","postDate":"1360648976","msgId":5990,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PEQ1RkQyMjE5LTIzNEEtNDlFNS04MjBDLTVDNkQ0QzkxNDAyMkB1d3lvLmVkdT4=","inReplyToHeader":"PGtmNzc5ZStxY2o0QGVHcm91cHMuY29tPg==","referencesHeader":"PGtmNzc5ZStxY2o0QGVHcm91cHMuY29tPg=="},"prevInTopic":5989,"nextInTopic":5992,"prevInTime":5989,"nextInTime":5991,"topicId":5976,"numMessagesInTopic":30,"msgSnippet":"Hello all, As Ken mentioned, we ve discussed these issues in private. I m going to include some of my comments from one of those email threads with slight","rawEmail":"Return-Path: &lt;jclune@...&gt;\r\nX-Sender: jclune@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 84728 invoked from network); 12 Feb 2013 06:03:05 -0000\r\nX-Received: from unknown (10.193.84.135)\n  by m3.grp.bf1.yahoo.com with QMQP; 12 Feb 2013 06:03:05 -0000\r\nX-Received: from unknown (HELO mail-ia0-f171.google.com) (209.85.210.171)\n  by mta1.grp.bf1.yahoo.com with SMTP; 12 Feb 2013 06:03:05 -0000\r\nX-Received: by mail-ia0-f171.google.com with SMTP id z13so7225934iaz.2\n        for &lt;neat@yahoogroups.com&gt;; Mon, 11 Feb 2013 22:03:04 -0800 (PST)\r\nX-Received: by 10.50.88.129 with SMTP id bg1mr1076194igb.33.1360648984544;\n        Mon, 11 Feb 2013 22:03:04 -0800 (PST)\r\nReturn-Path: &lt;jclune@...&gt;\r\nX-Received: from [10.0.1.3] (host-69-146-94-113.lar-wy.client.bresnan.net. [69.146.94.113])\n        by mx.google.com with ESMTPS id mj6sm30196866igc.9.2013.02.11.22.02.53\n        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);\n        Mon, 11 Feb 2013 22:03:03 -0800 (PST)\r\nContent-Type: multipart/alternative; boundary=&quot;Apple-Mail=_3A102778-E47E-45B2-8FF3-2B8FCF1EEA4F&quot;\r\nMessage-Id: &lt;D5FD2219-234A-49E5-820C-5C6D4C914022@...&gt;\r\nMime-Version: 1.0 (Mac OS X Mail 6.2 &#92;(1499&#92;))\r\nDate: Mon, 11 Feb 2013 23:02:56 -0700\r\nReferences: &lt;kf779e+qcj4@...&gt;\r\nTo: neat users group group &lt;neat@yahoogroups.com&gt;\r\nIn-Reply-To: &lt;kf779e+qcj4@...&gt;\r\nX-Mailer: Apple Mail (2.1499)\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nX-eGroups-From: Jeff Clune &lt;jeffclune@...&gt;\r\nFrom: Jeff Clune &lt;jclune@...&gt;\r\nSubject: Re: [neat] New paper on why modules evolve, and how to evolve modular artificial neural networks\r\nX-Yahoo-Group-Post: member; u=464818732; y=n1TEBBbvHYliy-F7um78VM38Z0J4gjEGuU9048p5XBtvhWemev5f\r\nX-Yahoo-Profile: jeffreyclune\r\n\r\n\r\n--Apple-Mail=_3A102778-E47E-45B2-8FF3-2B8FCF1EEA4F\r\nContent-Transfer-Encoding: quoted-printable\r\nContent-Type: text/plain;\n\tcharset=windows-1252\r\n\r\nHello all,\n\nAs Ken mentioned, we&#39;ve discussed these issues in private. I&#39;m =\r\ngoing to include some of my comments from one of those email threads with s=\r\nlight modification, as I believe they summarize the views of Jean-Baptiste =\r\nand I on the issues Ken raises. I&#39;ll then respond to a few individual comme=\r\nnts by Ken afterwards. \n\n---------------\nKen,\n\nIt&#39;s great to hear your feed=\r\nback on our paper. Thanks for sending it.\n\nFirst off, thanks for the kind w=\r\nords. We&#39;re very glad you liked the paper and think it is important. \n\nRega=\r\nrding a selection pressure vs. an encoding bias. We&#39;re not convinced that a=\r\nn initial encoding bias is a good way to encourage properties that one want=\r\ns in phenotypes throughout evolution, such as modularity. If there is any d=\r\neceptiveness (or even neutrality) regarding modularity at any point during =\r\nthe run then the bias will disappear, and then for the rest of evolutionary=\r\n time nothing will encourage modularity. We are more convinced of the power=\r\n of mutational bias in the encoding (i.e. a constant encoding bias instead =\r\nof just an initial encoding bias), and we think it would be interesting to =\r\ninvestigate area. However, if the encoding bias is under selection, then yo=\r\nu have the same issue where it might evolve away. Selective pressures are i=\r\nnteresting because they are constant, so you&#39;re more likely to get what you=\r\n want. That raises the point you mention about our pressure being too stron=\r\ng, such that evolution could not deviate when it would be beneficial not to=\r\n have modularity. That might be a problem if the pressure is too strong, bu=\r\nt it seems likely that in many cases the benefits in terms of performance f=\r\nor being non-modular will outweigh the cost. In other words, evolution can =\r\ndecide to pay the cost of non-modularity when it is useful (e.g. in your ex=\r\nample of a hub of connections between modules).\n\nRegarding playing with an =\r\nencoding being safer than playing with selection pressures. Our view is tha=\r\nt both are very complicated and can have unintended consequences, so playin=\r\ng with one is just as bad as the other. I think our field is more familiar =\r\nwith unintended consequences of selective pressures just because we histori=\r\ncally tend to play with them more (and make simple encodings), but it is al=\r\nso very hard to intuit the consequences of choices regarding biases in comp=\r\nlex encodings. In your case the consequences are relatively intuitive, prec=\r\nisely because they are so minimally interventionist...but that is also why =\r\nI think they are not strong enough to cause modularity except in cases (lik=\r\ne retina) where all you have to do is initially place evolution in the righ=\r\nt attractor basin.\n\nRegarding the resource hog waste of having a cost objec=\r\ntive. I have to have a little fun here and point out the irony of the co-ch=\r\nampion of novelty search worrying about the resources consumed by non-high-=\r\nperforming individuals! Hehe. As you&#39;ve persuaded me, I&#39;m more interested i=\r\nn an algorithm that is interesting or that works than spending a little com=\r\nputation inefficiently.\n\nI&#39;d also like to point out an innovation we came u=\r\np with to mitigate the problem of preventing evolution from exploring solut=\r\nions that are contrary to one of the objectives. We recognized that the cos=\r\nt objective is ultimately less important than the performance objective. We=\r\n wanted evolution to periodically ignore the cost objective to explore step=\r\nping stones that had higher connectivity. To do that, we invented a techniq=\r\nue that involves &quot;probabilistic pareto dominance&quot;, wherein secondary object=\r\nives (in this case cost) are factored into pareto dominance only a small pe=\r\nrcentage of the time. That won&#39;t solve the problem you mention if you have =\r\nto take a long, many-multi-generational walk through high-connectivity area=\r\ns of the search space, but it does allow quick forays into that terrain wit=\r\nhout any fitness penalty. This technique could be used for any cost (or oth=\r\ner) objective, so it is not specific to connectivity costs. \n\nSee below for=\r\n a few specific responses to your comments. I should note that below this t=\r\nhe thoughts are my own and Jean-Baptiste should not be blamed for any of th=\r\nem! (Feel free to blame him for things above this line=85we went over that =\r\ntext together a while back). ;-)\n\n&gt; More generally the issue is the usual p=\r\nroblem of deception, which is compounded by anything you do with fitness. F=\r\nor example, in a complex search space, there is a reasonable chance that th=\r\ne stepping stone to a good low-connectivity solution is something with high=\r\ner connectivity. By manipulating fitness, you are cutting out all chances o=\r\nf encountering such a deceptive stepping stone. But even if you don&#39;t belie=\r\nve that could be true, the single-mindedness of always favoring low-connect=\r\nivity could deceive you from many parts of the search space that might be s=\r\ntepping stones to something worthwhile, relating to connection density or n=\r\not.\n&gt; \n&gt; \nTrue. But the same exact thing can be said for biases in the enco=\r\nding: they prevent you from searching large areas of the search space. You =\r\nmay reply that it is only a bias, not a strict ban, but of course we know t=\r\nhat in large search spaces biases hugely affect the landscape such that cer=\r\ntain areas will practically never be visited. \n\n&gt; On the other hand, manipu=\r\nlating the encoding is different because in effect it actually reorganizes =\r\nthe structure of the search space itself, which seems to me a more principl=\r\ned thing to do (if you can figure out a way to do it). Because the thing is=\r\n, in that case, you do not need to worry about a permanent dead weight taki=\r\nng up some proportion of your population forever. Instead, while the encodi=\r\nng may *tend* to produce e.g. low-connectivity solutions, it can still esca=\r\npe that tendency without any penalty to fitness.\n&gt; \nMy instincts tell me th=\r\nat we create dead weight with encoding biases too. For example, an overly r=\r\negular generative encoding (e.g. context free L-systems) is great if good s=\r\nolutions are perfectly regular, but if what is required is a mix of regular=\r\nity and irregularity, then you spend your entire time producing only highly=\r\n regular phenotypes that never wander into the appropriately irregular area=\r\ns of the search space. Our IEEE TEC paper, for example, shows that HyperNEA=\r\nT can spend thousands of generations spinning its wheels never generating s=\r\nolutions that HybrID could easily generate, demonstrating a &quot;overly regular=\r\n&quot; dead weight associated with the biases of even the best known* generative=\r\n encoding! Both fitness penalties and biases can cause you to focus your se=\r\narch in unproductive areas=85which is ultimately what dead weight is. \n\n* i=\r\nn our opinion! :0)\n\n&gt; Furthermore, in reality the best situation regarding =\r\nmodularity and connectivity is probably rather subtle, with most of the bra=\r\nin respecting the principle of low connectivity, but with a number of criti=\r\ncal exceptions in key areas, such as major inter-module hubs. A sophisticat=\r\ned encoding can allow its bias to bend to make such nuanced exceptions (e.g=\r\n. based on locations within a geometry), whereas a fitness penalty is a hea=\r\nvy hand and blunt instrument that cannot but help always to demand global a=\r\nnd holistic subservience to dogmatic universals (unless you are willing to =\r\ntake a hit in fitness).\n&gt; \n&gt; \nI think the last clause you offer is the key =\r\nexception though. As I mentioned above, if a certain phenotype pays for its=\r\n wiring by increasing fitness, it can add high-connectivity areas anywhere =\r\nthat they are useful (without even needing to carve out that area in geomet=\r\nric space, which is often a difficult task for CPPNs). Instead of being a b=\r\nlunt instrument, a fitness penalty can be quite subtle, because it can allo=\r\nw connection-by-connection exceptions if they produce fitness improvements,=\r\n and do so without any search overhead. \n\n&gt; An interesting question in natu=\r\nre (where our brains evolved modular structure) is whether its tendency tow=\r\nards low connectivity is a result of an aspect of fitness in the wild, or a=\r\nn aspect of encoding bias. I think there is a lot of room in this question =\r\nfor arguing either way, but my hunch is that the bias is mostly in the enco=\r\nding. My logic is that I think the reason that the connectivity of the brai=\r\nn is so much lower than what it could be (e.g. it is a tiny fraction of eve=\r\nrything-to-everything connectivity) is an artifact of physics rather than a=\r\nn artifact of fitness. It is simply physically impossible for a giant 100-b=\r\nillion-to-100-billion connectivity to fit in a head anything close to our s=\r\nize. And physical impossibility is in some sense a property of encoding. Th=\r\nat is, mutations that could step from a low-connectivity brain to a high on=\r\ne are few and far between simply because of physical constraint. So high-co=\r\nnnectivity structures are simply a very small part of the search space of b=\r\nrains in the physical universe. However, at the same time, you can still ge=\r\nt long-range connections from time to time because there is no universal pe=\r\nnalty for doing so, just a lower a priori probability of such mutations occ=\r\nurring.\n&gt; \n&gt; \nHere I completely disagree with you. I see the force preventi=\r\nng the volume of neural connections from getting too large as a direct fitn=\r\ness cost, not an encoding bias. If mutations increase the size of the head,=\r\n the baby and the mother are more likely to die in childbirth. Anthropologi=\r\nsts have long known that evolution&#39;s desire to have larger and larger brain=\r\ns is the main reason why humans have such ridiculously high maternal and in=\r\nfant mortality &quot;in the wild&quot; (pre modern health care, and even post). Our e=\r\nncoding keeps producing such mutants, and it&#39;s death (via the physical cons=\r\ntraints of the pelvis) that keep them from being kept around. The historica=\r\nl accident of birthing through the pelvis aside, there would still be fitne=\r\nss consequences for more vastly neural connections (e.g. neurons are metabo=\r\nlically expensive, neural connections require energy to build and maintain,=\r\n and housing such large brains would create a large, clunky bobble head of =\r\na being that would be ungainly). It is possible that low connectivity is an=\r\n encoding bias, but were that true I think it would look like some sort of =\r\ngrowth rule that only grew a few connections per neuron (e.g. 10k). Evoluti=\r\non could have learned such a rule, and canalized that rule in a way that ma=\r\nkes it unlikely to have mutations that produce orders of magnitude more neu=\r\nrons, but my guess is that if it has done so, it was because of the fitness=\r\n costs associated with large numbers of neurons, not because of evolvabilit=\r\ny (or due to some historical accident). I do think it is interesting to stu=\r\ndy whether such biases exist in the encoding of neural growth rules, but ev=\r\nen if they do exist I don&#39;t think that shows that a fitness cost was not th=\r\ne ultimate cause. Note: I recognize that you admit that this could be argue=\r\nd &quot;either way&quot;=85are these the sorts of arguments you envisioned as being t=\r\nhe other way?\n\n&gt; In summary, the key difference between the alternatives is=\r\n that with fitness you are saying &quot;stay out of this part of the search spac=\r\ne&quot; whereas with encoding you are saying &quot;this part of the search space is m=\r\nuch smaller and hence less likely to encounter.&quot;\n&gt; \n&gt; \nI don&#39;t precisely un=\r\nderstand your latter clause, but I think I understand the spirit of it. I d=\r\nisagree, though. The reasons biases work is because they do bias search tow=\r\nards some areas and away from others: so I think both encoding biases and f=\r\nitness penalties have similar effects in this regard. \n\n&gt; So, my speculatio=\r\nn is that if you want to bias the search in highly complex domains, the bes=\r\nt way is through the encoding. Fitness is a nasty quagmire that is deceptiv=\r\nely tempting to manipulate, but never plays by the rules you wish it would.=\r\n Of course, these are merely my own unproven intuitions and their veracity =\r\nremains to be demonstrated. But at least it&#39;s something to think about.\n&gt; \n=\r\n&gt; \n\nThanks again for your feedback. As always, I appreciate your input and =\r\nenjoy discussing these fascinating subjects. \n\nI want to end by clarifying =\r\nthat I agree that there are positives and negatives to both approaches. I d=\r\no not see it as such an obvious choice between the two as you do. As I ment=\r\nioned up top: I definitely don&#39;t think initial encoding biases that selecti=\r\non can get rid of will get us very far. For simple problems they will work,=\r\n but for any challenging problem the initial bias will have long disappeare=\r\nd by the time it will matter. What we need is some constant force encouragi=\r\nng search to take promising paths. A fitness cost is one way to do that, bu=\r\nt a *constant* (or, at least, *periodic*) encoding bias could do that just =\r\nas well, and perhaps better. \n\n\nBest regards,\nJeff Clune\n\nAssistant Profess=\r\nor\nComputer Science\nUniversity of Wyoming\njeffclune@...\njeffclune.com\n=\r\n\n\n&gt; Best,\n&gt; \n&gt; ken\n&gt; \n&gt; --- In neat@yahoogroups.com, Alexandre Devert wrote=\r\n:\n&gt; &gt;\n&gt; &gt; Hi,\n&gt; &gt; \n&gt; &gt; =C2  Simple, clean experiment, with sharp results, c=\r\nongrats on that, definitely\n&gt; &gt; a step forward ! Of course, it begs for mor=\r\ne questions. I would love to hear\n&gt; &gt; you on such (fairly open) questions\n&gt;=\r\n &gt; \n&gt; &gt; =C2  =C2 1) Do you think that selection pressure for low connectivi=\r\nty is sufficient in\n&gt; &gt; itself to evolve large coherent networks, or is it =\r\njust a piece of the puzzle ?\n&gt; &gt; =C2  =C2 2) Do you see your work as an ind=\r\nication that any approach biased to low\n&gt; &gt; connectivity would reproduce th=\r\ne result ? Or does the way you guys enforced\n&gt; &gt; this bias matters ?\n&gt; &gt; \n&gt;=\r\n &gt; To me=C2 \n&gt; &gt; 1) =3D&gt; Part of the puzzle. Should see how well it scales =\r\nfor increasingly\n&gt; &gt; complex task, when the connection graph gets bigger. A=\r\n randomized=C2 \n&gt; &gt; search process=C2 on large graph sounds not so efficien=\r\nt, need something to guide it.\n&gt; &gt; I advocate construction process that hav=\r\ne a feedback from what the neuron=C2 \n&gt; &gt; network is computing. Don&#39;t know =\r\nhow to do it without creepling computational\n&gt; &gt; cost tho...\n&gt; &gt; 2) =3D&gt; I =\r\nguess that the bias alone is enough, the way to introduce it might\n&gt; &gt; not =\r\nbe such a big deal.=C2 \n&gt; &gt; \n&gt; &gt; Again, great work, very helpful contributi=\r\non :)\n&gt; &gt; \n&gt; &gt; Alex\n&gt; &gt; =C2 \n&gt; &gt; Dr. Devert Alexandre\n&gt; &gt; Researcher at the=\r\n Nature Inspired Computation and Applications Laboratory (NICAL)\n&gt; &gt; Lectur=\r\ner at School Of Software Engineering of USTC\n&gt; &gt; --------------------------=\r\n--------------------------\n&gt; &gt; Homepage :=C2 http://www.marmakoide.org\n&gt; &gt; =\r\n----------------------------------------------------\n&gt; &gt; 166 Renai Road, Du=\r\nshu Lake Higher Education Town\n&gt; &gt; Suzhou Industrial Park,\n&gt; &gt; Suzhou, Jian=\r\ngsu, People&#39;s Republic of China\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; _____________________________=\r\n___\n&gt; &gt; From: Jeff Clune \n&gt; &gt; To: neat users group group \n&gt; &gt; Cc: Jean-Bapt=\r\niste Mouret ; Hod Lipson \n&gt; &gt; Sent: Thursday, February 7, 2013 1:57 AM\n&gt; &gt; =\r\nSubject: [neat] New paper on why modules evolve, and how to evolve modular =\r\nartificial neural networks\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; =C2  \n&gt; &gt; Hello all,\n&gt; &gt; \n&gt; &gt; I&#39;m =\r\nextremely pleased to announce a new paper on a subject that many--including=\r\n myself--think is critical to making significant progress in our field: the=\r\n evolution of modularity.=C2 \n&gt; &gt; \n&gt; &gt; Jean-Baptiste Mouret, Hod Lipson and=\r\n I have a new paper that=C2 \n&gt; &gt; \n&gt; &gt; 1) sheds light on why modularity may =\r\nevolve in biological networks (e.g. neural, genetic, metabolic, protein-pro=\r\ntein, etc.)\n&gt; &gt; \n&gt; &gt; 2) provides a simple technique for evolving neural net=\r\nworks that are modular and have increased evolvability, in that they adapt =\r\nfaster to new environments. The modules that formed solved subproblems in t=\r\nhe domain.=C2 \n&gt; &gt; Cite:=C2 Clune J, Mouret J-B, Lipson H (2013) The evolut=\r\nionary origins of modularity. Proceedings of the Royal Society B. 280: 2012=\r\n2863.=C2 http://dx.doi.org/10.1098/rspb.2012.2863=C2 (pdf)\n&gt; &gt; \n&gt; &gt; Abstrac=\r\nt: A central biological question is how natural organisms are so evolvable =\r\n(capable of quickly adapting to new environments). A key driver of evolvabi=\r\nlity is the widespread modularity of biological networks=E2=80&quot;their organi=\r\nzation as functional, sparsely connected subunits=E2=80&quot;but there is no con=\r\nsensus regarding why modularity itself evolved. Although most hypotheses as=\r\nsume indirect selection for evolvability, here we demonstrate that the ubiq=\r\nuitous, direct selection pressure to reduce the cost of connections between=\r\n network nodes causes the emergence of modular networks. Computational evol=\r\nution experiments with selection pressures to maximize network performance =\r\nand minimize connection costs yield networks that are significantly more mo=\r\ndular and more evolvable than control experiments that only select for perf=\r\normance. These results will catalyse research in numerous disciplines, such=\r\n as neuroscience and genetics, and enhance our ability to harness\n&gt; &gt; evolu=\r\ntion for engineering purposes.\n&gt; &gt; \n&gt; &gt; Video:=C2 http://www.youtube.com/wa=\r\ntch?feature=3Dplayer_embedded&v=3DSG4_aW8LMng\n&gt; &gt; \n&gt; &gt; There has been some =\r\nnice coverage of this work in the popular press, in case you are interested=\r\n:\n&gt; &gt; National Geographic:=C2 http://phenomena.nationalgeographic.com/2013/=\r\n01/30/the-parts-of-life/MIT&#39;s Technology Review:=C2 http://www.technologyre=\r\nview.com/view/428504/computer-scientists-reproduce-the-evolution-of-evolvab=\r\nility/=C2 Fast Company:=C2 http://www.fastcompany.com/3005313/evolved-brain=\r\ns-robots-creep-closer-animal-learningCornell Chronicle:=C2 http://www.news.=\r\ncornell.edu/stories/Jan13/modNetwork.htmlScienceDaily:=C2 http://www.scienc=\r\nedaily.com/releases/2013/01/130130082300.htm\n&gt; &gt; \n&gt; &gt; Please let me know wh=\r\nat you think and if you have any questions. I hope this work will help our =\r\nfield move forward!\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; Best regards,\n&gt; &gt; Jeff Clune\n&gt; =\r\n&gt; \n&gt; &gt; Assistant Professor\n&gt; &gt; Computer Science\n&gt; &gt; University of Wyoming\n&gt;=\r\n &gt; jclune@...\n&gt; &gt; jeffclune.com\n&gt; &gt;\n&gt; \n&gt; \n&gt; \n\n\r\n--Apple-Mail=_3A102778-E47E-45B2-8FF3-2B8FCF1EEA4F\r\nContent-Transfer-Encoding: quoted-printable\r\nContent-Type: text/html;\n\tcharset=windows-1252\r\n\r\n&lt;html&gt;&lt;head&gt;&lt;meta http-equiv=3D&quot;Content-Type&quot; content=3D&quot;text/html charset=\r\n=3Dwindows-1252&quot;&gt;&lt;/head&gt;&lt;body style=3D&quot;word-wrap: break-word; -webkit-nbsp-=\r\nmode: space; -webkit-line-break: after-white-space; &quot;&gt;&lt;span style=3D&quot;font-s=\r\nize: 14px;&quot;&gt;Hello all,&lt;/span&gt;&lt;div&gt;&lt;span style=3D&quot;font-size: 14px;&quot;&gt;&lt;br&gt;&lt;/sp=\r\nan&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=3D&quot;font-size: 14px;&quot;&gt;As Ken mentioned, we&#39;ve disc=\r\nussed these issues in private. I&#39;m going to include some of my comments fro=\r\nm one of those email threads with slight modification, as I believe they su=\r\nmmarize the views of Jean-Baptiste and I on the issues Ken raises. I&#39;ll the=\r\nn respond to a few individual comments by Ken afterwards.&nbsp;&lt;/span&gt;&lt;/div=\r\n&gt;&lt;div&gt;&lt;span style=3D&quot;font-size: 14px;&quot;&gt;&lt;br&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=3D=\r\n&quot;font-size: 14px;&quot;&gt;---------------&lt;br&gt;&lt;/span&gt;&lt;div&gt;&lt;span style=3D&quot;font-size:=\r\n 14px;&quot;&gt;Ken,&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=3D&quot;font-size: 14px;&quot;&gt;&lt;br&gt;&lt;/span&gt;&lt;=\r\n/div&gt;&lt;div&gt;&lt;span style=3D&quot;font-size: 14px;&quot;&gt;It&#39;s great to hear your feedback=\r\n on our paper. Thanks for sending it.&lt;br&gt;&lt;br&gt;First off, thanks for the kind=\r\n words. We&#39;re very glad you liked the paper and think it is important.&nbsp=\r\n;&lt;br&gt;&lt;br&gt;Regarding a selection pressure vs. an encoding bias. We&#39;re not con=\r\nvinced that an initial encoding bias is a good way to encourage properties =\r\nthat one wants in phenotypes&nbsp;throughout evolution, such as modularity.=\r\n If there is any deceptiveness (or even neutrality) regarding modularity at=\r\n any point during the run then the bias will disappear, and then for the re=\r\nst of evolutionary&nbsp;time nothing will encourage modularity. We are more=\r\n convinced of the power of mutational bias in the encoding (i.e. a constant=\r\n encoding bias instead of just an initial encoding&nbsp;bias), and we think=\r\n it would be interesting to investigate area. However, if the encoding bias=\r\n is under selection, then you have the same issue where it might evolve awa=\r\ny.&nbsp;Selective pressures are interesting because they are constant, so y=\r\nou&#39;re more likely to get what you want. That raises the point you mention a=\r\nbout our pressure being too strong,&nbsp;such that evolution could not devi=\r\nate when it would be beneficial not to have modularity. That might be a pro=\r\nblem if the pressure is too strong, but it seems likely that in many&nbsp;c=\r\nases the benefits in terms of performance for being non-modular will outwei=\r\ngh the cost. In other words, evolution can decide to pay the cost of non-mo=\r\ndularity when it is useful (e.g. in your example of a hub of connections be=\r\ntween modules).&lt;br&gt;&lt;br&gt;Regarding playing with an encoding being safer than =\r\nplaying with selection pressures. Our view is that both are very complicate=\r\nd and can have unintended consequences, so&nbsp;playing with one is just as=\r\n bad as the other. I think our field is more familiar with unintended conse=\r\nquences of selective pressures just because we historically tend to play wi=\r\nth&nbsp;them more (and make simple encodings), but it is also very hard to =\r\nintuit the consequences of choices regarding biases in complex encodings. I=\r\nn your case the consequences are relatively intuitive, precisely&nbsp;becau=\r\nse they are so minimally interventionist...but that is also why I think the=\r\ny are not strong enough to cause modularity except in cases (like retina) w=\r\nhere all you have to do is&nbsp;initially place evolution in the right attr=\r\nactor basin.&lt;br&gt;&lt;br&gt;Regarding the resource hog waste of having a cost objec=\r\ntive. I have to have a little fun here and point out the irony of the co-ch=\r\nampion of novelty search worrying about the resources&nbsp;consumed by non-=\r\nhigh-performing individuals! Hehe. As you&#39;ve persuaded me, I&#39;m more interes=\r\nted in an algorithm that is interesting or that works than spending a littl=\r\ne&nbsp;computation inefficiently.&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=3D&quot;font-size=\r\n: 14px;&quot;&gt;&lt;br&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=3D&quot;font-size: 14px;&quot;&gt;I&#39;d also li=\r\nke to point out an innovation we came up with to mitigate the problem of pr=\r\neventing evolution from exploring solutions that are contrary to one of the=\r\n objectives. We recognized&nbsp;that&nbsp;the cost objective is ultimately =\r\nless important than the performance&nbsp;objective. We wanted evolution to =\r\nperiodically ignore the cost objective to explore stepping stones that had =\r\nhigher connectivity. To do that, we invented a technique that involves &quot;pro=\r\nbabilistic&nbsp;pareto dominance&quot;, wherein secondary objectives (in&nbsp;th=\r\nis&nbsp;case cost) are factored into pareto&nbsp;dominance&nbsp;only&nbsp;a=\r\n small percentage of the time. That won&#39;t solve the problem you mention if =\r\nyou have to take a long, many-multi-generational walk through high-connecti=\r\nvity areas of the search space, but it does allow quick forays into that te=\r\nrrain without any fitness penalty. This technique could be used for any cos=\r\nt (or other) objective, so it is not specific to connectivity costs.&nbsp;&lt;=\r\n/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=3D&quot;font-size: 14px;&quot;&gt;&lt;br&gt;See below for a few s=\r\npecific responses to your comments. I should note that below this the thoug=\r\nhts are my own and Jean-Baptiste should not be blamed for any of them! (Fee=\r\nl free to blame him for things above this line=85we went over that text tog=\r\nether a while back). ;-)&lt;/span&gt;&lt;/div&gt;&lt;div apple-content-edited=3D&quot;true&quot;&gt;&lt;di=\r\nv style=3D&quot;color: rgb(0, 0, 0); font-variant: normal; letter-spacing: norma=\r\nl; line-height: normal; orphans: 2; text-align: -webkit-auto; text-indent: =\r\n0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0p=\r\nx; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; word-wra=\r\np: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-sp=\r\nace; &quot;&gt;&lt;div style=3D&quot;color: rgb(0, 0, 0); font-variant: normal; letter-spac=\r\ning: normal; line-height: normal; orphans: 2; text-align: -webkit-auto; tex=\r\nt-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-s=\r\npacing: 0px; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px=\r\n; word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: afte=\r\nr-white-space; &quot;&gt;&lt;span class=3D&quot;Apple-style-span&quot; style=3D&quot;border-collapse:=\r\n separate; border-spacing: 0px; &quot;&gt;&lt;div style=3D&quot;word-wrap: break-word; -web=\r\nkit-nbsp-mode: space; -webkit-line-break: after-white-space; &quot;&gt;&lt;span class=\r\n=3D&quot;Apple-style-span&quot; style=3D&quot;border-collapse: separate; color: rgb(0, 0, =\r\n0); font-variant: normal; letter-spacing: normal; line-height: normal; orph=\r\nans: 2; text-align: -webkit-auto; text-indent: 0px; text-transform: none; w=\r\nhite-space: normal; widows: 2; word-spacing: 0px; border-spacing: 0px; -web=\r\nkit-text-decorations-in-effect: none; -webkit-text-size-adjust: auto; -webk=\r\nit-text-stroke-width: 0px; &quot;&gt;&lt;div style=3D&quot;word-wrap: break-word; -webkit-n=\r\nbsp-mode: space; -webkit-line-break: after-white-space; &quot;&gt;&lt;span class=3D&quot;Ap=\r\nple-style-span&quot; style=3D&quot;border-collapse: separate; color: rgb(0, 0, 0); fo=\r\nnt-variant: normal; letter-spacing: normal; line-height: normal; orphans: 2=\r\n; text-align: -webkit-auto; text-indent: 0px; text-transform: none; white-s=\r\npace: normal; widows: 2; word-spacing: 0px; border-spacing: 0px; -webkit-te=\r\nxt-decorations-in-effect: none; -webkit-text-size-adjust: auto; -webkit-tex=\r\nt-stroke-width: 0px; &quot;&gt;&lt;div style=3D&quot;word-wrap: break-word; -webkit-nbsp-mo=\r\nde: space; -webkit-line-break: after-white-space; &quot;&gt;&lt;span class=3D&quot;Apple-st=\r\nyle-span&quot; style=3D&quot;border-collapse: separate; color: rgb(0, 0, 0); font-var=\r\niant: normal; letter-spacing: normal; line-height: normal; orphans: 2; text=\r\n-align: -webkit-auto; text-indent: 0px; text-transform: none; white-space: =\r\nnormal; widows: 2; word-spacing: 0px; border-spacing: 0px; -webkit-text-dec=\r\norations-in-effect: none; -webkit-text-size-adjust: auto; -webkit-text-stro=\r\nke-width: 0px; font-size: 14px;&quot;&gt;&lt;div style=3D&quot;word-wrap: break-word; -webk=\r\nit-nbsp-mode: space; -webkit-line-break: after-white-space; &quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;/s=\r\npan&gt;&lt;/div&gt;&lt;/span&gt;&lt;/div&gt;&lt;/span&gt;&lt;/div&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;blockquo=\r\nte type=3D&quot;cite&quot;&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;div style=3D&quot;background-color: rgb(255, 255=\r\n, 255); position: static; z-index: auto; &quot;&gt;\n&lt;span style=3D&quot;display:none&quot;&gt;&n=\r\nbsp;&lt;/span&gt;\n\n\n\n    &lt;div id=3D&quot;ygrp-text&quot;&gt;&lt;p&gt;&lt;font face=3D&quot;Times&quot; style=3D&quot;f=\r\nont-size: 14px;&quot;&gt;More generally the issue is the usual problem of deception=\r\n, which is compounded by anything you do with fitness.  For example, in a c=\r\nomplex search space, there is a reasonable chance that the stepping stone t=\r\no a good low-connectivity solution is something with higher connectivity.  =\r\nBy manipulating fitness, you are cutting out all chances of encountering su=\r\nch a deceptive stepping stone.  But even if you don&#39;t believe that could be=\r\n true, the single-mindedness of always favoring low-connectivity could dece=\r\nive you from many parts of the search space that might be stepping stones t=\r\no something worthwhile, relating to connection density or not.&lt;br&gt;\n&lt;br&gt;&lt;/fo=\r\nnt&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/blockquote&gt;&lt;div&gt;&lt;span style=3D&quot;font-size: =\r\n14px;&quot;&gt;True. But the same exact thing can be said for biases in the encodin=\r\ng: they prevent you from searching large areas of the search space. You may=\r\n reply that it is only a bias, not a strict ban, but of course we know that=\r\n in large search spaces biases hugely affect the landscape such that certai=\r\nn areas will practically never be visited.&nbsp;&lt;/span&gt;&lt;/div&gt;&lt;span style=3D=\r\n&quot;font-size: 14px;&quot;&gt;&lt;br&gt;&lt;/span&gt;&lt;blockquote type=3D&quot;cite&quot;&gt;&lt;div style=3D&quot;backg=\r\nround-color: rgb(255, 255, 255); position: static; z-index: auto; &quot;&gt;&lt;div id=\r\n=3D&quot;ygrp-mlmsg&quot; style=3D&quot;position: relative; &quot;&gt;&lt;div id=3D&quot;ygrp-msg&quot; style=\r\n=3D&quot;z-index: 1;&quot;&gt;&lt;div id=3D&quot;ygrp-text&quot;&gt;&lt;p&gt;&lt;font face=3D&quot;Times&quot; style=3D&quot;fon=\r\nt-size: 14px;&quot;&gt;\nOn the other hand, manipulating the encoding is different b=\r\necause in effect it actually reorganizes the structure of the search space =\r\nitself, which seems to me a more principled thing to do (if you can figure =\r\nout a way to do it).  Because the thing is, in that case, you do not need t=\r\no worry about a permanent dead weight taking up some proportion of your pop=\r\nulation forever.  Instead, while the encoding may *tend* to produce e.g. lo=\r\nw-connectivity solutions, it can still escape that tendency without any pen=\r\nalty to fitness.&lt;/font&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/blockquote&gt;&lt;div&gt;&lt;span =\r\nstyle=3D&quot;font-size: 14px;&quot;&gt;My instincts tell me that we create dead weight =\r\nwith encoding biases too. For example, an overly regular generative encodin=\r\ng (e.g. context free L-systems) is great if good solutions are perfectly re=\r\ngular, but if what is required is a mix of regularity and irregularity, the=\r\nn you spend your entire time producing only highly regular phenotypes that =\r\nnever wander into the appropriately irregular areas of the search space. Ou=\r\nr IEEE TEC paper, for example, shows that HyperNEAT can spend thousands of =\r\ngenerations spinning its wheels never generating solutions that HybrID coul=\r\nd easily generate, demonstrating a &quot;overly regular&quot; dead weight associated =\r\nwith the biases of even the best known* generative encoding! Both fitness p=\r\nenalties and biases can cause you to focus your search in unproductive area=\r\ns=85which is ultimately what dead weight is.&nbsp;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span s=\r\ntyle=3D&quot;font-size: 14px;&quot;&gt;&lt;br&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=3D&quot;font-size: 1=\r\n4px;&quot;&gt;* in our opinion! :0)&lt;/span&gt;&lt;/div&gt;&lt;span style=3D&quot;font-size: 14px;&quot;&gt;&lt;b=\r\nr&gt;&lt;/span&gt;&lt;blockquote type=3D&quot;cite&quot;&gt;&lt;div style=3D&quot;background-color: rgb(255,=\r\n 255, 255); position: static; z-index: auto; &quot;&gt;&lt;div id=3D&quot;ygrp-mlmsg&quot; style=\r\n=3D&quot;position: relative; &quot;&gt;&lt;div id=3D&quot;ygrp-msg&quot; style=3D&quot;z-index: 1;&quot;&gt;&lt;div i=\r\nd=3D&quot;ygrp-text&quot;&gt;&lt;p&gt;&lt;font face=3D&quot;Times&quot; style=3D&quot;font-size: 14px;&quot;&gt;  Furthe=\r\nrmore, in reality the best situation regarding modularity and connectivity =\r\nis probably rather subtle, with most of the brain respecting the principle =\r\nof low connectivity, but with a number of critical exceptions in key areas,=\r\n such as major inter-module hubs.  A sophisticated encoding can allow its b=\r\nias to bend to make such nuanced exceptions (e.g. based on locations within=\r\n a geometry), whereas a fitness penalty is a heavy hand and blunt instrumen=\r\nt that cannot but help always to demand global and holistic subservience to=\r\n dogmatic universals (unless you are willing to take a hit in fitness).&lt;br&gt;=\r\n\n&lt;br&gt;&lt;/font&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/blockquote&gt;&lt;div&gt;&lt;span style=3D&quot;fo=\r\nnt-size: 14px;&quot;&gt;I think the last clause you offer is the key exception thou=\r\ngh. As I mentioned above, if a certain phenotype pays for its wiring by inc=\r\nreasing fitness, it can add high-connectivity areas anywhere that they are =\r\nuseful (without even needing to carve out that area in geometric space, whi=\r\nch is often a difficult task for CPPNs). Instead of being a blunt instrumen=\r\nt, a fitness penalty can be quite subtle, because it can allow connection-b=\r\ny-connection exceptions&nbsp;if they produce fitness improvements, and do s=\r\no&nbsp;without any search overhead.&nbsp;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=3D&quot;f=\r\nont-size: 14px;&quot;&gt;&lt;br&gt;&lt;/span&gt;&lt;/div&gt;&lt;blockquote type=3D&quot;cite&quot;&gt;&lt;div style=3D&quot;b=\r\nackground-color: rgb(255, 255, 255); position: static; z-index: auto; &quot;&gt;&lt;di=\r\nv id=3D&quot;ygrp-mlmsg&quot; style=3D&quot;position: relative; &quot;&gt;&lt;div id=3D&quot;ygrp-msg&quot; sty=\r\nle=3D&quot;z-index: 1;&quot;&gt;&lt;div id=3D&quot;ygrp-text&quot;&gt;&lt;p&gt;&lt;font face=3D&quot;Times&quot; style=3D&quot;f=\r\nont-size: 14px;&quot;&gt;\nAn interesting question in nature (where our brains evolv=\r\ned modular structure) is whether its tendency towards low connectivity is a=\r\n result of an aspect of fitness in the wild, or an aspect of encoding bias.=\r\n  I think there is a lot of room in this question for arguing either way, b=\r\nut my hunch is that the bias is mostly in the encoding.  My logic is that I=\r\n think the reason that the connectivity of the brain is so much lower than =\r\nwhat it could be (e.g. it is a tiny fraction of everything-to-everything co=\r\nnnectivity) is an artifact of physics rather than an artifact of fitness.  =\r\nIt is simply physically impossible for a giant 100-billion-to-100-billion c=\r\nonnectivity to fit in a head anything close to our size.  And physical impo=\r\nssibility is in some sense a property of encoding.  That is, mutations that=\r\n could step from a low-connectivity brain to a high one are few and far bet=\r\nween simply because of physical constraint.  So high-connectivity structure=\r\ns are simply a very small part of the search space of brains in the physica=\r\nl universe.  However, at the same time, you can still get long-range connec=\r\ntions from time to time because there is no universal penalty for doing so,=\r\n just a lower a priori probability of such mutations occurring.&lt;br&gt;\n&lt;br&gt;&lt;/f=\r\nont&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/blockquote&gt;&lt;div&gt;&lt;span style=3D&quot;font-size:=\r\n 14px;&quot;&gt;Here I completely disagree with you. I see the force preventing the=\r\n volume of neural connections from getting too large as a direct fitness co=\r\nst, not an encoding bias. If mutations increase the size of the head, the b=\r\naby and the mother are more likely to die in childbirth. Anthropologists ha=\r\nve long known that evolution&#39;s desire to have larger and larger brains is t=\r\nhe main reason why humans have such ridiculously high maternal and infant m=\r\nortality &quot;in the wild&quot; (pre modern health care, and even post). Our encodin=\r\ng keeps producing such mutants, and it&#39;s death (via the physical constraint=\r\ns of the pelvis) that keep them from being kept around. The historical acci=\r\ndent of birthing through the pelvis aside, there would still be fitness con=\r\nsequences for more vastly neural connections (e.g. neurons are metabolicall=\r\ny expensive, neural connections require energy to build and maintain, and h=\r\nousing such large brains would create a large, clunky bobble head of a bein=\r\ng that would be ungainly). It is possible that low connectivity is an encod=\r\ning bias, but were that true I think it would look like some sort of growth=\r\n rule that only grew a few connections per neuron (e.g. 10k). Evolution cou=\r\nld have learned such a rule, and canalized that rule in a way that makes it=\r\n unlikely to have mutations that produce orders of magnitude more neurons, =\r\nbut my guess is that if it has done so, it was because of the fitness costs=\r\n associated with large numbers of neurons, not because of evolvability (or =\r\ndue to some historical accident). I do think it is interesting to study whe=\r\nther such biases exist in the encoding of neural growth rules, but even if =\r\nthey do exist I don&#39;t think that shows that a fitness cost was not the ulti=\r\nmate cause. Note: I recognize that you admit that this could be argued &quot;eit=\r\nher way&quot;=85are these the sorts of arguments you envisioned as being the oth=\r\ner way?&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=3D&quot;font-size: 14px;&quot;&gt;&lt;br&gt;&lt;/span&gt;&lt;/div&gt;=\r\n&lt;blockquote type=3D&quot;cite&quot;&gt;&lt;div style=3D&quot;background-color: rgb(255, 255, 255=\r\n); position: static; z-index: auto; &quot;&gt;&lt;div id=3D&quot;ygrp-mlmsg&quot; style=3D&quot;posit=\r\nion: relative; &quot;&gt;&lt;div id=3D&quot;ygrp-msg&quot; style=3D&quot;z-index: 1;&quot;&gt;&lt;div id=3D&quot;ygrp=\r\n-text&quot;&gt;&lt;p&gt;&lt;font face=3D&quot;Times&quot; style=3D&quot;font-size: 14px;&quot;&gt;\nIn summary, the =\r\nkey difference between the alternatives is that with fitness you are saying=\r\n &quot;stay out of this part of the search space&quot; whereas with encoding you are =\r\nsaying &quot;this part of the search space is much smaller and hence less likely=\r\n to encounter.&quot;&lt;br&gt;\n&lt;br&gt;&lt;/font&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/blockquote&gt;&lt;di=\r\nv&gt;&lt;span style=3D&quot;font-size: 14px;&quot;&gt;I don&#39;t precisely understand your latter=\r\n clause, but I think I understand the spirit of it. I disagree, though. The=\r\n reasons biases work is because they do bias search towards some areas and =\r\naway from others: so I think both encoding biases and fitness penalties hav=\r\ne similar effects in this regard.&nbsp;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=3D&quot;fon=\r\nt-size: 14px;&quot;&gt;&lt;br&gt;&lt;/span&gt;&lt;/div&gt;&lt;blockquote type=3D&quot;cite&quot;&gt;&lt;div style=3D&quot;bac=\r\nkground-color: rgb(255, 255, 255); position: static; z-index: auto; &quot;&gt;&lt;div =\r\nid=3D&quot;ygrp-mlmsg&quot; style=3D&quot;position: relative; &quot;&gt;&lt;div id=3D&quot;ygrp-msg&quot; style=\r\n=3D&quot;z-index: 1;&quot;&gt;&lt;div id=3D&quot;ygrp-text&quot;&gt;&lt;p&gt;&lt;font face=3D&quot;Times&quot; style=3D&quot;fon=\r\nt-size: 14px;&quot;&gt;\nSo, my speculation is that if you want to bias the search i=\r\nn highly complex domains, the best way is through the encoding.  Fitness is=\r\n a nasty quagmire that is deceptively tempting to manipulate, but never pla=\r\nys by the rules you wish it would.  Of course, these are merely my own unpr=\r\noven intuitions and their veracity remains to be demonstrated.  But at leas=\r\nt it&#39;s something to think about.&lt;br&gt;\n&lt;br&gt;&lt;/font&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div=\r\n&gt;&lt;/blockquote&gt;&lt;div&gt;&lt;span style=3D&quot;font-size: 14px;&quot;&gt;&lt;br&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;=\r\ndiv&gt;&lt;span style=3D&quot;font-size: 14px;&quot;&gt;Thanks again for your feedback. As alw=\r\nays, I appreciate your input and enjoy discussing these fascinating subject=\r\ns.&nbsp;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=3D&quot;font-size: 14px;&quot;&gt;&lt;br&gt;&lt;/span&gt;&lt;/div=\r\n&gt;&lt;div&gt;&lt;span style=3D&quot;font-size: 14px;&quot;&gt;I want to end by clarifying that I a=\r\ngree that there are positives and negatives to both approaches. I do not se=\r\ne it as such an obvious choice between the two as you do. As I mentioned up=\r\n top: I definitely don&#39;t think initial encoding biases that selection can g=\r\net rid of will get us very far. For simple problems they will work, but for=\r\n any challenging problem the initial bias will have long disappeared by the=\r\n time it will matter. What we need is some constant force encouraging searc=\r\nh to take promising paths. A fitness cost is one way to do that, but a *con=\r\nstant* (or, at least, *periodic*) encoding bias could do that just as well,=\r\n and perhaps better.&nbsp;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=3D&quot;font-size: 14px;=\r\n&quot;&gt;&lt;br&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=3D&quot;font-size: 14px;&quot;&gt;&lt;br&gt;&lt;/span&gt;&lt;=\r\n/div&gt;&lt;div&gt;&lt;div&gt;&lt;div style=3D&quot;word-wrap: break-word; -webkit-nbsp-mode: spac=\r\ne; -webkit-line-break: after-white-space; &quot;&gt;&lt;div style=3D&quot;word-wrap: break-=\r\nword; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; &quot;&gt;&lt;s=\r\npan class=3D&quot;Apple-style-span&quot; style=3D&quot;border-collapse: separate; border-s=\r\npacing: 0px; &quot;&gt;&lt;div style=3D&quot;word-wrap: break-word; -webkit-nbsp-mode: spac=\r\ne; -webkit-line-break: after-white-space; &quot;&gt;&lt;span class=3D&quot;Apple-style-span=\r\n&quot; style=3D&quot;border-collapse: separate; border-spacing: 0px; &quot;&gt;&lt;div style=3D&quot;=\r\nword-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-=\r\nwhite-space; &quot;&gt;&lt;span class=3D&quot;Apple-style-span&quot; style=3D&quot;border-collapse: s=\r\neparate; border-spacing: 0px; &quot;&gt;&lt;div style=3D&quot;word-wrap: break-word; -webki=\r\nt-nbsp-mode: space; -webkit-line-break: after-white-space; &quot;&gt;&lt;span class=3D=\r\n&quot;Apple-style-span&quot; style=3D&quot;border-collapse: separate; border-spacing: 0px;=\r\n font-size: 14px;&quot;&gt;&lt;div style=3D&quot;word-wrap: break-word; -webkit-nbsp-mode: =\r\nspace; -webkit-line-break: after-white-space; &quot;&gt;Best regards,&lt;br&gt;&lt;font clas=\r\ns=3D&quot;Apple-style-span&quot; color=3D&quot;#0a5d19&quot;&gt;Jeff Clune&lt;/font&gt;&lt;br&gt;&lt;br&gt;Assistant=\r\n Professor&lt;br&gt;Computer Science&lt;/div&gt;&lt;div style=3D&quot;word-wrap: break-word; -w=\r\nebkit-nbsp-mode: space; -webkit-line-break: after-white-space; &quot;&gt;University=\r\n of Wyoming&lt;br&gt;&lt;a href=3D&quot;mailto:jeffclune@...&quot;&gt;jeffclune@...&lt;/a&gt;=\r\n&lt;br&gt;jeffclune.com&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;/span&gt;&lt;/div&gt;&lt;/span&gt;&lt;/div&gt;&lt;/span&gt;&lt;/di=\r\nv&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;span style=3D&quot;font-size: 14px;&quot;&gt;&lt;br&gt;&lt;/spa=\r\nn&gt;&lt;blockquote type=3D&quot;cite&quot;&gt;&lt;div style=3D&quot;background-color: rgb(255, 255, 2=\r\n55); position: static; z-index: auto; &quot;&gt;&lt;div id=3D&quot;ygrp-mlmsg&quot; style=3D&quot;pos=\r\nition: relative; &quot;&gt;&lt;div id=3D&quot;ygrp-msg&quot; style=3D&quot;z-index: 1;&quot;&gt;&lt;div id=3D&quot;yg=\r\nrp-text&quot;&gt;&lt;p&gt;&lt;font face=3D&quot;Times&quot; style=3D&quot;font-size: 14px;&quot;&gt;\nBest,&lt;br&gt;\n&lt;br&gt;=\r\n\nken&lt;br&gt;\n&lt;br&gt;\n--- In &lt;a href=3D&quot;mailto:neat%40yahoogroups.com&quot;&gt;neat@yahoogr=\r\noups.com&lt;/a&gt;, Alexandre Devert  wrote:&lt;br&gt;\n&gt;&lt;br&gt;\n&gt; Hi,&lt;br&gt;\n&gt; &lt;br&gt;\n=\r\n&gt; =C2&nbsp; Simple, clean experiment, with sharp results, congrats on th=\r\nat, definitely&lt;br&gt;\n&gt; a step forward ! Of course, it begs for more questi=\r\nons. I would love to hear&lt;br&gt;\n&gt; you on such (fairly open) questions&lt;br&gt;\n=\r\n&gt; &lt;br&gt;\n&gt; =C2&nbsp; =C2&nbsp;1) Do you think that selection pressure f=\r\nor low connectivity is sufficient in&lt;br&gt;\n&gt; itself to evolve large cohere=\r\nnt networks, or is it just a piece of the puzzle ?&lt;br&gt;\n&gt; =C2&nbsp; =C2&n=\r\nbsp;2) Do you see your work as an indication that any approach biased to lo=\r\nw&lt;br&gt;\n&gt; connectivity would reproduce the result ? Or does the way you gu=\r\nys enforced&lt;br&gt;\n&gt; this bias matters ?&lt;br&gt;\n&gt; &lt;br&gt;\n&gt; To me=C2&nbsp;&lt;=\r\nbr&gt;\n&gt; 1) =3D&gt; Part of the puzzle. Should see how well it scales for i=\r\nncreasingly&lt;br&gt;\n&gt; complex task, when the connection graph gets bigger. A=\r\n randomized=C2&nbsp;&lt;br&gt;\n&gt; search process=C2&nbsp;on large graph sounds =\r\nnot so efficient, need something to guide it.&lt;br&gt;\n&gt; I advocate construct=\r\nion process that have a feedback from what the neuron=C2&nbsp;&lt;br&gt;\n&gt; net=\r\nwork is computing. Don&#39;t know how to do it without creepling computational&lt;=\r\nbr&gt;\n&gt; cost tho...&lt;br&gt;\n&gt; 2) =3D&gt; I guess that the bias alone is eno=\r\nugh, the way to introduce it might&lt;br&gt;\n&gt; not be such a big deal.=C2&nbsp=\r\n;&lt;br&gt;\n&gt; &lt;br&gt;\n&gt; Again, great work, very helpful contribution :)&lt;br&gt;\n&g=\r\nt; &lt;br&gt;\n&gt; Alex&lt;br&gt;\n&gt; =C2&nbsp;&lt;br&gt;\n&gt; Dr. Devert Alexandre&lt;br&gt;\n&gt;=\r\n Researcher at the Nature Inspired Computation and Applications Laboratory =\r\n(NICAL)&lt;br&gt;\n&gt; Lecturer at School Of Software Engineering of USTC&lt;br&gt;\n&gt=\r\n; ----------------------------------------------------&lt;br&gt;\n&gt; Homepage :=\r\n=C2&nbsp;&lt;a href=3D&quot;http://www.marmakoide.org/&quot;&gt;http://www.marmakoide.org&lt;/=\r\na&gt;&lt;br&gt;\n&gt; ----------------------------------------------------&lt;br&gt;\n&gt; 1=\r\n66 Renai Road, Dushu Lake Higher Education Town&lt;br&gt;\n&gt; Suzhou Industrial =\r\nPark,&lt;br&gt;\n&gt; Suzhou, Jiangsu, People&#39;s Republic of China&lt;br&gt;\n&gt; &lt;br&gt;\n&g=\r\nt; &lt;br&gt;\n&gt; ________________________________&lt;br&gt;\n&gt;  From: Jeff Clune &lt;b=\r\nr&gt;\n&gt; To: neat users group group  &lt;br&gt;\n&gt; Cc: Jean-Baptiste Mouret ; Ho=\r\nd Lipson  &lt;br&gt;\n&gt; Sent: Thursday, February 7, 2013 1:57 AM&lt;br&gt;\n&gt; Subje=\r\nct: [neat] New paper on why modules evolve, and how to evolve modular artif=\r\nicial neural networks&lt;br&gt;\n&gt;  &lt;br&gt;\n&gt; &lt;br&gt;\n&gt; =C2&nbsp; &lt;br&gt;\n&gt; Hel=\r\nlo all,&lt;br&gt;\n&gt; &lt;br&gt;\n&gt; I&#39;m extremely pleased to announce a new paper on=\r\n a subject that many--including myself--think is critical to making signifi=\r\ncant progress in our field: the evolution of modularity.=C2&nbsp;&lt;br&gt;\n&gt; =\r\n&lt;br&gt;\n&gt; Jean-Baptiste Mouret, Hod Lipson and I have a new paper that=C2&n=\r\nbsp;&lt;br&gt;\n&gt; &lt;br&gt;\n&gt; 1) sheds light on why modularity may evolve in biol=\r\nogical networks (e.g. neural, genetic, metabolic, protein-protein, etc.)&lt;br=\r\n&gt;\n&gt; &lt;br&gt;\n&gt; 2) provides a simple technique for evolving neural network=\r\ns that are modular and have increased evolvability, in that they adapt fast=\r\ner to new environments. The modules that formed solved subproblems in the d=\r\nomain.=C2&nbsp;&lt;br&gt;\n&gt; Cite:=C2&nbsp;Clune J, Mouret J-B, Lipson H (2013)=\r\n The evolutionary origins of modularity. Proceedings of the Royal Society B=\r\n. 280: 20122863.=C2&nbsp;&lt;a href=3D&quot;http://dx.doi.org/10.1098/rspb.2012.286=\r\n3&quot;&gt;http://dx.doi.org/10.1098/rspb.2012.2863&lt;/a&gt;=C2&nbsp;(pdf)&lt;br&gt;\n&gt; &lt;br&gt;=\r\n\n&gt; Abstract: A central biological question is how natural organisms are =\r\nso evolvable (capable of quickly adapting to new environments). A key drive=\r\nr of evolvability is the widespread modularity of biological networks=E2=80=\r\n&quot;their organization as functional, sparsely connected subunits=E2=80&quot;but th=\r\nere is no consensus regarding why modularity itself evolved. Although most =\r\nhypotheses assume indirect selection for evolvability, here we demonstrate =\r\nthat the ubiquitous, direct selection pressure to reduce the cost of connec=\r\ntions between network nodes causes the emergence of modular networks. Compu=\r\ntational evolution experiments with selection pressures to maximize network=\r\n performance and minimize connection costs yield networks that are signific=\r\nantly more modular and more evolvable than control experiments that only se=\r\nlect for performance. These results will catalyse research in numerous disc=\r\niplines, such as neuroscience and genetics, and enhance our ability to harn=\r\ness&lt;br&gt;\n&gt;  evolution for engineering purposes.&lt;br&gt;\n&gt; &lt;br&gt;\n&gt; Video:=\r\n=C2&nbsp;&lt;a href=3D&quot;http://www.youtube.com/watch?feature=3Dplayer_embedded&=\r\namp;v=3DSG4_aW8LMng&quot;&gt;http://www.youtube.com/watch?feature=3Dplayer_embedded=\r\n&amp;v=3DSG4_aW8LMng&lt;/a&gt;&lt;br&gt;\n&gt; &lt;br&gt;\n&gt; There has been some nice covera=\r\nge of this work in the popular press, in case you are interested:&lt;br&gt;\n&gt; =\r\nNational Geographic:=C2&nbsp;&lt;a href=3D&quot;http://phenomena.nationalgeographic=\r\n.com/2013/01/30/the-parts-of-life/MIT&quot;&gt;http://phenomena.nationalgeographic.=\r\ncom/2013/01/30/the-parts-of-life/MIT&lt;/a&gt;&#39;s Technology Review:=C2&nbsp;&lt;a hr=\r\nef=3D&quot;http://www.technologyreview.com/view/428504/computer-scientists-repro=\r\nduce-the-evolution-of-evolvability/&quot;&gt;http://www.technologyreview.com/view/4=\r\n28504/computer-scientists-reproduce-the-evolution-of-evolvability/&lt;/a&gt;=C2&n=\r\nbsp;Fast Company:=C2&nbsp;&lt;a href=3D&quot;http://www.fastcompany.com/3005313/evo=\r\nlved-brains-robots-creep-closer-animal-learningCornell&quot;&gt;http://www.fastcomp=\r\nany.com/3005313/evolved-brains-robots-creep-closer-animal-learningCornell&lt;/=\r\na&gt; Chronicle:=C2&nbsp;&lt;a href=3D&quot;http://www.news.cornell.edu/stories/Jan13/=\r\nmodNetwork.htmlScienceDaily:&quot;&gt;http://www.news.cornell.edu/stories/Jan13/mod=\r\nNetwork.htmlScienceDaily:&lt;/a&gt;=C2&nbsp;&lt;a href=3D&quot;http://www.sciencedaily.co=\r\nm/releases/2013/01/130130082300.htm&quot;&gt;http://www.sciencedaily.com/releases/2=\r\n013/01/130130082300.htm&lt;/a&gt;&lt;br&gt;\n&gt; &lt;br&gt;\n&gt; Please let me know what you =\r\nthink and if you have any questions. I hope this work will help our field m=\r\nove forward!&lt;br&gt;\n&gt; &lt;br&gt;\n&gt; &lt;br&gt;\n&gt; &lt;br&gt;\n&gt; &lt;br&gt;\n&gt; Best regards,=\r\n&lt;br&gt;\n&gt; Jeff Clune&lt;br&gt;\n&gt; &lt;br&gt;\n&gt; Assistant Professor&lt;br&gt;\n&gt; Comput=\r\ner Science&lt;br&gt;\n&gt; University of Wyoming&lt;br&gt;\n&gt; jclune@...&lt;br&gt;\n&gt; &lt;a h=\r\nref=3D&quot;http://jeffclune.com&quot;&gt;jeffclune.com&lt;/a&gt;&lt;br&gt;\n&gt;&lt;br&gt;\n&lt;br&gt;\n&lt;/font&gt;&lt;/p=\r\n&gt;\n\n    &lt;/div&gt;\n     \n\n    \n\n&lt;/div&gt;\n\n\n\n&lt;!-- end group email --&gt;\n\n&lt;/blockquote=\r\n&gt;&lt;/div&gt;&lt;br&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;\r\n--Apple-Mail=_3A102778-E47E-45B2-8FF3-2B8FCF1EEA4F--\r\n\n"}}