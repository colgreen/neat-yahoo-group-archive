{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":344770313,"authorName":"Colin Green","from":"Colin Green &lt;colin.green1@...&gt;","profile":"alienseedpod","replyTo":"LIST","senderId":"RNJN3PsdsR9O0pDovJjRG4AnQr2_ZSXKMTCvoLhiB7R0FLehmbtRIPR_tD-H6dIAjou3VwEUIdGkCz4FFXeFW3Cegvuk3GMhQrZvGp7KHpA","spamInfo":{"isSpam":false,"reason":"4"},"subject":"Re: [neat] Doble/Single digit precision NEAT","postDate":"1242754382","msgId":4673,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDcyN2E0MDZjMDkwNTE5MTAzM3Q3NjExMWRlY3hlMTQ1Yzc3MGQwMTgxYzZAbWFpbC5nbWFpbC5jb20+","inReplyToHeader":"PEM4NkZFOEY0OEMxNzQ5QjI4RUZGQzEzRTc2MTc0MDM1QHdhdHRwND4=","referencesHeader":"PGd1c2xlZSt1dDU4QGVHcm91cHMuY29tPiA8Qzg2RkU4RjQ4QzE3NDlCMjhFRkZDMTNFNzYxNzQwMzVAd2F0dHA0Pg=="},"prevInTopic":4672,"nextInTopic":4674,"prevInTime":4672,"nextInTime":4674,"topicId":4671,"numMessagesInTopic":7,"msgSnippet":"For the record the default neural net implementation in SharpNeat uses single precision floats although there are implementations that use double, and also the","rawEmail":"Return-Path: &lt;colin.green1@...&gt;\r\nX-Sender: colin.green1@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 86882 invoked from network); 19 May 2009 17:34:03 -0000\r\nX-Received: from unknown (69.147.108.201)\n  by m3.grp.re1.yahoo.com with QMQP; 19 May 2009 17:34:03 -0000\r\nX-Received: from unknown (HELO yw-out-1718.google.com) (74.125.46.154)\n  by mta2.grp.re1.yahoo.com with SMTP; 19 May 2009 17:34:03 -0000\r\nX-Received: by yw-out-1718.google.com with SMTP id 9so2247259ywk.76\n        for &lt;neat@yahoogroups.com&gt;; Tue, 19 May 2009 10:33:03 -0700 (PDT)\r\nMIME-Version: 1.0\r\nX-Received: by 10.100.177.15 with SMTP id z15mr617790ane.27.1242754382984; Tue, \n\t19 May 2009 10:33:02 -0700 (PDT)\r\nIn-Reply-To: &lt;C86FE8F48C1749B28EFFC13E76174035@wattp4&gt;\r\nReferences: &lt;guslee+ut58@...&gt; &lt;C86FE8F48C1749B28EFFC13E76174035@wattp4&gt;\r\nDate: Tue, 19 May 2009 18:33:02 +0100\r\nMessage-ID: &lt;727a406c0905191033t76111decxe145c770d0181c6@...&gt;\r\nTo: neat@yahoogroups.com\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 2:4:8:0:0\r\nFrom: Colin Green &lt;colin.green1@...&gt;\r\nSubject: Re: [neat] Doble/Single digit precision NEAT\r\nX-Yahoo-Group-Post: member; u=344770313; y=lYFoid-UBjUjiGk668XlfN8VEosbidpfozayCvt7kgapBjCx1J8l\r\nX-Yahoo-Profile: alienseedpod\r\n\r\nFor the record the default neural net implementation in SharpNeat uses\nsingle precision floats although there are implementations that use\ndouble, and also the genome class uses doubles. Unless there is some\noverriding reason you may be aware of (e.g. for the problem domain in\nquestion) then using doubles is probably unnecessary. E.g. I was\nrecently experimenting with a gradient descent algorithm and noticed\nthe algorithm stopped with very small learning rates when using single\nprecision - the reason being that the update increments were simply\ntoo small, that is, when adding a tiny value to a non-zero variable\nthe true result may require more precision than is available and so\nthe variable becomes &#39;stuck&#39;.\n\nI chose floats for a small performance gain on standard CPUs, it&#39;s not\nnecessarily the CPU arithmetic that is faster since CPUs are designed\nto do double precision very fast these days, but there will always be\na 2x memory bandwidth benefit with floats and of course gains when\nusing CUDA, Larabee, SIMD instructions etc.\n\nColin.\n\n"}}