{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"ODEKY891a96NZ6ZBbpN9-w1IVyIlpYO0qBPSVV0er1Muv1B-XsqXrnmodsPiGa06EInHF1OKG4fOHMujHOlb52NSyv9-p6SmgD8rHQxLTYw7","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: NEAT X Cascade Correlation","postDate":"1165900138","msgId":2882,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGVsbGRoYStxN2FyQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGQyNzhlM2FkMDYxMjExMDk0M3YxNzg3MTA1OGdmMjM2MDEzZWI4ODlhNDBiQG1haWwuZ21haWwuY29tPg=="},"prevInTopic":2880,"nextInTopic":2884,"prevInTime":2881,"nextInTime":2883,"topicId":2878,"numMessagesInTopic":5,"msgSnippet":"I think a significant difference is that CC is geared more naturally to supervised training problems.  That is, examples must be presented to the CC network as","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 74368 invoked from network); 12 Dec 2006 05:10:54 -0000\r\nReceived: from unknown (66.218.67.34)\n  by m27.grp.scd.yahoo.com with QMQP; 12 Dec 2006 05:10:54 -0000\r\nReceived: from unknown (HELO n16c.bullet.sp1.yahoo.com) (69.147.64.123)\n  by mta8.grp.scd.yahoo.com with SMTP; 12 Dec 2006 05:10:54 -0000\r\nReceived: from [216.252.122.219] by n16.bullet.sp1.yahoo.com with NNFMP; 12 Dec 2006 05:08:59 -0000\r\nReceived: from [66.218.69.5] by t4.bullet.sp1.yahoo.com with NNFMP; 12 Dec 2006 05:08:59 -0000\r\nReceived: from [66.218.66.70] by t5.bullet.scd.yahoo.com with NNFMP; 12 Dec 2006 05:08:59 -0000\r\nDate: Tue, 12 Dec 2006 05:08:58 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;elldha+q7ar@...&gt;\r\nIn-Reply-To: &lt;d278e3ad0612110943v17871058gf236013eb889a40b@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: Re: NEAT X Cascade Correlation\r\nX-Yahoo-Group-Post: member; u=54567749; y=nsfa1DjseH7Frus8cQEhWEdWLpE-LlK-dn7u5UMvpdGB0wtxWgqh\r\nX-Yahoo-Profile: kenstanley01\r\n\r\nI think a significant difference is that CC is geared more naturally \nto su=\r\npervised training problems.  That is, examples must be \npresented to the CC=\r\n network as input/target training pairs, just \nlike in backpropagation.  Th=\r\nus, it is more difficult to apply CC \ndirectly to reinforcement learning pr=\r\noblems where there is no \ntraining target but rather only sparse reinforcem=\r\nent signals such \nas &quot;win/lose&quot; or how fast some task was completed by a ca=\r\nndidate \nnetwork.  NEAT on the other hand is well-suited to such problems \n=\r\nbecause fitness functions naturally capture sparse reinforcement \nsignals f=\r\nor training purposes.  Thus, the scope of problems for \nwhich CC is easiest=\r\n to apply is more limited.\n\nHowever, if I was doing a classic supervised-le=\r\narning problem with \ntraining targets, I would not be sure which method to =\r\nchoose.  It is \npossible NEAT may do better even there because it is not co=\r\nnstrained \nto a single topological motif, but I am not certain.\n\nken\n\n--- I=\r\nn neat@yahoogroups.com, &quot;Rafael C.P.&quot; &lt;kurama.youko.br@...&gt; \nwrote:\n&gt;\n&gt; The=\r\nre are variations with recurrence and other things, they&#39;re on \nthe first\n&gt;=\r\n link I&#39;ve posted. But at a first glance, I agree that NEAT is more \npowerf=\r\null\n&gt; (by its generality).\n&gt; \n&gt; Rafael C.P.\n&gt; \n&gt; 2006/12/11, Derek James &lt;d=\r\njames@...&gt;:\n&gt; &gt;\n&gt; &gt;   Yeah, I read about Cascade Correlation for the first =\r\ntime in a \nclass\n&gt; &gt; this semester.  It seems like a good early step in alg=\r\norithms \nfor exploring\n&gt; &gt; good topologies, but it&#39;s obviously far simpler =\r\nand more limited \nthan\n&gt; &gt; something like NEAT.  As far as I understand it,=\r\n you basically \nuse\n&gt; &gt; multi-layer perceptrons, train them with backprop, =\r\nand when the \nlearning\n&gt; &gt; stagnates you add in another hidden node...rinse=\r\n and repeat \nuntil you solve\n&gt; &gt; the problem or reach diminishing returns.\n=\r\n&gt; &gt;\n&gt; &gt; So, it doesn&#39;t handle recurrency, and it only explores a small \nran=\r\nge of\n&gt; &gt; possible topologies.  I haven&#39;t played around with it myself.  \nS=\r\neems better\n&gt; &gt; than your standard MLP using trial and error to figure out =\r\nhow \nmany hidden\n&gt; &gt; nodes you need, but it also seems far less powerful th=\r\nan NEAT.\n&gt; &gt;\n&gt; &gt; --Derek\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; On 12/11/06, Rafael C.P. &lt;kurama.youko=\r\n.br@...&gt; wrote:\n&gt; &gt; &gt;\n&gt; &gt; &gt;   What do you thing about the pros and cons of =\r\nNEAT and CC \n(Cascade\n&gt; &gt; &gt; Correlation)? Why use one or another?\n&gt; &gt; &gt;\n&gt; &gt;=\r\n &gt; Links about CC:\n&gt; &gt; &gt; \nhttp://www.lix.polytechnique.fr/~liberti/public/c=\r\nomputing/neural/snns\n/UserManual/node164.html\n&gt; &gt; &gt;\n&gt; &gt; &gt; &lt;http://www.lix.p=\r\nolytechnique.fr/%\n7Eliberti/public/computing/neural/snns/UserManual/node164=\r\n.html&gt;\n&gt; &gt; &gt; http://en.wikipedia.org/wiki/Cascade_correlation_algorithm\n&gt; &gt;=\r\n &gt;\n&gt; &gt; &gt; --\n&gt; &gt; &gt; =3D=3D=3D=3D=3D=3D=3D=3D=3D\n&gt; &gt; &gt; Rafael C.P.\n&gt; &gt; &gt; =3D=\r\n=3D=3D=3D=3D=3D=3D=3D=3D\n&gt; &gt; &gt;\n&gt; &gt;\n&gt; &gt;  \n&gt; &gt;\n&gt; \n&gt; \n&gt; \n&gt; -- \n&gt; =3D=3D=3D=3D=\r\n=3D=3D=3D=3D=3D\n&gt; Rafael C.P.\n&gt; =3D=3D=3D=3D=3D=3D=3D=3D=3D\n&gt;\n\n\n\n"}}