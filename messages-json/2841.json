{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":119941855,"authorName":"cpchristenson","from":"&quot;cpchristenson&quot; &lt;cpchristenson@...&gt;","profile":"cpchristenson","replyTo":"LIST","senderId":"8rvXUz1pNoebQEUb8URAzvInzQ7z_Nwj6hs-1qn30sJTSRDJChLwcjGKengTvnJi4o6EDltmHzfwN6bmvVYZdHpiI8BN5sPAbS7LcPA8yek","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Some questions from beginner in NEAT","postDate":"1164121430","msgId":2841,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGVqdjRnbSs3N2pkQGVHcm91cHMuY29tPg==","inReplyToHeader":"PEMxODc1NjFELjE1RTIzJWpjbHVuZUBtc3UuZWR1Pg=="},"prevInTopic":2838,"nextInTopic":2843,"prevInTime":2838,"nextInTime":2843,"topicId":2819,"numMessagesInTopic":9,"msgSnippet":"It was. Incremental Evolution of Trainable Neural Networks that are Backwards Compatible Fifth  IASTED International Conference on Artificial Intelligence","rawEmail":"Return-Path: &lt;cpchristenson@...&gt;\r\nX-Sender: cpchristenson@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 99221 invoked from network); 21 Nov 2006 15:05:52 -0000\r\nReceived: from unknown (66.218.66.218)\n  by m21.grp.scd.yahoo.com with QMQP; 21 Nov 2006 15:05:52 -0000\r\nReceived: from unknown (HELO n15c.bullet.sp1.yahoo.com) (69.147.64.120)\n  by mta3.grp.scd.yahoo.com with SMTP; 21 Nov 2006 15:05:50 -0000\r\nReceived: from [216.252.122.217] by n15.bullet.sp1.yahoo.com with NNFMP; 21 Nov 2006 15:03:52 -0000\r\nReceived: from [66.218.69.5] by t2.bullet.sp1.yahoo.com with NNFMP; 21 Nov 2006 15:03:52 -0000\r\nReceived: from [66.218.66.81] by t5.bullet.scd.yahoo.com with NNFMP; 21 Nov 2006 15:03:50 -0000\r\nDate: Tue, 21 Nov 2006 15:03:50 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;ejv4gm+77jd@...&gt;\r\nIn-Reply-To: &lt;C187561D.15E23%jclune@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0\r\nFrom: &quot;cpchristenson&quot; &lt;cpchristenson@...&gt;\r\nSubject: Re: Some questions from beginner in NEAT\r\nX-Yahoo-Group-Post: member; u=119941855; y=Wuh4YfaP-t30PdKytE1EptxF3so4z5nZncujgWNlwk_m2zdRiWjhzg\r\nX-Yahoo-Profile: cpchristenson\r\n\r\nIt was.\n\n&quot;Incremental Evolution of Trainable Neural Networks that are \nBack=\r\nwards Compatible&quot; Fifth  IASTED International Conference on \nArtificial Int=\r\nelligence and Applications (AIA 2006), February 2006\n\nI have added the publ=\r\nished document to the files section of the \ngroup.  It is a condensed versi=\r\non, only 6 pages.\n\n--- In neat@yahoogroups.com, Jeff Clune &lt;jclune@...&gt; wro=\r\nte:\n&gt;\n&gt; Cool stuff. Was this work published? I would like to read and cite =\r\n\nit.\n&gt; \n&gt; \n&gt; \n&gt; \n&gt; Cheers,\n&gt; Jeff Clune\n&gt; \n&gt; Digital Evolution Lab\n&gt; jclune=\r\n@...\n&gt; 517.214.1060 (cell)\n&gt; \n&gt; \n&gt; &gt; From: cpchristenson &lt;cpchristenson@...=\r\n&gt;\n&gt; &gt; Reply-To: &quot;neat@yahoogroups.com&quot; &lt;neat@yahoogroups.com&gt;\n&gt; &gt; Date: Mon=\r\n, 20 Nov 2006 16:54:31 -0000\n&gt; &gt; To: &quot;neat@yahoogroups.com&quot; &lt;neat@yahoogrou=\r\nps.com&gt;\n&gt; &gt; Subject: [neat] Re: Some questions from beginner in NEAT\n&gt; &gt; \n&gt;=\r\n &gt; I still read, just never comment.  I did have success combining\n&gt; &gt; evol=\r\nution and learning.  However, unlike what you described, my\n&gt; &gt; thesis incl=\r\nuded the learning within the life of the network \nrather\n&gt; &gt; than after the=\r\n evolution is complete.  So, evolution would \ncreate a\n&gt; &gt; network, then th=\r\nat network would learn to perform a task, then it\n&gt; &gt; was evaluated based o=\r\nn its ability to learn the task.  The \nlearned\n&gt; &gt; weights were NOT passed =\r\nonto the next generation.  The idea was \nto\n&gt; &gt; evolve a network that was b=\r\netter at learning.  I was able to show\n&gt; &gt; this was possible using the task=\r\n of learning to solve \npolynomials.\n&gt; &gt; A very simple example, but function=\r\nal.\n&gt; &gt; \n&gt; &gt; --- In neat@yahoogroups.com, &quot;Kenneth Stanley&quot; &lt;kstanley@&gt; wro=\r\nte:\n&gt; &gt;&gt; \n&gt; &gt;&gt; I know people have combined NEAT with BP in the past.  For\n&gt;=\r\n &gt; example, \n&gt; &gt;&gt; Chris Christenson had some success with it, but I&#39;m not s=\r\nure \nhe&#39;s\n&gt; &gt;&gt; still reading this group.\n&gt; &gt;&gt; \n&gt; &gt;&gt; In any case, the number=\r\n of generations (and hence the length of\n&gt; &gt;&gt; time) it takes to solve a pro=\r\nblem depends on problem difficulty,\n&gt; &gt; so \n&gt; &gt;&gt; it&#39;s difficult to give an =\r\naccurate estimate, but 7 hours is\n&gt; &gt; probably \n&gt; &gt;&gt; a reasonable expectati=\r\non for the problem you cite.\n&gt; &gt;&gt; \n&gt; &gt;&gt; In fact, supervised classification =\r\nproblems tend to be faster \nthan\n&gt; &gt;&gt; control problems since they don&#39;t inv=\r\nolve a domain simulator, so\n&gt; &gt; you \n&gt; &gt;&gt; can expect it to be possibly a lo=\r\nt faster.  However, if you use \nBP\n&gt; &gt;&gt; you have to factor in the time that=\r\n adds to each evaluation,\n&gt; &gt;&gt; depending how you fold it into your procedur=\r\ne.\n&gt; &gt;&gt; \n&gt; &gt;&gt; I&#39;m also guessing you will not need two hidden layers of 8 \nh=\r\nidden\n&gt; &gt;&gt; nodes each to solve your problem.  NEAT usually discovers that \n=\r\nit\n&gt; &gt;&gt; takes a lot less than you expect to solve problems that seem \nlike\n=\r\n&gt; &gt;&gt; they should be difficult.\n&gt; &gt;&gt; \n&gt; &gt;&gt; Finally, NEAT has been applied to=\r\n real world industrial problems\n&gt; &gt; in \n&gt; &gt;&gt; simulation, but I&#39;m not sure i=\r\nf it&#39;s been literally used in e.g \na\n&gt; &gt;&gt; real factory or something like th=\r\nat.  But a 300 instance\n&gt; &gt;&gt; classification sounds fairly straightforward.\n=\r\n&gt; &gt;&gt; \n&gt; &gt;&gt; ken \n&gt; &gt;&gt; \n&gt; &gt;&gt; --- In neat@yahoogroups.com, &quot;aimike002&quot; &lt;aimike=\r\n002@&gt; wrote:\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt; Sorry , just to clarify for (3) if we took an exa=\r\nmple with\n&gt; &gt;&gt;&gt; say 300 training samples, with say 100 samples in the test =\r\nset.\n&gt; &gt;&gt;&gt; Just to try and get an idea.\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt; Mike\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt; --- =\r\nIn neat@yahoogroups.com, &quot;aimike002&quot; &lt;aimike002@&gt; wrote:\n&gt; &gt;&gt;&gt;&gt; \n&gt; &gt;&gt;&gt;&gt; Hi =\r\nI&#39;m taking a look for possibly using in for my Masters\n&gt; &gt;&gt; project,\n&gt; &gt;&gt;&gt;&gt;=\r\n which  needs evolving the architecture for a neural net for a\n&gt; &gt;&gt; real\n&gt; =\r\n&gt;&gt;&gt;&gt; world application.\n&gt; &gt;&gt;&gt;&gt; I have read some of Ken Stanley&#39;s papers on =\r\nthe web but beyond\n&gt; &gt;&gt; that\n&gt; &gt;&gt;&gt;&gt; I&#39;m a beginner, so if I may ask some be=\r\nginner&#39;s questions:\n&gt; &gt;&gt;&gt;&gt; 1) I think my neural net may be evolved using GA=\r\ns , but fine\n&gt; &gt;&gt; tuned to\n&gt; &gt;&gt;&gt;&gt; optimise using standard BP (ie hybrid app=\r\nroach). Has anybody\n&gt; &gt;&gt; else been\n&gt; &gt;&gt;&gt;&gt; down that path using NEAT, taking=\r\n e.g. the weights evolved\n&gt; &gt; using \n&gt; &gt;&gt; NEAT\n&gt; &gt;&gt;&gt;&gt; & feeding them into s=\r\nome standard BP software?\n&gt; &gt;&gt;&gt;&gt; 2) Has NEAT been used in any &#39;real world&#39;/=\r\nindustrial\n&gt; &gt;&gt; applications yet?\n&gt; &gt;&gt;&gt;&gt; \n&gt; &gt;&gt;&gt;&gt; and a &#39;how long is a piece=\r\n of string question&#39;...\n&gt; &gt;&gt;&gt;&gt; \n&gt; &gt;&gt;&gt;&gt; 3)  As yet  have no feel for how lon=\r\ng the evolution might\n&gt; &gt; take. \n&gt; &gt;&gt; I&#39;ve\n&gt; &gt;&gt;&gt;&gt; ran a few of the download=\r\ned sample executables they seem to\n&gt; &gt; take \n&gt; &gt;&gt; quite\n&gt; &gt;&gt;&gt;&gt; a while to r=\r\nun. My neural net has 8 inputs, one output and if\n&gt; &gt; we \n&gt; &gt;&gt; guess\n&gt; &gt;&gt;&gt;&gt;=\r\n at a final neural net of 2 hidden layers with 8 nodes per\n&gt; &gt; layer. \n&gt; &gt;&gt;=\r\n If we\n&gt; &gt;&gt;&gt;&gt; used say a 2GHz PC doing nothing else - could we guess at 7\n&gt;=\r\n &gt; hours\n&gt; &gt;&gt;&gt;&gt; (i.e. an overnight run) to evolve or is it more likely to b=\r\ne\n&gt; &gt;&gt; several\n&gt; &gt;&gt;&gt;&gt; days? Would appreciate any best guesses or experience=\r\n on this.\n&gt; &gt;&gt;&gt;&gt; \n&gt; &gt;&gt;&gt;&gt; Thanks in advance\n&gt; &gt;&gt;&gt;&gt; \n&gt; &gt;&gt;&gt;&gt; Mike\n&gt; &gt;&gt;&gt;&gt; \n&gt; &gt;&gt;=\r\n&gt; \n&gt; &gt;&gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt;\n&gt;\n\n\n\n\n"}}