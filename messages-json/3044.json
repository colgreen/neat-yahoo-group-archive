{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":283334584,"authorName":"petar_chervenski","from":"&quot;petar_chervenski&quot; &lt;petar_chervenski@...&gt;","profile":"petar_chervenski","replyTo":"LIST","senderId":"KtmvY2YN2lvlQBY-bm9TaVdInJuq__IyTg9hlZMDw6z_Lw8pB3ife3NHtOk5O7uQVEW5RKLsipFFyH4sCdZ9AjYqoo-TzZnD_OROsWuIAOS0tl2w1Iw","spamInfo":{"isSpam":false,"reason":"6"},"subject":"RE : [neat] Re: HyperNEAT: Creating Neural Networks with CPPNs","postDate":"1175004576","msgId":3044,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGV1YjhqMCtnMmJmQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDY5NjYzMy42NzA0NS5xbUB3ZWI1NzAwMy5tYWlsLnJlMy55YWhvby5jb20+"},"prevInTopic":3043,"nextInTopic":3045,"prevInTime":3043,"nextInTime":3045,"topicId":3028,"numMessagesInTopic":34,"msgSnippet":"... NEAT has shown that it s better to search in a minimal space and then expand that space to find the right complexity for the task. What we have here is a","rawEmail":"Return-Path: &lt;petar_chervenski@...&gt;\r\nX-Sender: petar_chervenski@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 48562 invoked from network); 27 Mar 2007 14:14:41 -0000\r\nReceived: from unknown (66.218.67.34)\n  by m31.grp.scd.yahoo.com with QMQP; 27 Mar 2007 14:14:41 -0000\r\nReceived: from unknown (HELO n15c.bullet.sp1.yahoo.com) (69.147.64.120)\n  by mta8.grp.scd.yahoo.com with SMTP; 27 Mar 2007 14:14:41 -0000\r\nReceived: from [216.252.122.217] by n15.bullet.sp1.yahoo.com with NNFMP; 27 Mar 2007 14:09:40 -0000\r\nReceived: from [209.73.164.83] by t2.bullet.sp1.yahoo.com with NNFMP; 27 Mar 2007 14:09:40 -0000\r\nReceived: from [66.218.66.83] by t7.bullet.scd.yahoo.com with NNFMP; 27 Mar 2007 14:09:40 -0000\r\nDate: Tue, 27 Mar 2007 14:09:36 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;eub8j0+g2bf@...&gt;\r\nIn-Reply-To: &lt;696633.67045.qm@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0\r\nFrom: &quot;petar_chervenski&quot; &lt;petar_chervenski@...&gt;\r\nSubject: RE : [neat] Re: HyperNEAT: Creating Neural Networks with CPPNs\r\nX-Yahoo-Group-Post: member; u=283334584; y=9uFOqrlWzi6E2LjG0zsOeVvgA35JYYH5XSiNvj15xw1AfkLMJtNW9MGoZw\r\nX-Yahoo-Profile: petar_chervenski\r\n\r\n&gt; Searching a network that reach a given fitness and\n&gt; searching the minima=\r\nl neural network that reach the\n&gt; same fitness are two differents problems.=\r\n\n\nNEAT has shown that it&#39;s better to search in a minimal space and then \nex=\r\npand that space to find the right complexity for the task.\nWhat we have her=\r\ne is a fixed space - the substrate. We search in this \nspace only.. Even if=\r\n not all connections are expressed, that space is \nconstrained - it has a l=\r\nimit. So here comes the need for a &quot;feel&quot; of \nhow many nodes are necesarry =\r\nfor a problem. \nThe search of HyperNEAT is actually just like the Genetic A=\r\nrt stuff.. \nIt can raise complexity, but the pictures are still the same X/=\r\nY \nsize..\nWhat if HyperNEAT has the ability to scale the substrate and add =\r\n\nneurons by itself? Some kind of mutation maybe.. Just like in the \norigina=\r\nl NEAT, first start with extremely simple substrate, then \nscale it further=\r\n and add nodes to it.. The nodes are added in the \nsame space, so it should=\r\nn&#39;t be a problem.. I have to give this some \nmore thought.. \n\nPeter\n\n--- In=\r\n neat@yahoogroups.com, Alexandre Devert &lt;marmakoide@...&gt; wrote:\n&gt;\n&gt; \n&gt; --- =\r\npetar_chervenski &lt;petar_chervenski@...&gt; a\n&gt; =E9crit :\n&gt; \n&gt; &gt; Hi, thanks for=\r\n the quick response, \n&gt; &gt; But I know that the CPPN is used like a &quot;genome&quot; =\r\nfor\n&gt; &gt; the big network, \n&gt; &gt; and the CPPNs evolve with the NEAT method. \n&gt;=\r\n Sorry to break down open doors, that&#39;s an old habit :D\n&gt; \n&gt; &gt; I was\n&gt; &gt; ju=\r\nst wondering why \n&gt; &gt; so big phenotypes at first? Does it really matter? \n&gt;=\r\n &gt; I know that the  fitness function tells me which \n&gt; &gt; genome is better, =\r\nbut consider some \n&gt; &gt; simple task.. like the &quot;go to the food&quot; one.. \n&gt; &gt; C=\r\nan this method be applied succesfuly where direct\n&gt; &gt; encodings like the \n&gt;=\r\n &gt; original NEAT perform best? \n&gt; &gt;\n&gt; Yes, for a one bit XOR or food retrie=\r\nval for a two\n&gt; sensors robots, it&#39;s a bit ovesized. But let&#39;s for a\n&gt; 256 =\r\nbits XOR, it&#39;s mandatory. It would interesting,\n&gt; for the XOR problem, when=\r\n HyperNEAT beats NEAT in time\n&gt; to reach a given fitness ceil. I have other=\r\n problems\n&gt; in mind where the solution is a big neural network.\n&gt; Then, tha=\r\nt&#39;s only a personal intuition, it&#39;s maybe\n&gt; much easier to find a big, regu=\r\nlar neural network for\n&gt; a given task, rather than find the minimal one.\n&gt; =\r\nSearching a network that reach a given fitness and\n&gt; searching the minimal =\r\nneural network that reach the\n&gt; same fitness are two differents problems.\n&gt;=\r\n \n&gt; \n&gt; &gt; Can HyperNEAT be extended so it can alter the\n&gt; &gt; substrate (like =\r\nadding \n&gt; &gt; more neurons), just like the original NEAT alters\n&gt; &gt; the searc=\r\nh space by \n&gt; &gt; adding more nodes and connections?\n&gt; &gt; \n&gt; Developmental app=\r\nroaches, here we come :D\n&gt; \n&gt; &gt; Peter.\n&gt; &gt; \n&gt; &gt; --- In neat@yahoogroups.com=\r\n, Alexandre Devert\n&gt; &gt; &lt;marmakoide@&gt; wrote:\n&gt; &gt; &gt;\n&gt; &gt; &gt; Hi\n&gt; &gt; &gt;  Here the =\r\nphenotype is the million connections\n&gt; &gt; neural\n&gt; &gt; &gt; network, but the geno=\r\ntype is a much more simple,\n&gt; &gt; &gt; initialy minimal CPNN. Additive mutations=\r\n acts on\n&gt; &gt; the\n&gt; &gt; &gt; CPNN. So there is any violations of the NEAT\n&gt; &gt; &gt; p=\r\nhilosophy : minimal genotype with well conceived\n&gt; &gt; &gt; mutations. Of course=\r\n, small modifications of the\n&gt; &gt; CPNN\n&gt; &gt; &gt; lead to huge modifications in t=\r\nhe target\n&gt; &gt; phenotype, as\n&gt; &gt; &gt; in all compact neural networks encodings =\r\nlike\n&gt; &gt; cellular\n&gt; &gt; &gt; encoding. The HyperNEAT seems to be a new step in\n&gt;=\r\n &gt; this\n&gt; &gt; &gt; familly of encodings :D\n&gt; &gt; &gt; \n&gt; &gt; &gt; The hidden nodes of the =\r\ngenotype (the CPNN) are\n&gt; &gt; added\n&gt; &gt; &gt; by NEAT as usual. The hidden nodes =\r\nof the\n&gt; &gt; phenotype\n&gt; &gt; &gt; (the jumbo sized neural network) are never added=\r\n,\n&gt; &gt; it&#39;s\n&gt; &gt; &gt; up to the substrate conception to provide enought\n&gt; &gt; &gt; ne=\r\nurons. The HyperNEAT allows to put thousands of\n&gt; &gt; &gt; neurons, so we can be=\r\n generous with the substrate.\n&gt; &gt; &gt; \n&gt; &gt; &gt; --- petar_chervenski &lt;petar_cher=\r\nvenski@&gt; a\n&gt; &gt; &gt; =E9crit :\n&gt; &gt; &gt; \n&gt; &gt; &gt; &gt; Hi, \n&gt; &gt; &gt; &gt; a have a quiestion a=\r\nbout this.. The evolution in\n&gt; &gt; &gt; &gt; this indirect method \n&gt; &gt; &gt; &gt; evolves =\r\nthe geometry of the connections, right?\n&gt; &gt; But\n&gt; &gt; &gt; &gt; even in the first \n=\r\n&gt; &gt; &gt; &gt; population there may be phenotypes with millions\n&gt; &gt; of\n&gt; &gt; &gt; &gt; con=\r\nnections. Does \n&gt; &gt; &gt; &gt; this violate the third principle of NEAT, that\n&gt; &gt; =\r\n&gt; &gt; initial genomes must \n&gt; &gt; &gt; &gt; be as small as possible? \n&gt; &gt; &gt; &gt; What ab=\r\nout hidden nodes? In this approach, a set\n&gt; &gt; of\n&gt; &gt; &gt; &gt; nodes has to be \n&gt;=\r\n &gt; &gt; &gt; chosen and the evolution then evolves only the\n&gt; &gt; &gt; &gt; connections b=\r\netween \n&gt; &gt; &gt; &gt; them. There are no additive mutations for nodes\n&gt; &gt; and\n&gt; &gt;=\r\n &gt; &gt; stuff. \n&gt; &gt; &gt; &gt; I know that the inner workings of this system is\n&gt; &gt; &gt;=\r\n &gt; NEAT, starting \n&gt; &gt; &gt; &gt; minimally and so on, but when we look at the\n&gt; &gt;=\r\n actual\n&gt; &gt; &gt; &gt; phenotypes, when \n&gt; &gt; &gt; &gt; we see how they evolve, it doesn&#39;=\r\nt look like\n&gt; &gt; NEAT\n&gt; &gt; &gt; &gt; starting from a \n&gt; &gt; &gt; &gt; minimal point and slo=\r\nwly adding structure.. A\n&gt; &gt; single\n&gt; &gt; &gt; &gt; small mutation \n&gt; &gt; &gt; &gt; in the =\r\nCPPN genome changes the entire structure\n&gt; &gt; of\n&gt; &gt; &gt; &gt; the phenotype. Is \n=\r\n&gt; &gt; &gt; &gt; this right?\n&gt; &gt; &gt; &gt; Or the comlexification here is about moving to\n=\r\n&gt; &gt; &gt; &gt; higher resolutions of \n&gt; &gt; &gt; &gt; the networks? \n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; Pete=\r\nr\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; --- In neat@yahoogroups.com, &quot;Kenneth Stanley&quot;\n&gt; &gt; &gt; &gt; &lt;=\r\nkstanley@&gt; wrote:\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; Hi everyone, as some of you know, we =\r\nhave been\n&gt; &gt; &gt; &gt; working for a while \n&gt; &gt; &gt; &gt; &gt; now on turning the pattern=\r\ns output by CPPNs\n&gt; &gt; into\n&gt; &gt; &gt; &gt; neural networks.  \n&gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; =\r\nI am excited to report that we have found a \n&gt; &gt; &gt; &gt; &gt; straightforward and =\r\nprincipled approach to\n&gt; &gt; &gt; &gt; interpreting CPPN output \n&gt; &gt; &gt; &gt; &gt; as a con=\r\nnectivity pattern rather than a\n&gt; &gt; spatial\n&gt; &gt; &gt; &gt; pattern.    We are \n&gt; &gt;=\r\n &gt; &gt; &gt; calling this method HyperNEAT, which stands\n&gt; &gt; for\n&gt; &gt; &gt; &gt; Hypercub=\r\ne-based \n&gt; &gt; &gt; &gt; &gt; NEAT.  With HyperNEAT, we have been able to\n&gt; &gt; &gt; &gt; prod=\r\nuce working neural \n&gt; &gt; &gt; &gt; &gt; networks with millions of connections.\n&gt; &gt; &gt; =\r\n&gt; &gt; \n&gt; &gt; &gt; &gt; &gt; Because we have just recently had two\n&gt; &gt; publications\n&gt; &gt; &gt;=\r\n &gt; on this method \n&gt; &gt; &gt; &gt; &gt; accepted at GECCO-2007, we can finally\n&gt; &gt; dis=\r\nclose\n&gt; &gt; &gt; &gt; our work and share \n&gt; &gt; &gt; &gt; &gt; it with others.  The publicatio=\r\nns are\n&gt; &gt; available on\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; the &quot;Publications&quot; page at\n&gt; &gt; h=\r\nttp://eplex.cs.ucf.edu\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; They are also availabl=\r\ne through my homepage \n&gt; &gt; &gt; &gt; &gt; http://www.cs.ucf.edu/~kstanley/#publicati=\r\nons\n&gt; &gt; &gt; &gt; (under Publications, \n&gt; &gt; &gt; &gt; &gt; Conference and Symposium Papers=\r\n), or directly\n&gt; &gt; at\n&gt; &gt; &gt; &gt; the following \n&gt; &gt; &gt; &gt; &gt; address, which I am =\r\nsure yahoo is going to\n&gt; &gt; mangle:\n&gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; http://eplex.cs.ucf=\r\n.edu/index.php?\n&gt; &gt; &gt; &gt; &gt; option=3Dcom_content&task=3Dview&id=3D14&Itemid=\r\n=3D28\n&gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; The two papers, which present separate\n&gt; &gt; exper=\r\niments\n&gt; &gt; &gt; &gt; are:\n&gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &quot;Generating Large-Scale Neural Net=\r\nworks\n&gt; &gt; Through\n&gt; &gt; &gt; &gt; Discovering \n&gt; &gt; &gt; &gt; &gt; Geometric Regularities&quot; by=\r\n Jason J. Gauci and\n&gt; &gt; &gt; &gt; Kenneth O. Stanley\n&gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; and \n&gt; =\r\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; &quot;A Novel Generative Encoding for Exploiting\n&gt; &gt; Neural\n&gt;=\r\n &gt; &gt; &gt; Network Sensor \n&gt; &gt; &gt; &gt; &gt; and Output Geometry&quot; by David B. D&#39;Ambrosi=\r\no\n&gt; &gt; and\n&gt; &gt; &gt; &gt; Kenneth O. Stanley\n&gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; David D&#39;Ambriosio=\r\n and Jason Gauci are both\n&gt; &gt; Ph.D.\n&gt; &gt; &gt; &gt; students in the \n&gt; &gt; &gt; &gt; &gt; EPle=\r\nx lab who did an enormous amount of work\n&gt; &gt; to\n&gt; &gt; &gt; &gt; make this possible.=\r\n\n&gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; HyperNEAT is based on a surprisingly simple\n&gt; &gt; &gt; &gt; p=\r\nrinciple: Connectivity \n&gt; &gt; &gt; &gt; &gt; patterns are actually no different from\n&gt;=\r\n &gt; spatial\n&gt; &gt; &gt; &gt; patterns in a \n&gt; &gt; &gt; &gt; higher-\n&gt; &gt; &gt; &gt; &gt; dimensional spa=\r\nce.  In other words, a\n&gt; &gt; &gt; &gt; 2-dimensional connectivity \n&gt; &gt; &gt; &gt; &gt; patter=\r\nn is the same thing as a 4-dimensional\n&gt; &gt; &gt; &gt; spatial pattern.  (The \n&gt; &gt; =\r\n&gt; &gt; &gt; technical term is that they are &quot;isomorphic.&quot;)\n&gt; &gt; \n&gt; &gt; &gt; &gt; Therefore=\r\n, if we \n&gt; &gt; &gt; &gt; &gt; simply produce spatial patterns within a 4D\n&gt; &gt; &gt; &gt; hype=\r\nrcube instead of a \n&gt; &gt; &gt; &gt; &gt; 2D plane, they can be mapped directly to\n&gt; &gt; =\r\n&gt; &gt; connectivity patterns with \n&gt; &gt; &gt; &gt; &gt; all the nice properties that we h=\r\nave seen from\n&gt; &gt; &gt; &gt; CPPNs.  Thus, they \n&gt; &gt; &gt; &gt; &gt; require no special &quot;ima=\r\nge recognition&quot; type\n&gt; &gt; &gt; &gt; tricks.  The mapping is \n&gt; &gt; &gt; &gt; &gt; perfectly i=\r\nsomorphic.  We are calling these 4D\n&gt; &gt; &gt; &gt; CPPNs &quot;connective \n&gt; &gt; &gt; &gt; &gt; CP=\r\nPNs&quot; since they are interpreted as\n&gt; &gt; connectivity\n&gt; &gt; &gt; &gt; patterns.\n&gt; &gt; &gt;=\r\n &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; This insight yields a handful of unprecedented\n&gt; &gt; &gt; &gt; capa=\r\nbilities.  For \n&gt; &gt; &gt; &gt; &gt; example, with connective CPPNs, you can\n&gt; &gt; recre=\r\nate\n&gt; &gt; &gt; &gt; the same solution \n&gt; &gt; &gt; &gt; &gt; network at a different resolution =\r\nwithout\n&gt; &gt; further\n&gt; &gt; &gt; &gt; evolution!  You \n&gt; &gt; &gt; &gt; &gt; can also place input=\r\ns and outputs in\n&gt; &gt; meaningful\n&gt; &gt; &gt; &gt; geometric \n&gt; &gt; &gt; &gt; &gt; configurations=\r\n that HyperNEAT can exploit for\n&gt; &gt; &gt; &gt; symmetries and \n&gt; &gt; &gt; &gt; &gt; regularit=\r\nies.  \n&gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; In any case, I am excited about this new\n&gt; &gt; ad=\r\nvance\n&gt; &gt; &gt; &gt; and these papers \n&gt; &gt; &gt; &gt; &gt; are a first step in exploring the=\r\n new\n&gt; &gt; approach.\n&gt; &gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &gt; ken\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; \n&gt; &gt;=\r\n &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; &gt; _______________________________\n&gt; &gt; &gt; Marmakoide =\r\naka Alexandre Devert \n&gt; &gt; &gt; _______________________________\n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt;=\r\n &gt; &gt; \t\n&gt; &gt; &gt; \n&gt; &gt; &gt; \t\n&gt; &gt; &gt; \t\t\n&gt; &gt; &gt; \n&gt; &gt;\n&gt; \n______________________________=\r\n________________________________________\n&gt; &gt; _____ \n&gt; &gt; &gt; D=E9couvrez une n=\r\nouvelle fa=E7on d&#39;obtenir des\n&gt; &gt; r=E9ponses =E0 toutes vos \n&gt; &gt; questions =\r\n! \n&gt; &gt; &gt; Profitez des connaissances, des opinions et des\n&gt; &gt; exp=E9riences =\r\ndes \n&gt; &gt; internautes sur Yahoo! Questions/R=E9ponses \n&gt; &gt; &gt; http://fr.answe=\r\nrs.yahoo.com\n&gt; &gt; &gt;\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; \n&gt; \n&gt; _______________________________\n&gt;=\r\n Marmakoide aka Alexandre Devert \n&gt; _______________________________\n&gt; \n&gt; \n&gt;=\r\n \t\n&gt; \n&gt; \t\n&gt; \t\t\n&gt; \n_________________________________________________________=\r\n_____________\n_____ \n&gt; D=E9couvrez une nouvelle fa=E7on d&#39;obtenir des r=E9p=\r\nonses =E0 toutes vos \nquestions ! \n&gt; Profitez des connaissances, des opinio=\r\nns et des exp=E9riences des \ninternautes sur Yahoo! Questions/R=E9ponses \n&gt;=\r\n http://fr.answers.yahoo.com\n&gt;\n\n\n\n"}}