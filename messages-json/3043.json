{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":235544548,"authorName":"Alexandre Devert","from":"Alexandre Devert &lt;marmakoide@...&gt;","profile":"marmakoide","replyTo":"LIST","senderId":"uPIrkZtKEw-W65txSj7bf8A5m2zqX2RhWfBvxwIdNEdoSOMTZIUZQxfSjipFdJWg_elGYCKx8_1nj4u7hwQYseJt3NNHeoMDIUD9OiM","spamInfo":{"isSpam":false,"reason":"0"},"subject":"RE : [neat] Re: HyperNEAT: Creating Neural Networks with CPPNs","postDate":"1175003379","msgId":3043,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDY5NjYzMy42NzA0NS5xbUB3ZWI1NzAwMy5tYWlsLnJlMy55YWhvby5jb20+","inReplyToHeader":"PGV1YjNiOCtvZjRpQGVHcm91cHMuY29tPg=="},"prevInTopic":3042,"nextInTopic":3044,"prevInTime":3042,"nextInTime":3044,"topicId":3028,"numMessagesInTopic":34,"msgSnippet":"... Sorry to break down open doors, that s an old habit :D ... Yes, for a one bit XOR or food retrieval for a two sensors robots, it s a bit ovesized. But","rawEmail":"Return-Path: &lt;marmakoide@...&gt;\r\nX-Sender: marmakoide@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 52224 invoked from network); 27 Mar 2007 13:49:44 -0000\r\nReceived: from unknown (66.218.66.68)\n  by m26.grp.scd.yahoo.com with QMQP; 27 Mar 2007 13:49:43 -0000\r\nReceived: from unknown (HELO web57003.mail.re3.yahoo.com) (66.196.97.107)\n  by mta11.grp.scd.yahoo.com with SMTP; 27 Mar 2007 13:49:43 -0000\r\nReceived: (qmail 67173 invoked by uid 60001); 27 Mar 2007 13:49:39 -0000\r\nX-YMail-OSG: L7Z7b4sVM1ngCi12pdwUPfX46v3rL5g.UaVwIOWa.HuGZLnlttDx0PwKWAPVHUwbGRDbhyfrF9hkHyzbGcEA1FMIEBED3npKktGKub33kaqYgdvnm2UeLmATu20chw--\r\nReceived: from [129.175.5.146] by web57003.mail.re3.yahoo.com via HTTP; Tue, 27 Mar 2007 15:49:39 CEST\r\nDate: Tue, 27 Mar 2007 15:49:39 +0200 (CEST)\r\nTo: neat@yahoogroups.com\r\nIn-Reply-To: &lt;eub3b8+of4i@...&gt;\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=iso-8859-1\r\nContent-Transfer-Encoding: 8bit\r\nMessage-ID: &lt;696633.67045.qm@...&gt;\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Alexandre Devert &lt;marmakoide@...&gt;\r\nSubject: RE : [neat] Re: HyperNEAT: Creating Neural Networks with CPPNs\r\nX-Yahoo-Group-Post: member; u=235544548; y=km7_EvsExoFH5Zqrd0ojrFGRNbHHx7koNoi_av1ShffsrMWI9w\r\nX-Yahoo-Profile: marmakoide\r\n\r\n\n--- petar_chervenski &lt;petar_chervenski@...&gt; a\n�crit :\n\n&gt; Hi, thanks for the quick response, \n&gt; But I know that the CPPN is used like a &quot;genome&quot; for\n&gt; the big network, \n&gt; and the CPPNs evolve with the NEAT method. \nSorry to break down open doors, that&#39;s an old habit :D\n\n&gt; I was\n&gt; just wondering why \n&gt; so big phenotypes at first? Does it really matter? \n&gt; I know that the  fitness function tells me which \n&gt; genome is better, but consider some \n&gt; simple task.. like the &quot;go to the food&quot; one.. \n&gt; Can this method be applied succesfuly where direct\n&gt; encodings like the \n&gt; original NEAT perform best? \n&gt;\nYes, for a one bit XOR or food retrieval for a two\nsensors robots, it&#39;s a bit ovesized. But let&#39;s for a\n256 bits XOR, it&#39;s mandatory. It would interesting,\nfor the XOR problem, when HyperNEAT beats NEAT in time\nto reach a given fitness ceil. I have other problems\nin mind where the solution is a big neural network.\nThen, that&#39;s only a personal intuition, it&#39;s maybe\nmuch easier to find a big, regular neural network for\na given task, rather than find the minimal one.\nSearching a network that reach a given fitness and\nsearching the minimal neural network that reach the\nsame fitness are two differents problems.\n\n\n&gt; Can HyperNEAT be extended so it can alter the\n&gt; substrate (like adding \n&gt; more neurons), just like the original NEAT alters\n&gt; the search space by \n&gt; adding more nodes and connections?\n&gt; \nDevelopmental approaches, here we come :D\n\n&gt; Peter.\n&gt; \n&gt; --- In neat@yahoogroups.com, Alexandre Devert\n&gt; &lt;marmakoide@...&gt; wrote:\n&gt; &gt;\n&gt; &gt; Hi\n&gt; &gt;  Here the phenotype is the million connections\n&gt; neural\n&gt; &gt; network, but the genotype is a much more simple,\n&gt; &gt; initialy minimal CPNN. Additive mutations acts on\n&gt; the\n&gt; &gt; CPNN. So there is any violations of the NEAT\n&gt; &gt; philosophy : minimal genotype with well conceived\n&gt; &gt; mutations. Of course, small modifications of the\n&gt; CPNN\n&gt; &gt; lead to huge modifications in the target\n&gt; phenotype, as\n&gt; &gt; in all compact neural networks encodings like\n&gt; cellular\n&gt; &gt; encoding. The HyperNEAT seems to be a new step in\n&gt; this\n&gt; &gt; familly of encodings :D\n&gt; &gt; \n&gt; &gt; The hidden nodes of the genotype (the CPNN) are\n&gt; added\n&gt; &gt; by NEAT as usual. The hidden nodes of the\n&gt; phenotype\n&gt; &gt; (the jumbo sized neural network) are never added,\n&gt; it&#39;s\n&gt; &gt; up to the substrate conception to provide enought\n&gt; &gt; neurons. The HyperNEAT allows to put thousands of\n&gt; &gt; neurons, so we can be generous with the substrate.\n&gt; &gt; \n&gt; &gt; --- petar_chervenski &lt;petar_chervenski@...&gt; a\n&gt; &gt; �crit :\n&gt; &gt; \n&gt; &gt; &gt; Hi, \n&gt; &gt; &gt; a have a quiestion about this.. The evolution in\n&gt; &gt; &gt; this indirect method \n&gt; &gt; &gt; evolves the geometry of the connections, right?\n&gt; But\n&gt; &gt; &gt; even in the first \n&gt; &gt; &gt; population there may be phenotypes with millions\n&gt; of\n&gt; &gt; &gt; connections. Does \n&gt; &gt; &gt; this violate the third principle of NEAT, that\n&gt; &gt; &gt; initial genomes must \n&gt; &gt; &gt; be as small as possible? \n&gt; &gt; &gt; What about hidden nodes? In this approach, a set\n&gt; of\n&gt; &gt; &gt; nodes has to be \n&gt; &gt; &gt; chosen and the evolution then evolves only the\n&gt; &gt; &gt; connections between \n&gt; &gt; &gt; them. There are no additive mutations for nodes\n&gt; and\n&gt; &gt; &gt; stuff. \n&gt; &gt; &gt; I know that the inner workings of this system is\n&gt; &gt; &gt; NEAT, starting \n&gt; &gt; &gt; minimally and so on, but when we look at the\n&gt; actual\n&gt; &gt; &gt; phenotypes, when \n&gt; &gt; &gt; we see how they evolve, it doesn&#39;t look like\n&gt; NEAT\n&gt; &gt; &gt; starting from a \n&gt; &gt; &gt; minimal point and slowly adding structure.. A\n&gt; single\n&gt; &gt; &gt; small mutation \n&gt; &gt; &gt; in the CPPN genome changes the entire structure\n&gt; of\n&gt; &gt; &gt; the phenotype. Is \n&gt; &gt; &gt; this right?\n&gt; &gt; &gt; Or the comlexification here is about moving to\n&gt; &gt; &gt; higher resolutions of \n&gt; &gt; &gt; the networks? \n&gt; &gt; &gt; \n&gt; &gt; &gt; Peter\n&gt; &gt; &gt; \n&gt; &gt; &gt; --- In neat@yahoogroups.com, &quot;Kenneth Stanley&quot;\n&gt; &gt; &gt; &lt;kstanley@&gt; wrote:\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; Hi everyone, as some of you know, we have been\n&gt; &gt; &gt; working for a while \n&gt; &gt; &gt; &gt; now on turning the patterns output by CPPNs\n&gt; into\n&gt; &gt; &gt; neural networks.  \n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; I am excited to report that we have found a \n&gt; &gt; &gt; &gt; straightforward and principled approach to\n&gt; &gt; &gt; interpreting CPPN output \n&gt; &gt; &gt; &gt; as a connectivity pattern rather than a\n&gt; spatial\n&gt; &gt; &gt; pattern.    We are \n&gt; &gt; &gt; &gt; calling this method HyperNEAT, which stands\n&gt; for\n&gt; &gt; &gt; Hypercube-based \n&gt; &gt; &gt; &gt; NEAT.  With HyperNEAT, we have been able to\n&gt; &gt; &gt; produce working neural \n&gt; &gt; &gt; &gt; networks with millions of connections.\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; Because we have just recently had two\n&gt; publications\n&gt; &gt; &gt; on this method \n&gt; &gt; &gt; &gt; accepted at GECCO-2007, we can finally\n&gt; disclose\n&gt; &gt; &gt; our work and share \n&gt; &gt; &gt; &gt; it with others.  The publications are\n&gt; available on\n&gt; &gt; &gt; \n&gt; &gt; &gt; &gt; the &quot;Publications&quot; page at\n&gt; http://eplex.cs.ucf.edu\n&gt; &gt; &gt; \n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; They are also available through my homepage \n&gt; &gt; &gt; &gt; http://www.cs.ucf.edu/~kstanley/#publications\n&gt; &gt; &gt; (under Publications, \n&gt; &gt; &gt; &gt; Conference and Symposium Papers), or directly\n&gt; at\n&gt; &gt; &gt; the following \n&gt; &gt; &gt; &gt; address, which I am sure yahoo is going to\n&gt; mangle:\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; http://eplex.cs.ucf.edu/index.php?\n&gt; &gt; &gt; &gt; option=com_content&task=view&id=14&Itemid=28\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; The two papers, which present separate\n&gt; experiments\n&gt; &gt; &gt; are:\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &quot;Generating Large-Scale Neural Networks\n&gt; Through\n&gt; &gt; &gt; Discovering \n&gt; &gt; &gt; &gt; Geometric Regularities&quot; by Jason J. Gauci and\n&gt; &gt; &gt; Kenneth O. Stanley\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; and \n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; &quot;A Novel Generative Encoding for Exploiting\n&gt; Neural\n&gt; &gt; &gt; Network Sensor \n&gt; &gt; &gt; &gt; and Output Geometry&quot; by David B. D&#39;Ambrosio\n&gt; and\n&gt; &gt; &gt; Kenneth O. Stanley\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; David D&#39;Ambriosio and Jason Gauci are both\n&gt; Ph.D.\n&gt; &gt; &gt; students in the \n&gt; &gt; &gt; &gt; EPlex lab who did an enormous amount of work\n&gt; to\n&gt; &gt; &gt; make this possible.\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; HyperNEAT is based on a surprisingly simple\n&gt; &gt; &gt; principle: Connectivity \n&gt; &gt; &gt; &gt; patterns are actually no different from\n&gt; spatial\n&gt; &gt; &gt; patterns in a \n&gt; &gt; &gt; higher-\n&gt; &gt; &gt; &gt; dimensional space.  In other words, a\n&gt; &gt; &gt; 2-dimensional connectivity \n&gt; &gt; &gt; &gt; pattern is the same thing as a 4-dimensional\n&gt; &gt; &gt; spatial pattern.  (The \n&gt; &gt; &gt; &gt; technical term is that they are &quot;isomorphic.&quot;)\n&gt; \n&gt; &gt; &gt; Therefore, if we \n&gt; &gt; &gt; &gt; simply produce spatial patterns within a 4D\n&gt; &gt; &gt; hypercube instead of a \n&gt; &gt; &gt; &gt; 2D plane, they can be mapped directly to\n&gt; &gt; &gt; connectivity patterns with \n&gt; &gt; &gt; &gt; all the nice properties that we have seen from\n&gt; &gt; &gt; CPPNs.  Thus, they \n&gt; &gt; &gt; &gt; require no special &quot;image recognition&quot; type\n&gt; &gt; &gt; tricks.  The mapping is \n&gt; &gt; &gt; &gt; perfectly isomorphic.  We are calling these 4D\n&gt; &gt; &gt; CPPNs &quot;connective \n&gt; &gt; &gt; &gt; CPPNs&quot; since they are interpreted as\n&gt; connectivity\n&gt; &gt; &gt; patterns.\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; This insight yields a handful of unprecedented\n&gt; &gt; &gt; capabilities.  For \n&gt; &gt; &gt; &gt; example, with connective CPPNs, you can\n&gt; recreate\n&gt; &gt; &gt; the same solution \n&gt; &gt; &gt; &gt; network at a different resolution without\n&gt; further\n&gt; &gt; &gt; evolution!  You \n&gt; &gt; &gt; &gt; can also place inputs and outputs in\n&gt; meaningful\n&gt; &gt; &gt; geometric \n&gt; &gt; &gt; &gt; configurations that HyperNEAT can exploit for\n&gt; &gt; &gt; symmetries and \n&gt; &gt; &gt; &gt; regularities.  \n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; In any case, I am excited about this new\n&gt; advance\n&gt; &gt; &gt; and these papers \n&gt; &gt; &gt; &gt; are a first step in exploring the new\n&gt; approach.\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; ken\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; _______________________________\n&gt; &gt; Marmakoide aka Alexandre Devert \n&gt; &gt; _______________________________\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \t\n&gt; &gt; \n&gt; &gt; \t\n&gt; &gt; \t\t\n&gt; &gt; \n&gt;\n______________________________________________________________________\n&gt; _____ \n&gt; &gt; D�couvrez une nouvelle fa�on d&#39;obtenir des\n&gt; r�ponses � toutes vos \n&gt; questions ! \n&gt; &gt; Profitez des connaissances, des opinions et des\n&gt; exp�riences des \n&gt; internautes sur Yahoo! Questions/R�ponses \n&gt; &gt; http://fr.answers.yahoo.com\n&gt; &gt;\n&gt; \n&gt; \n&gt; \n\n\n_______________________________\nMarmakoide aka Alexandre Devert \n_______________________________\n\n\n\t\n\n\t\n\t\t\n___________________________________________________________________________ \nD�couvrez une nouvelle fa�on d&#39;obtenir des r�ponses � toutes vos questions ! \nProfitez des connaissances, des opinions et des exp�riences des internautes sur Yahoo! Questions/R�ponses \nhttp://fr.answers.yahoo.com\n\n"}}