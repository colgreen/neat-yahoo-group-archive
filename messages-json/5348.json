{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":469058923,"authorName":"garrerc","from":"&quot;garrerc&quot; &lt;garrerc@...&gt;","profile":"garrerc","replyTo":"LIST","senderId":"OyVHToQGEGUZAOybUQVKqICLkULjf7ss3SPCA9oBI1wOqL1jj7oge0UFYMbEHywnkKCs4J5V1uiATO2T-KrroIyRdQw","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Serialization within HyperNEAT","postDate":"1288719861","msgId":5348,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGlhcGlsbCttN2IwQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGlhcGkzdCs5ZmMyQGVHcm91cHMuY29tPg=="},"prevInTopic":5347,"nextInTopic":5349,"prevInTime":5347,"nextInTime":5349,"topicId":5342,"numMessagesInTopic":8,"msgSnippet":"Actually, looking at the mpimain.cpp, I think that s given me some guidance on how to proceed.  Specifically, it looks like MPI is just used as the transport","rawEmail":"Return-Path: &lt;garrerc@...&gt;\r\nX-Sender: garrerc@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 42532 invoked from network); 2 Nov 2010 17:44:21 -0000\r\nX-Received: from unknown (98.137.34.45)\n  by m4.grp.sp2.yahoo.com with QMQP; 2 Nov 2010 17:44:21 -0000\r\nX-Received: from unknown (HELO n38b.bullet.mail.sp1.yahoo.com) (66.163.168.152)\n  by mta2.grp.sp2.yahoo.com with SMTP; 2 Nov 2010 17:44:21 -0000\r\nX-Received: from [69.147.65.174] by n38.bullet.mail.sp1.yahoo.com with NNFMP; 02 Nov 2010 17:44:21 -0000\r\nX-Received: from [98.137.34.35] by t12.bullet.mail.sp1.yahoo.com with NNFMP; 02 Nov 2010 17:44:21 -0000\r\nDate: Tue, 02 Nov 2010 17:44:21 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;iapill+m7b0@...&gt;\r\nIn-Reply-To: &lt;iapi3t+9fc2@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;garrerc&quot; &lt;garrerc@...&gt;\r\nSubject: Re: Serialization within HyperNEAT\r\nX-Yahoo-Group-Post: member; u=469058923; y=SuR0x6wXb_sWbp_ucIbyLD3Qq6LOXQJKlgHnA0LnZ6wMxQ\r\nX-Yahoo-Profile: garrerc\r\n\r\nActually, looking at the mpimain.cpp, I think that&#39;s given me some guidance=\r\n on how to proceed.  Specifically, it looks like MPI is just used as the tr=\r\nansport mechanism for a string or byte array.  The code in there just creat=\r\nes an istingstream from an incoming buffer and deserializes individual Gene=\r\nticIndividuals from that.  Thanks for the tips!\n\n\n--- In neat@yahoogroups.c=\r\nom, &quot;garrerc&quot; &lt;garrerc@...&gt; wrote:\n&gt;\n&gt; Doesn&#39;t it still require a parallel =\r\nprogram, though?\n&gt; \n&gt; \n&gt; --- In neat@yahoogroups.com, Wesley Tansey &lt;tansey=\r\n@&gt; wrote:\n&gt; &gt;\n&gt; &gt; Hi Chris,\n&gt; &gt; \n&gt; &gt; MPI is a distributed memory, distribut=\r\ned processor architecture. You \n&gt; &gt; pass messages between machines. Maybe y=\r\nou are thinking of OpenMP?\n&gt; &gt; \n&gt; &gt; However, MPI is still designed for clus=\r\nters. It doesn&#39;t handle partial \n&gt; &gt; failure, lag, or latency in a robust w=\r\nay.\n&gt; &gt; \n&gt; &gt; Wesley\n&gt; &gt; \n&gt; &gt; On 11/2/2010 1:13 PM, garrerc wrote:\n&gt; &gt; &gt;\n&gt; &gt;=\r\n &gt; Thanks for the pointer, but I thought that the whole point of MPI was \n&gt;=\r\n &gt; &gt; that it was a virtually shared memory model for a compute cluster. I \n=\r\n&gt; &gt; &gt; don&#39;t have access to a cluster, so I quickly ruled out MPI as a \n&gt; &gt; =\r\n&gt; mechanism for doing this. Maybe I need to read up a bit more on it.\n&gt; &gt; &gt;=\r\n\n&gt; &gt; &gt; Thanks again!\n&gt; &gt; &gt;\n&gt; &gt; &gt; -- Chris\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt;\n&gt;\n\n\n\n"}}