{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":29390171,"authorName":"Mattias Fagerlund","from":"&quot;Mattias Fagerlund&quot; &lt;mattias@...&gt;","profile":"mattias_fagerlund","replyTo":"LIST","senderId":"8tcgw3RTZsvlkaLdA4uzOGwTOaCGGXxHAfdtIwI-TZ5I5YRzT4-Hszo6227KGhp2mkE3yuxeC7ht3vNDIBLXmdiVCbLmq0TnBc69bGjkCsURRhN8hw","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] Neuron functions","postDate":"1099832944","msgId":1697,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PFdvcmxkQ2xpZW50LUYyMDA0MTEwNzE0MDkuQUEwOTA0MDAwOUBvY3RhZ2F0ZS5jb20+","inReplyToHeader":"PDYuMS4yLjAuMC4yMDA0MTEwMjExNTgzMC4wMjUxNDcwOEBwb3AubWFpbC55YWhvby5jby51az4=","referencesHeader":"PDQxODY0NDM3LjEwNTA2MDJAZHNsLnBpcGV4LmNvbT4gPDYuMS4yLjAuMC4yMDA0MTEwMjExNTgzMC4wMjUxNDcwOEBwb3AubWFpbC55YWhvby5jby51az4="},"prevInTopic":1695,"nextInTopic":1713,"prevInTime":1696,"nextInTime":1698,"topicId":1668,"numMessagesInTopic":20,"msgSnippet":"Did I mention that DelphiNEAT does many different operators (Sigmoid, Gaussian, Mul, Div, Add, Sub, etc) and has done so for several years? Just thought I d","rawEmail":"Return-Path: &lt;mattias@...&gt;\r\nX-Sender: mattias@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 84522 invoked from network); 7 Nov 2004 13:13:48 -0000\r\nReceived: from unknown (66.218.66.218)\n  by m2.grp.scd.yahoo.com with QMQP; 7 Nov 2004 13:13:48 -0000\r\nReceived: from unknown (HELO endeavour.mit.um.bostream.net) (81.26.227.9)\n  by mta3.grp.scd.yahoo.com with SMTP; 7 Nov 2004 13:13:47 -0000\r\nReceived: from bredin.nu (h208n6c1o285.bredband.skanova.com [81.227.101.208])\n\tby endeavour.mit.um.bostream.net (Postfix) with ESMTP id 6F4E768089\n\tfor &lt;neat@yahoogroups.com&gt;; Sun,  7 Nov 2004 14:13:37 +0100 (CET)\r\nReceived: from WorldClient by bredin.nu\n\t(MDaemon.PRO.v7.2.0.R)\n\twith ESMTP id md50000027077.msg\n\tfor &lt;neat@yahoogroups.com&gt;; Sun, 07 Nov 2004 14:09:06 +0100\r\nReceived: from [127.0.0.1] via WorldClient with HTTP;\n\tSun, 07 Nov 2004 14:09:04 +0100\r\nDate: Sun, 07 Nov 2004 14:09:04 +0100\r\nTo: neat@yahoogroups.com\r\nMIME-Version: 1.0\r\nContent-Type: text/plain\r\nMessage-ID: &lt;WorldClient-F200411071409.AA09040009@...&gt;\r\nX-Mailer: WorldClient 7.2.0\r\nIn-Reply-To: &lt;6.1.2.0.0.20041102115830.02514708@...&gt;\r\nReferences: &lt;41864437.1050602@...&gt; &lt;6.1.2.0.0.20041102115830.02514708@...&gt;\r\nX-Authenticated-Sender: mattias@...\r\nX-Spam-Processed: bredin.nu, Sun, 07 Nov 2004 14:09:06 +0100\n\t(not processed: message from valid local sender)\r\nX-MDRemoteIP: 127.0.0.1\r\nX-Return-Path: mattias@...\r\nX-MDaemon-Deliver-To: neat@yahoogroups.com\r\nX-MDAV-Processed: bredin.nu, Sun, 07 Nov 2004 14:09:08 +0100\r\nX-eGroups-Remote-IP: 81.26.227.9\r\nFrom: &quot;Mattias Fagerlund&quot; &lt;mattias@...&gt;\r\nSubject: Re: [neat] Neuron functions\r\nX-Yahoo-Group-Post: member; u=29390171\r\nX-Yahoo-Profile: mattias_fagerlund\r\n\r\nDid I mention that DelphiNEAT does many different operators (Sigmoid,\nGaussian, Mul, Div, Add, Sub, etc) and has done so for several years? Just\nthought I&#39;d mention it again.\n\ncheers,\nmattias\n\n-----Original Message-----\nFrom: Ian Badcoe &lt;ian_badcoe@...&gt;\nTo: neat@yahoogroups.com\nDate: Tue, 02 Nov 2004 14:09:20 +0000\nSubject: Re: [neat] Neuron functions\n\n&gt; \n&gt; Hi,\n&gt;          Whilst I&#39;m glad to see this idea getting explored.  I wonder \n&gt; whether you need to think through what the objective is.\n&gt; \n&gt;          e.g. what I mean is, when we have a statement like:\n&gt; \n&gt; &gt;Multiply (can also perform division by using input&lt;1.0)\n&gt; \n&gt;          (which I guess was originally me?) then I realized that the\n&gt; part \n&gt; in braces is only true if we also have a means to find the reciprocal\n&gt; of \n&gt; the input.  _BUT_ reciprocal is a very unnatural operator to include,\n&gt; e.g. \n&gt; because natural neurones have a fixed range of activation (call it 0 -\n&gt; 1) \n&gt; and (i) reciprocal would need to output infinity sometimes, and (ii)\n&gt; even \n&gt; if we regard it as more like an inverse, outputting 1 continually just \n&gt; because all your inputs are off doesn&#39;t sound plausible for a\n&gt; biological \n&gt; system (e.g. burning energy to do nothing).\n&gt; \n&gt;          So, you might want to think about whether your overall\n&gt; objective \n&gt; is &quot;nerve-like&quot; or &quot;maths-like&quot; and select your function set \n&gt; accordingly.  Personally I would go with nerve-like, because maths-like\n&gt; would be very like GP and we know GP is very fragile w.r.t mutation. \n&gt; Now \n&gt; once again I have to say I&#39;m not just backing natural approaches\n&gt; because \n&gt; they are natural.  I just come to this with an intuition and end up[\n&gt; using \n&gt; nature to explain it....\n&gt; \n&gt;          w.r.t your plan for collector functions and activation\n&gt; functions, \n&gt; I like it.  I&#39;m not sure I like so much adding a whole new layer just\n&gt; to \n&gt; support &quot;leaky integrators&quot; however.\n&gt; \n&gt;          Did you consider making it an homologous pool of functions. \n&gt; e.g. \n&gt; not distinguishing collectors from activators but allowing them to be \n&gt; plugged in any order.  That way if the system needs linearity through\n&gt; some \n&gt; sub-net, it does not need to select a whole load of &quot;linear&quot;\n&gt; activators, it \n&gt; just omits the activation functions altogether.  That would cover the\n&gt; leaky \n&gt; integrator as well...\n&gt; \n&gt;          Alternatively, you could try some sort of single neurone which\n&gt; could cover the whole (or a lot) of what you intend.  But I would only\n&gt; do \n&gt; that if you can come up with a single neurone design which forms a\n&gt; coherent \n&gt; whole and doesn&#39;t look like several unrelated ideas bolded together...\n&gt; \n&gt; How about:\n&gt;          one type of neurone with several &quot;input channels&quot;, allow\n&gt; multiple \n&gt; connections to each channel and sum them.\n&gt; \n&gt;          Sum_Channel - like in a standard ANN\n&gt;          Act_Channel - makes the neurone more activatable by tightening\n&gt; the \n&gt; sigmoid\n&gt;          Scale_Channel - multiplies the input\n&gt; \n&gt;          (scale and act are different in that when act is very low, the\n&gt; sigmoid becomes linear, but when scale is very low, the sigmoid just \n&gt; becomes very shallow)\n&gt; \n&gt;          Wiring Act and Scale just to the bias would be the same as\n&gt; having \n&gt; fixed, mutatable &quot;curve shape&quot; parameters on the node.  Wiring them to \n&gt; other inputs could give you potentiation/depotentiation (those are\n&gt; where X \n&gt; does not trigger Y but does increase the ability of Z to trigger Y).\n&gt; \n&gt;          Just rambling,\n&gt; \n&gt;                  Ian B\n&gt; \n&gt; p.s. OTOH, mean, median, min and max collectors feel &quot;right&quot; to me,\n&gt; &quot;mode&quot; \n&gt; is less intuitive...\n&gt; \n&gt; At 14:12 01/11/2004, you wrote:\n&gt; \n&gt; &gt;I want to start looking into using a range of different functions with\n&gt; &gt;neurons in the hope that this will expand NEAT&#39;s ability to find\n&gt; &gt;solutions to particular types of problems, e.g. multiplication and\n&gt; &gt;vector cross product.\n&gt; &gt;\n&gt; &gt;To begin with I want to just distinguish between &#39;input collector&#39;\n&gt; &gt;functions and activation functions. A collection function describes\n&gt; how\n&gt; &gt;the input signals on a neuron are combined, e.g. traditionally we add\n&gt; &gt;the inputs, but we could do other things such as multiply them. Input\n&gt; &gt;Collection Function seems like a good term to use, I wonder though if\n&gt; &gt;there is an existing term I should be using?\n&gt; &gt;\n&gt; &gt;Once the inputs have been combined we apply the activation function,\n&gt; and\n&gt; &gt;again there are all kinds of functions we could use here.\n&gt; &gt;\n&gt; &gt;The recent discussion on leaky integrator neurons got me thinking\n&gt; about\n&gt; &gt;where these would fit into this model. Although they could be\n&gt; described\n&gt; &gt;as an activation function I think it might be more useful to extend\n&gt; out\n&gt; &gt;neuron model to include a pre-activation function - and to apply the\n&gt; &gt;leaky-integrator funtion there.\n&gt; &gt;\n&gt; &gt;So the complete model of a neuron would be:\n&gt; &gt;\n&gt; &gt;       &#92;  |  /       [Inputs]\n&gt; &gt;         &#92;|/\n&gt; &gt;[Input collection function]\n&gt; &gt;          |\n&gt; &gt;          |\n&gt; &gt;[Pre-activation function]\n&gt; &gt;          |\n&gt; &gt;          |\n&gt; &gt;[Activation function]\n&gt; &gt;          |\n&gt; &gt;    output signal\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;So that now the shape of the activation function is completely\n&gt; &gt;independent of how we combine the inputs and whether we use a\n&gt; &gt;leaky-integrator function.\n&gt; &gt;\n&gt; &gt;Some obvious questions then are:\n&gt; &gt;\n&gt; &gt;1) What set of functions should we provide NEAT  with at each stage?\n&gt; &gt;[Collection Functions] - Some of these taken from an ealier post by\n&gt; Ian B.\n&gt; &gt;-----------------------\n&gt; &gt;Add\n&gt; &gt;Multiply (can also perform division by using input&lt;1.0)\n&gt; &gt;Logical AND\n&gt; &gt;Logical OR\n&gt; &gt;Variance\n&gt; &gt;Min\n&gt; &gt;Max\n&gt; &gt;Sum squares\n&gt; &gt;\n&gt; &gt;[Pre-activation functions]\n&gt; &gt;--------------------------\n&gt; &gt;Linear (do nothing).\n&gt; &gt;Leaky-integrator (probably with fixed parameters)\n&gt; &gt;\n&gt; &gt;[Activation functions]\n&gt; &gt;----------------------\n&gt; &gt;Linear\n&gt; &gt;Inverse Linear\n&gt; &gt;Sigmoid\n&gt; &gt;Step\n&gt; &gt;\n&gt; &gt;or instead of Inverse Linear, perhaps the inverse of each fn should be\n&gt; &gt;made available instead?\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;I&#39;d be interested in other people thoughts on this. Does the neuron\n&gt; &gt;model make sense? Are there other functions that should be in the\n&gt; above\n&gt; &gt;lists, or should I drop some of the functions?\n&gt; &gt;\n&gt; &gt;Keep in mind that you could (I think) ultimately drop all of the\n&gt; &gt;functions (except the sigmoid) and still be able to describe all of\n&gt; the\n&gt; &gt;functions using combinations of a standard neuron. The point is that I\n&gt; &gt;want to try and take some of the pressure of finding solutions away\n&gt; from\n&gt; &gt;NEAT&#39;s topology searching, because certain functions are hard to\n&gt; &gt;describe (and therefore discover) using standard neurons - a simple\n&gt; case\n&gt; &gt;being multiplication.\n&gt; &gt;\n&gt; &gt;Colin.\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;Yahoo! Groups Links\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; \n&gt; \n&gt; \n&gt; Living@Home - Open Source Evolving Organisms - \n&gt; http://livingathome.sourceforge.net/\n&gt; \n&gt; \n&gt; \n&gt; \n&gt; \n&gt; \n&gt;  \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; \n&gt;  \n&gt; \n&gt; \n&gt; \n\n\n\n"}}