{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":281645563,"authorName":"afcarl2","from":"&quot;afcarl2&quot; &lt;a.carl@...&gt;","profile":"afcarl2","replyTo":"LIST","senderId":"tH-z2-hD5U0SQudOiaSORqrTQiHl4MfHlkTF0Dpy2_eW2DOazeKREUhbdsf10eP7eB0MI79cHriLYc42S9WgUsw","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: A Few Thoughts on HyperNEAT","postDate":"1177903573","msgId":3217,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGYxM25rbCtoMm50QGVHcm91cHMuY29tPg==","inReplyToHeader":"PGYxM2ViOCtsbTB1QGVHcm91cHMuY29tPg=="},"prevInTopic":3216,"nextInTopic":3218,"prevInTime":3216,"nextInTime":3218,"topicId":3214,"numMessagesInTopic":27,"msgSnippet":"First, it is somewhat frustrating receiving responses which cherry- pick issues raised, doubly so when they appear to be tangents. There is no disparaging","rawEmail":"Return-Path: &lt;a.carl@...&gt;\r\nX-Sender: a.carl@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 66598 invoked from network); 30 Apr 2007 03:26:19 -0000\r\nReceived: from unknown (66.218.67.33)\n  by m51.grp.scd.yahoo.com with QMQP; 30 Apr 2007 03:26:19 -0000\r\nReceived: from unknown (HELO n4b.bullet.sp1.yahoo.com) (69.147.64.183)\n  by mta7.grp.scd.yahoo.com with SMTP; 30 Apr 2007 03:26:19 -0000\r\nReceived: from [216.252.122.216] by n4.bullet.sp1.yahoo.com with NNFMP; 30 Apr 2007 03:26:15 -0000\r\nReceived: from [209.73.164.83] by t1.bullet.sp1.yahoo.com with NNFMP; 30 Apr 2007 03:26:15 -0000\r\nReceived: from [66.218.66.87] by t7.bullet.scd.yahoo.com with NNFMP; 30 Apr 2007 03:26:15 -0000\r\nDate: Mon, 30 Apr 2007 03:26:13 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;f13nkl+h2nt@...&gt;\r\nIn-Reply-To: &lt;f13eb8+lm0u@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0\r\nFrom: &quot;afcarl2&quot; &lt;a.carl@...&gt;\r\nSubject: Re: A Few Thoughts on HyperNEAT\r\nX-Yahoo-Group-Post: member; u=281645563; y=pedTckE9ETAVoV0czmOCj-hFDch7hJ-xt1RuIWj6PHJPeQ\r\nX-Yahoo-Profile: afcarl2\r\n\r\nFirst, it is somewhat frustrating receiving responses which cherry-\npick is=\r\nsues raised, doubly so when they appear to be tangents. There \nis no dispar=\r\naging intent regarding whether or not hidden nodes are \npresently addressed=\r\n in the substrate network of hyperNEAT. The \nfixation on geometry dictating=\r\n substrate topology seems limiting, \nthough probably motivated by a legitim=\r\nate class of useful problems \nill-suited to conventional NEAT. &quot;Geometry&quot; i=\r\ns a subset of variables \ntaken together to describe a state, such as coordi=\r\nnates plus a time \nstamp can express a trajectory. There are any number of =\r\nadditional \ndescriptors and/or parameters, whether or not expressly represe=\r\nnted \nas inputs, which may be considered &quot;geometry&quot; in a higher order \ndesi=\r\ngn space.\n\nMy primary objective is to address how the employed hypercube \nf=\r\nunctional vocabulary effects the search space requirements of useful \nprobl=\r\nems looking for an answer. And the features required to address \nsaid funct=\r\nional vocabulary. There is a door that appears to be \nunopened, though it i=\r\ns shouting to be cracked. As I observe the \npublic work performed, it keeps=\r\n avoiding the issue, for reasons I can \nonly imagine are associated with ad=\r\nded complexity, that for multiple \nreasons, nodes need to employ multiple u=\r\nnique inputs for hidden and \noutput nodes, each of which maintain independe=\r\nnt summation of \nconnection inputs, and multiple unique outputs for sensor =\r\nand hidden \nnodes. \n\nThe fundamental properties of the NEAT approach, combi=\r\nned with the \nhypercube methodology of seeking higher or more global levels=\r\n of \nmathematical expression, assuming an adequately rich functional \nvocab=\r\nulary, is both applicable and extensible to a much larger set of \nuseful pr=\r\noblems looking for an answer than are currently being \nrepresented for Hype=\r\nrNEAT.\n\nThat&#39;s the point. Please respond to that point.\n\nThanks,\n   Andy Ca=\r\nrl\n\n\n\n--- In neat@yahoogroups.com, &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt; wrote:\n=\r\n&gt;\n&gt; Oh well, I was trying to avoid getting too technical about No Free \n&gt; L=\r\nunch because I figured that would be veering too far off the point \n&gt; that =\r\npeople were discussing.  But yes if you want to get into the \n&gt; details of =\r\ncourse there are a number of considerations.  That&#39;s why \nI \n&gt; said it &quot;may=\r\n&quot; be better for evolving very large scale brains.  \n&gt; The &quot;may&quot; hinges on t=\r\nhe issues you bring up and others.  However, \n&gt; even the opportunity to be =\r\nbetter is an advantage over having no \nsuch \n&gt; opportunity.  The opportunit=\r\ny of course can be squandered with the \n&gt; wrong a priori knowledge.   Yet p=\r\nart of my point is that it will \n&gt; often be the case that the natural geome=\r\ntry of a task is all you \nneed \n&gt; to provide a powerful bias (or at least a=\r\n bias that is better than \n&gt; nothing), so it will often be possible to seiz=\r\ne the opportunity \n&gt; without a great deal of effort.\n&gt; \n&gt; It is true too th=\r\nat NEAT and other neural network algorithms do \n&gt; indeed allow for some inc=\r\nlusion of prior knowledge through their \n&gt; input/output encoding.  However,=\r\n note how I phrased my \n&gt; claim: &quot;HyperNEAT is not subject to the No Free L=\r\nunch theorem when \n&gt; comparing to algorithms that do not allow injecting su=\r\nch a priori \n&gt; knowledge.&quot;  That is, among algorithms that allow you to dec=\r\nide on \nan \n&gt; input encoding in the traditional way, the provision of such =\r\n\nencoding \n&gt; does not give one algorithm a leg up over another since they a=\r\nll \n&gt; allow for such knowledge to be included.  HyperNEAT, on the other \n&gt; =\r\nhand, allows a new kind of knowledge (i.e. geometry) to be included \n&gt; and =\r\ntherefore does have a potential leg up on that class of \n&gt; algorithms.  Of =\r\ncourse it depends on how well the user takes \n&gt; advantage of the opportunit=\r\ny, but the opportunity is now there.  \nThis \n&gt; fact does indeed mean that s=\r\ntatements about HyperNEAT vs. other \n&gt; neuroevolution (or even machine lear=\r\nning) algorithms can cite an \n&gt; opportunity to genuinely be better on avera=\r\nge, which in effect \nbrings \n&gt; it outside NFL in one particular sense.  \n&gt; =\r\n\n&gt; In a way, this claim is nothing surprising, because it only says \nthat \n=\r\n&gt; the algorithm is biased to a certain type of problem, which is \n&gt; exactly=\r\n what Wolpert said algorithms need to be to have any real \n&gt; advantage.  Ho=\r\nwever, when you consider what type of problem we are \n&gt; talking about, it&#39;s=\r\n a pretty significant observation.\n&gt; \n&gt; ken\n&gt; \n&gt; \n&gt; --- In neat@yahoogroups=\r\n.com, Joseph Reisinger &lt;joeraii@&gt; wrote:\n&gt; &gt;\n&gt; &gt; &gt; While it may be viewed a=\r\ns a weakness that the user must decide \nthe\n&gt; &gt; &gt; node layout, my view is t=\r\nhat it is actually quite a bonus, \n&gt; because it\n&gt; &gt; &gt; means we have an oppo=\r\nrtuntiy to inject intuitive relationships \n&gt; into\n&gt; &gt; &gt; the learning proces=\r\ns from the get-go.  The most interesting\n&gt; &gt; &gt; consequence of this ability =\r\nis that HyperNEAT is not subject to \n&gt; the\n&gt; &gt; &gt; No Free Lunch theorem when=\r\n comparing to algorithms that do not \n&gt; allow\n&gt; &gt; &gt; injecting such a priori=\r\n knowledge, which justifies the \nexpectation\n&gt; &gt; &gt; that HyperNEAT actually =\r\nmay be &quot;better&quot; for evolving very-large-\n&gt; scale\n&gt; &gt; &gt; brains.\n&gt; &gt; &gt;\n&gt; &gt; &gt; =\r\nNo Free Lunch is a theorem showing that no single black-box \nsearch\n&gt; &gt; &gt; m=\r\nethod can be better than any other when averaged over all \n&gt; possible\n&gt; &gt; &gt;=\r\n problems.  However, HyperNEAT escapes this trap because it is no\n&gt; &gt; &gt; lon=\r\nger a black box algorithm, thanks to the ability to inject a\n&gt; &gt; &gt; priori r=\r\nelationships at the start.  In many cases this a priori\n&gt; &gt; &gt; knowledge is =\r\nvery simple to include because it follows directly \n&gt; from\n&gt; &gt; &gt; the obviou=\r\ns geometry of the task (such as a visual field or game\n&gt; &gt; &gt; board being Ca=\r\nrtesian in an self-evident arrangement).  Yet the\n&gt; &gt; &gt; significance of suc=\r\nh knowledge (as opposed to not having it) is\n&gt; &gt; &gt; priceless.  So having th=\r\ne capacity to arrange sensors and \noutputs \n&gt; in\n&gt; &gt; &gt; the way you want is =\r\nquite a powerful new capability.\n&gt; &gt; \n&gt; &gt; Hi Ken,\n&gt; &gt; \n&gt; &gt; I&#39;d be really ca=\r\nreful in how you word this statement. The point \nyou \n&gt; are \n&gt; &gt; trying to =\r\nmake, I think, is that for any specific problem you&#39;d \n&gt; want to \n&gt; &gt; solve=\r\n with HyperNEAT, the experimenter will always inject the \n&gt; &gt; /appropriate/=\r\n prior knowledge for that problem, and thus \nHyperNEAT \n&gt; will \n&gt; &gt; /never/=\r\n be used in a black-box setting (e.g. in situations where \nno \n&gt; prior \n&gt; &gt;=\r\n knowledge is available). In this case, yes, NFL no longer holds \n&gt; because=\r\n, \n&gt; &gt; in a sense, you are not applying the same algorithm to each \n&gt; probl=\r\nem. \n&gt; &gt; Rather, you are peeking at the problem, and then selecting the \nmo=\r\nst \n&gt; &gt; appropriate settings for HyperNEAT to solve that problem.\n&gt; &gt; \n&gt; &gt; =\r\nSince your argument relies on the use of an &quot;oracle&quot; experimenter \n&gt; to kno=\r\nw \n&gt; &gt; the correct properties of the problem to use as prior knowledge, \nI =\r\n\n&gt; think \n&gt; &gt; you are being a little disingenuous. What if the experimenter=\r\n \n&gt; messes up \n&gt; &gt; and puts in the wrong prior knowledge? Or, even worse, p=\r\nuts in \n&gt; almost \n&gt; &gt; correct prior knowledge that s/he assumes generalizes=\r\n from other \n&gt; problems. \n&gt; &gt; In this case, without any other limitations p=\r\nlaced on the space \nof \n&gt; &gt; problems we may be trying, we are back to NFL l=\r\nand. Based on this \nI \n&gt; don&#39;t \n&gt; &gt; think you are justified in saying Hyper=\r\nNEAT is not a black box \n&gt; algorithm.\n&gt; &gt; \n&gt; &gt; Furthermore, I would argue t=\r\nhat HyperNEAT&#39;s ability to inject \nprior \n&gt; &gt; knowledge is not something sp=\r\necial to that algorithm. You can do \n&gt; this in \n&gt; &gt; NEAT as well, albeit to=\r\n a less spectacular extent, by \nmanipulating \n&gt; the \n&gt; &gt; input coding. The =\r\nsame problem can be made arbitrarily difficult \nby \n&gt; using \n&gt; &gt; &quot;dumb&quot; inp=\r\nut codings, i.e. input codings that you might use if \nyou \n&gt; didn&#39;t \n&gt; &gt; ha=\r\nve any prior knowledge of the problem.  Nate has done some \n&gt; interesting \n=\r\n&gt; &gt; work in this area.\n&gt; &gt; \n&gt; &gt; In any case, I do agree with your original =\r\npoint: HyperNEAT \n&gt; probably has a \n&gt; &gt; bias which makes it better for larg=\r\ne scale problems, given the \n&gt; appropriate \n&gt; &gt; prior knowledge, and NEAT p=\r\nrobably has a bias towards simpler \n&gt; problems. I \n&gt; &gt; would leave out the =\r\nappeal to &quot;injecting prior knowledge&quot; and \njust \n&gt; say: \n&gt; &gt; &quot;HyperNEAT is =\r\nexpected to perform better on the class of large-\n&gt; scale \n&gt; &gt; problems wit=\r\nh regular structure.&quot; This statement can be made NFL-\n&gt; proof in \n&gt; &gt; and o=\r\nf itsel0f, simply by ensuring that that class of problems is \n&gt; not \n&gt; &gt; cl=\r\nosed-under-permutation, which it probably isn&#39;t: See, for \n&gt; instance, Marc=\r\n \n&gt; &gt; Toussaint&#39;s work on compressible search landscapes (i.e. problems \n&gt; =\r\nwith \n&gt; &gt; regular structure) and how the class of such landscapes is not \nC=\r\nUP \n&gt; and \n&gt; &gt; thus NFL does not apply.\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; Ok, phew. Let me reca=\r\np my argument just in case I lost anyone: \nKen, \n&gt; I \n&gt; &gt; think your intuit=\r\nion here about HyperNEAT is perfectly correct, \nbut \n&gt; your \n&gt; &gt; justificat=\r\nion and statement that HyperNEAT does not fall under \nthe \n&gt; purview \n&gt; &gt; o=\r\nf NFL is tenuous at best.\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; -- Joe\n&gt; &gt; \n&gt; &gt; -- \n&gt; &gt; \n&gt; &gt; Joseph=\r\n Reisinger\n&gt; &gt; http://www.cs.utexas.edu/~joeraii\n&gt; &gt;\n&gt;\n\n\n\n"}}