{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":199382914,"authorName":"Mike Woodhouse","from":"&quot;Mike Woodhouse&quot; &lt;mikewoodhouse@...&gt;","profile":"mikewoodhouse","replyTo":"LIST","senderId":"EPO_NCb9MI1iUlgwHEjCWbsu_p9SD7tNqh8FnhEYW4HL2dhqcn-pyV46aS-HikX-gH90yBZDMVY7kMXVYx6dbAYG7KjW-IUNz9o5oqFlkSTD","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: NEAT enhancements","postDate":"1157031760","msgId":2720,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGVkNnAwZyt0cjFzQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDQ0RjIzOEFDLjcwMjAxQGRzbC5waXBleC5jb20+"},"prevInTopic":2713,"nextInTopic":2721,"prevInTime":2719,"nextInTime":2721,"topicId":2684,"numMessagesInTopic":17,"msgSnippet":"... Good point. ... I can t see anything wrong with that. The main thing is to get away from input values that are scaled differently for different shares, ","rawEmail":"Return-Path: &lt;mikewoodhouse@...&gt;\r\nX-Sender: mikewoodhouse@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 91673 invoked from network); 31 Aug 2006 13:42:45 -0000\r\nReceived: from unknown (66.218.66.216)\n  by m28.grp.scd.yahoo.com with QMQP; 31 Aug 2006 13:42:45 -0000\r\nReceived: from unknown (HELO n14a.bullet.scd.yahoo.com) (66.94.237.28)\n  by mta1.grp.scd.yahoo.com with SMTP; 31 Aug 2006 13:42:45 -0000\r\nReceived: from [66.218.69.3] by n14.bullet.scd.yahoo.com with NNFMP; 31 Aug 2006 13:42:41 -0000\r\nReceived: from [66.218.66.79] by t3.bullet.scd.yahoo.com with NNFMP; 31 Aug 2006 13:42:41 -0000\r\nDate: Thu, 31 Aug 2006 13:42:40 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;ed6p0g+tr1s@...&gt;\r\nIn-Reply-To: &lt;44F238AC.70201@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0\r\nFrom: &quot;Mike Woodhouse&quot; &lt;mikewoodhouse@...&gt;\r\nSubject: Re: NEAT enhancements\r\nX-Yahoo-Group-Post: member; u=199382914; y=7g3s5-hGg87DyKqatQL45hX7bgQ_WlK7pJE_AhfGtgL0vgI5X0epQA\r\nX-Yahoo-Profile: mikewoodhouse\r\n\r\n--- In neat@yahoogroups.com, Colin Green &lt;cgreen@...&gt; wrote:\n&gt; Hi Mike, Emy=\r\nr, All\n&gt; \n&gt; This definitely seems to be the most sound approach to take to =\r\nme. It \n&gt; does introduce the problem of having to calculate (sample) the me=\r\nan and \n&gt; standard deviation from a sample set of data which, as I was sayi=\r\nng \n&gt; before, you should then avoid for training and testing purposes - whi=\r\nch \n&gt; is a problem if, like me, you already have limited data.\n\nGood point.=\r\n\n\n&gt; Currently I&#39;m using the daily percentage change in price as an input \n&gt;=\r\n signal as you suggested, but I use the raw figure as it&#39;s typically  \n&gt; wi=\r\nthin the range -1 to 1 anyway (in fact more like +-0.1 of course), \n\nI can&#39;=\r\nt see anything wrong with that. The main thing is to get away\nfrom input va=\r\nlues that are scaled differently for different shares,\nwhich is likely to c=\r\nonfuse the dickens out of your networks.\n\n&gt; with the occasional outlying fi=\r\ngure which could be anywhere from +-0.5 \n&gt; to +3. Using the number of stand=\r\nard deviations would generate a similar \n&gt; number for these outlying cases,=\r\n instead of the rather arbitrary number \n&gt; that would get used (input to an=\r\n ANN) otherwise.\n&gt; \n&gt; For volume and # of trades Z-scores are even more app=\r\nropriate since we \n&gt; cannot really use daily percentage change for these si=\r\ngnals because\nthey \n&gt; can change by such large amount from day to day. E.g.=\r\n you might have a \n&gt; quiet trading day with a few thousand shares traded an=\r\nd several million \n&gt; traded the next day due to some news/event. Currently =\r\nI use \n&gt; log10(volume) to generate a signal appropriate for feeding into an=\r\n ANN, \n&gt; but on reflection a Z-score is a far more mathematically sound cho=\r\nice.\n\nI have always felt that having some idea of the distribution of data\n=\r\npoints historically ought to lead to identifying the most appropriate\ntrans=\r\nformations. When you have a strange distribution, say one where\nthere are f=\r\nrequent small positive values and occasional large negative\nones, most &quot;usu=\r\nal&quot; transformations will compress the normal case into\na very small range w=\r\nhich may in fact be removing information. I&#39;ve\ntried in the past applying a=\r\n preprocessor that tranforms inputs into\npercentiles from a given historic =\r\nrange, so that the small normal\nrange is expanded. It might be better to ap=\r\nply some sort of continuous\nfunction; this is an area where my knowledge ne=\r\neds some significant\nincrease, I&#39;m afraid.\n\nI have played around with the i=\r\ndea of extending the power of the input\nnode to include a genetically-coded=\r\n input transformation. I haven&#39;t\nmanaged to get beyond playing yet - there =\r\nreally do need to be at\nleast three or four hours in a day.\n\nMike\n\n\n\n\n"}}