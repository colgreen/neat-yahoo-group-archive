{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":200957992,"authorName":"Jason Gauci","from":"&quot;Jason Gauci&quot; &lt;jgmath2000@...&gt;","profile":"jgmath2000","replyTo":"LIST","senderId":"CuBJZw2oovFBEPvBMobcK8Eec09LU4sYAUykADRCDVB0ttX58vbJUYpeMzxutQLjxIvcFdRiT4ToMUiEmubIjRtKstpKhKKonH6f","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: hyperneat questions","postDate":"1250089225","msgId":4822,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGg1dWxlOStrZ3U3QGVHcm91cHMuY29tPg==","inReplyToHeader":"PGg1dWcwbiszaXZuQGVHcm91cHMuY29tPg=="},"prevInTopic":4820,"nextInTopic":4824,"prevInTime":4821,"nextInTime":4823,"topicId":4808,"numMessagesInTopic":10,"msgSnippet":"Hey Nikolai, I think the key to implementing HyperNEAT on the GPU would be to take advantage of the fact that substrates are so regular.  A sheet of","rawEmail":"Return-Path: &lt;jgmath2000@...&gt;\r\nX-Sender: jgmath2000@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 91992 invoked from network); 12 Aug 2009 15:00:50 -0000\r\nX-Received: from unknown (69.147.108.201)\n  by m5.grp.re1.yahoo.com with QMQP; 12 Aug 2009 15:00:50 -0000\r\nX-Received: from unknown (HELO n4-vm6.bullet.mail.sp2.yahoo.com) (67.195.135.100)\n  by mta2.grp.re1.yahoo.com with SMTP; 12 Aug 2009 15:00:50 -0000\r\nX-Received: from [67.195.134.238] by n4.bullet.mail.sp2.yahoo.com with NNFMP; 12 Aug 2009 15:00:27 -0000\r\nX-Received: from [69.147.65.171] by t3.bullet.mail.sp2.yahoo.com with NNFMP; 12 Aug 2009 15:00:26 -0000\r\nX-Received: from [98.137.34.36] by t13.bullet.mail.sp1.yahoo.com with NNFMP; 12 Aug 2009 15:00:26 -0000\r\nDate: Wed, 12 Aug 2009 15:00:25 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;h5ule9+kgu7@...&gt;\r\nIn-Reply-To: &lt;h5ug0n+3ivn@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;Jason Gauci&quot; &lt;jgmath2000@...&gt;\r\nSubject: Re: hyperneat questions\r\nX-Yahoo-Group-Post: member; u=200957992; y=4HrtZzqiVdka5evP8fSqe6RNxXWLw9OU3EHI0O-Ym6148Zv6og\r\nX-Yahoo-Profile: jgmath2000\r\n\r\nHey Nikolai,\n\nI think the key to implementing HyperNEAT on the GPU would be=\r\n to take advantage of the fact that substrates are so regular.  A sheet of =\r\nconnections between two layers on a substrate of size (a,b) and (x,y) can b=\r\ne represented as a 4d texture of size (a,b,x,y) where the pixel values on t=\r\nhe texture hold the link weights.  The activation levels of the substrate c=\r\nan be represented by textures of size (a,b) and (x,y).  Then maybe the GPU =\r\ncan optimize multiplying the activation level texture by the weight texture=\r\n and summing that and executing the sigmoid, then stuffing that into the ne=\r\nxt layer.\n\nDo you know if something like this is possible on the GPU?  I do=\r\nn&#39;t have any GPU programming experience.\n\n--- In neat@yahoogroups.com, &quot;Nik=\r\nolai&quot; &lt;ker_31_toluca@...&gt; wrote:\n&gt;\n&gt; Thank you Ken.\n&gt; No, i have no problem=\r\n with concepts, I am rather trying to understand the exact implementation o=\r\nf your hyperneat version, to code exactly like that in my own parallel, GPU=\r\n based implementation. \n&gt; Thanks again & best regards.\n&gt; Nikolai\n&gt; \n&gt; --- I=\r\nn neat@yahoogroups.com, &quot;Kenneth Stanley&quot; &lt;kstanley@&gt; wrote:\n&gt; &gt;\n&gt; &gt; Nikola=\r\ni, I think your question is essentially about vocabulary.  You are asking w=\r\nhether the substrate would still be called an ANN if it contained non-sigmo=\r\nidal nodes.  I believe no one has evolved such substrates (i.e. with differ=\r\nent activation functions), so I think it is a theoretical question, but I t=\r\nhink what you are saying is probably correct.  The substrate does not need =\r\nto be an ANN.  Of course, some ANNs do not use sigmoids (such as RBF networ=\r\nks), so just because the nodes are not sigmoids does not guarantee that it =\r\nis not an ANN, but in some cases I would agree it is not an ANN.  \n&gt; &gt; \n&gt; &gt;=\r\n I would be careful with the term &quot;tree&quot; because most graphs are not &quot;trees=\r\n,&quot; so I would not call it an &quot;expression tree&quot; unless it really was somehow=\r\n forced to produce trees or somehow output a tree as the optimal result.  W=\r\nhether it would be called &quot;simple&quot; is also probably a matter of opinion.  G=\r\nenerally, if there are many connections it may not really be simple, but it=\r\n may be simple if the connection weights of those connections adhere to a s=\r\nimple underlying pattern.\n&gt; &gt; \n&gt; &gt; I hope this answer helps clarify some of=\r\n your questions.\n&gt; &gt; \n&gt; &gt; ken\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; --- In neat@...=\r\nm, &quot;Nikolai&quot; &lt;ker_31_toluca@&gt; wrote:\n&gt; &gt; &gt;\n&gt; &gt; &gt; --- In neat@...=\r\nm, &quot;Kenneth Stanley&quot; &lt;kstanley@&gt; wrote:\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; Nikolai, you have t=\r\nhe main idea right but the one aspect of your \n&gt; &gt; &gt; &gt; description that I w=\r\nould correct is the idea that the substrate must\n&gt; &gt; &gt; &gt; be a perceptron.\n&gt;=\r\n &gt; &gt; \n&gt; &gt; &gt; thanks Ken, as I understood, you call &#39;substrate&#39; a final ANN t=\r\nhat evaluates the solution to the problem activating each node one by one w=\r\nhere each node uses the same activation function.\n&gt; &gt; &gt; In a paper about CP=\r\nPNs you say that CPPN is not an ANN and this is why it is called &#39;CPPN&#39;, i =\r\nagree. But when a CPPN produces weights for an ANN, the result (substrate) =\r\nmust be an ANN. If each node of the substrate is not perceptron type (i.e d=\r\noes not have a sigmoid activation function), then we can not call it an ANN=\r\n, right ? It could be called as some sort of an &quot;expression tree&quot;? And if i=\r\nt does not have many hidden layers, it would represent a simple (repetitive=\r\n, symmetrical, etc) expression. A &#39;simple expression at large scale&#39; if it =\r\nis dealing with large number of connections. \n&gt; &gt; &gt; Please correct me if i =\r\nam missing something.\n&gt; &gt; &gt; Thanks\n&gt; &gt; &gt; Nikolai\n&gt; &gt; &gt;\n&gt; &gt;\n&gt;\n\n\n\n"}}