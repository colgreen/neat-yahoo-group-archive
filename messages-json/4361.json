{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"lXtuRtu8azhUoFqdScEG2vWXuceD0BrP4Guy3GnJ2O9-FgGtz1HxaKveli_tgThhzGs4MkB7piOxrWk2am_gcyxezIZLs1-jIvGBR9Mczru8","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: grammatical evolution","postDate":"1223769139","msgId":4361,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGdjcmU3aitycWtvQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDEyODBjZjZhMDgxMDExMTM1OXIxYzdjNjg3NmoxYThkZmY1ZjExZmFmM2YwQG1haWwuZ21haWwuY29tPg=="},"prevInTopic":4360,"nextInTopic":4362,"prevInTime":4360,"nextInTime":4362,"topicId":4352,"numMessagesInTopic":11,"msgSnippet":"Thomas, you make a valid point.  Some encodings are easier to decipher for humans than others.  However, here is my guess:  If an encoding is powerful enough","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 70524 invoked from network); 11 Oct 2008 23:52:20 -0000\r\nX-Received: from unknown (66.218.67.95)\n  by m46.grp.scd.yahoo.com with QMQP; 11 Oct 2008 23:52:20 -0000\r\nX-Received: from unknown (HELO n13b.bullet.sp1.yahoo.com) (69.147.64.113)\n  by mta16.grp.scd.yahoo.com with SMTP; 11 Oct 2008 23:52:20 -0000\r\nX-Received: from [69.147.65.151] by n13.bullet.sp1.yahoo.com with NNFMP; 11 Oct 2008 23:52:20 -0000\r\nX-Received: from [66.218.69.3] by t5.bullet.mail.sp1.yahoo.com with NNFMP; 11 Oct 2008 23:52:20 -0000\r\nX-Received: from [69.147.65.148] by t3.bullet.scd.yahoo.com with NNFMP; 11 Oct 2008 23:52:20 -0000\r\nX-Received: from [66.218.66.82] by t11.bullet.mail.sp1.yahoo.com with NNFMP; 11 Oct 2008 23:52:20 -0000\r\nDate: Sat, 11 Oct 2008 23:52:19 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;gcre7j+rqko@...&gt;\r\nIn-Reply-To: &lt;1280cf6a0810111359r1c7c6876j1a8dff5f11faf3f0@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: Re: grammatical evolution\r\nX-Yahoo-Group-Post: member; u=54567749; y=He-a-32BJgjsgV01IhXo7EO1nd4BhyDI_q4OwNkW8SPvRpmRmezq\r\nX-Yahoo-Profile: kenstanley01\r\n\r\nThomas, you make a valid point.  Some encodings are easier to decipher\nfor =\r\nhumans than others.  However, here is my guess:  If an encoding is\npowerful=\r\n enough to evolve and describe structures of significant\ncomplexity, the de=\r\nscription will be opaque (i.e. too difficult to\nunderstand) regardless of t=\r\nhe way the encoding is expressed.\n\nAt one time people argued for decision t=\r\nrees over neural networks for\nthe similar reason that you can actually read=\r\n a decision tree and\nunderstand its reasoning.  However, when decision tree=\r\ns get very big\nand complicated, they start to become too esoteric and &quot;inhu=\r\nman&quot; to\nreally make much sense of them anyway.  Nevertheless, of course the=\r\nre\nis more hope in understand a decision tree than a neural network. \nIt&#39;s =\r\njust that it isn&#39;t really that much better when problems get\ncomplicated.\n\n=\r\nI think the first thing I&#39;d want to ask about an encoding is whether\nit can=\r\n solve your problem, i.e. if it has the right bias.  Next I\nwould ask wheth=\r\ner it has other desirable properties.  However, if it\ncan&#39;t solve the probl=\r\nem it won&#39;t help much to have other nice\nproperties.  The problem is that o=\r\nften these special properties (like\nreadability) make the encoding induce a=\r\n worse search space. \nTherefore, there is often a trade-off that is difficu=\r\nlt to control. \nWhen it comes to extremely difficult problems (e.g. like ev=\r\nolving\nsomething that makes millions of dollars in the stock market), I&#39;d b=\r\ne\nhappy just to have something that works, and anything else is icing on\nth=\r\ne cake.\n\nken\n\n--- In neat@yahoogroups.com, &quot;Thomas Johnson&quot; &lt;thomas.j.johns=\r\non@...&gt;\nwrote:\n&gt;\n&gt; I&#39;m not sure that it&#39;s necessary to argue for GE from a =\r\npurely\n&gt; encoding-based or search-space-based framework. It depends on what=\r\n\n&gt; kind of attributes you value for your solutions. If you want a\n&gt; solutio=\r\nn that is &quot;merely&quot; a good mapping of future inputs to future\n&gt; outputs over=\r\n some kind of input domain, then you desire a smooth\n&gt; search space which w=\r\nill maximize the probability of getting a solution\n&gt; like that. But GE has =\r\nthe advantage that its solutions can be more\n&gt; easily understood by humans =\r\nthan NNs. So if I&#39;m concerned about the\n&gt; intuition behind the &quot;black box&quot; =\r\nof the solution, or I want to analyze\n&gt; the kinds of domains over which the=\r\n solution might not perform well\n&gt; (e.g., modeling financial risk), GE coul=\r\nd be a better methodology even\n&gt; if its searching power is somewhat inferio=\r\nr.\n&gt; \n&gt; On Sat, Oct 11, 2008 at 3:14 PM, Kenneth Stanley &lt;kstanley@...&gt; wro=\r\nte:\n&gt; &gt; I understand that &quot;hierarchical, modular, and formalized&quot; are\n&gt; &gt; s=\r\nometimes viewed as advantages for Grammatical Encoding (a similar\n&gt; &gt; argum=\r\nent is also made for L-systems, which is also grammatical).\n&gt; &gt; While these=\r\n are clearly properties you want in a programming language,\n&gt; &gt; are these r=\r\neally good things for a genetic encoding? The challenge\n&gt; &gt; with encodings =\r\nis not only for them to theoretically express all\n&gt; &gt; possible procedures, =\r\nbut also to induce a nicely coupled (smooth)\n&gt; &gt; search space. It seems tha=\r\nt the space induced by a strictly modular\n&gt; &gt; and hierarchical encodings is=\r\n unlikely to be smooth, leading to\n&gt; &gt; brittleness. You can have things lik=\r\ne a massive new module being\n&gt; &gt; doubled in one shot (which would usually b=\r\ne a mess, like being born\n&gt; &gt; with two heads) or a change at one level of t=\r\nhe hierarchy breaking the\n&gt; &gt; rest of it entirely.\n&gt; &gt;\n&gt; &gt; I am not dismiss=\r\ning GE, but I would want to hear an argument\n&gt; &gt; supporting it more from th=\r\ne encoding and search spaces side rather\n&gt; &gt; than from the formal languages=\r\n side.\n&gt; &gt;\n&gt; &gt; ken\n&gt; &gt;\n&gt; &gt; --- In neat@yahoogroups.com, Jean-Baptiste Moure=\r\nt / Mandor\n&gt; &gt; &lt;mandor@&gt; wrote:\n&gt; &gt;&gt;\n&gt; &gt;&gt; From: &quot;shaflidason&quot; &lt;styrmis@&gt;\n&gt; =\r\n&gt;\n&gt; &gt;&gt; &gt; &lt;shanemcdonaldryan@&gt; wrote:\n&gt; &gt;&gt; &gt; &gt;\n&gt; &gt;&gt; &gt; &gt; Has anyone ever comp=\r\nared the performance of grammatical\nevolution to\n&gt; &gt;&gt; &gt; &gt; neuro evolution? =\r\nPerhaps on the standard problems\n&gt; &gt; pole-balancing, xor.\n&gt; &gt;&gt; &gt; &gt;\n&gt; &gt;&gt; &gt; &gt;=\r\n It seems quite promising and I can post my results when they are\n&gt; &gt; done.=\r\n\n&gt; &gt;&gt; &gt; &gt; But I wanted to see if anyone else has looked into it before\nI do=\r\n.\n&gt; &gt;&gt; &gt; &gt; I&#39;ve invested quite a bit of enery in NE but GE looks very\n&gt; &gt; p=\r\nromising,\n&gt; &gt;&gt; &gt; &gt; and I like the fact that it produces actual code rather =\r\nthan\na black\n&gt; &gt;&gt; &gt; &gt; box. Which makes it more appropriate for some domains=\r\n. ie\ncommercial\n&gt; &gt;&gt; &gt; &gt; video games.\n&gt; &gt;&gt;\n&gt; &gt;&gt; I did some comparaisons of =\r\nNEAT with regards to our attribute grammar\n&gt; &gt;&gt; based encoding (this is not=\r\n grammatical evolution but it&#39;s\nrelated). I\n&gt; &gt;&gt; used a pole-balancing task=\r\n and a task in which a multi-dof\nrobotic arm\n&gt; &gt;&gt; had to move to defined po=\r\nsitions. Our system performed a bit better\n&gt; &gt;&gt; than NEAT on these tasks bu=\r\nt NEAT could probably be tweaked\n&gt; &gt;&gt; more. While I&#39;m very happy with the t=\r\nheoritical features of the\n&gt; &gt;&gt; encodings (it&#39;s hierarchical, modular and f=\r\normalized), more\nbenchmarks\n&gt; &gt;&gt; will be needed to draw any final conclusio=\r\nn about the &quot;efficiency&quot;\n&gt; &gt;&gt; (btw, Ken previously highlighted the problems=\r\n with benchmarks such as\n&gt; &gt;&gt; those).\n&gt; &gt;&gt;\n&gt; &gt;&gt; See : http://dx.doi.org/10.=\r\n1007/s12065-008-0015-7\n&gt; &gt;&gt;\n&gt; &gt;&gt; Best regards,\n&gt; &gt;&gt; --\n&gt; &gt;&gt; Jean-Baptiste M=\r\nouret / Mandor\n&gt; &gt;&gt; http://animatlab.lip6.fr/~mouret\n&gt; &gt;&gt; tel : (+33) 6 28 =\r\n35 10 49\n&gt; &gt;&gt;\n&gt; &gt;\n&gt; &gt;\n&gt;\n\n\n\n"}}