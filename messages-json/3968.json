{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"ldPPVvkQILl4qgSgGWYZBWF9Di_tbRpdgoVX9P0HH8UZGrdPE5w96TNg1PqYt0lATH4Qe9nRc6EJPZYV9UN20qGq3iokDCP6IzQuxE0f0jxi","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: New HyperNEAT Paper in the Domain of Checkers","postDate":"1208642676","msgId":3968,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGZ1ZHE5aytmcjFoQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDEyMTUzMDE2ODkyODYyQG1haWwuaWRuZXQubmV0LnVrPg=="},"prevInTopic":3967,"nextInTopic":3969,"prevInTime":3967,"nextInTime":3969,"topicId":3942,"numMessagesInTopic":27,"msgSnippet":"Ian, your questions expose a tricky distinction that perhaps we could have made more clear.  The distinction is between the idea of learning the geometry and","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 83022 invoked from network); 19 Apr 2008 22:04:37 -0000\r\nX-Received: from unknown (66.218.67.95)\n  by m56.grp.scd.yahoo.com with QMQP; 19 Apr 2008 22:04:37 -0000\r\nX-Received: from unknown (HELO n28a.bullet.sp1.yahoo.com) (209.131.38.246)\n  by mta16.grp.scd.yahoo.com with SMTP; 19 Apr 2008 22:04:37 -0000\r\nX-Received: from [216.252.122.217] by n28.bullet.sp1.yahoo.com with NNFMP; 19 Apr 2008 22:04:37 -0000\r\nX-Received: from [66.218.69.1] by t2.bullet.sp1.yahoo.com with NNFMP; 19 Apr 2008 22:04:37 -0000\r\nX-Received: from [66.218.67.197] by t1.bullet.scd.yahoo.com with NNFMP; 19 Apr 2008 22:04:37 -0000\r\nDate: Sat, 19 Apr 2008 22:04:36 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;fudq9k+fr1h@...&gt;\r\nIn-Reply-To: &lt;12153016892862@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: Re: New HyperNEAT Paper in the Domain of Checkers\r\nX-Yahoo-Group-Post: member; u=54567749; y=y-CaFOExd9PqZb4pzDCW-d6eCd_yfgrngpnab90PnJ3xBi6Mc1zl\r\nX-Yahoo-Profile: kenstanley01\r\n\r\nIan, your questions expose a tricky distinction that perhaps we could\nhave =\r\nmade more clear.  The distinction is between the idea of\n&quot;learning the geom=\r\netry&quot; and &quot;learning from the geometry.&quot;\n\nMy interpretation of HyperNEAT is =\r\nthat it does not learn the geometry\nbut that it does learn FROM the geometr=\r\ny, and that ability to learn\nregularities from the geometry is one of its n=\r\novel beneficial\ncapabilities.  \n\nIn other words, I believe the claim is cor=\r\nrect that the geometry of\nthe checkers board is given a priori to HyperNEAT=\r\n by the human, and\ntherefore HyperNEAT did not figure out the geometry of t=\r\nhe checkers\nboard itself.  Yet that is exactly what is so useful about the\n=\r\ntechnique: We can present it with the actual geometry of a problem so\nthat =\r\nit does not have to waste time figuring it out for itself.  Then\nit can use=\r\n that geometry to get down to the more important business of\ndescribing the=\r\n regularities of the task as they relate to the\ngeometry.  It is this abili=\r\nty to learn regularities directly from the\ngeometry such that they do not n=\r\need to be identified a priori by the\nhuman user that is notable.  \n\nA probl=\r\nem with direct encodings (like regular NEAT) and some indirect\nencodings is=\r\n that you cannot tell them the geometry of the inputs (or\noutputs).  For ex=\r\nample, in checkers, NEAT cannot know a priori which\ninput is adjacent to wh=\r\nich in the geometry of the checkers board. \nTherefore, in fact, regular NEA=\r\nT is the technique that has to learn\nthe geometry (which is a bad thing).  =\r\nThat is, it has to figure out on\nits own over many experiences how the diff=\r\nerent inputs are\ngeometrically related.  That implicitly becomes part of it=\r\ns internal\nrepresentation over many generations.  Unfortunately, also, it h=\r\nas to\nlearn all of these geometric relations separately from each other.\n\nS=\r\no one of the recommending aspect of HyperNEAT in my view is that\nthere IS a=\r\n way for the human to simply say &quot;here is how the problem\nlooks&quot; and that w=\r\nay avoid the ridiculous situation where the learner\ncan&#39;t even tell how any=\r\n of the inputs or outputs are relatively\nsituated and yet is expected to le=\r\narn effectively nevertheless.  Why\nindeed would we want to withhold such in=\r\nformation from a learning\nalgorithm when it is readily obvious for many tas=\r\nks (such as\ncheckers)?  In fact, one of the few positive recommendations th=\r\nat come\nout of the relatively depressing NFL theorems is that we should str=\r\nive\nto provide our learning algorithms domain-specific biases.  What more\ns=\r\ntraightforward bias than to provide the actual geometry of the\nproblem doma=\r\nin?\n\nI agree that we should explore various alternative geometric systems\nt=\r\no see their effects, e.g. polar vs. Cartesian.  In fact, that is an\nissue e=\r\nxplored in, &quot;A Novel Generative Encoding for Exploiting Neural\nNetwork Sens=\r\nor and Output Geometry,&quot; from last year (with David\nD&#39;Ambrosio):\n\nhttp://ep=\r\nlex.cs.ucf.edu/papers/dambrosio_gecco07.pdf\n\nThe issue that comes to light =\r\nis that the activation function set you\nchoose is like a &quot;language&quot; for des=\r\ncribing relationships within the\ncoordinate system you choose as well.  Som=\r\ne languages are more natural\nfor describing relationships in one coordinate=\r\n system or another, and\nthat is what really recommends one type of represen=\r\ntation over another\n(although in some domains the will be less obvious than=\r\n in others). \nCheckers looks particularly Cartesian to me, although it migh=\r\nt be\ninteresting to rotate the board 45 degrees since then what were\nentire=\r\nly diagonal relationships become aligned along axis-parallel\nlines (hence i=\r\nt may be easier to describe regularities in such a system).\n\nAlternative hi=\r\ndden node geometries are also interesting since as you\npoint out it is less=\r\n obvious what the &quot;right&quot; geometry is for hidden\nnodes.\n\nAbout kings, we di=\r\nd allow kings and we did input a different value to\nrepresent kings.  I bel=\r\nieve that is explained in the &quot;Experiments&quot;\nsection of the paper and it is =\r\nbased on the precedent established by\nChellapilla and Fogel.  If I am recal=\r\nling right, they used the value\n1.3 for kings and we followed their lead.  =\r\n(I can&#39;t check right now\nsince it appears the eplex server is down :( )\n\nke=\r\nn\n\n--- In neat@yahoogroups.com, Ian Badcoe &lt;ian_badcoe@...&gt; wrote:\n&gt;\n&gt; Hi a=\r\nll,\n&gt; \n&gt; A few, I hope, different thoughts about hyperNEAT in the checkers\n=\r\ndomain...\n&gt; \n&gt; Did hyperNEAT _really_ discover the regularities of the boar=\r\nd, or \n&gt; where they still encoded (or partly encoded) in the setup of the \n=\r\n&gt; system?  I&#39;ll readily admit that if they were encoded then it was far \n&gt; =\r\nless directly than in other AI approaches, but...\n&gt; \n&gt; What I&#39;m getting at =\r\nis that the choice of coordinate space for the \n&gt; hidden layer (2D) and the=\r\n choice of sampling pattern and interval \n&gt; (8x8 square grid) were supplied=\r\n in the experimental setup...\n&gt; \n&gt; I don&#39;t have a full fix for this, but wh=\r\nat I&#39;d like to see is \n&gt; experiments along the lines of (some of these over=\r\nlap):\n&gt; \n&gt; a) use hidden layers of differing size and dimensionality.  This=\r\n may \n&gt; not degrade performance at all, in that there is no reason for the =\r\n\n&gt; nodes in the hidden layer to be entirely geometric in their \n&gt; &quot;meaning&quot;=\r\n.  e.g. there may be a node in the hidden layer which means \n&gt; &quot;number of m=\r\ny pieces in corner locations&quot; -- the meaning may not be \n&gt; directly related=\r\n to the area of the input layer corresponding to the \n&gt; node in the hidden =\r\nlayer...  I&#39;d be inclined to try:\n&gt; \n&gt; a1) 1d hidden layer of various lengt=\r\nhs (akin to degrading to a \n&gt; non-hyper NEAT approach except you still have=\r\n half the capability to \n&gt; exploit geometry, e.g. the CPPN can still exploi=\r\nt regularities in \n&gt; coordinates on the input layer)\n&gt; a2) 3d hidden layer =\r\nof similar volume to the original but different \n&gt; shapes, e.g. original wa=\r\ns 8x8 =3D 64, 4x4x4 is a different shape but\nsame size...\n&gt; a3) high-dimens=\r\nional but small hidden layer, such as \n&gt; 2x2x2x2x2x2.  You may want to look=\r\n to the types of function in the \n&gt; CPPN for this, e.g. because each ordina=\r\nte on the hidden layer is \n&gt; binary, you may want binary operators in the C=\r\nPPN\n&gt; \n&gt; b) change the sample grid on the hidden layer:\n&gt; \n&gt; b1) over sampl=\r\ne x2, x3\n&gt; b2) sample the 7x7 or 9x9 grids that correspond to the square co=\r\nrners \n&gt; on the input layer\n&gt; b3) under sample by x2, x3\n&gt; b4) offset each =\r\nsample position in the hidden layer by a constant \n&gt; random vector (differe=\r\nnt for each sample but fixed for the \n&gt; experiment), vary the size 0.01, 0.=\r\n1, 0.2, 0.5...\n&gt; \n&gt; c) change the coordinate system.  I do wonder whether i=\r\nn defining the \n&gt; CPPN in terms of 2D coordinates (OK, 2x 2D) you have acci=\r\ndentally \n&gt; built in fore-knowledge of the geometry...  Try:\n&gt; \n&gt; c1) input=\r\n and hidden layers sampled at same points but using polar\ncoordinates\n&gt; c2)=\r\n as previous but one polar and one Cartesian\n&gt; c3) rotate the sample points=\r\n in the hidden layer by: 90 degrees \n&gt; (should have no effect?), 45 degrees=\r\n, or 10 degrees\n&gt; \n&gt; I think experiments such as these would hammer down wh=\r\nether hyperNEAT \n&gt; is fully discovering the geometry, or whether it was gif=\r\nted with a \n&gt; geometric system.  In the latter case it is still discovering=\r\n \n&gt; regularities, but it is doing it within a geometric framework that it \n=\r\n&gt; already knew.\n&gt; \n&gt; Great experiment as it stands though!  And congratulat=\r\nions for \n&gt; getting it closer to the mainstream (not that we need them :-) =\r\n).\n&gt; \n&gt; Just a quick question, were &quot;kings&quot; used in the rule base, e.g. whe=\r\nn \n&gt; a piece reaches the back row it becomes a &quot;king&quot; and can move \n&gt; backw=\r\nards...  If so I guess you used input values of greater \n&gt; magnitude for th=\r\nose?\n&gt; \n&gt; Regards,\n&gt; \n&gt; Ian\n&gt;\n\n\n\n"}}