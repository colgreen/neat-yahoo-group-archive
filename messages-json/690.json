{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"pZ0iWx1wtZIsSMgeQxVDYmKbfhrEJyDyKgoxeQOerZLXDBx9kXV3ecNQkvC_lX7ym183y0h3kTMseKWdsQoHQ7wFqow5Bcr_FyAjhBrLgsgN","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: autonomous virtual humans project","postDate":"1082680538","msgId":690,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGM2OW9jcityamt2QGVHcm91cHMuY29tPg==","inReplyToHeader":"PDIwMDQwNDIyMDUwNTMwLjI4NTk0LnFtYWlsQHdlYjQwNTA3Lm1haWwueWFob28uY29tPg=="},"prevInTopic":688,"nextInTopic":691,"prevInTime":689,"nextInTime":691,"topicId":657,"numMessagesInTopic":32,"msgSnippet":"One of the themes that seems to come up in the discussion of sensors for the virtual human is the questions of what senses we humans might plausibly have.  I","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 21229 invoked from network); 23 Apr 2004 00:36:39 -0000\r\nReceived: from unknown (66.218.66.217)\n  by m21.grp.scd.yahoo.com with QMQP; 23 Apr 2004 00:36:39 -0000\r\nReceived: from unknown (HELO n8.grp.scd.yahoo.com) (66.218.66.92)\n  by mta2.grp.scd.yahoo.com with SMTP; 23 Apr 2004 00:36:39 -0000\r\nReceived: from [66.218.67.172] by n8.grp.scd.yahoo.com with NNFMP; 23 Apr 2004 00:35:41 -0000\r\nDate: Fri, 23 Apr 2004 00:35:38 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;c69ocr+rjkv@...&gt;\r\nIn-Reply-To: &lt;20040422050530.28594.qmail@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Transfer-Encoding: quoted-printable\r\nContent-Length: 3975\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-eGroups-Remote-IP: 66.218.66.92\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: Re: autonomous virtual humans project\r\nX-Yahoo-Group-Post: member; u=54567749\r\nX-Yahoo-Profile: kenstanley01\r\n\r\nOne of the themes that seems to come up in the discussion of sensors\nfor th=\r\ne virtual human is the questions of what senses we humans might\nplausibly h=\r\nave.  I am not against making sensors plausible, but at\nleast the point sho=\r\nuld made that it&#39;s not really important what&#39;s\nplausible if your main goal =\r\nis for it to work.  While that point is\nsomewhat obvious, the more subtle p=\r\noint is that a lot of the time two\ndifferent types of sensor are really jus=\r\nt coordiante transformations\nof each other (e.g. polar vs. cartesian coordi=\r\nnates).  In other words,\nthey are essentially the same thing represented in=\r\n different ways. \nAnd because of that, the real question is not what&#39;s &quot;act=\r\nually&quot; going\non, but rather what is the most convenient representation from=\r\n the\nperspective of an NN.  And that&#39;s a bit of a different question than\n&quot;=\r\nwhat senses do humans use&quot; (although that is certainly a good source\nfor in=\r\nspiration).  But we need to be careful about jumping at complex\nor convolut=\r\ned representations that are really just transformations of\nsome simple conc=\r\nept.  \n\nFor example, if all that really matters is how high your head is of=\r\nf\nthe ground, then *that* is what should be fed to the NN, rather than a\nco=\r\nmplex of angles and vestibular information which ultimately boils\ndown the =\r\nsame piece of information.  I&#39;m not claiming that this\nexample is true (i.e=\r\n. that we don&#39;t need the extra stuff), but just\nmaking the point that it&#39;s =\r\nthe simplest most straightforward\nrepresentation of the key concept that ne=\r\neds to be conveyed to the\nnetwork, and sometimes it is not necessary to use=\r\n exactly the same\nrepresentation scheme as biology.\n\nken\n\n--- In neat@yahoo=\r\ngroups.com, Tyler Streeter &lt;tylerstreeter@y...&gt;\nwrote:\n&gt; \n&gt; --- Colin Green=\r\n &lt;cgreen@d...&gt; wrote:\n&gt; &gt; Tyler Streeter wrote:\n&gt; &gt; \n&gt; &gt; &gt;&gt;...unless you we=\r\nre intending to\n&gt; &gt; &gt;&gt;transfer the whole \n&gt; &gt; &gt;&gt;project to a real-world sce=\r\nnario I see eye\n&gt; &gt; &gt;&gt;simulation as unnecessary.\n&gt; &gt; &gt;&gt;    \n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;\n&gt; =\r\n&gt; &gt;I&#39;m reading the book Evolutionary Robotics right\n&gt; &gt; now\n&gt; &gt; &gt;which focu=\r\nses heavily on using senses robots could\n&gt; &gt; &gt;actually use.  My mind is kin=\r\nd of geared in that\n&gt; &gt; &gt;direction at present.  I guess I would like to\n&gt; &gt;=\r\n leave\n&gt; &gt; &gt;that option (transferring neural nets to real\n&gt; &gt; robots)\n&gt; &gt; &gt;=\r\nopen, so I am trying to focus on more realistic\n&gt; &gt; &gt;senses.  \n&gt; &gt; &gt;\n&gt; &gt; &gt; =\r\n \n&gt; &gt; &gt;\n&gt; &gt; Hi Tyler,\n&gt; &gt; \n&gt; &gt; My concern though is that you are allowing t=\r\nhe scope\n&gt; &gt; of your project to \n&gt; &gt; increase and encompass a lot more than=\r\n can be\n&gt; &gt; achieved in one go. The \n&gt; &gt; stuff being discussed on this grou=\r\np about fitness\n&gt; &gt; functions and the type \n&gt; &gt; of senses needed is all goo=\r\nd stuff and represents a\n&gt; &gt; significant problem \n&gt; &gt; and interesting piece=\r\n of research in itself. By\n&gt; &gt; adding the extra factors \n&gt; &gt; of ray tracing=\r\n and image analysis you are\n&gt; &gt; dramatically increasing the \n&gt; &gt; complexity=\r\n of the problem. Image analysis by itself\n&gt; &gt; is a long-standing \n&gt; &gt; branc=\r\nh of research AKAIK and I&#39;m just thinking about\n&gt; &gt; detecting lines and \n&gt; =\r\n&gt; objects in 2D images, in order to build a 3D model\n&gt; &gt; in your head you h=\r\nave \n&gt; &gt; to combine two 2D images (stereoscopic), you could\n&gt; &gt; do this wit=\r\nh code \n&gt; &gt; rather than in the NN&#39;s but it is CPU intensive work\n&gt; &gt; especi=\r\nally if you \n&gt; &gt; include the ray tracing!\n&gt; &gt; \n&gt; &gt; Scope creep is the death=\r\n of many a project. My\n&gt; &gt; advice would be to keep \n&gt; &gt; your scope focused =\r\nand you can move on to the other\n&gt; &gt; stuff with a new \n&gt; &gt; project later.\n&gt;=\r\n &gt; \n&gt; &gt; Hope I didn&#39;t come across as *too* much like a\n&gt; &gt; nay-sayer there =\r\n:)\n&gt; \n&gt; Nope, not at all.  I definitely need to keep things\n&gt; simple, espec=\r\nially at first.  One thing I&#39;m interested\n&gt; in is figuring out these differ=\r\nent problems and trying\n&gt; to combine them into a single agent.  I&#39;ll be sur=\r\ne to\n&gt; keep the search space size in check.\n&gt; \n&gt; Tyler\n&gt; \n&gt; \n&gt; \t\n&gt; \t\t\n&gt; ___=\r\n_______________________________\n&gt; Do you Yahoo!?\n&gt; Yahoo! Photos: High-qual=\r\nity 4x6 digital prints for 25=A2\n&gt; http://photos.yahoo.com/ph/print_splash\n=\r\n\n\n"}}