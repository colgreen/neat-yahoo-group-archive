{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"4Fyc-7wOM7ZRLU6BXuXDK1jvdPmBUfDtbKDY5QUCQvgTlMp9F9g4Qed8omdRQBFlmRt_aBnxP26Pc0wdaNO4qCkQ4TzR0KoNoGWnNYBrbUnJ","spamInfo":{"isSpam":false,"reason":"6"},"subject":"RE : [neat] Re: HyperNEAT: Creating Neural Networks with CPPNs","postDate":"1175025863","msgId":3049,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGV1YnRjNys1MGhuQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDQ2MDk1NzQxLjgwNDA1MDZAd2FpdHMubmV0Pg=="},"prevInTopic":3048,"nextInTopic":3050,"prevInTime":3048,"nextInTime":3050,"topicId":3028,"numMessagesInTopic":34,"msgSnippet":"... of ... # ... on ... the ... Even without automatic node placement, I m anticipating that in the long run it will turn out that there are a few standard","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 53152 invoked from network); 27 Mar 2007 20:07:44 -0000\r\nReceived: from unknown (66.218.66.71)\n  by m45.grp.scd.yahoo.com with QMQP; 27 Mar 2007 20:07:44 -0000\r\nReceived: from unknown (HELO n17a.bullet.sp1.yahoo.com) (69.147.64.124)\n  by mta13.grp.scd.yahoo.com with SMTP; 27 Mar 2007 20:07:44 -0000\r\nReceived: from [216.252.122.216] by n17.bullet.sp1.yahoo.com with NNFMP; 27 Mar 2007 20:04:24 -0000\r\nReceived: from [66.218.69.2] by t1.bullet.sp1.yahoo.com with NNFMP; 27 Mar 2007 20:04:24 -0000\r\nReceived: from [66.218.66.92] by t2.bullet.scd.yahoo.com with NNFMP; 27 Mar 2007 20:04:24 -0000\r\nDate: Tue, 27 Mar 2007 20:04:23 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;eubtc7+50hn@...&gt;\r\nIn-Reply-To: &lt;46095741.8040506@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: RE : [neat] Re: HyperNEAT: Creating Neural Networks with CPPNs\r\nX-Yahoo-Group-Post: member; u=54567749; y=I4S3nOS9blVbFzd1s9s-ysRnosdb-0Hq0X4_iPh2JcPQPTJKU_Wz\r\nX-Yahoo-Profile: kenstanley01\r\n\r\n--- In neat@yahoogroups.com, Stephen Waits &lt;steve@...&gt; wrote:\n&gt;\n&gt; Kenneth S=\r\ntanley wrote:\n&gt; &gt; \n&gt; &gt; Petar, I agree that it would be interesting to intro=\r\nduce a kind \nof\n&gt; &gt; mutation that automatically bumps up the substrate dens=\r\nity (i.e. \n#\n&gt; &gt; of nodes in the substrate). It would also be interesting t=\r\no allow\n&gt; &gt; HyperNEAT to decide the locations of the nodes in the substrate=\r\n \non\n&gt; &gt; its own.\n&gt; \n&gt; I think this is important.  Part of the draw to NEAT=\r\n, for me, is  \nthe \n&gt; low level of human configuration and design required.=\r\n\n&gt; \n\nEven without automatic node placement, I&#39;m anticipating that in the \nl=\r\nong run it will turn out that there are a few standard substrate \nconfigura=\r\ntions for certain types of problems (e.g. robot control, \nvisual recognitio=\r\nn, board games) that you can just start with \nwithout thinking about it.  T=\r\nhe actual connectivity in the end will \nstill be determined by HyperNEAT.  =\r\n\n\nHowever, I think the customaizability of the substrate is a real \nopportu=\r\nnity.  With this capability we can just say, &quot;Look, here&#39;s \nhow you should =\r\nconceive the world, now take advantage of it.&quot;\n\nThe density choice is also =\r\npotentially powerful.  It means you can \ne.g. change the number of sensors =\r\non an old neural network and still \nhave the system work.  It means you can=\r\n increase the density of a \nbrain and still have it generally respect the s=\r\name pattern!  That is \nan unprecedented capability.  \n\nI believe the most s=\r\nignificant advantage of increasing density will \nindeed be a more powerful =\r\nkind of complexification.  It is unlikely \nthat adding one connection at a =\r\ntime will ever scale to millions or \nbillions of connections.  However, den=\r\nsity increases can \nrealistically scale by orders of magnitude.  The end go=\r\nal is not to \nbe able to simply do the same thing at higher resolution, but=\r\n to be \nable to further evolve something that mostly still works, now with =\r\n\nmillions more connections available for improving functionality.\n\n&gt; Ken, I=\r\n had another question related to varying node density on the \n&gt; substrate. =\r\n The CPNN may (likely) connect &#39;regions&#39; heavily, and \nso as \n&gt; node densit=\r\ny increases, so will these regional connections.  My \nquestion \n&gt; is, is th=\r\nere evidence of this in biology?  That is, of groups of \nneurons \n&gt; regiona=\r\nlly strongly connected to other groups of neurons?  I \nrealize \n&gt; it&#39;s prob=\r\nably mostly irrelevant, but I&#39;m curious if it&#39;s something \n&gt; you&#39;ve researc=\r\nhed.\n&gt; \n\nThere is evidencve for high density regional connections.  \n(i.e. =\r\n&quot;group-wise connectivity.&quot;)  For example, the optic nerve that \nconnects fr=\r\nom the back of the retina into the lateral geniculate \nnucleus has a number=\r\n of neurons proportional to the density of \nvisual resolution.  It has even=\r\n been shown that some animals are \nborn with more or less resolution (of va=\r\nrious types: rods, cones, \netc..) than their parents, and that the density =\r\nof the optic nerve \nadjusts proprtionally during development.  The evidence=\r\n for this is \ncited in our Taxonomy paper in the section on canalization; i=\r\nt \ninvolved experiments with wild cats.\n\nPerhaps the greatest group-wise co=\r\nnnector of all is the corpus \ncallosum, which connects one hemisphere of th=\r\ne brain to the other. \n\nWe have determined in unpublished work how group-wi=\r\nse connectivity \nis effectively represented by connective CPPNs.  Hopefully=\r\n we will \nbe able to publish that soon.\n\nken\n\n\n\n"}}