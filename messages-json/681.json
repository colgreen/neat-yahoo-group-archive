{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":174783294,"authorName":"Tyler Streeter","from":"Tyler Streeter &lt;tylerstreeter@...&gt;","profile":"tylerstreeter","replyTo":"LIST","senderId":"Qrcw-zYxzuYH__Wf6L7WhoeXHnCu95j7Alkv2xmLXHTS3EErOxAgmSK-4fMYKxAE17UBqnhQlt56Yr0QeuhpKJX97DA1z34liJJZREqlcA","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] autonomous virtual humans project","postDate":"1082562452","msgId":681,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDIwMDQwNDIxMTU0NzMyLjE0MzE1LnFtYWlsQHdlYjQwNTE0Lm1haWwueWFob28uY29tPg==","inReplyToHeader":"PDIwMDQwNDIwMjA1NDQ4LjU4MTI1LnFtYWlsQHdlYjkwMDA0Lm1haWwuc2NkLnlhaG9vLmNvbT4="},"prevInTopic":679,"nextInTopic":682,"prevInTime":680,"nextInTime":682,"topicId":657,"numMessagesInTopic":32,"msgSnippet":"... Assuming the walking behavior eventually works well... when I attempt to get them to walk through more complex environments, I plan to experiment with some","rawEmail":"Return-Path: &lt;tylerstreeter@...&gt;\r\nX-Sender: tylerstreeter@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 49993 invoked from network); 21 Apr 2004 15:48:33 -0000\r\nReceived: from unknown (66.218.66.167)\n  by m4.grp.scd.yahoo.com with QMQP; 21 Apr 2004 15:48:33 -0000\r\nReceived: from unknown (HELO web40514.mail.yahoo.com) (66.218.78.131)\n  by mta6.grp.scd.yahoo.com with SMTP; 21 Apr 2004 15:48:33 -0000\r\nMessage-ID: &lt;20040421154732.14315.qmail@...&gt;\r\nReceived: from [129.186.232.168] by web40514.mail.yahoo.com via HTTP; Wed, 21 Apr 2004 08:47:32 PDT\r\nDate: Wed, 21 Apr 2004 08:47:32 -0700 (PDT)\r\nTo: neat@yahoogroups.com\r\nIn-Reply-To: &lt;20040420205448.58125.qmail@...&gt;\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=us-ascii\r\nX-eGroups-Remote-IP: 66.218.78.131\r\nFrom: Tyler Streeter &lt;tylerstreeter@...&gt;\r\nSubject: Re: [neat] autonomous virtual humans project\r\nX-Yahoo-Group-Post: member; u=174783294\r\nX-Yahoo-Profile: tylerstreeter\r\n\r\n\n&gt; These inputs are all good, and I would personally\n&gt; think they&#39;d all be important.  But unless the agent\n&gt; is always going to be walking in a predictable,\n&gt; uniform environment (which I assume it is not),\n&gt; there&#39;s going to need to be some form of input\n&gt; describing the external environment.  Your agent is\n&gt; effectively blind and deaf, and could probably be\n&gt; trained to move across an even surface, but\n&gt; locomotion\n&gt; requires information about the path ahead, or else\n&gt; it&#39;s going to walk off cliffs and fall down a lot.\n\nAssuming the walking behavior eventually works well...\nwhen I attempt to get them to walk through more\ncomplex environments, I plan to experiment with some\nsort of vision input system (ray-casting or image\nprocessing).\n\n&gt; I&#39;d also be more concerned about your fitness\n&gt; function\n&gt; with complex behaviors like walking (or jumping\n&gt; without falling down).  With jumping and standing,\n&gt; you&#39;re simply using the average vertical position of\n&gt; the head over time to determine fitness...is this\n&gt; correct?\n\nStanding fitness function = average head height (each\ntrial has a maximum time and gets cut short if the\nhead goes below or above a certain threshold; penalty\nfor getting stopped early like this).\n\nJumping fitness function = highest head height\nachieved (stops early if head falls below a certain\nthreshold; penalty for stopping early).\n\n&gt; And with walking, are you simply measuring movement\n&gt; in\n&gt; a given horizontal direction?\n\nFor the demo I posted, yes.  Last semester I\nexperimented with several walking fitness functions. \n(I think they were being held back by an\noverly-unrealistic human model.)\n\nThings I tried for walking were combinations of these:\n\n* forward distance\n* penalty/early cutoff if torso fell below a threshold\n* bonus for # of steps taken (a foot going below a\nthreshold counted as a step)\n* only count alternating foot steps\n\nI&#39;ll mess with that more later.\n\n&gt; How hard is it to manually program one of your\n&gt; agents\n&gt; to walk or jump?  Have you done this?  If so, it\n&gt; might\n&gt; be interesting to evolve neural network controllers\n&gt; that best mimic the movements of the preprogrammed\n&gt; ones, to at least give them a certain level of\n&gt; proficiency that can then be built upon.\n\nThat would probably help.  It seems like that would\nhelp steer the search quite a bit in the early stages,\nat which point they could be set free from having to\nmimic the preprogrammed behavior.  I may try this some\nday.\n\nThanks for the suggestions!\n\nTyler\n\n\n\t\n\t\t\n__________________________________\nDo you Yahoo!?\nYahoo! Photos: High-quality 4x6 digital prints for 25ï¿½\nhttp://photos.yahoo.com/ph/print_splash\n\n"}}