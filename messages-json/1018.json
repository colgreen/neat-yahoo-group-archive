{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":7192225,"authorName":"Ian Badcoe","from":"Ian Badcoe &lt;ian_badcoe@...&gt;","profile":"ian_badcoe","replyTo":"LIST","senderId":"KqYpKuwDJJnrzGakwleg9DNqIGWN-C7lOH8pJE5D0HvknWsWXMWZkw1c4zbiKFj0Vj0e0jyJF3rnaKarJOF09QbJvMDtf5qH-Rk","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] Re: Computation Time","postDate":"1086646378","msgId":1018,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDUuMS4wLjE0LjAuMjAwNDA2MDcyMjAzMjcuMDBhOThlYzBAcG9wLm1haWwueWFob28uY28udWs+","inReplyToHeader":"PEJBWTItRjI3b2xwbG1Va0ptSTcwMDA0NDQxNEBob3RtYWlsLmNvbT4="},"prevInTopic":999,"nextInTopic":1031,"prevInTime":1017,"nextInTime":1019,"topicId":845,"numMessagesInTopic":99,"msgSnippet":"Hi, ... So it is a PERL script that writes a C function.  That s not a bad approach but you have to factor the time required for the C compile into your ","rawEmail":"Return-Path: &lt;ian_badcoe@...&gt;\r\nX-Sender: ian_badcoe@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 41595 invoked from network); 7 Jun 2004 22:11:46 -0000\r\nReceived: from unknown (66.218.66.218)\n  by m1.grp.scd.yahoo.com with QMQP; 7 Jun 2004 22:11:46 -0000\r\nReceived: from unknown (HELO cmailg3.svr.pol.co.uk) (195.92.195.173)\n  by mta3.grp.scd.yahoo.com with SMTP; 7 Jun 2004 22:11:46 -0000\r\nReceived: from modem-3240.llama.dialup.pol.co.uk ([217.135.188.168] helo=giles.yahoo.co.uk)\n\tby cmailg3.svr.pol.co.uk with esmtp (Exim 4.14)\n\tid 1BXSL5-0006zb-Px\n\tfor neat@yahoogroups.com; Mon, 07 Jun 2004 23:11:32 +0100\r\nMessage-Id: &lt;5.1.0.14.0.20040607220327.00a98ec0@...&gt;\r\nX-Sender: ian_badcoe@...\r\nX-Mailer: QUALCOMM Windows Eudora Version 5.1\r\nDate: Mon, 07 Jun 2004 23:12:58 +0100\r\nTo: neat@yahoogroups.com\r\nIn-Reply-To: &lt;BAY2-F27olplmUkJmI700044414@...&gt;\r\nMime-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;us-ascii&quot;; format=flowed\r\nX-eGroups-Remote-IP: 195.92.195.173\r\nFrom: Ian Badcoe &lt;ian_badcoe@...&gt;\r\nSubject: Re: [neat] Re: Computation Time\r\nX-Yahoo-Group-Post: member; u=7192225\r\nX-Yahoo-Profile: ian_badcoe\r\n\r\nHi,\n\nAt 12:44 04/06/2004 -0700, you wrote:\n\n&gt;What I intend to do is the equivalent of a partial-evaluator.  For a given\n&gt;network, the weights are fixed.  At least, for now...  So, I can leverage\n&gt;that knowledge.  I will write a script that takes in the definition of a\n&gt;network and writes out a function (or maybe a class) that calculates the\n&gt;result.  It will take in an array of node values (just doubles, not\n&gt;structures).  It won&#39;t have to look up the weights, the function will know\n&gt;the weights, they will be hard-coded.  It will not use any loops, it will\n&gt;fetch the value of a node, multiply it by the known weight, and add it to an\n&gt;accumulator.  Once it has finished the accumulation for one node, it will\n&gt;apply the appropriate (e.g. sigmoid) function to it, then save that as that\n&gt;node&#39;s new value.  The processing order will be such that an entire\n&gt;feed-forward network is calculated in a single pass.\n\nSo it is a PERL script that writes a C function.  That&#39;s not a bad approach \nbut you have to factor the time required for the C compile into your \nrun-time.  Compiling and linking a small C program is never going to take \nless than a few seconds...\n\nYou want to build the weights into the function as constants.  OK, but:\n\n1) constant data is not always treated as immediate data.  RISC CPUs do not \nhave immediate data.  And even if the CPU has it, the compiler is not \nrequired to use it.\n\n2) TANSTAAFL: even if you do turn the weights into immediate data that data \nhas to be stored somewhere and loaded into the CPU.  So immediate data in \nthe instructions increases the size of the function.  There&#39;s more bytes of \ninstructions to be loaded into the CPU and more micro-ops to be executed in \nthe CPU.\n\n&gt;I think the FASTEST way to do it is:\n&gt;\n&gt;initialize a register variable to 0\n&gt;get the value of the first node, multiply times weight, add to register\n&gt;do same for second, third, and fourth node.\n&gt;(weights and relative address of node values are hard-coded in function)\n&gt;apply sigmoid\n&gt;save variable to node value\n&gt;(repeat for all nodes)\n\nHuh?  Fastest compared to what?  There&#39;s just too little information here \nto say anything about how fast this is.  You are fetching the nodes in a \nrandom-access manner (fixed, sure, but not from incrementing memory \nlocations) so that&#39;s burning cache accesses.  Also, you may have the \nwritten-back result of one node required immediately for the next, which \ncan cause CPU stalls while it waits for the write to memory to \ncomplete.  Similarly, the summing into a single register of several \nmultiplication products can force stalls because later multiplication \nresults have to wait for the previous add to complete.\n\n&gt; &gt;3) when they were rolling out the extended SIMD instruction sets (MMX and\n&gt; &gt;the later floating-point version, which I think was called SMME -- for\n&gt; &gt;streaming multi-media extensions) they covered some problems quite like\n&gt; &gt;this in the tutorials they gave away.  They took a purely intel-centric\n&gt; &gt;view, of course, but many of their general points should apply.\n&gt;\n&gt;If there is a way to do it using those fancy extension instructions that\n&gt;might be faster, I&#39;d be happy to give it a try.  Though that might mean that\n&gt;my laptop (a P2) couldn&#39;t participate in the network evaluation.  But my\n&gt;workhorse system, a dual P4 xeon, should be able to, and if it gives a\n&gt;significant speed boost, I&#39;d use it there.\n&gt;\n&gt;You know where I can get my hands on one of those tutorials?\n\nThey may still be somewhere on the intel web-site.  Tutorials for their \nlatest stuff certainly will be.  I was not going to recommend moving to \nfancy instruction extensions however because:\n\n1) they are hard to use\n2) sometimes they are assembly only, or at-least you have to get the intel \nC compiler which I believe is no-longer free\n3) they only work on some CPUs\n4) you can get LSB differences in some contexts\n5) exp is not covered by them, so you&#39;ll still wait for that.\n\nBut the explanations of general CPU efficiency issues and, in particular, \nrearranging the order of processing and the order of data in memory might \nteach you a lot of what you need to know.\n\n&gt; &gt;4) since you are running the same network many times with different input\n&gt; &gt;data then you might get a big speed-up if you can re-arrange the order of\n&gt; &gt;processing.  I assume that at the moment...[snip]\n&gt;\n&gt;At the moment, I still haven&#39;t built it yet.  I&#39;m building an implementation\n&gt;of NEAT (in perl) that is conducive to distributed processing.  Once that&#39;s\n&gt;done, and tested using XOR and maybe pole-balencing, THEN I&#39;ll start working\n&gt;on the image enlargement experiment.\n\nAny sort of distributed processing will depend on being able to hand each \nremote processor a chunk which is large-enough to offset the network \ncommunications overheads.  For genetic approaches by far the most natural \nways to distribute involve splitting the population between different \nmachines (e.g. as separate &quot;tournaments&quot;, or even &quot;islands&quot;).\n\n&gt; &gt;you evaluate a whole network, then\n&gt; &gt;another whole network, then another, etc etc.  My first thought it to\n&gt; &gt;reorder this so that one link is evaluated for every point in the image,\n&gt; &gt;then the next link and so on.  This way you are performing a simple\n&gt; &gt;operation on big arrays (and it is important to sort the data so that it is\n&gt; &gt;in contiguous arrays).  Thus, if you had a trivial net:\n&gt; &gt;\n&gt; &gt;IMAGE --w1-&gt; NODE --w2-&gt; OUTPUT\n&gt; &gt;\n&gt; &gt;Then instead of.\n&gt; &gt;\n&gt; &gt;for (x,y) = every_point_in_image\n&gt; &gt;          NODE = IMAGE[x] * w[1];\n&gt; &gt;          NODE = ActivationFunction(NODE);\n&gt; &gt;          OUTPUT[x] = NODE * w[2];\n&gt; &gt;next x\n&gt; &gt;\n&gt; &gt;-- which has a random data access pattern (OK, so not very random but the\n&gt; &gt;more nodes you add the more it is) --\n&gt;\n&gt;The network will have 81 pixel inputs, a 9x9 grid, extraced from the image\n&gt;to be enlarged.  Technically, they won&#39;t be pixels, though, they will be\n&gt;&#39;relative brightness&#39; that have been scaled so that the brightest pixel is 1\n&gt;and the darkest is 0, with some exceptions.\n&gt;\n&gt;And it will have 4 other inputs, whose role is to specify a region in that\n&gt;space (upper-left to lower-right).  The network must approximate a function\n&gt;that is represented by the 81 pixel inputs, and then as output return the\n&gt;average of the area of that function represented by the region inputs.\n&gt;\n&gt;In order to encourage robustness, I will evaluate the network once for each\n&gt;of several different variations of each input.  Normal, negative/inverse,\n&gt;rotated, and mirrored.  I believe that there are 16 different possible\n&gt;combinations, or maybe it was 8.  For each such input, it will have to\n&gt;calculate a result given a large number of variations of the region, from\n&gt;larger to smaller, closer to center to farther to the outside.  I will\n&gt;weight the result by distance from the center.  I expect the network to be\n&gt;better at predicting the center-most pixels than it will be at the edges.\n&gt;\n&gt;It&#39;s a HUGE problem domain.  And it&#39;s a huge amount of work to evaluate the\n&gt;fitness of one single potential network.    I&#39;m going to have to take some\n&gt;shortcuts.  There are billions of possible input/output pairs possible\n&gt;within a single image, and I have thousands of images available to me.  So\n&gt;I&#39;m going to have to simplify.\n&gt;\n&gt;First of all, the network should be able to recreate its inputs.  If it\n&gt;can&#39;t do that, there&#39;s no chance it can do enlargement.  if I specify\n&gt;0,0,1,1 as the output region, it should output  a single value that\n&gt;corresponds to the average of the 81 inputs.  If I specify 1/8,1/8, 7/8,7/8,\n&gt;it should output a value corresponding to the average of the inner 49\n&gt;inputs.  Repeat down to the center 1 pixel.  If the fitness on that task was\n&gt;too low (not better than random), the testing on that input is done, and the\n&gt;result is 1 plus a fraction corresponding to how close it was.\n\nDo you know what the score of &quot;random&quot; would be, it&#39;s not always easy to \ncalculate in these complex cases.\n\n&gt;Otherwise, it continues evaluation, examining each possible 8x8, 7x7, 6x6,\n&gt;... 1x1 areas of the inputs, weighted by distance from center.  If it can&#39;t\n&gt;do better than random, it returns 2 plus a fraction.\n\nDo you really need to examine them all?\n\nAlso, why strain your evolution system making it evolve identical \nprocessing of each input separately.  You know that there is some \n&quot;symmetry&quot; between the input pixels.  I don&#39;t mean the image is symmetric, \nI mean there is no justification for treating all 81 inputs as requiring \n&quot;unique&quot; treatment.  e.g. what justification could there be for doing some \ncalculation on the top-left input, which you do not also do on the bottom \nright?\n\nAlso, also, you do realize in an information theoretic manner, than you are \nasking the network to invent information which is not in the input.  E.g. \nwhen the original image was averaged, information was deleted from it, and \nthere can be no process which -- given only the averaged image -- would \nrecreate the lost information (you can prove this because there&#39;s more than \none input image which would average to the same reduced image, and the \nnetwork can only produce _one_ output for that input).  So you could train \none network to recreate one image, but the network has no generality and \ndoes not apply to any other image (also, there&#39;s no point supplying it with \nthe averaged image in this case, since it could equally be trained to \nreproduce the image with no input).  Of course, if all the images are \nrelated in some way (all faces, all landscapes, all fractals) then there \nare more constraints and maybe the network can take advantage of those.\n\nAlso, also, also [:-)] are you justified in equating &quot;ability to average \nthe input pixels&quot; with &quot;ability to guess values between the input \npixels&quot;?  I did not question it at first, but the more I think about it, \nthe less it seems to be a logical connection.  You are supposing that any \nnetwork which can fill in between pixels will also be able to average.  Why?\n\n&gt;Now, the original image / expected results will be 144x144 pixels.  The\n&gt;inputs will be the 9x9 scaled down version of that.  Once it has gotten this\n&gt;far, it starts over, but looks more carefully.  instead of 1/8,1/8, 7/8,7/8,\n&gt;and repeating down towards the center, it can start at\n&gt;1/144,1/144,143/144,143/144 and work its way down.  If it can&#39;t do better\n&gt;than random, it returns 3 plus a fraction.\n&gt;\n&gt;Now, if it is still better than random, it evaluates the network for each of\n&gt;the 144x144 expected pixels, weighting the results by distance from center.\n&gt;If it can&#39;t do better than random, it returns 4 plus a fraction.\n&gt;\n&gt;If it got this far, it&#39;s a good network.  It manages to do a better than\n&gt;random job of 16x enlargement.  So to differentiate it from others that are\n&gt;equally good...\n\n16x seems like a _vast_ degree to start with, (your reduced image contains \nonly 1/256 of the original data) why not start with 18x18 reduced to 9x9?\n\n&gt;Evaluate it for every possible 2x2 area of the 144x144 expected space.  And\n&gt;3x3, and 4x4, and 5x5...15x15.  That will take a long time.The fitness then\n&gt;becomes 5 plus a fraction, where the fraction is between 0 and 1, where 0 is\n&gt;about the same as random, and 1 is &#39;perfect score!&#39;\n\nWhy are you asking the network to average pixels?  We already know how to \naverage pixels (and there are choices there like applying frequency \nfilters) so once the network can answer for an y of your 144x144 pixels, \nwhy not do the averaging by simple maths?\n\n&gt;And of course, the same network is evaluated against a mirrored, rotated,\n&gt;and negated version of that one input sample, making an average fitness for\n&gt;that one input.\n\nThat&#39;s a valid confirmation, but is it any more valid than simply moving on \nto another picture?\n\n&gt;There are TRILLIONS of those 9x9/144x144 test data pairs to choose from.\n&gt;Obviously, I can&#39;t evaluate each network against all of them.  And I should\n&gt;probably evaluate each network against the same ones.  So what I should\n&gt;probably do is:\n&gt;\n&gt;pick an input sample at random.\n&gt;evaluate all networks against that input sample.\n&gt;do\n&gt;     pick another at random\n&gt;     evaluate all networks against that input sample\n&gt;     calculate the average fitness of each network so far\n&gt;     order the organisms by their relative fitness values\n&gt;     calculate who would pass on their genes to the next generation\n&gt;while the list of who passes on their genes has changed\n&gt;\n&gt;Initially, no one passes on their genes, so there will be a minimum of 3\n&gt;input evaluations per generation.\n&gt;\n&gt;Now, since the input and the structure AND the weights are all known for a\n&gt;LARGE number of activations of the network, I may decide to do further\n&gt;optimization of the evaluation of one network.  I may generate code based on\n&gt;constants for input values, so that the compiler can optimize the heck out\n&gt;of the function.  Yeah...I like it...  :)\n&gt;\n&gt;Have a script generate a .c file that contains the dynamic &#39;evaluate\n&gt;network&#39; function given the expected output range.  The rest of the C source\n&gt;is a static algorithm like what was described above.  Initially, since\n&gt;fitnesses will be low and execution times will therefore be shorter, I just\n&gt;compile and run it locally.  Once a solution begins to appear, I can make it\n&gt;work in a distributed fashion, bzipping the source and shipping it out to\n&gt;client machines which compile it, run it, and return the results.  The\n&gt;closer we get to a solution, the slower it progresses, but the more\n&gt;resources I can employ in the search.\n\nI think you&#39;ll need to run generations faster than that.  Also, on a \npractical note, when I start these experiments, I never get them right \nfirst time, but I don&#39;t discover the errors until I&#39;ve run them a bit, so \nthat&#39;s another reason why you need a fast turn around.\n\n&gt;So, there&#39;s a &quot;brain dump&quot; of my thoughts on how to go about this\n&gt;experiment.  Feedback on any aspect of it is welcome, not just on optimizing\n&gt;the network evaluation execution time.  :)\n&gt;\n&gt;\n&gt;\n&gt; &gt;You do this:\n&gt; &gt;\n&gt; &gt;// DATA contains IMAGE sorted into a linear array\n&gt; &gt;\n&gt; &gt;for x = 0 to number_of_pixels\n&gt; &gt;          NODE[x] = DATA[x] * w[1]\n&gt; &gt;next x\n&gt; &gt;for x = 0 to number_of_pixels\n&gt; &gt;          NODE[x] = ActivationFunction(NODE[x])\n&gt; &gt;next x\n&gt; &gt;for x = 0 to number_of_pixels\n&gt; &gt;          OUTPUT[x] = NODE[x] * w(2)\n&gt; &gt;next x\n&gt; &gt;\n&gt; &gt;-- which accesses data as three straight runs through a set of contiguous\n&gt; &gt;arrays.\n&gt; &gt;\n&gt; &gt;The change that makes to data caching can be considerable.  If you like,\n&gt; &gt;I&#39;ll explain in more detail off list, unless this is of general\n&gt; &gt;interest?  It&#39;s one of the things people used to do to get oomph out of the\n&gt; &gt;vector-pipelines on supercomputers...\n&gt;\n&gt;I&#39;d love to hear more details, on- or off-list, depending on other people&#39;s\n&gt;interest.\n\nLet&#39;s wait for them to throw us off.\n\nCertainly any system for &quot;early out&quot; elimination of whole networks is a \ngood thing.  But that&#39;s a high-level speed-up and we were talking low-level \nstuff.  E.g. if your project succeeds, you must at some point arrive at \nnetworks which don&#39;t get eliminated by any of the early stages.  And then \nyou need to do the heavy work efficiently.\n\nWhat I was proposing was a way to rearrange the order of processing to \nimprove memory access.\n\nE.g.  The routine you outlined above does this (you unrolled it and \nhard-coded the addresses and weights but that doesn&#39;t change the node and \nimaged data access patterns)\n\nfor each 9x9 part of image\n   for each test case\n     for each internal node\n       clear_register\n       for each link on node\n         if from_node\n           get_node_value\n         else\n           get_image_value\n         endif\n         multiply\n         sum\n       next\n       activation_function\n     next\n   next\nnext\n\nwhich is essentially a random-order memory access pattern.  On the other \nhand, each node&#39;s equation can be broken down into a set of sub \nequations.  Thus:\n\nNode7 = Activate(Node4 * w23 + Node3 * w24 + Image(1,3) * w25)\n\nbreaks into:\n\nT1 = Node4 * w23\nT2 = Node3 * w24\nT3 = Image(1,3)\nT4 = T3 * w25\nT5 = T1 + T2 + T4\nNode7 = Activate(T5)\n\nWhich is algebraically and computationally identical.  (T is for temp)\n\nNow, we could stick that hacked-up equation into our original loops:\n\nfor each 9x9 part of image\n   for each test case\n     // some nodes already processed\n     T1 = Node4 * w23\n     T2 = Node3 * w24\n     T3 = Image(1,3)\n     T4 = T3 * w25\n     T5 = T1 + T2 + T4\n     Node7 = Activate(T5)\n     // and similarly for other internal nodes\n   next\nnext\n\nWhich is essentially your current plan, however, the memory access is still \nrandom, however, if each of those temporary variables is converted into an \narray, then you can do this:\n\nfor each 9x9 part of image\n   for each test case\n     // some nodes already processed\n   next\n   for each test case\n     T1[case] = Node4[case] * w23\n   next\n   for each test case\n     T2[case] = Node3[case] * w24\n   next\n   for each test case\n     T3[case] = Image_1_3[case]\n   next\n   for each test case\n     T4[case] = T3[case] * w25\n   next\n   for each test case\n     T5[case] = T1[case] + T2[case] + T4[case]\n   next\n   for each test case\n     Node7[case] = Activate(T5[case])\n   next\n   for each test case\n     // and similarly for other internal nodes\n   next\nnext\n\nAnd that has done exactly the same processing as our original loop, but \nonly ever accessed memory in a strictly incremental order.  Which, since \nmemory accesses are far slower than CPU cycles, should (repeat should) be \nfaster.\n\nThere is an extra overhead, in that you have to presort the image data into \n81 arrays (image_1_1 to image_9_9) with a different test case in each slot, \nand you may need to do something similar in reverse to read the output back \ninto a format you like, but even so...\n\n&gt; &gt;p.s. What language are you using?\n&gt;\n&gt;Probably C, for speed.  Except for the actual NEAT code, which will be Perl,\n&gt;for coding convenience.   I&#39;d generate ASM code if I thought it could help!\n&gt;:)\n\nThat&#39;s good, but never go to ASM before it is proven you need to.\n\n         Ian B\n\n\n\nhttp://livingathome.sourceforge.net/ - evolution for the desktop\n\n\n\n"}}