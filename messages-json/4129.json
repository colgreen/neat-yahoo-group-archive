{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"veRcBqbLpiySsiGtB11mdqm7hEVcsSZHIt4eFFm1GPPjFgsiXcjkx_TH-kHyxn3nuThiz5vHjWFbKrX-0kdRSaQXf9TbEMLGXpxrmhyD1ifC","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Can novelty search be treated as an optimization technique?","postDate":"1212718742","msgId":4129,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGcyYTZxbSsyaTY1QGVHcm91cHMuY29tPg==","inReplyToHeader":"PDIzMGU0NjNlMDgwNTMwMDUxMHExZDJkNjZiYnQxOTdmMjE5N2FiMmU5OTg0QG1haWwuZ21haWwuY29tPg=="},"prevInTopic":4127,"nextInTopic":4130,"prevInTime":4128,"nextInTime":4130,"topicId":4113,"numMessagesInTopic":6,"msgSnippet":"Julian, Thanks for the further insight on CMA-ES.  I also think there may be a way to combine it properly with a variable-dimension encoding. Somehow the","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 55326 invoked from network); 6 Jun 2008 02:19:04 -0000\r\nX-Received: from unknown (66.218.67.96)\n  by m41.grp.scd.yahoo.com with QMQP; 6 Jun 2008 02:19:04 -0000\r\nX-Received: from unknown (HELO n11d.bullet.scd.yahoo.com) (66.218.67.60)\n  by mta17.grp.scd.yahoo.com with SMTP; 6 Jun 2008 02:19:04 -0000\r\nX-Received: from [66.218.69.3] by n22.bullet.scd.yahoo.com with NNFMP; 06 Jun 2008 02:19:03 -0000\r\nX-Received: from [66.218.66.73] by t3.bullet.scd.yahoo.com with NNFMP; 06 Jun 2008 02:19:03 -0000\r\nDate: Fri, 06 Jun 2008 02:19:02 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;g2a6qm+2i65@...&gt;\r\nIn-Reply-To: &lt;230e463e0805300510q1d2d66bbt197f2197ab2e9984@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: Re: Can novelty search be treated as an optimization technique?\r\nX-Yahoo-Group-Post: member; u=54567749; y=satlogwTYyT-2nav3U0seKytCOoirbZCy8C46NatQIoH4nOcXAkB\r\nX-Yahoo-Profile: kenstanley01\r\n\r\nJulian,\n\nThanks for the further insight on CMA-ES.  I also think there may =\r\nbe a\nway to combine it &quot;properly&quot; with a variable-dimension encoding. \nSome=\r\nhow the dimensions for which there is no information (e.g. because\nthey wer=\r\ne just added) need to be possible to incorporate into the\nexisting model se=\r\namlessly on the fly, instead of starting over again.\n It seems like in prin=\r\nciple that should be possible?\n\nAnyway, like you said, it is still not a pa=\r\nnacea since it can be greedy.\n\nken\n\n--- In neat@yahoogroups.com, &quot;Julian To=\r\ngelius&quot; &lt;julian@...&gt; wrote:\n&gt;\n&gt; Hi Peter and Ken,\n&gt; \n&gt; We actually just rec=\r\nently combined CMA-ES with topology search (though\n&gt; not NEAT-based) in a p=\r\naper submitted to PPSN. I&#39;ll make the paper\n&gt; available as soon as it&#39;s acc=\r\nepted and revised, but send me a private\n&gt; mail if you want a draft before =\r\nthat.\n&gt; \n&gt; Basically, what we found in this respect is that for two standar=\r\nd\n&gt; reinforcement learning problems, our &quot;memetic CMA-ES&quot; did much worse\n&gt; =\r\nthan standard CMA. This is probably because it takes time to get a\n&gt; good c=\r\novariance matrix, typically many hundreds of evaluations, and\n&gt; everytime y=\r\nou change the topology you have to restart the CMA-ES.\n&gt; However, in a vers=\r\nion of the problem with lots of extra inputs (and\n&gt; thus lots of deceptiven=\r\ness) the memetic CMA-ES could outperform both a\n&gt; normal CMA-ES and a memet=\r\nic ES.\n&gt; \n&gt; Finding a way of dealing &quot;properly&quot; with varying number of dime=\r\nnsions\n&gt; would be very interesting indeed. But I also agree with Ken that\n&gt;=\r\n CMA-ES is not a panacea in any way, and in fact might be too greedy\n&gt; for =\r\nmany problems.\n&gt; \n&gt; By the way, I just dropped down in Hong Kong - any memb=\r\ners of this\n&gt; group attending WCCI? Maybe even know of any interesting pape=\r\nrs being\n&gt; presented?\n&gt; \n&gt; Julian\n&gt; \n&gt; \n&gt; On 26/05/2008, Kenneth Stanley &lt;k=\r\nstanley@...&gt; wrote:\n&gt; &gt;\n&gt; &gt;   Peter,\n&gt; &gt;\n&gt; &gt; I also think NEAT could work w=\r\nith a CMA-ES (or some kind of EDA, which\n&gt; &gt; seems to do something similar)=\r\n. While EANT cycles through phases of\n&gt; &gt; topology and weight optimization,=\r\n I think maybe there is a more\n&gt; &gt; elegant way to just combine the two by d=\r\nealing properly with variable\n&gt; &gt; numbers of dimensions in the CMA-ES.\n&gt; &gt;\n=\r\n&gt; &gt; Anyway, that&#39;s not the main point you&#39;re making. Your main question\n&gt; &gt;=\r\n is whether something like CMA-ES, which focuses on optimization, can\n&gt; &gt; b=\r\ne used in a novelty search. Of course, novelty search is\n&gt; &gt; philosophicall=\r\ny almost the antithesis of optimization. However, in\n&gt; &gt; practice the searc=\r\nh for novelty itself could be viewed as the\n&gt; &gt; objective, and then, in pri=\r\nnciple, an optimization method might be\n&gt; &gt; able to optimize to that end.\n&gt;=\r\n &gt;\n&gt; &gt; It is true that from one generation to the next, the scores change a=\r\nnd\n&gt; &gt; therefore you cannot rely on a constant gradient. However, isn&#39;t it\n=\r\n&gt; &gt; true that CMA-ES (and EDAs) computes its covariance matrix from only\n&gt; =\r\n&gt; the current generation (i.e. current population). If so, it could\n&gt; &gt; com=\r\npute how genetic parameters correlate to novelty scores, and\n&gt; &gt; thereby tr=\r\ny to optimize novelty for that particular population. In\n&gt; &gt; other words, i=\r\nn a fixed population, I don&#39;t see a reason it couldn&#39;t\n&gt; &gt; be applied (and =\r\nthen reapplied for the next population all over\n&gt; &gt; again). However, I gues=\r\ns you are saying that you can&#39;t just launch a\n&gt; &gt; &quot;phase&quot; of CMA-ES on its =\r\nown since it would not optimize topologies.\n&gt; &gt; But I don&#39;t think that woul=\r\nd necessarily matter. Still, even better\n&gt; &gt; would be to seamlessly combine=\r\n the CMA-ES with NEAT instead of\n&gt; &gt; splitting them into phases.\n&gt; &gt;\n&gt; &gt; Wh=\r\nether or not CMA-ES works better than stochastic mutation over the\n&gt; &gt; long=\r\n run is a different question, but it&#39;s a question even in regular\n&gt; &gt; objec=\r\ntive-based optimization. I actually think that CMA-ES and the\n&gt; &gt; like are =\r\ngreedy, although you say it is not. It computes the most\n&gt; &gt; promising vect=\r\nor based exclusively on the current distribution. Thus\n&gt; &gt; it is in a sense=\r\n overly focused on the initial population\n&gt; &gt; distribution, which may be hi=\r\nghly misleading with respect to the\n&gt; &gt; ultimate objective. Basically, it&#39;s=\r\n the problem of deception as\n&gt; &gt; usual, but I think CMA-ES is even more sus=\r\nceptible to it because it\n&gt; &gt; actively seeks out a model of the deceptive d=\r\nistribution. That is\n&gt; &gt; pretty greedy in my view.\n&gt; &gt;\n&gt; &gt; Now, if the prob=\r\nlem is not deceptive, then that is great, because the\n&gt; &gt; model will be way=\r\n better than random perturbations. So you will find\n&gt; &gt; some problems where=\r\n it will be fantastic. But in highly deceptive\n&gt; &gt; problems I am not sure. =\r\nSome of the results for CMA-ES reported to\n&gt; &gt; date may be misleading becau=\r\nse they do not focus on deceptive\n&gt; &gt; problems. One way to see this fact is=\r\n that they use extremely small\n&gt; &gt; populations. That simply cannot work wel=\r\nl in a highly deceptive\n&gt; &gt; domain. In fact, as you can see in my dissertat=\r\nion, regular NEAT is\n&gt; &gt; also extremely fast at pole balancing problems wit=\r\nh a tiny population.\n&gt; &gt; So that tells you more about pole balancing having=\r\n a really large\n&gt; &gt; basin of attraction than about either method in general=\r\n.\n&gt; &gt;\n&gt; &gt; Anyway, I think ultimately that it can be combined with NEAT and =\r\nthat\n&gt; &gt; would be interesting, and such a system could even work with novel=\r\nty\n&gt; &gt; search. Whether it could work better is an open question.\n&gt; &gt;\n&gt; &gt; ke=\r\nn\n&gt; &gt;\n&gt; &gt; --- In neat@yahoogroups.com &lt;neat%40yahoogroups.com&gt;,\n&quot;peterberri=\r\nngton&quot;\n&gt; &gt; &lt;peterberrington@&gt;\n&gt; &gt; wrote:\n&gt; &gt; &gt;\n&gt; &gt; &gt; I&#39;ve been tinkering wi=\r\nth many optimization techniques and\nconsidering\n&gt; &gt; &gt; the possibility of in=\r\ntegrating some with the mechanics of neat,\nin the\n&gt; &gt; &gt; vein of EANT, which=\r\n sadly has no open source implementation.\n&gt; &gt; &gt;\n&gt; &gt; &gt; While I&#39;m trying to a=\r\ndd that functionality to the neat-python\n&gt; &gt; &gt; implementation I use, I&#39;m re=\r\nally wondering if its at all\napplicable to\n&gt; &gt; &gt; novelty search. In novelty=\r\n search, there is a specific pressure\nto do\n&gt; &gt; &gt; something new and indeed =\r\nwe do assign a numeric novelty value to\neach\n&gt; &gt; &gt; individual behaviour at =\r\nthe time of its evaluation for addition\nto the\n&gt; &gt; &gt; archive. However, the =\r\nscore an individual receives depends on\nwho its\n&gt; &gt; &gt; up against, so its no=\r\nt objective. Without a function to\nminimize, the\n&gt; &gt; &gt; dynamics which allow=\r\n optimization techniques to work may be\ndamaged to\n&gt; &gt; &gt; the point of rende=\r\nring it no better than random search.\n&gt; &gt; &gt;\n&gt; &gt; &gt; I&#39;m still puzzled on the =\r\ndiscussion Peter C raised about\nreevaluating\n&gt; &gt; &gt; the novelty of behaviour=\r\ns in the archive at future intervals.\n&gt; &gt; &gt; Shouldn&#39;t the novelty score ass=\r\nigned to each point be useless after\n&gt; &gt; &gt; we&#39;ve decided whether or not to =\r\narchive the behaviour? I&#39;m not sure\n&gt; &gt; &gt; that joint angles over every time=\r\nstep is really the best way the\n&gt; &gt; &gt; characterize behaviour for a 3d ragdo=\r\nll rig, as you are essentially\n&gt; &gt; &gt; providing no clear way to distinguish =\r\na good behaviour from a bad\n&gt; &gt; &gt; behaviour. Although I haven&#39;t been able t=\r\no use optimized libraries\n&gt; &gt; &gt; with my current simulation configuration (t=\r\nhis is hopefully changing\n&gt; &gt; &gt; very soon), I have noticed a significant bo=\r\nost in speed with my\n&gt; &gt; &gt; ragdoll evolution by defining behaviour simply a=\r\ns the final x y z\n&gt; &gt; &gt; triplet for the center of mass. In this way the sea=\r\nrch quickly\n&gt; &gt; &gt; exhausts all the easy ways of falling close to its origin=\r\n and\npressure\n&gt; &gt; &gt; mounts for it explore end locations successively farthe=\r\nr and farther\n&gt; &gt; &gt; away from its origin; in essence the objective function=\r\n is\nrealized in\n&gt; &gt; &gt; that characterization of behavioural distance.\n&gt; &gt; &gt;\n=\r\n&gt; &gt; &gt; In any case, we are explicitly trying to reward novelty by selecting\n=\r\n&gt; &gt; &gt; for further evaluation any individuals which satisfy a minimum\n&gt; &gt; &gt; =\r\nthreshold of how different their behaviour is. Something about that\n&gt; &gt; &gt; g=\r\nives me a feeling that maybe optimization techniques are\napplicable,\n&gt; &gt; &gt; =\r\nbut only if the dynamics of optimization can be adapted to the\n&gt; &gt; &gt; unwiel=\r\ndy workings of novelty search.\n&gt; &gt; &gt;\n&gt; &gt; &gt; Rather than maximizing fitness, =\r\noptimization techniques usually\n&gt; &gt; &gt; minimize an objective function so the=\r\n fitness scale is simply\nreversed\n&gt; &gt; &gt; with a perfect score being 0.0\n&gt; &gt; =\r\n&gt; Since in this domain theres no such thing as perfect novelty, can\n&gt; &gt; &gt; a=\r\nnyone provide any insights as to a framework where optimization\n&gt; &gt; &gt; techn=\r\niques can be harnessed to maximize novelty? I feel this is a\n&gt; &gt; &gt; really i=\r\nmportant area to focus attention on, as it could potentially\n&gt; &gt; &gt; lead to =\r\na dramatic increase in the fitness per number of\nevaluations.\n&gt; &gt; &gt;\n&gt; &gt; &gt; A=\r\ns some background: I&#39;ve long been tempted after reading papers on\n&gt; &gt; &gt; EAN=\r\nT to try dropping in a more advanced optimization function into\n&gt; &gt; &gt; NEAT,=\r\n like some of the new variations on the covariance matrix\n&gt; &gt; &gt; adaption ev=\r\nolution strategy. The way this was implemented in\nEANT was\n&gt; &gt; &gt; by splitti=\r\nng NEATs main loop into a &quot;structural exploration&quot;\nloop for\n&gt; &gt; &gt; building =\r\nnetworks, and optimizing the weight connection values\n&gt; &gt; &gt; separately with=\r\nin a nested loop, using CMA-ES. This division of work\n&gt; &gt; &gt; permits the net=\r\n to be treated as an n-dimensional equation where\nn is\n&gt; &gt; &gt; the number of =\r\nconnection or node weights (or other properties)\nwe wish\n&gt; &gt; &gt; to optimize =\r\n(i.e. everything that isn&#39;t defining the topology of the\n&gt; &gt; &gt; net). I thin=\r\nk it can be argued that standard fitness based neat is a\n&gt; &gt; &gt; greedier app=\r\nroach because it cartwheels through topological\nspace and\n&gt; &gt; &gt; weight para=\r\nmeter space at the same time. I want to plug in an\n&gt; &gt; &gt; optimization funct=\r\nion, but I can&#39;t think of how anymore, or even if\n&gt; &gt; &gt; the two search tech=\r\nniques are reconcilable.\n&gt; &gt; &gt;\n&gt; &gt; &gt; Theres no point optimizing within a sp=\r\necific net topology in novelty\n&gt; &gt; &gt; search because you want to kind of pus=\r\nh out and fill behavioural\nspace\n&gt; &gt; &gt; as evenly as possible, so theres no =\r\nnook or crack which escapes the\n&gt; &gt; &gt; poking and prodding of your search; w=\r\nithout the\n&gt; &gt; &gt; competition/coevolutionary aspect of defining fitness agai=\r\nnst the\n&gt; &gt; &gt; backdrop of current rivals and ancestors, I doubt you&#39;d see a=\r\nnything\n&gt; &gt; &gt; more effective than random selection. If you can&#39;t optimize e=\r\nach net\n&gt; &gt; &gt; topology in a vacuum, is there another way fitness-based\nsear=\r\nches and\n&gt; &gt; &gt; novelty-based searches can reach a compromise where each com=\r\nplements\n&gt; &gt; &gt; the other?\n&gt; &gt; &gt;\n&gt; &gt; &gt; I&#39;ve given some thought to the idea o=\r\nf phased searching, but you\nwould\n&gt; &gt; &gt; have to find some way of defining s=\r\ntopping critera for when\nyou&#39;d want\n&gt; &gt; &gt; to optimize an objective function=\r\n and when you&#39;d want to diversify\n&gt; &gt; &gt; from bottom up complexity wise.\n&gt; &gt;=\r\n &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt; As an interesting side note, to anyone whos had a sneak peak=\r\n at\nPeter\n&gt; &gt; &gt; C&#39;s maze navigation novelty search program, isn&#39;t it uncann=\r\ny, the\n&gt; &gt; &gt; resemblance between the past behaviours that have accumulated,=\r\n\nand the\n&gt; &gt; &gt; growth of a plant? As more organisms fill up the space of po=\r\nssible\n&gt; &gt; &gt; behaviours its tempting to imagine it as molasses which slowly=\r\n oozes\n&gt; &gt; &gt; its way into every crack and crevice it can reach. Perhaps the=\r\nre\nis a\n&gt; &gt; &gt; useful insight to be drawn from that, I have no idea, I just =\r\nthought\n&gt; &gt; &gt; it looked very &quot;organic&quot;.\n&gt; &gt; &gt;\n&gt; &gt;\n&gt; &gt; \n&gt; &gt;\n&gt; \n&gt; \n&gt; \n&gt; -- \n&gt;=\r\n Julian Togelius\n&gt; IDSIA\n&gt; Galleria 2\n&gt; 6928 Manno-Lugano\n&gt; Switzerland\n&gt; j=\r\nulian@...\n&gt; http://julian.togelius.com\n&gt; http://www.idsia.ch/~togelius\n&gt; +4=\r\n1-764-110679\n&gt; +46-705-192088\n&gt;\n\n\n\n"}}