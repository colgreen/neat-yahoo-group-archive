{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":279942280,"authorName":"mneylon01","from":"&quot;mneylon01&quot; &lt;mneylon01@...&gt;","profile":"mneylon01","replyTo":"LIST","senderId":"m_wB9jD2FhP3Rq4LoQcC6dAXK8a8cIe7C_RoFjTJ6aDyOVy9St8pKI2tMTvvgfVoA39LtEqBJ6ZohzQSVh0f4Onja-Ny2xNU","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: NEAT and highly recurrent networks","postDate":"1156792464","msgId":2717,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGVjdmZhZytqcWtmQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGVjdDBtOSs1NWtyQGVHcm91cHMuY29tPg=="},"prevInTopic":2714,"nextInTopic":2723,"prevInTime":2716,"nextInTime":2718,"topicId":2711,"numMessagesInTopic":7,"msgSnippet":"So I ve had a chance to do what Ken suggested below - start from the basic (2+1bias)-1-1 network for the static XOR problem, randomizing the test data, and","rawEmail":"Return-Path: &lt;mneylon01@...&gt;\r\nX-Sender: mneylon01@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 57340 invoked from network); 28 Aug 2006 19:15:38 -0000\r\nReceived: from unknown (66.218.66.218)\n  by m36.grp.scd.yahoo.com with QMQP; 28 Aug 2006 19:15:38 -0000\r\nReceived: from unknown (HELO n20c.bullet.sc5.yahoo.com) (66.163.187.211)\n  by mta3.grp.scd.yahoo.com with SMTP; 28 Aug 2006 19:15:38 -0000\r\nReceived: from [66.163.187.123] by n20.bullet.sc5.yahoo.com with NNFMP; 28 Aug 2006 19:14:26 -0000\r\nReceived: from [66.218.69.1] by t4.bullet.sc5.yahoo.com with NNFMP; 28 Aug 2006 19:14:25 -0000\r\nReceived: from [66.218.66.73] by t1.bullet.scd.yahoo.com with NNFMP; 28 Aug 2006 19:14:25 -0000\r\nDate: Mon, 28 Aug 2006 19:14:24 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;ecvfag+jqkf@...&gt;\r\nIn-Reply-To: &lt;ect0m9+55kr@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0\r\nFrom: &quot;mneylon01&quot; &lt;mneylon01@...&gt;\r\nSubject: Re: NEAT and highly recurrent networks\r\nX-Yahoo-Group-Post: member; u=279942280; y=dirL9QQOQTJyH-UBOVPskaYpRCtJxAVbWWKqZlIO5TmZLuhn\r\nX-Yahoo-Profile: mneylon01\r\n\r\nSo I&#39;ve had a chance to do what Ken suggested below - start from the\nbasic =\r\n(2+1bias)-1-1 network for the static XOR problem, randomizing\nthe test data=\r\n, and disabling any node or connection creation or\ntoggling to see how my n=\r\network worked.  I&#39;m using what I believe are\nthe same parameters in Ken&#39;s v=\r\narious papers: 80% chance of a child&#39;s\nweights being changed, with 90% chan=\r\nce of perturbation, 10% chance of\nmutation.  I used 150 species, and for th=\r\ne species calculation, I used\n1.0 for the excess and disjoint coefficients,=\r\n 0.4 for the common\nweight differences, and set N to 1 (as noted by Ken bel=\r\now), with the\ntolerance set to 3.0.\n\nWhat was interesting is that I found t=\r\nhere to be a VERY strong\nconnection between two parameters that I don&#39;t see=\r\n mentioned much in\nthe papers: the weight cap, and the type and amount of p=\r\nerturbation or\nmutation done.   (Note that I&#39;ve tried both the sigmoid stee=\r\npness of\n4.9 and 1, this seems to have less of an impact that these other\np=\r\narameters).\n\nMy original code capped the weight at 2, which I saw slow if a=\r\nny\napproach to a good network.  Increasing the cap to insane values like\n10=\r\n0 gave a great speed to evolution convergence, as this allowed for a\nwider =\r\nrange of weight values to be explored by the NEAT method, but\nleads to some=\r\n networks that one questions their ability to work.  \nUsing a cap of 5 to 1=\r\n0 gave a more reasonable set of networks while\nstill converging quickly (ab=\r\nout 50 generations to get the RMS under\n0.1 on the 4 cases).  I&#39;ve seen the=\r\n cap of 5 tossed around here\nbefore, but does this seem to be a good practi=\r\ncal number?\n\nAlso, with my weight mutations, I found that using gaussian\ndi=\r\nstributions for both initial weights and mutated weights, and\npertubations =\r\nin weights was better than straight random.  (Presently\nI&#39;m using N rolls o=\r\nf the RNG and getting the average to simulate\ngaussian distribution, with l=\r\narger N to reduce the standard\ndeviation).  The quesiton I have, since I&#39;m =\r\nhaving trouble seeing how\nit&#39;s done in either the C++ or C# code, is what t=\r\nhe good effective\nmagnitude is.  The above cases, for the mutated/initial w=\r\neights, I&#39;m\nusing a gaussian random number from -5 to 5, std deviation of a=\r\nbout 1,\nand for the perturbed weights, from -5 to 5 with a std deviation of=\r\n\nabout 0.1-0.5.    These cases seem to work, but I can&#39;t seem how they\neasi=\r\nly match up with the above code.  Even with the weight caps, I\nalways seeme=\r\nd to have weights that wanted to hit the caps.  Any good\nsuggestions on wha=\r\nt are good weight ranges and adjustments to make. \nAnother thing I noticed =\r\nis that if the weight cap is too low, using\nthe values for the species tole=\r\nrance calculation I give above, I\nrarely got more than 1 or two species.\n\nA=\r\ndditionally, I find it interesting in how one approaches the input\ndata par=\r\nt.  I tried the standard XOR with a fixed set of data, and it\ncan converge =\r\nquickly, but when you then make the data randomly\npresented (re-evaluating =\r\nweights of genomes that carry over to account\nfor repeated runs), the perfo=\r\nrmance of the NEAT method is poor.   That\nis, when fixed, I almost always g=\r\not a recurrent connection (I don&#39;t\nprevent these from being formed), sugges=\r\nting it&#39;s trying to mimic the\ninput pattern order and not the actual data p=\r\natterns.  I found that if\na random order was used, using a larger test patt=\r\nern (in my case, 50\ninitially randomly generated XOR cases and then present=\r\ned in a random\norder to the networks) seemed to help.  I still get some rec=\r\nurrancies,\nbut the network size seemed to stay small and avoid new node\nfor=\r\nmation.  (Obviously, when I use recurrent data as like the delayed\nXOR prob=\r\nlem, the data order will be fixed, but the size of the data\nset needs to be=\r\n large enough that it would be impractical for the\ngenome to try to learn t=\r\nhe overall data set pattern as opposed to the\nactual pattern that the datas=\r\net contains...)\n\n--- In neat@yahoogroups.com, &quot;Kenneth Stanley&quot; &lt;kstanley@.=\r\n..&gt; wrote:\n&gt;\n&gt; It sounds like the main issue before you can really start lo=\r\noking at \n&gt; delayed-XOR is to get XOR working more closely to my own NEAT\n&gt;=\r\n (and others developed since).  Certainly the 500-1000 generations \n&gt; it&#39;s =\r\ntaking you indicates somethign is likely wrong in your \n&gt; implementation.\n&gt;=\r\n \n&gt; I would suggest the following experiment:  Start evolution with  a \n&gt; p=\r\nopulation of networks that already have the correct topology for a \n&gt; solut=\r\nion to XOR and turn off structural mutations.  Then run your \n&gt; version of =\r\nNEAT as usual, except in this case, it will only be \n&gt; searching over weigh=\r\nt space.  If it takes forever, it tells you that \n&gt; the problem is in the w=\r\nay your weights are being mutated, or the way \n&gt; they are being combined in=\r\n crossover.  It may also indicate a \n&gt; problem in speciation (related to we=\r\night comparison).  In any case, \n&gt; it will greatly narrow down the problem.=\r\n\n&gt; \n&gt; Once you get it working on pure weight-evolution, then you can move \n=\r\n&gt; to the normal topology evolution, and you will either see it work \n&gt; righ=\r\nt away, or find that there is a problem in adding structure as \n&gt; well.\n&gt; \n=\r\n&gt; About compatibility testing for speciation, when you menion &quot;N,&quot; do \n&gt; yo=\r\nu mean the normalization term in my papers?  Regrettably, many \n&gt; people mi=\r\nss that I did not use N (for normalization) in practice, \n&gt; that is, I set =\r\nN to 1 in all cases.  That may explain why you using \n&gt; the same coefficien=\r\nts as me does not work.  If you look at my papers \n&gt; closely, they say that=\r\n N can be set to 1 if genomes are not too \n&gt; large.  I have found in practi=\r\nce that it always works fine with it \n&gt; set to 1, so that&#39;s what I&#39;ve done.=\r\n  This confusion is my fault and \n&gt; I apologize for it- the papers should b=\r\ne more clear- but I wanted \n&gt; people to be aware of N and the option for no=\r\nrmalization in case it \n&gt; indeed does come into play with very large genome=\r\ns.\n&gt; \n&gt; As for deviations from typical values, you can see all the values \n=\r\n&gt; I&#39;ve used in the appendix to my dissertation.  There is some \n&gt; explanati=\r\non there too for why different values were chosen.  One \n&gt; consideration mi=\r\nght be whether very fine grained weight changed are \n&gt; key or not in a part=\r\nicular problem.  If they are, you might want the \n&gt; coefficient of weight d=\r\nifferences to be higher.\n&gt; \n&gt; Once you get XOR working, please let the grou=\r\np know how things go \n&gt; with the delayed XOR!  By the way, what language/pl=\r\natform did you \n&gt; use for your version of NEAT?\n&gt; \n&gt; ken\n&gt; \n&gt; --- In neat@y=\r\nahoogroups.com, &quot;mneylon01&quot; &lt;mneylon01@&gt; wrote:\n&gt; &gt;\n&gt; &gt; I&#39;ve been working o=\r\nn my own implimentation of NEAT (having to fit \n&gt; to a\n&gt; &gt; predescribed fra=\r\nmework) and while it&#39;s mostly working, I&#39;m looking \n&gt; at\n&gt; &gt; a couple of qu=\r\nestions.\n&gt; &gt; \n&gt; &gt; First, I know that the networks that NEAT generates can h=\r\nave\n&gt; &gt; recurrency (feedback), and this is likely why some of the robot \n&gt; =\r\ngame\n&gt; &gt; examples work well.  However, I&#39;m looking at trying to use NEAT to=\r\n\n&gt; &gt; find patterns in time series data, as one would use fully recurrent\n&gt; =\r\n&gt; networks for (In these, also known as Elmen networks, all of the\n&gt; &gt; hidd=\r\nen layer and output layer values are &#39;propagated&#39; into the next\n&gt; &gt; time st=\r\nep to give the network memory, with full connectivity \n&gt; between\n&gt; &gt; all th=\r\ne input and previous nodes to the hidden/output nodes).\n&gt; &gt; \n&gt; &gt; Such a net=\r\nwork should be possible to be generated by NEAT, though I\n&gt; &gt; figure that n=\r\not every recurrent type problem needs a fully \n&gt; recurrent\n&gt; &gt; network.  So=\r\n I&#39;m trying to use NEAT to generate such, using the\n&gt; &gt; classic delayed-XOR=\r\n problem (such that the result of xor of the two\n&gt; &gt; current inputs will be=\r\n the actual output some time steps away).  \n&gt; Fully\n&gt; &gt; recurrent networks =\r\ncan be trained to do this, but I want to \n&gt; generate a\n&gt; &gt; NEAT network tha=\r\nt, after running through the fixed data series a\n&gt; &gt; number of relaxation t=\r\nimes, that the weights have already been \n&gt; trained\n&gt; &gt; through NEAT evolut=\r\nion such that I don&#39;t have to perform additional\n&gt; &gt; training on the networ=\r\nk.  \n&gt; &gt; \n&gt; &gt; Has anyone had any success directly in generating such recurr=\r\nent\n&gt; &gt; networks?  I know my fitnesses improve with time, but it takes a \n&gt;=\r\n lot\n&gt; &gt; of generations (1000+ with a 150 member population) to even see\n&gt; =\r\n&gt; something, and even then, it&#39;s not anywhere close to what simple\n&gt; &gt; recu=\r\nrrent training can provide.  (This may be also related to my\n&gt; &gt; second que=\r\nstion).\n&gt; &gt; \n&gt; &gt; The other question I had was about convergence times.  I&#39;m=\r\n trying \n&gt; to\n&gt; &gt; test my network on the normal XOR problem (non-recurrent =\r\nmode) and\n&gt; &gt; find that it takes many more evolution generations for the fi=\r\ntness \n&gt; to\n&gt; &gt; get to acceptable levels (based solely on the distance of e=\r\nxpected \n&gt; vs\n&gt; &gt; observed output), exceptionaly more than listed in the NE=\r\nAT papers\n&gt; &gt; (500-1000 evolution steps as opposed to 10-30 steps) even whe=\r\nn \n&gt; using\n&gt; &gt; what I believe are the same values described by Kenneth in h=\r\nis \n&gt; papers.\n&gt; &gt;  I&#39;ve tried nearly every parameter, and the only one that=\r\n I know I\n&gt; &gt; want to keep low is the new node probability to avoid excessi=\r\nve \n&gt; growth\n&gt; &gt; of the network.  Anyone have any pointers on what paramete=\r\nrs are\n&gt; &gt; critical to help with rapid convergence on the best network?  Mi=\r\nnd\n&gt; &gt; you, it could still be something in my code which I&#39;ve been \n&gt; pound=\r\ning\n&gt; &gt; through to try to find differences.\n&gt; &gt; \n&gt; &gt; Another related questi=\r\non is on the species comparison expression \n&gt; and\n&gt; &gt; tolerance.  I tend to=\r\n use N=3Dnumber of genes in largest species\n&gt; &gt; regardless of the case, and=\r\n for that I have to play with the \n&gt; tolerance\n&gt; &gt; as to get 5 or more spec=\r\nies in a population of 150.  Is there an \n&gt; ideal\n&gt; &gt; average number of spe=\r\ncies that you want to carry through in the\n&gt; &gt; population in order to take =\r\nadvantage of NEAT&#39;s use of species?  \n&gt; And\n&gt; &gt; when do people move away fr=\r\nom the &#39;typical&#39; values of the \n&gt; coefficients\n&gt; &gt; (1 and 1 for disjoint an=\r\nd excess elements, 0.4 for weight \n&gt; difference\n&gt; &gt; average)\n&gt; &gt;\n&gt;\n\n\n\n\n\n\n"}}