{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":115403844,"authorName":"John Arrowwood","from":"&quot;John Arrowwood&quot; &lt;jarrowwx@...&gt;","profile":"jarrowwx","replyTo":"LIST","senderId":"gFkKIxX7C8rUIyQiezq7OwK6A3YYHT7IBy7v2gYYyc0L-UDUmyQVH2hon7ooyYbgRfWBh6Tj5TSffhuBIF98FB0BLLInlb7chFxT6jpd","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] Neural network optimization details","postDate":"1087951557","msgId":1125,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PEJBWTItRjMxcTNZejFzbndSSHQwMDAwMDlmOUBob3RtYWlsLmNvbT4="},"prevInTopic":1124,"nextInTopic":1128,"prevInTime":1124,"nextInTime":1126,"topicId":1106,"numMessagesInTopic":7,"msgSnippet":"... Agreed. ... Not exactly. ... The use of the propagation wave technique should work equally well for controllers and recurrent networks as it does for the","rawEmail":"Return-Path: &lt;jarrowwx@...&gt;\r\nX-Sender: jarrowwx@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 56982 invoked from network); 23 Jun 2004 00:45:58 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m25.grp.scd.yahoo.com with QMQP; 23 Jun 2004 00:45:58 -0000\r\nReceived: from unknown (HELO hotmail.com) (65.54.247.31)\n  by mta5.grp.scd.yahoo.com with SMTP; 23 Jun 2004 00:45:58 -0000\r\nReceived: from mail pickup service by hotmail.com with Microsoft SMTPSVC;\n\t Tue, 22 Jun 2004 17:45:58 -0700\r\nReceived: from 64.122.44.102 by by2fd.bay2.hotmail.msn.com with HTTP;\n\tWed, 23 Jun 2004 00:45:57 GMT\r\nX-Originating-Email: [jarrowwx@...]\r\nX-Sender: jarrowwx@...\r\nTo: neat@yahoogroups.com\r\nBcc: \r\nDate: Tue, 22 Jun 2004 17:45:57 -0700\r\nMime-Version: 1.0\r\nContent-Type: text/plain; format=flowed\r\nMessage-ID: &lt;BAY2-F31q3Yz1snwRHt000009f9@...&gt;\r\nX-OriginalArrivalTime: 23 Jun 2004 00:45:58.0085 (UTC) FILETIME=[72C4DF50:01C458BB]\r\nX-eGroups-Remote-IP: 65.54.247.31\r\nFrom: &quot;John Arrowwood&quot; &lt;jarrowwx@...&gt;\r\nReply-To: john@...\r\nSubject: Re: [neat] Neural network optimization details\r\nX-Yahoo-Group-Post: member; u=115403844\r\nX-Yahoo-Profile: jarrowwx\r\n\r\n&gt;From: Colin Green &lt;cgreen@...&gt;\n\n&gt;I accept the point that using the technique of propagating an activation\n&gt;&#39;wave&#39; through a network requires far less operations and is therefore\n&gt;going to be faster than a time-step approach, and  I&#39;ll be honest and\n&gt;admit I haven&#39;t put much thought into this approach yet, and what its\n&gt;implications are. What I will say though is that I don&#39;t see one\n&gt;technique as being better than the other, I think they are both valid\n&gt;techniques...\n\nAgreed.\n\n&gt;and each is more applicable to certain types of experiment.\n\nNot exactly.\n\n&gt;e.g. in continuous control experiments you might feed a signal into a\n&gt;network on each timestep and read the output on each timestep, there\n&gt;will be a lag if the network is deep but that doesn&#39;t matter if the\n&gt;timestep is very small,  this is biologically inspired and this type of\n&gt;experiment fits in better with the time-step network. Alternatively you\n&gt;might want to apply an input signal, allow the signals to propogate all\n&gt;the way through the net and THEN read the outputs - this is better\n&gt;suited to the &#39;propagation wave&#39; technique.\n\nThe use of the &#39;propagation wave&#39; technique should work equally well for \ncontrollers and recurrent networks as it does for the other.\n\n&gt;Right now I&#39;m invested in the time-step approach, (A) because it handles\n&gt;recurrent connections very simply\n\nAgreed, it is simpler.\n\nBut handling recurrency in the propagation wave technique isn&#39;t hard.  You \nsimply check for it when you are traversing the topology in order to \ndetermine activation sequence, so as to avoid getting caught in an endless \nloop.  You will want to do that anyway, as a matter of good programming \npractice.  Then, when you activate, you treat it no different.  Just \nactivate in the prescribed order, using the values as you generate them, \nrather than storing the values in a temporary location and then copying them \ninto place at the end.\n\nThere ARE consequences to how things are propagated in a recurrent network, \nhowever.  Some nodes might use the old value of the node in their \ncalculations, while others use the new.  As long as your mechanism for \ndetermining activation order is deterministic, this isn&#39;t a problem.  It \nsimply becomes an influence on the evolution.  The evolution will work \naround it.  It will probably even take advantage of it.\n\n&gt;and (B) it is applicable to both types\n&gt;of experiment described above.\n\nAs I indicated, both techniques will work for both types of experiments.\n\n&gt;So I suppose what I&#39;m saying is - I see\n&gt;the time-step net as general purpose, and the wave-propagation net as\n&gt;specialist. I accept the later to be more efficient though for certian\n&gt;types of experiment.\n\nFor controllers, it does potentially change the behavior of the network.  \nFor example, that lag or latency may be completely eliminated, which may be \na good thing in a real-time system (meaning a real-world application of the \nresultant network).  Also, since there is no lag, the network doesn&#39;t have \nto accomodate the lag.  That in and of itself may make the network able to \ndo a better job with less topology.  If someone is trying to evolve \ncontrollers that must operate in the real-world where they need as little \nlag-time as possible and/or lowest possible complexity/activation cost, it \nmight be worth giving it a try to see if the propagation wave activation \ndoes offer advantages.\n\n&gt;Here&#39;s another interesting piece of code I unturfed on the net. Say you\n&gt;want to add all of the integers in an array, you might do this:\n&gt;\n&gt;--------------------------------------\n&gt;int sum=0;\n&gt;for(int i=0; i&lt;myarray.length; i++)\n&gt;    sum += myarray[i];\n&gt;--------------------------------------\n&gt;\n&gt;Now because the result of each addition is being stored in &#39;sum&#39; the CPU\n&gt;can&#39;t use it&#39;s multiple ALU&#39;s (Arithmetic & Logic Unit&#39;s) in parallel.\n&gt;Not unless the compiler does something clever equivalent to the\n&gt;following code (Assuming 3 ALU&#39;s are available as in AMD Athlon):\n&gt;--------------------------------------\n&gt;int sum=0;\n&gt;int sum0=0;\n&gt;int sum1=0;\n&gt;int sum2=0;\n&gt;for(int i=0; i&lt;myarray.length; i+=3)\n&gt;{\n&gt;    sum0 += myarray[i];\n&gt;    sum1 += myarray[i+1];\n&gt;    sum2 += myarray[i+2];\n&gt;}\n&gt;sum = sum0+sum1+sum2;\n&gt;--------------------------------------\n&gt;Note that the array length has to be a multiple of 3 in this case to\n&gt;prevent an overrun.\n&gt;\n&gt;It just so happens that the Athlon has 3 FPUs also, so definitely worth\n&gt;some investigation this one.\n\nI had thought of doing something like that.  But since I&#39;m using generated \ncode, I can replace the above with\n\nsum = myarray[1]+myarray[2]+myarray[3]+...+myarray[n];\n\nThen let the compiler optimize it according to whatever technique is \navailable to it for the CPU in question.\n\n-- John\n\n\n\n"}}