{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":283334584,"authorName":"petar_chervenski","from":"&quot;petar_chervenski&quot; &lt;petar_chervenski@...&gt;","profile":"petar_chervenski","replyTo":"LIST","senderId":"SL4t63FWmTqfTnxG4epQf_jv68sx9frNOHyVUcpy_XDmczCh1NCqOFIDhyx6oomtoKvzsabxfOg8FBPWdTiJs3dmJ-hjaWLbG9ay1R5KWReiK9hMlO4","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Introducing a New Approach to Search: Novelty Search (New Paper)","postDate":"1210519050","msgId":4039,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGcwNzJtYStyNDYxQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGcwMGI4MStmdTY4QGVHcm91cHMuY29tPg=="},"prevInTopic":4038,"nextInTopic":4040,"prevInTime":4038,"nextInTime":4040,"topicId":4038,"numMessagesInTopic":26,"msgSnippet":"Hi Joal and Ken, This papers and the results presented there really turned my understandings for EC upside down. This kind of search is really innovative","rawEmail":"Return-Path: &lt;petar_chervenski@...&gt;\r\nX-Sender: petar_chervenski@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 51210 invoked from network); 11 May 2008 15:17:32 -0000\r\nX-Received: from unknown (66.218.67.96)\n  by m46.grp.scd.yahoo.com with QMQP; 11 May 2008 15:17:32 -0000\r\nX-Received: from unknown (HELO n18.bullet.mail.re1.yahoo.com) (69.147.102.101)\n  by mta17.grp.scd.yahoo.com with SMTP; 11 May 2008 15:17:31 -0000\r\nX-Received: from [68.142.237.90] by n18.bullet.mail.re1.yahoo.com with NNFMP; 11 May 2008 15:17:31 -0000\r\nX-Received: from [66.218.69.2] by t6.bullet.re3.yahoo.com with NNFMP; 11 May 2008 15:17:31 -0000\r\nX-Received: from [66.218.66.82] by t2.bullet.scd.yahoo.com with NNFMP; 11 May 2008 15:17:31 -0000\r\nDate: Sun, 11 May 2008 15:17:30 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;g072ma+r461@...&gt;\r\nIn-Reply-To: &lt;g00b81+fu68@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;petar_chervenski&quot; &lt;petar_chervenski@...&gt;\r\nSubject: Re: Introducing a New Approach to Search: Novelty Search (New Paper)\r\nX-Yahoo-Group-Post: member; u=283334584; y=FVgtw9lmDIEfHIarxDtnPJysW0Il8_Ct28kouN-BmXSASueDFQh0u-t3cg\r\nX-Yahoo-Profile: petar_chervenski\r\n\r\nHi Joal and Ken,\n\nThis papers and the results presented there really turned=\r\n my \nunderstandings for EC upside down. This kind of search is really \ninno=\r\nvative concept that needs to be explored. But I have several \nquestions abo=\r\nut it. \n\nIt is obvious that you have to define what is a network&#39;s behaviou=\r\nr \nin the task domain and find a way to measure distance between two \nbehav=\r\niours. I think this is the hardest part of setting up a novelty \nsearch. Fo=\r\nr example, in the task domain of maze navigation, you can \nsimply say that =\r\na robot&#39;s ending point is its behaviour, but I can \nargue that this is not =\r\nall, but the way the robot reaches this point \ncan also be considered a beh=\r\naviour. For example the robot may go from \npoint A to point B straight forw=\r\nard, but it can reach the same point \nby going forward and spinning. Your m=\r\nodel will consider both as the \nsame behaviour while it is obvious the beha=\r\nviours are different. \n\nAnother thing is that as the task becomes more comp=\r\nlex, it is nearly \nimpossible to define what a behaviour *is* and how to me=\r\nasure \ndistance between them. Consider the tennis playing robot. Or the \nch=\r\neckers playing network. How can you define the behaviour then? And \nmore, h=\r\now can you measure *distance* between behaviours? It&#39;s like \nyou have two p=\r\nersons that you know very well and you try to say &quot;the \ndistance between yo=\r\nur behaviours is 56.1 units&quot;... I don&#39;t say it is \nimpossible, it is just t=\r\noo hard. \n\nThe simplest approach is to track the network&#39;s output pattern a=\r\nnd \nhave this as its behaviour. But then a slight change will mean a \ndiffe=\r\nrent behaviour and the algorithm will end up adding and adding \nmore member=\r\ns to the pool, nearly every new thing in fact, thus the \nalgorithm will end=\r\n up as a slightly optimized version of exhaustive \nsearch. \n\nBut it is real=\r\nly interesting to see novelty search combined with \nCPPN/HyperNEAT on tasks=\r\n like my biped or just regular NEAT with \nnovelty search on the cool demos =\r\nI saw in DephiNEAT like the Hopper \nand the snake. \n\nBy the way I support t=\r\nhe idea that novelty search should sometimes be \nbiased with fitness-based =\r\nsearch, once a good number of approximate \nsolutions is found. Especially i=\r\nn domains where the search/behaviour \nspace is vast. \n\nI believe that the s=\r\nearch space that NEAT searches directly (the \ngenotype space) is much more =\r\nbig than the behaviour space, simply \nbecause many many different structure=\r\ns/weight configurations can \nrepresent the same exact behaviour. The task d=\r\nomain can restrict the \npossible behaviours further, but not in all task do=\r\nmains. But even if \nthere are infinite possible behaviours in a space, nove=\r\nlty search is \nstill better than random search because it searches explicit=\r\nly for \nnew behaviours, so new genomes that do the same thing will not be \n=\r\nrewarded. \n\nPeter\n\n--- In neat@yahoogroups.com, &quot;Kenneth Stanley&quot; &lt;kstanley=\r\n@...&gt; wrote:\n&gt;\n&gt; Joel Lehman and I are excited to announce our new publicat=\r\nion to\n&gt; appear in the Eleventh International Conference on Articifial Life=\r\n\n&gt; (ALIFE XI), called &quot;Exploiting Open-Endedness to Solve Problems\n&gt; Throug=\r\nh the Search for Novelty.&quot;\n&gt; \n&gt; The paper is here:\n&gt; \n&gt; http://eplex.cs.ucf=\r\n.edu/publications.html#lehman.alife08\n&gt; \n&gt; Direct link: http://eplex.cs.ucf=\r\n.edu/papers/lehman_alife08.pdf\n&gt; \n&gt; This paper is about a new kind of searc=\r\nh (which works with NEAT) \nthat\n&gt; abandons the longstanding notion in all o=\r\nf machine learning that the\n&gt; gradient of search should be measured with re=\r\nspect to the ultimate\n&gt; objective.  In other words, it entirely abandons ob=\r\njectives and\n&gt; thereby also abandons fitness functions as the impetus for s=\r\nearch. \n&gt; Yet remarkably, we still show that such an algorithm can perform\n=\r\n&gt; *better* than one that actually tries to achieve the objective!  I\n&gt; beli=\r\neve this strange result has fascinating implications for machine\n&gt; learning=\r\n, artificial life, and even biology.\n&gt; \n&gt; Lately on this forum we have ofte=\r\nn discussed the nagging problem \nthat\n&gt; the fitness function often does not=\r\n properly recognize or reward the\n&gt; stepping stones on the way to the solut=\r\nion.  I went as far as\n&gt; suggesting that the fitness function can become an=\r\n *obstacle* to\n&gt; success (e.g. when we discussed creativity in Picbreeder).=\r\n\n&gt; \n&gt; While this discussion was largely philosophical, Joel Lehman and I\n&gt; =\r\ndecided to make it concrete and actually introduce an algorithm that\n&gt; make=\r\ns an automated evolutionary process in *any* domain behave like\n&gt; humans in=\r\n Picbreeder, that is, like open-ended evolution.  This\n&gt; approach is called=\r\n &quot;novelty search.&quot;  The algorithm simply searches\n&gt; for behavior that is no=\r\nvel with respect to what has come before.\n&gt; \n&gt; The benefit of this approach=\r\n is that it is immune to deception \nbecause\n&gt; it does not even try to achie=\r\nve the objective.  I know it seems\n&gt; strange but, counterintuitively, we sh=\r\now that in fact it is far more\n&gt; effective at solving a difficult problem i=\r\nn a deceptive landscape \nthan\n&gt; fitness-based search.  \n&gt; \n&gt; In other words=\r\n, what we are saying is that to achieve some of the \nmost\n&gt; ambitious objec=\r\ntives we might have for evolution, we must abandon\n&gt; trying to explicitly a=\r\nchieve them.  To quote from the end of our\n&gt; Discussion section:\n&gt; \n&gt; &quot;In s=\r\nummary, almost like a riddle, novelty search suggests\n&gt; a surprising new pe=\r\nrspective on achievement: To achieve\n&gt; your highest goals, you must be will=\r\ning to abandon them.&quot;\n&gt; \n&gt; I believe this lesson is true in practice and is=\r\n therefore beyond a\n&gt; philosophical curiosity.  In fact, we are instinctive=\r\nly familiar \nwith\n&gt; it in life in general when people say things like &quot;You =\r\nare trying \ntoo\n&gt; hard&quot; or when we focus so much on something so far ahead =\r\nof us in \nlife\n&gt; that we forget completely to solve the short term problems=\r\n that \nstand\n&gt; in our way.  It is no less true in evolution or search in ge=\r\nneral.\n&gt; \n&gt; For NEAT, novelty search should open up new opportunities for\n&gt;=\r\n discovery that were previously closed off to us.  \n&gt; \n&gt; I look forward to =\r\nhearing your thoughts on this work.\n&gt; \n&gt; ken\n&gt;\n\n\n\n"}}