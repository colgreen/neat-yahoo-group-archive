{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":82117382,"authorName":"Jim O&#39;Flaherty, Jr.","from":"&quot;Jim O&#39;Flaherty, Jr.&quot; &lt;jim_oflaherty_jr@...&gt;","profile":"jim_oflaherty_jr","replyTo":"LIST","senderId":"jLjGkFzylOJI9aI6A7onkqFiORb9_5DMHBPEDLzrsUo8PFoYGxVbFmxm_y734-QIIOkbqDb_PDI-dYNEA6w27VYuX6TSL371jP7bqnDRxKuQDmLE5jpm65g","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] Re: autonomous virtual humans project","postDate":"1082695507","msgId":692,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDAyMjAwMWM0MjhlZCRjMzViMzA3MCQzMzAxYThjMEBORVdBR0U+","referencesHeader":"PGM2OW9jcityamt2QGVHcm91cHMuY29tPg=="},"prevInTopic":691,"nextInTopic":693,"prevInTime":691,"nextInTime":693,"topicId":657,"numMessagesInTopic":32,"msgSnippet":"Ken, I think I see your point.  However, if you only put in the head height input, you could end up with some ambulatory sequences that look very odd, but work","rawEmail":"Return-Path: &lt;jim_oflaherty_jr@...&gt;\r\nX-Sender: jim_oflaherty_jr@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 77805 invoked from network); 23 Apr 2004 04:46:17 -0000\r\nReceived: from unknown (66.218.66.217)\n  by m22.grp.scd.yahoo.com with QMQP; 23 Apr 2004 04:46:17 -0000\r\nReceived: from unknown (HELO smtp017.mail.yahoo.com) (216.136.174.114)\n  by mta2.grp.scd.yahoo.com with SMTP; 23 Apr 2004 04:46:17 -0000\r\nReceived: from unknown (HELO NEWAGE) (jim?oflaherty?jr@24.1.159.151 with login)\n  by smtp017.mail.yahoo.com with SMTP; 23 Apr 2004 04:45:12 -0000\r\nMessage-ID: &lt;022001c428ed$c35b3070$3301a8c0@NEWAGE&gt;\r\nTo: &lt;neat@yahoogroups.com&gt;\r\nReferences: &lt;c69ocr+rjkv@...&gt;\r\nDate: Thu, 22 Apr 2004 23:45:07 -0500\r\nMIME-Version: 1.0\r\nContent-Type: multipart/alternative;\n\tboundary=&quot;----=_NextPart_000_021B_01C428C3.D7EE50F0&quot;\r\nX-Priority: 3\r\nX-MSMail-Priority: Normal\r\nX-Mailer: Microsoft Outlook Express 6.00.2720.3000\r\nX-MimeOLE: Produced By Microsoft MimeOLE V6.00.2739.300\r\nX-eGroups-Remote-IP: 216.136.174.114\r\nFrom: &quot;Jim O&#39;Flaherty, Jr.&quot; &lt;jim_oflaherty_jr@...&gt;\r\nSubject: Re: [neat] Re: autonomous virtual humans project\r\nX-Yahoo-Group-Post: member; u=82117382\r\nX-Yahoo-Profile: jim_oflaherty_jr\r\n\r\n\r\n------=_NextPart_000_021B_01C428C3.D7EE50F0\r\nContent-Type: text/plain;\n\tcharset=&quot;iso-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\nKen,\n\nI think I see your point.  However, if you only put in the head heigh=\r\nt input, you could end up with some ambulatory sequences that look very odd=\r\n, but work okay; inverted knee walking, one hand one foot in odd jerky sequ=\r\nences, dog walking using all fours, crab walking using all fours, undulatin=\r\ng body lying flat with stationary head, etc.\n\nMy guess is that some aspect =\r\nof &quot;balance&quot;, &quot;symmetry&quot; and &quot;moderately constrained ranges of motion&quot; (whi=\r\nch will be quite challenging to define abstractly) will be desirable if the=\r\n goal is to eventually have the resultant &quot;figures&quot; have some level of cons=\r\nistency and biological similarity, even to that of a dog, or horse, much le=\r\nss a human\n\nIf I am understanding your point, you are saying that achieving=\r\n these ends might be more &quot;efficiently&quot; achieved without having to expand t=\r\nhe input set dramatically to introduce all the addition information about m=\r\nuscle contraction and resistance combined with bone elasticity, bone connec=\r\ntivity, etc.\n\nJust my $0.02 worth in the whole discussion.\n\n\nJim O&#39;Flaherty=\r\n, Jr.\n\n\n  ----- Original Message ----- \n  From: Kenneth Stanley \n  To: neat=\r\n@yahoogroups.com \n  Sent: Thursday, April 22, 2004 7:35 PM\n  Subject: [neat=\r\n] Re: autonomous virtual humans project\n\n\n  One of the themes that seems to=\r\n come up in the discussion of sensors\n  for the virtual human is the questi=\r\nons of what senses we humans might\n  plausibly have.  I am not against maki=\r\nng sensors plausible, but at\n  least the point should made that it&#39;s not re=\r\nally important what&#39;s\n  plausible if your main goal is for it to work.  Whi=\r\nle that point is\n  somewhat obvious, the more subtle point is that a lot of=\r\n the time two\n  different types of sensor are really just coordiante transf=\r\normations\n  of each other (e.g. polar vs. cartesian coordinates).  In other=\r\n words,\n  they are essentially the same thing represented in different ways=\r\n. \n  And because of that, the real question is not what&#39;s &quot;actually&quot; going\n=\r\n  on, but rather what is the most convenient representation from the\n  pers=\r\npective of an NN.  And that&#39;s a bit of a different question than\n  &quot;what se=\r\nnses do humans use&quot; (although that is certainly a good source\n  for inspira=\r\ntion).  But we need to be careful about jumping at complex\n  or convoluted =\r\nrepresentations that are really just transformations of\n  some simple conce=\r\npt.  \n\n  For example, if all that really matters is how high your head is o=\r\nff\n  the ground, then *that* is what should be fed to the NN, rather than a=\r\n\n  complex of angles and vestibular information which ultimately boils\n  do=\r\nwn the same piece of information.  I&#39;m not claiming that this\n  example is =\r\ntrue (i.e. that we don&#39;t need the extra stuff), but just\n  making the point=\r\n that it&#39;s the simplest most straightforward\n  representation of the key co=\r\nncept that needs to be conveyed to the\n  network, and sometimes it is not n=\r\necessary to use exactly the same\n  representation scheme as biology.\n\n  ken=\r\n\n\n  --- In neat@yahoogroups.com, Tyler Streeter &lt;tylerstreeter@y...&gt;\n  wrot=\r\ne:\n  &gt; \n  &gt; --- Colin Green &lt;cgreen@d...&gt; wrote:\n  &gt; &gt; Tyler Streeter wrote=\r\n:\n  &gt; &gt; \n  &gt; &gt; &gt;&gt;...unless you were intending to\n  &gt; &gt; &gt;&gt;transfer the whole=\r\n \n  &gt; &gt; &gt;&gt;project to a real-world scenario I see eye\n  &gt; &gt; &gt;&gt;simulation as =\r\nunnecessary.\n  &gt; &gt; &gt;&gt;    \n  &gt; &gt; &gt;&gt;\n  &gt; &gt; &gt;\n  &gt; &gt; &gt;I&#39;m reading the book Evol=\r\nutionary Robotics right\n  &gt; &gt; now\n  &gt; &gt; &gt;which focuses heavily on using sen=\r\nses robots could\n  &gt; &gt; &gt;actually use.  My mind is kind of geared in that\n  =\r\n&gt; &gt; &gt;direction at present.  I guess I would like to\n  &gt; &gt; leave\n  &gt; &gt; &gt;that=\r\n option (transferring neural nets to real\n  &gt; &gt; robots)\n  &gt; &gt; &gt;open, so I a=\r\nm trying to focus on more realistic\n  &gt; &gt; &gt;senses.  \n  &gt; &gt; &gt;\n  &gt; &gt; &gt;  \n  &gt; =\r\n&gt; &gt;\n  &gt; &gt; Hi Tyler,\n  &gt; &gt; \n  &gt; &gt; My concern though is that you are allowing=\r\n the scope\n  &gt; &gt; of your project to \n  &gt; &gt; increase and encompass a lot mor=\r\ne than can be\n  &gt; &gt; achieved in one go. The \n  &gt; &gt; stuff being discussed on=\r\n this group about fitness\n  &gt; &gt; functions and the type \n  &gt; &gt; of senses nee=\r\nded is all good stuff and represents a\n  &gt; &gt; significant problem \n  &gt; &gt; and=\r\n interesting piece of research in itself. By\n  &gt; &gt; adding the extra factors=\r\n \n  &gt; &gt; of ray tracing and image analysis you are\n  &gt; &gt; dramatically increa=\r\nsing the \n  &gt; &gt; complexity of the problem. Image analysis by itself\n  &gt; &gt; i=\r\ns a long-standing \n  &gt; &gt; branch of research AKAIK and I&#39;m just thinking abo=\r\nut\n  &gt; &gt; detecting lines and \n  &gt; &gt; objects in 2D images, in order to build=\r\n a 3D model\n  &gt; &gt; in your head you have \n  &gt; &gt; to combine two 2D images (st=\r\nereoscopic), you could\n  &gt; &gt; do this with code \n  &gt; &gt; rather than in the NN=\r\n&#39;s but it is CPU intensive work\n  &gt; &gt; especially if you \n  &gt; &gt; include the =\r\nray tracing!\n  &gt; &gt; \n  &gt; &gt; Scope creep is the death of many a project. My\n  =\r\n&gt; &gt; advice would be to keep \n  &gt; &gt; your scope focused and you can move on t=\r\no the other\n  &gt; &gt; stuff with a new \n  &gt; &gt; project later.\n  &gt; &gt; \n  &gt; &gt; Hope =\r\nI didn&#39;t come across as *too* much like a\n  &gt; &gt; nay-sayer there :)\n  &gt; \n  &gt;=\r\n Nope, not at all.  I definitely need to keep things\n  &gt; simple, especially=\r\n at first.  One thing I&#39;m interested\n  &gt; in is figuring out these different=\r\n problems and trying\n  &gt; to combine them into a single agent.  I&#39;ll be sure=\r\n to\n  &gt; keep the search space size in check.\n  &gt; \n  &gt; Tyler\n  &gt; \n  &gt; \n  &gt;  =\r\n     \n  &gt;             \n  &gt; __________________________________\n  &gt; Do you Ya=\r\nhoo!?\n  &gt; Yahoo! Photos: High-quality 4x6 digital prints for 25=A2\n  &gt; http=\r\n://photos.yahoo.com/ph/print_splash\n\n\n\n------------------------------------=\r\n------------------------------------------\n  Yahoo! Groups Links\n\n    a.. T=\r\no visit your group on the web, go to:\n    http://groups.yahoo.com/group/nea=\r\nt/\n      \n    b.. To unsubscribe from this group, send an email to:\n    nea=\r\nt-unsubscribe@yahoogroups.com\n      \n    c.. Your use of Yahoo! Groups is s=\r\nubject to the Yahoo! Terms of Service. \n\n\n\r\n------=_NextPart_000_021B_01C428C3.D7EE50F0\r\nContent-Type: text/html;\n\tcharset=&quot;iso-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\n&lt;!DOCTYPE HTML PUBLIC &quot;-//W3C//DTD HTML 4.0 Transitional//EN&quot;&gt;\n&lt;HTML&gt;&lt;HEAD&gt;=\r\n\n&lt;META http-equiv=3DContent-Type content=3D&quot;text/html; charset=3Diso-8859-1=\r\n&quot;&gt;\n&lt;META content=3D&quot;MSHTML 6.00.2737.800&quot; name=3DGENERATOR&gt;\n&lt;STYLE&gt;&lt;/STYLE&gt;=\r\n\n&lt;/HEAD&gt;\n&lt;BODY bgColor=3D#ffffff&gt;\n&lt;DIV&gt;&lt;FONT face=3DArial size=3D2&gt;Ken,&lt;/FO=\r\nNT&gt;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;FONT face=3DArial size=3D2&gt;&lt;/FONT&gt;&nbsp;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;FONT f=\r\nace=3DArial size=3D2&gt;I think I see your point.&nbsp; However, if you \nonly =\r\nput in the head height input, you could end up with some ambulatory \nsequen=\r\nces that look very odd, but work okay; inverted knee walking, one hand one =\r\n\nfoot in odd jerky sequences, dog walking using all fours, crab walking usi=\r\nng all \nfours, undulating body lying flat with stationary head, etc.&lt;/FONT&gt;=\r\n&lt;/DIV&gt;\n&lt;DIV&gt;&lt;FONT face=3DArial size=3D2&gt;&lt;/FONT&gt;&nbsp;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;FONT face=\r\n=3DArial size=3D2&gt;My guess is that some aspect of \n&quot;balance&quot;,&nbsp;&quot;symmetr=\r\ny&quot; and &quot;moderately constrained ranges of motion&quot; (which \nwill be quite chal=\r\nlenging to define abstractly)&nbsp;will be desirable if the \ngoal is to eve=\r\nntually have the resultant &quot;figures&quot; have some level of \nconsistency and bi=\r\nological similarity, even to that of a dog, or horse, much \nless a human&lt;/F=\r\nONT&gt;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;FONT face=3DArial size=3D2&gt;&lt;/FONT&gt;&nbsp;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;FONT =\r\nface=3DArial size=3D2&gt;If I am understanding your point, you are saying \ntha=\r\nt achieving these ends might be more &quot;efficiently&quot; achieved without having =\r\nto \nexpand the input set dramatically to introduce all the addition informa=\r\ntion \nabout muscle contraction and resistance combined with bone elasticity=\r\n, bone \nconnectivity, etc.&lt;/FONT&gt;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;FONT face=3DArial size=3D2&gt;&lt;/=\r\nFONT&gt;&nbsp;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;FONT face=3DArial size=3D2&gt;Just my $0.02 worth in t=\r\nhe whole \ndiscussion.&lt;/FONT&gt;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;FONT face=3DArial size=3D2&gt;&lt;/FONT&gt;=\r\n&nbsp;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;FONT face=3DArial size=3D2&gt;&lt;/FONT&gt;&nbsp;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;FON=\r\nT face=3DArial size=3D2&gt;Jim O&#39;Flaherty, Jr.&lt;/FONT&gt;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;FONT face=3D=\r\nArial size=3D2&gt;&lt;/FONT&gt;&nbsp;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;FONT face=3DArial size=3D2&gt;&lt;/FONT&gt;=\r\n&nbsp;&lt;/DIV&gt;\n&lt;BLOCKQUOTE \nstyle=3D&quot;PADDING-RIGHT: 0px; PADDING-LEFT: 5px; M=\r\nARGIN-LEFT: 5px; BORDER-LEFT: #000000 2px solid; MARGIN-RIGHT: 0px&quot;&gt;\n  &lt;DIV=\r\n style=3D&quot;FONT: 10pt arial&quot;&gt;----- Original Message ----- &lt;/DIV&gt;\n  &lt;DIV \n  s=\r\ntyle=3D&quot;BACKGROUND: #e4e4e4; FONT: 10pt arial; font-color: black&quot;&gt;&lt;B&gt;From:&lt;=\r\n/B&gt; \n  &lt;A title=3Dkstanley@... href=3D&quot;mailto:kstanley@....=\r\nedu&quot;&gt;Kenneth \n  Stanley&lt;/A&gt; &lt;/DIV&gt;\n  &lt;DIV style=3D&quot;FONT: 10pt arial&quot;&gt;&lt;B&gt;To:=\r\n&lt;/B&gt; &lt;A title=3Dneat@yahoogroups.com \n  href=3D&quot;mailto:neat@yahoogroups.com=\r\n&quot;&gt;neat@yahoogroups.com&lt;/A&gt; &lt;/DIV&gt;\n  &lt;DIV style=3D&quot;FONT: 10pt arial&quot;&gt;&lt;B&gt;Sent=\r\n:&lt;/B&gt; Thursday, April 22, 2004 7:35 \n  PM&lt;/DIV&gt;\n  &lt;DIV style=3D&quot;FONT: 10pt =\r\narial&quot;&gt;&lt;B&gt;Subject:&lt;/B&gt; [neat] Re: autonomous virtual \n  humans project&lt;/DIV=\r\n&gt;\n  &lt;DIV&gt;&lt;BR&gt;&lt;/DIV&gt;&lt;TT&gt;One of the themes that seems to come up in the discu=\r\nssion \n  of sensors&lt;BR&gt;for the virtual human is the questions of what sense=\r\ns we humans \n  might&lt;BR&gt;plausibly have.&nbsp; I am not against making senso=\r\nrs plausible, but \n  at&lt;BR&gt;least the point should made that it&#39;s not really=\r\n important \n  what&#39;s&lt;BR&gt;plausible if your main goal is for it to work.&nbsp=\r\n; While that \n  point is&lt;BR&gt;somewhat obvious, the more subtle point is that=\r\n a lot of the time \n  two&lt;BR&gt;different types of sensor are really just coor=\r\ndiante \n  transformations&lt;BR&gt;of each other (e.g. polar vs. cartesian coordi=\r\nnates).&nbsp; \n  In other words,&lt;BR&gt;they are essentially the same thing rep=\r\nresented in \n  different ways. &lt;BR&gt;And because of that, the real question i=\r\ns not what&#39;s \n  &quot;actually&quot; going&lt;BR&gt;on, but rather what is the most conveni=\r\nent representation \n  from the&lt;BR&gt;perspective of an NN.&nbsp; And that&#39;s a =\r\nbit of a different \n  question than&lt;BR&gt;&quot;what senses do humans use&quot; (althoug=\r\nh that is certainly a \n  good source&lt;BR&gt;for inspiration).&nbsp; But we need=\r\n to be careful about jumping \n  at complex&lt;BR&gt;or convoluted representations=\r\n that are really just \n  transformations of&lt;BR&gt;some simple concept.&nbsp; &lt;=\r\nBR&gt;&lt;BR&gt;For example, if all \n  that really matters is how high your head is =\r\noff&lt;BR&gt;the ground, then *that* is \n  what should be fed to the NN, rather t=\r\nhan a&lt;BR&gt;complex of angles and \n  vestibular information which ultimately b=\r\noils&lt;BR&gt;down the same piece of \n  information.&nbsp; I&#39;m not claiming that =\r\nthis&lt;BR&gt;example is true (i.e. that we \n  don&#39;t need the extra stuff), but j=\r\nust&lt;BR&gt;making the point that it&#39;s the \n  simplest most straightforward&lt;BR&gt;r=\r\nepresentation of the key concept that needs \n  to be conveyed to the&lt;BR&gt;net=\r\nwork, and sometimes it is not necessary to use \n  exactly the same&lt;BR&gt;repre=\r\nsentation scheme as biology.&lt;BR&gt;&lt;BR&gt;ken&lt;BR&gt;&lt;BR&gt;--- In \n  neat@yahoogroups.c=\r\nom, Tyler Streeter \n  &lt;tylerstreeter@y...&gt;&lt;BR&gt;wrote:&lt;BR&gt;&gt; &lt;BR&gt;&gt;=\r\n --- Colin Green \n  &lt;cgreen@d...&gt; wrote:&lt;BR&gt;&gt; &gt; Tyler Streeter =\r\nwrote:&lt;BR&gt;&gt; &gt; \n  &lt;BR&gt;&gt; &gt; &gt;&gt;...unless you were intending t=\r\no&lt;BR&gt;&gt; &gt; \n  &gt;&gt;transfer the whole &lt;BR&gt;&gt; &gt; &gt;&gt;project =\r\nto a real-world \n  scenario I see eye&lt;BR&gt;&gt; &gt; &gt;&gt;simulation as un=\r\nnecessary.&lt;BR&gt;&gt; \n  &gt; &gt;&gt;&nbsp;&nbsp;&nbsp; &lt;BR&gt;&gt; &gt; &gt;&gt=\r\n;&lt;BR&gt;&gt; &gt; \n  &gt;&lt;BR&gt;&gt; &gt; &gt;I&#39;m reading the book Evolutionary R=\r\nobotics right&lt;BR&gt;&gt; \n  &gt; now&lt;BR&gt;&gt; &gt; &gt;which focuses heavily on=\r\n using senses robots \n  could&lt;BR&gt;&gt; &gt; &gt;actually use.&nbsp; My mind =\r\nis kind of geared in \n  that&lt;BR&gt;&gt; &gt; &gt;direction at present.&nbsp; I=\r\n guess I would like \n  to&lt;BR&gt;&gt; &gt; leave&lt;BR&gt;&gt; &gt; &gt;that option (=\r\ntransferring neural nets \n  to real&lt;BR&gt;&gt; &gt; robots)&lt;BR&gt;&gt; &gt; &gt;o=\r\npen, so I am trying to focus on \n  more realistic&lt;BR&gt;&gt; &gt; &gt;senses.&=\r\nnbsp; &lt;BR&gt;&gt; &gt; &gt;&lt;BR&gt;&gt; &gt; \n  &gt;&nbsp; &lt;BR&gt;&gt; &gt; &gt;&lt;BR&gt;&=\r\ngt; &gt; Hi Tyler,&lt;BR&gt;&gt; &gt; &lt;BR&gt;&gt; \n  &gt; My concern though is that =\r\nyou are allowing the scope&lt;BR&gt;&gt; &gt; of your \n  project to &lt;BR&gt;&gt; &gt;=\r\n increase and encompass a lot more than can be&lt;BR&gt;&gt; \n  &gt; achieved in =\r\none go. The &lt;BR&gt;&gt; &gt; stuff being discussed on this group \n  about fitn=\r\ness&lt;BR&gt;&gt; &gt; functions and the type &lt;BR&gt;&gt; &gt; of senses \n  needed i=\r\ns all good stuff and represents a&lt;BR&gt;&gt; &gt; significant problem \n  &lt;BR&gt;&=\r\ngt; &gt; and interesting piece of research in itself. By&lt;BR&gt;&gt; &gt; \n  ad=\r\nding the extra factors &lt;BR&gt;&gt; &gt; of ray tracing and image analysis you =\r\n\n  are&lt;BR&gt;&gt; &gt; dramatically increasing the &lt;BR&gt;&gt; &gt; complexity of=\r\n the \n  problem. Image analysis by itself&lt;BR&gt;&gt; &gt; is a long-standing &lt;=\r\nBR&gt;&gt; \n  &gt; branch of research AKAIK and I&#39;m just thinking about&lt;BR&gt;&gt=\r\n; &gt; \n  detecting lines and &lt;BR&gt;&gt; &gt; objects in 2D images, in order =\r\nto build a 3D \n  model&lt;BR&gt;&gt; &gt; in your head you have &lt;BR&gt;&gt; &gt; to =\r\ncombine two 2D \n  images (stereoscopic), you could&lt;BR&gt;&gt; &gt; do this wit=\r\nh code &lt;BR&gt;&gt; &gt; \n  rather than in the NN&#39;s but it is CPU intensive wor=\r\nk&lt;BR&gt;&gt; &gt; especially \n  if you &lt;BR&gt;&gt; &gt; include the ray tracing!&lt;=\r\nBR&gt;&gt; &gt; &lt;BR&gt;&gt; &gt; Scope \n  creep is the death of many a project. M=\r\ny&lt;BR&gt;&gt; &gt; advice would be to keep \n  &lt;BR&gt;&gt; &gt; your scope focused =\r\nand you can move on to the other&lt;BR&gt;&gt; &gt; \n  stuff with a new &lt;BR&gt;&gt; =\r\n&gt; project later.&lt;BR&gt;&gt; &gt; &lt;BR&gt;&gt; &gt; Hope \n  I didn&#39;t come across=\r\n as *too* much like a&lt;BR&gt;&gt; &gt; nay-sayer there \n  :)&lt;BR&gt;&gt; &lt;BR&gt;&gt; N=\r\nope, not at all.&nbsp; I definitely need to keep \n  things&lt;BR&gt;&gt; simple, =\r\nespecially at first.&nbsp; One thing I&#39;m \n  interested&lt;BR&gt;&gt; in is figuri=\r\nng out these different problems and \n  trying&lt;BR&gt;&gt; to combine them into =\r\na single agent.&nbsp; I&#39;ll be sure \n  to&lt;BR&gt;&gt; keep the search space size=\r\n in check.&lt;BR&gt;&gt; &lt;BR&gt;&gt; Tyler&lt;BR&gt;&gt; \n  &lt;BR&gt;&gt; &lt;BR&gt;&gt; &nbsp;&nbsp;=\r\n&nbsp;&nbsp;&nbsp; &lt;BR&gt;&gt; \n  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&=\r\nnbsp;&nbsp;&nbsp; &lt;BR&gt;&gt; \n  __________________________________&lt;BR&gt;&gt; Do=\r\n you Yahoo!?&lt;BR&gt;&gt; Yahoo! \n  Photos: High-quality 4x6 digital prints for =\r\n25=A2&lt;BR&gt;&gt; &lt;A \n  href=3D&quot;http://photos.yahoo.com/ph/print_splash&quot;&gt;http:/=\r\n/photos.yahoo.com/ph/print_splash&lt;/A&gt;&lt;BR&gt;&lt;BR&gt;&lt;/TT&gt;&lt;/BODY&gt;&lt;/HTML&gt;\n\r\n------=_NextPart_000_021B_01C428C3.D7EE50F0--\r\n\n"}}