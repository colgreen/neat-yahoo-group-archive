{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":183620858,"authorName":"Derek James","from":"&quot;Derek James&quot; &lt;djames@...&gt;","profile":"blue5432","replyTo":"LIST","senderId":"ZnDobMU0xEw-7OZg0_oFdiqfKE9PAnPPSAXRQNSK93dygQ4x8q63z0wcGzNAE7s0OkI-CC9IV8OgIV9wBKMBIum93Ktt29M","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: Computation Time","postDate":"1085841014","msgId":897,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGM5YTZwbStoOTZtQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDQwQjg5ODQzLjUwNTAxMDJAZHNsLnBpcGV4LmNvbT4="},"prevInTopic":896,"nextInTopic":898,"prevInTime":896,"nextInTime":898,"topicId":845,"numMessagesInTopic":99,"msgSnippet":"... Just because they re more complex, doesn t necessarily mean they re bloated .  It s only bloat if the added structure doesn t carry with it proportional","rawEmail":"Return-Path: &lt;djames@...&gt;\r\nX-Sender: djames@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 99670 invoked from network); 29 May 2004 14:31:11 -0000\r\nReceived: from unknown (66.218.66.217)\n  by m6.grp.scd.yahoo.com with QMQP; 29 May 2004 14:31:11 -0000\r\nReceived: from unknown (HELO n16.grp.scd.yahoo.com) (66.218.66.71)\n  by mta2.grp.scd.yahoo.com with SMTP; 29 May 2004 14:31:11 -0000\r\nReceived: from [66.218.67.252] by n16.grp.scd.yahoo.com with NNFMP; 29 May 2004 14:30:15 -0000\r\nDate: Sat, 29 May 2004 14:30:14 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;c9a6pm+h96m@...&gt;\r\nIn-Reply-To: &lt;40B89843.5050102@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Length: 2590\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-eGroups-Remote-IP: 66.218.66.71\r\nFrom: &quot;Derek James&quot; &lt;djames@...&gt;\r\nSubject: Re: Computation Time\r\nX-Yahoo-Group-Post: member; u=183620858\r\nX-Yahoo-Profile: blue5432\r\n\r\n--- In neat@yahoogroups.com, Colin Green &lt;cgreen@d...&gt; wrote:\n&gt; Even with the blended runs the graphs in the paper show that the \n&gt; population now has a wide range of genomes from very simple ones \n&gt; to very complex (bloated) ones. \n\nJust because they&#39;re more complex, doesn&#39;t necessarily mean \nthey&#39;re &quot;bloated&quot;.  It&#39;s only bloat if the added structure doesn&#39;t \ncarry with it proportional performance in fitness.\n\n&gt; A phased search would periodically remove ALL redundancy \n&gt; from the entire population, thus improving the efficiency of a \n&gt; search \n&gt; from a computational expense point of view.  Now you could argue \n&gt; (Derek \n&gt; made this  point) that the &#39;trim&#39; phase is going backwards in the \n&gt; search \n&gt; space and therefore it will take extra time to work back forwards \n&gt; again.\n\nThat&#39;s not exactly what I was arguing.  I was saying that if you do \ntwo phases, only-complexifying followed by only-simplifying, that&#39;s \ngoing to be less efficient than using both dynamics in the first \nplace.\n\nTake the numbers from our paper.  Computation time for complexifying \nruns was 216 minutes on average, while the blended runs were about \n170 minutes on average.\n\nFollowing the phased approach, as you complexify you&#39;ve got \nredundant structure in the population that&#39;s going to make your \nnetwork activation less efficient, slowing you down.  Your approach \nseems to want to let those inefficiencies evolve, then weed them out \nlater.  Wouldn&#39;t it be better to never let them evolve in the first \nplace?\n\nThat&#39;s basically what I&#39;m arguing that a blended approach does.\n\n&gt; I believe that allowing the population to complexify/bloat \n&gt; unbounded is \n&gt; a very important part of the NEAT search process, but that it \n&gt; becomes \n&gt; detremental in medium to long searches. \n\nI don&#39;t think allowing liberal additions to the network at any stage \nis very useful.  The whole philosophy of NEAT is that innovations \nare added incrementally, and verified for usefulness along the way. \n\n&gt; Phased searching should clean up \n&gt; all of the junk that hasn&#39;t been incorporated functionaly into the \n&gt; networks and should give search speeds a nice boost.\n\nAgain, the best approach would be to never let the junk pile up in \nthe first place.\n\nIt seems intuitive that it&#39;s going to take longer to let junk pile \nup and then clean it out, rather than never allowing it to pile up.\n\n&gt; Get the search technique right/optimized first and THEN \n&gt; optimize the nuts and bolts code. \n\nI&#39;d probably agree that optimizing the search is more important in \ngeneral than optimizing code...but they&#39;re both important.\n\nDerek\n\n\n"}}