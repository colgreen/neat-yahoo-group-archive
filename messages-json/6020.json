{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Ken","from":"&quot;Ken&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"voSlMnljQix8AN5ndfnRvDayGhhbFod8gjU9gzmgv6pxAIySUT76TWVLiKvHb8hVcJMxBqL0cpt-Ws2_5CqL5WMiC9D9","spamInfo":{"isSpam":false,"reason":"3"},"subject":"Re: New paper on why modules evolve, and how to evolve modular artif","postDate":"1362736333","msgId":6020,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGtoY2NjZCtmMHQ5QGVHcm91cHMuY29tPg==","inReplyToHeader":"PDlBQkU2N0ZDLUZBMTAtNDUwMy1BQjY2LUM3QzQ0MUM3M0E5REB1d3lvLmVkdT4="},"prevInTopic":6019,"nextInTopic":6021,"prevInTime":6019,"nextInTime":6021,"topicId":6011,"numMessagesInTopic":10,"msgSnippet":"Hi Jeff, You make a great case for your position, and like I said to Stef, the major point on how connection length relates to modularity that you and your","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 12840 invoked from network); 8 Mar 2013 09:52:14 -0000\r\nX-Received: from unknown (10.193.84.168)\n  by m2.grp.bf1.yahoo.com with QMQP; 8 Mar 2013 09:52:14 -0000\r\nX-Received: from unknown (HELO ng7-ip2.bullet.mail.bf1.yahoo.com) (98.139.165.48)\n  by mta6.grp.bf1.yahoo.com with SMTP; 8 Mar 2013 09:52:13 -0000\r\nX-Received: from [98.139.164.120] by ng7.bullet.mail.bf1.yahoo.com with NNFMP; 08 Mar 2013 09:52:13 -0000\r\nX-Received: from [10.193.94.45] by tg1.bullet.mail.bf1.yahoo.com with NNFMP; 08 Mar 2013 09:52:13 -0000\r\nDate: Fri, 08 Mar 2013 09:52:13 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;khcccd+f0t9@...&gt;\r\nIn-Reply-To: &lt;9ABE67FC-FA10-4503-AB66-C7C441C73A9D@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 2:3:4:0:0\r\nFrom: &quot;Ken&quot; &lt;kstanley@...&gt;\r\nSubject: Re: New paper on why modules evolve, and how to evolve modular artif\r\nX-Yahoo-Group-Post: member; u=54567749; y=r6TIWG0MDAyLGW2P3OMSQl6Fbta5SclDRpnG5gb_5dUWKFyaGraG\r\nX-Yahoo-Profile: kenstanley01\r\n\r\n\n\nHi Jeff,\n\nYou make a great case for your position, and like I said to Ste=\r\nf, the major point on how connection length relates to modularity that you =\r\nand your coauthors have contributed is important.  In my reply to Stef I ma=\r\nde a few general last points and I think we can safely assume we will have =\r\nsome very interesting further discussions at GECCO!\n\nBest,\n\nken\n\n\n--- In ne=\r\nat@yahoogroups.com, Jeff Clune &lt;jclune@...&gt; wrote:\n&gt;\n&gt; Thanks for clarifyin=\r\ng your positions Ken. I believe we have reached the point at which reasonab=\r\nle minds can respectably disagree, as you put it. Were I to respond, I beli=\r\neve I would mostly repeat myself as the ideas I believe answer your comment=\r\ns and questions I&#39;ve already expressed earlier in this thread. That usually=\r\n is an indication of coming to an agreement, even if the agreement is to di=\r\nsagree. :-) \n&gt; \n&gt; That said, I&#39;ll summarize my main points: I do think natu=\r\nre has default fitness penalties, and all we have done is change the defaul=\r\nt to one that encourages modularity and evolvability. As such, I don&#39;t thin=\r\nk we&#39;re subject to general attacks on fitness pressures, because by default=\r\n there are fitness pressures (e.g. a cost for materials), we just change th=\r\nem to be better with respect to modularity and evolvability. Moreover, I th=\r\nink a connection cost encourages modularity in a way similar to how things =\r\nwork in nature, even if nature more resembles a combination of performance =\r\nand costs into a single objective instead of multi-objective algorithms. Ou=\r\nr work is not dependent on an MOEA: we suspect a connection cost will encou=\r\nrage modularity in general irrespective of the details of the algorithm. As=\r\n I&#39;ve mentioned, I think nuance and subtlety are possible with a connection=\r\n cost because nature can pay for more connectivity via performance gains, a=\r\nllowing all sorts of exceptions to the general rule. I still don&#39;t see how =\r\nan initial encoding bias will have any long-term effect on evolution, so th=\r\ne only hope for canalization and helpful encoding biases is via fitness. I =\r\nthus believe that fitness penalties are a great way to encourage good encod=\r\nings via canalization. For example, we know that default penalties exist (s=\r\nuch as a default tendency to NOT produce modularity, which has been empiric=\r\nally observed in our systems repeatedly), and nature will not explore those=\r\n areas unless the fitness function is changed (just as nature never explore=\r\nd certain classes of extremely inefficient metabolisms, or birds with bones=\r\n made of lead). Ultimately, since the effect of your initial encoding bias =\r\nhint will vanish over millennia, your argument amounts to saying that we sh=\r\nould not try to bias evolution at all. But this whole conversation was on t=\r\nhe best way to bias evolution, so maybe your position is that we shouldn&#39;t =\r\nbe biasing evolution at all. That&#39;s fine if we don&#39;t have any default fitne=\r\nss penalties in the environment that will hurt us, but we have no guarantee=\r\n of that. As you point out, Nature has done impressive things, but it also =\r\nhad a very different environment than the environment in our setups so far.=\r\n What we&#39;ve shown in this paper is that some things that we originally thou=\r\nght just happened to exist in nature, but were unnecessary vis a vis evolva=\r\nbility (such as a cost for materials) actually may play a role in the evolu=\r\ntion of modularity and evolvability. I think it is worthwhile to investigat=\r\ne what else we may have skipped from the natural world that may be an impor=\r\ntant driver of evolvability and, ultimately, open-ended evolution.\n&gt; \n&gt; A f=\r\ninal point: your criticism that &#39;nothing we learn matters if we don&#39;t have =\r\nan open-ended algorithm&#39; discounts all the work that has been done to date =\r\nin computational evolutionary biology, not just our paper. You may be right=\r\n that all of our lessons are worthless once we figure out an open-ended evo=\r\nlutionary algorithm, but I doubt that will be the case. I think a lot of in=\r\nteresting, worthwhile understandings have been gained by simulated evolutio=\r\nn, and that much of that work will prove informative even if the underlying=\r\n algorithms change. \n&gt; \n&gt; \n&gt; Best regards,\n&gt; Jeff Clune\n&gt; \n&gt; Assistant Prof=\r\nessor\n&gt; Computer Science\n&gt; University of Wyoming\n&gt; jeffclune@...\n&gt; jeffclun=\r\ne.com\n&gt; \n&gt; On Mar 2, 2013, at 4:16 PM, Ken &lt;kstanley@...&gt; wrote:\n&gt; \n&gt; &gt; \n&gt; =\r\n&gt; \n&gt; &gt; Hi Jeff, Stef, and Martin, I hope you don&#39;t mind since all of you ad=\r\ndressed me if I try to reply to all of you at once to keep the thread (and =\r\nmy brain) from branching in three directions. Many of your points follow a =\r\nsimilar theme so I think it makes sense to respond collectively. This respo=\r\nnse is practically an article, but oh well, it&#39;s nice to get the ideas down=\r\n even if it&#39;s a bit too long (it just shows you are asking me great questio=\r\nns that are challenging).\n&gt; &gt; \n&gt; &gt; Martin offers a good unifying question: =\r\n&quot;My question to Ken would be here: what is the additional ingredient that m=\r\nakes a\n&gt; &gt; bias in the encoding better / more plausible than *any* implemen=\r\ntation of the bias in the fitness function?&quot;\n&gt; &gt; \n&gt; &gt; After some thought, I=\r\n believe one of the difficulties in this discussion is that we often confla=\r\nte artificial EC-style fitness-based experiments with open-ended scenarios =\r\nwhen these are entirely different situations (I take blame myself as well f=\r\nor this tendency). That is, when we talk about something being &quot;better&quot; or =\r\n&quot;solving&quot; a problem, we are often talking about artificial and unnatural ex=\r\nperimental setups that have little relationship to open-ended evolutionary =\r\nscenarios like nature. \n&gt; &gt; \n&gt; &gt; Why does that matter? It matters because i=\r\nn discussions that try to dovetail engineering-oriented mechanisms (like a =\r\nconnectivity penalty) with explanations of what happened in nature (such as=\r\n the emergence of modular connectivity), it cannot simply be ignored that n=\r\nature in fact is first and foremost an open-ended evolutionary system, and =\r\nthat that open-ended dynamic is a significant factor in the explanation of =\r\nits products. What that means to me is that if you think your proposed mech=\r\nanism actually *explains* something that happened in nature, then it is ess=\r\nential that the explanation speaks to the question of how the particular me=\r\nchanism you are advancing combined historically with the open-ended evoluti=\r\nonary dynamics in nature to produce the result you expect.\n&gt; &gt; \n&gt; &gt; But bec=\r\nause we conflate very closed-ended artificial scenarios with monumentally o=\r\npen-ended searches like nature, it leads to a lot of dangerous inferences. =\r\nSo ideas that would make sense in one context end up sounding reasonable wh=\r\nen they don&#39;t really make any sense in the other context. The difficulty of=\r\n squaring fitness-pressure objectives with nature is more serious when you =\r\nconsider it in this perspective. (Note that I am defining &quot;fitness pressure=\r\n&quot; as selection based on relative performance to other organisms on a measur=\r\ne of some property that varies over a range of possible values, such as deg=\r\nree of connectivity.)\n&gt; &gt; \n&gt; &gt; The problem is that fitness pressures that p=\r\nreserve a degenerate niche for eternity are definitively not like nature, s=\r\no whether they work or not, or whether I am somehow indicting them or not, =\r\nshould not be the issue. The issue should be that we should be worried that=\r\n nature does not use a mechanism even remotely like that yet still achieves=\r\n the &quot;same&quot; result (i.e. beautiful variations of pseudo-modular design). If=\r\n you are advancing the hypothesis that this kind of constant &quot;pressure&quot; is =\r\nsomehow essential to the emergence of modularity in nature, then you must s=\r\nomehow explain why you needed to use a setup with these bizarre and unnatur=\r\nal side effects (like eternal degenerate niches) instead of whatever nature=\r\n actually supposedly does use.\n&gt; &gt; \n&gt; &gt; And the fact that you cannot come u=\r\np with anything similar to what nature does, i.e. something that does not i=\r\nnvolve creating such a deadweight pocket, reasonably may suggest that your =\r\nhypothesis about nature could be wrong. That is, it may not be this endless=\r\n &quot;fitness pressure&quot; after all that explains what is happening there, becaus=\r\ne fitness pressure in general in EC is almost always creating some kind of =\r\nunintended deadweight niche.\n&gt; &gt; \n&gt; &gt; I think it is particularly fascinatin=\r\ng that in fact nature obtains not really the same result, but a far more aw=\r\nesome result (in terms of modularity or anything else), without such an ad =\r\nhoc mechanism. \n&gt; &gt; If you think about it, as long as you insist on cheerin=\r\ng for fitness pressure, it prevents you from asking how this could be - how=\r\n is it possible that you can get these kinds of results without such an unn=\r\natural side effect?\n&gt; &gt; \n&gt; &gt; I need to emphasize here the difference betwee=\r\nn being a better engineering mechanism and a better explanation. I am focus=\r\ning now primarily on the explanatory power of the proposed mechanism. But b=\r\necause nature is so much more accomplished than anything artificial, the ex=\r\nplanatory gap here implies a dangerous potential to overlook what will ulti=\r\nmately amount also to a major engineering gap as well. There is no evidence=\r\n that anything except nature in its open-ended way can create anything like=\r\n the connectivity of natural brains.\n&gt; &gt; \n&gt; &gt; Furthermore, it is always imp=\r\nortant to acknowledge nuance and subtlety in nature, which has not really b=\r\neen acknowledged yet in this conversation. Nature is almost never all one w=\r\nay. So it is misleading and potentially confusing to talk about brains as s=\r\nimply modular or not. The recent discussion on the Connectionists list, whe=\r\nre scientists have been giving all kinds of subtle and conflicting perspect=\r\nives on modularity in natural brains in response to Jeff and JBM&#39;s article,=\r\n echoes this nuance. The beauty of the human brain to me is not that it is =\r\nmodular, but that it is modular to an extent, but not entirely so, and what=\r\n modularity there is is hard to pin down. This kind of nuance is not to me =\r\na mere footnote to the achievement of nature, but the central point of it: =\r\nwhat nature achieves in spades is nuance. \n&gt; &gt; \n&gt; &gt; And the idea of a const=\r\nant pressure of any kind is directly in conflict with the achievement of nu=\r\nance, because nuance is a delicate balancing act that is easily tipped off =\r\nits perch if constant pressure in *any* direction is applied without relief=\r\n. Jeff is concerned with short-term versus long-term issues (which isn&#39;t re=\r\nally as clearly defined in an open-ended context), but even if we honor tha=\r\nt concern, it is potentially na=EFve to believe that pressure in either dir=\r\nection from the start, or even an encoding bias in either direction from th=\r\ne start, is somehow going to directly align with the level of nuance observ=\r\ned millions of years in the future. However, while fitness pressure is eter=\r\nnal, encoding bias is malleable, so pushing in the &quot;right&quot; direction from t=\r\nhe start is not essential for encoding. It&#39;s more like a hint to get you st=\r\narted, whereas fitness pressure is more like a gun forever pointed at your =\r\nback.\n&gt; &gt; \n&gt; &gt; For example, who is to say that we should not have the oppos=\r\nite short-term worry as Jeff does =96 he worries that an encoding bias towa=\r\nrds low connectivity &quot;might evolve away because of fitness pressure,&quot; but c=\r\nan&#39;t we just as easily worry about *too much* modularity? In that case, Jef=\r\nf&#39;s evil twin &quot;opposite-Jeff&quot; might be worried that an initial encoding bia=\r\ns towards *high* connectivity might evolve away. It is not clear nor establ=\r\nished fact (see Connectionists) that the exact form of the final &quot;solution&quot;=\r\n is particularly modular or non-modular. What it is, is subtle and somewher=\r\ne in the middle. So none of this kind of panicking about what nature &quot;needs=\r\n&quot; to harass it into such an astronomically complex future configuration mak=\r\nes much sense. We cannot say definitively the extent to which the final str=\r\nucture is &quot;closer&quot; to modular or non-modular, whatever that even means. For=\r\ntunately, an encoding that begins with a bias towards modularity can tone i=\r\nt down as needed, or ramp it up even more.\n&gt; &gt; \n&gt; &gt; Yet Jeff also worries a=\r\nbout about the radiation of evolutionary lineages being blocked because of =\r\nimplicit penalties: He says, &quot;You assume that evolution will branch out and=\r\n explore all these options even in the face of fitness penalties for that e=\r\nxploration. But that is not how evolution works.&quot;\n&gt; &gt; \n&gt; &gt; But branching ou=\r\nt and exploring many (not necessarily all of course) of the options is the =\r\nonly way that natural evolution works. That&#39;s what open-endedness is (unles=\r\ns you don&#39;t believe natural evolution to be open-ended). The tree of life i=\r\ns ever-branching. The worry about &quot;fitness penalties&quot; here is a red herring=\r\n because it originates from closed-ended artificial EC experiments where yo=\r\nu can end up on the wrong path. But nature does not have any single &quot;fitnes=\r\ns penalty&quot; or &quot;right path&quot; throughout its run because the landscape is alwa=\r\nys changing as it branches and branches. For example, before trees, being a=\r\nn extremely tall herbivore would incur a fitness penalty, but after trees g=\r\niraffes were perfectly viable. The penalty is not consistent.\n&gt; &gt; \n&gt; &gt; More=\r\n generally, how can there be what you call a &quot;default fitness penalty&quot; if t=\r\nhere is no final goal? Penalty with respect to what? Keep in mind here that=\r\n the origin of pseudo-modular organization in nature likely predates the em=\r\nergence even of neurons. The first neural structures piggy-backed on previo=\r\nusly evolved organizational structure that likely influenced the subtle pse=\r\nudo-modularity of connectivity from the start for reasons entirely unrelate=\r\nd to connection cost because these organizational conventions evolved long =\r\nbefore neurons even existed: the bias in the encoding was in part already t=\r\nhere.\n&gt; &gt; \n&gt; &gt; Which brings me back to the origin of all such conventions -=\r\n canalization - which is the key here. Stephane talks about a bias that exi=\r\nsts &quot;all along&quot; in evolution, but ultimately the ability to *change* bias e=\r\nclipses choosing one up front. Again, in the context of artificial scenario=\r\ns, it&#39;s a good engineering hack to force in some kind of bias into the enco=\r\nding or into fitness that you expect to control things for a moderate numbe=\r\nr of generations. But in nature the scope is so vast that it can&#39;t be the f=\r\ninal word; it&#39;s only the initial hint. While that hint can help, nature in =\r\nthe long term needs to choose and commit to its own biases, and to slither =\r\nout of them from time to time, and only encoding offers that potential. Can=\r\nalization is the way nature can make long-term (though not necessarily perm=\r\nanent) commitments. It&#39;s how conventions are established in specific lineag=\r\nes.\n&gt; &gt; \n&gt; &gt; In a genuine open-ended scenario like nature, modularity will =\r\nemerge and proliferate over vast stretches of time only if modularity leads=\r\n to more species emerging. Of course, the species we observe at the end are=\r\n the consequence of organizational principles that supported generating man=\r\ny species (which is almost tautological). So it need not relate to being be=\r\ntter or worse, or &quot;solving&quot; anything. It has to do with open-ended dynamics=\r\n. Air will escape a hole in a balloon if you wait long enough. If that hole=\r\n leads to a whole other world, you will eventually see that other world. Mo=\r\ndularity, to the extent it actually exists in nature, has served as such a =\r\nhole. But the only way such a hole can be exploited, the only way you can k=\r\neep focused on that area, is if it can be canalized. An encoding that can b=\r\ne canalized allows you to maintain the subtle convention that is responsibl=\r\ne for spreading diversity. \n&gt; &gt; \n&gt; &gt; Stef nevertheless reminds me that &quot;sel=\r\nection pressure has strong impact,&quot; and I entirely agree of course. But the=\r\nre are two very different classes of selection pressure. One is about pushi=\r\nng you towards the new, and the other is about forcing you to commit to the=\r\n old. There are many ways to push towards the new, and novelty search is ju=\r\nst one. In contrast, these things we call &quot;fitness pressures&quot; (whether part=\r\n of a MOEA or not) are the opposite =96 they are toxic strait jackets appli=\r\ned for eternity. They presume that we know what we need with no nuance what=\r\nsoever eons before anything remotely related has appeared. Again, in engine=\r\nering, fair enough =96 it can work. But it is not an *explanation* of the p=\r\nroducts of open-ended evolution in nature, and likely is not a good way to =\r\nproduce open-endedness artificially either. \n&gt; &gt; \n&gt; &gt; So the only escape I =\r\nsee here from my argument is if you can argue somehow that you can do all t=\r\nhese amazing things *without* open-ended evolution. Then all your pressures=\r\n and constraints might make sense. But I don&#39;t think you can argue that, wh=\r\nich, to finally circle back to Martin&#39;s broad question, is why encoding is =\r\nultimately superior. A canalizeable encoding is the perfect partner for an =\r\nopen-ended process. But it is not (as Martin puts it) because it makes a pa=\r\nrticular &quot;bias in the encoding better.&quot; Rather, it is because encoding lets=\r\n evolution delicately modify its own biases on the fly and explore all of t=\r\nhem in parallel. That is, the ability to change, the ability to flexible, t=\r\no commit but to uncommit in increments of subtlety, to radiate diversity wh=\r\nile still committing to certain biases in certain chains, is the power that=\r\n made everything happen. Any forced competition, any constant bias, any ete=\r\nrnal relative judgment, which are all things that constant fitness pressure=\r\n offers, will diminish that flexibility. It will not necessarily destroy th=\r\ne open-ended process, but it will reduce its power and ultimately therefore=\r\n cannot explain or account for it.\n&gt; &gt; \n&gt; &gt; Best,\n&gt; &gt; \n&gt; &gt; ken\n&gt; &gt; \n&gt; &gt; ---=\r\n In neat@yahoogroups.com, &quot;martin_pyka&quot; wrote:\n&gt; &gt; &gt;\n&gt; &gt; &gt; I just would lik=\r\ne to point out that, in my opinion, part of the disagreement between you an=\r\nd Jeff and Ken comes from the fact that Ken somehow made the statement &quot;it =\r\nis better to implement the bias in the encoding than in the fitness functio=\r\nn&quot; but in actual fact argues for a specific type of implementation in the e=\r\nncoding.\n&gt; &gt; &gt; \n&gt; &gt; &gt; Thus, I thing the discussion should not center around=\r\n the general question whether a bias should be incorporated in the fitness =\r\nor in the encoding because in both areas there are better and worse ways to=\r\n do it. The question is more, why a specific implementation (that Ken has o=\r\nbviously in mind, my impression was he thought about approaches similar to =\r\nLEO) is better than another.\n&gt; &gt; &gt; \n&gt; &gt; &gt; My question to Ken would be here:=\r\n what is the additional ingredient that makes a bias in the encoding better=\r\n / more plausible than *any* implementation of the bias in the fitness func=\r\ntion?\n&gt; &gt; &gt;\n&gt; &gt; \n&gt; &gt;\n&gt;\n\n\n\n"}}