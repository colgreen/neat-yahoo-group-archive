{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Kenneth Stanley","from":"&quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"QSLM-plY2bBXOf38pLnSm-PrYaNntadmTEUjr2MgII3RXBTMoHtZvxnxrdV6t8_5PKq0Cp8YR2u2NTXtxxe_oP8gMJSv-V9NZMyJmS349AHA","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: NEAT and highly recurrent networks","postDate":"1158824453","msgId":2755,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGVldGZtNStnNjI3QGVHcm91cHMuY29tPg==","inReplyToHeader":"PDIwMDYwOTE4MjEyOTM2LjY3ODk0LnFtYWlsQHdlYjU3MTA5Lm1haWwucmUzLnlhaG9vLmNvbT4="},"prevInTopic":2750,"nextInTopic":0,"prevInTime":2754,"nextInTime":2756,"topicId":2711,"numMessagesInTopic":7,"msgSnippet":"Michael, that s interesting about the partial supervised training helping to converge a bit but not too much.  What was the training algorithm?  Was it","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 97392 invoked from network); 21 Sep 2006 07:43:25 -0000\r\nReceived: from unknown (66.218.66.216)\n  by m38.grp.scd.yahoo.com with QMQP; 21 Sep 2006 07:43:25 -0000\r\nReceived: from unknown (HELO n5c.bullet.sc5.yahoo.com) (66.163.187.196)\n  by mta1.grp.scd.yahoo.com with SMTP; 21 Sep 2006 07:43:25 -0000\r\nReceived: from [66.163.187.120] by n5.bullet.sc5.yahoo.com with NNFMP; 21 Sep 2006 07:40:54 -0000\r\nReceived: from [66.218.69.1] by t1.bullet.sc5.yahoo.com with NNFMP; 21 Sep 2006 07:40:54 -0000\r\nReceived: from [66.218.66.80] by t1.bullet.scd.yahoo.com with NNFMP; 21 Sep 2006 07:40:54 -0000\r\nDate: Thu, 21 Sep 2006 07:40:53 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;eetfm5+g627@...&gt;\r\nIn-Reply-To: &lt;20060918212936.67894.qmail@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0\r\nFrom: &quot;Kenneth Stanley&quot; &lt;kstanley@...&gt;\r\nSubject: Re: NEAT and highly recurrent networks\r\nX-Yahoo-Group-Post: member; u=54567749; y=C4Pl0RfVeSUhMQWVGs1gP11LXQ4Futh9Cn-y8jMDaC7vCuiZWTsW\r\nX-Yahoo-Profile: kenstanley01\r\n\r\nMichael, that&#39;s interesting about the partial supervised training \nhelping =\r\nto converge a bit but not too much.  What was the training \nalgorithm?  Was=\r\n it recurrent backprop?\n\nken\n\n--- In neat@yahoogroups.com, Michael Neylon &lt;=\r\nmneylon01@...&gt; wrote:\n&gt;\n&gt; I finally figured out where things were going sou=\r\nth in my code and \ngot it to work nicely for the various known tests (stand=\r\nard XOR, \npole balancing) compariable to the papers out there.  The XOR was=\r\n \nadding too many nodes as I didn&#39;t prevent feedback though once fixed \nto =\r\n(optionally) only allow feedforward connections, that worked \nperfectly.\n&gt; =\r\n\n&gt; I found that for highly recurrent networks like for the delayed \nXOR pro=\r\nblem, the addition of training as a possible weight mutation \n(it technical=\r\nly is a mutation of the weights, but done in a \nlogical/mathematical fashio=\r\nn) was necessary to boost the speed of \nthe convergenace of the problem.   =\r\nTraining every network is not \ngood (you&#39;d favor networks with too many nod=\r\nes but that can match \nthe pattern of data presented, and not the data itse=\r\nlf), and you \ndon&#39;t want to train the network all the way either.  I found =\r\nthat \ntraining about 5-10% of the time with a dataset about 1/20th of the \n=\r\nfull time series (in addition to weight/new node/new connection \nmutations)=\r\n helped pushed the convergence along better, maybe abiyut \n2 to 10 times fe=\r\nwer generations.  Also, pushing up the new \nconnection likelihood and reduc=\r\ning the chance of connections being \ndisabled through crossover and mutatio=\r\nn helped to capture the \nrecurrent nature - while the number of nodes may b=\r\ne a bit higher \nthan needed,\n&gt;  there was the right number of nodes connect=\r\ned in the highly \nrecurrent fashion as expected to solve the problem.  I&#39;m =\r\nstill \nworking to get the feel for how the training mutation parameters can=\r\n \naffect the performance of the system, but it seems reasonable with \nwhat =\r\nI have listed above for far.\n&gt; \n&gt; Kenneth Stanley &lt;kstanley@...&gt; \nwrote:   =\r\n                               NEAT really shouldn&#39;t be \nallowed to form re=\r\ncurrent connections to \n&gt;  solve XOR.  It&#39;s not that there is anything &quot;wro=\r\nng&quot; or &quot;cheating&quot; \n&gt;  about it- it&#39;s just that the main point of XOR is to =\r\ncompare your \n&gt;  implementation with other implementations, and since the s=\r\ntandard \n&gt;  benchmark does not include recurrent connections, you obfuscate=\r\n \nthe \n&gt;  comparison by allowing them.  It sounds like your XOR is indeed \n=\r\n&gt;  memorizing the order of presentation.  That is definitely not the \n&gt;  in=\r\ntent of the XOR problem, and means your NNs are solving a very \n&gt;  differen=\r\nt problem.  They are memorizing a sequence by using \n&gt;  recurrent connectio=\r\nns.  That&#39;s interesting, but makes the \ncomparison \n&gt;  less meaningful.\n&gt;  =\r\n\n&gt;  Technically, networks should be flushed (all nodes set to zero) \n&gt;  bet=\r\nween input presentations in XOR (or any other similar \n&gt;  classification pr=\r\noblem) since XOR is order-independent (the \n&gt;  definition of the XOR proble=\r\nm says nothing about order of \n&gt;  presentation).  The flushing should precl=\r\nude the need to present \nthe \n&gt;  instances in random order.  Even if you wa=\r\nnt to use recurrent \n&gt;  connections, flushing should be part of the procedu=\r\nre.\n&gt;  \n&gt;  The weight cap of 5 sounds reasonable.  Up to 10 I think is \n&gt;  =\r\nreasonable.  My rule of thumb for the power of mutations is that \nit \n&gt;  sh=\r\nould take on average several mutations to travel from a weight \nof \n&gt;  0 to=\r\n the max or min weight.  \n&gt;  \n&gt;  If XOR weights are tending to max out, tha=\r\nt may be an artifact of \n&gt;  the XOR problem- though I&#39;m not sure- there may=\r\n be other reasons \nas \n&gt;  well.  But I would not worry a lot about it until=\r\n the XOR problem \nis \n&gt;  being presented in a customary way.\n&gt;  \n&gt;  --- In =\r\nneat@yahoogroups.com, &quot;mneylon01&quot; &lt;mneylon01@&gt; wrote:\n&gt;  &gt;\n&gt;  &gt; So I&#39;ve had=\r\n a chance to do what Ken suggested below - start \nfrom \n&gt;  the\n&gt;  &gt; basic (=\r\n2+1bias)-1-1 network for the static XOR problem, \nrandomizing\n&gt;  &gt; the test=\r\n data, and disabling any node or connection creation or\n&gt;  &gt; toggling to se=\r\ne how my network worked.  I&#39;m using what I \nbelieve \n&gt;  are\n&gt;  &gt; the same p=\r\narameters in Ken&#39;s various papers: 80% chance of a \n&gt;  child&#39;s\n&gt;  &gt; weights=\r\n being changed, with 90% chance of perturbation, 10% \nchance \n&gt;  of\n&gt;  &gt; mu=\r\ntation.  I used 150 species, and for the species calculation, \nI \n&gt;  used\n&gt;=\r\n  &gt; 1.0 for the excess and disjoint coefficients, 0.4 for the common\n&gt;  &gt; w=\r\neight differences, and set N to 1 (as noted by Ken below), \nwith \n&gt;  the\n&gt; =\r\n &gt; tolerance set to 3.0.\n&gt;  &gt; \n&gt;  &gt; What was interesting is that I found th=\r\nere to be a VERY strong\n&gt;  &gt; connection between two parameters that I don&#39;t=\r\n see mentioned \nmuch \n&gt;  in\n&gt;  &gt; the papers: the weight cap, and the type a=\r\nnd amount of \n&gt;  perturbation or\n&gt;  &gt; mutation done.   (Note that I&#39;ve trie=\r\nd both the sigmoid \nsteepness \n&gt;  of\n&gt;  &gt; 4.9 and 1, this seems to have les=\r\ns of an impact that these other\n&gt;  &gt; parameters).\n&gt;  &gt; \n&gt;  &gt; My original co=\r\nde capped the weight at 2, which I saw slow if any\n&gt;  &gt; approach to a good =\r\nnetwork.  Increasing the cap to insane \nvalues \n&gt;  like\n&gt;  &gt; 100 gave a gre=\r\nat speed to evolution convergence, as this \nallowed \n&gt;  for a\n&gt;  &gt; wider ra=\r\nnge of weight values to be explored by the NEAT method, \nbut\n&gt;  &gt; leads to =\r\nsome networks that one questions their ability to \nwork.  \n&gt;  &gt; Using a cap=\r\n of 5 to 10 gave a more reasonable set of networks \nwhile\n&gt;  &gt; still conver=\r\nging quickly (about 50 generations to get the RMS \nunder\n&gt;  &gt; 0.1 on the 4 =\r\ncases).  I&#39;ve seen the cap of 5 tossed around here\n&gt;  &gt; before, but does th=\r\nis seem to be a good practical number?\n&gt;  &gt; \n&gt;  &gt; Also, with my weight muta=\r\ntions, I found that using gaussian\n&gt;  &gt; distributions for both initial weig=\r\nhts and mutated weights, and\n&gt;  &gt; pertubations in weights was better than s=\r\ntraight random.  \n&gt;  (Presently\n&gt;  &gt; I&#39;m using N rolls of the RNG and getti=\r\nng the average to simulate\n&gt;  &gt; gaussian distribution, with larger N to red=\r\nuce the standard\n&gt;  &gt; deviation).  The quesiton I have, since I&#39;m having tr=\r\nouble \nseeing \n&gt;  how\n&gt;  &gt; it&#39;s done in either the C++ or C# code, is what =\r\nthe good \neffective\n&gt;  &gt; magnitude is.  The above cases, for the mutated/in=\r\nitial \nweights, \n&gt;  I&#39;m\n&gt;  &gt; using a gaussian random number from -5 to 5, s=\r\ntd deviation of \n&gt;  about 1,\n&gt;  &gt; and for the perturbed weights, from -5 to=\r\n 5 with a std \ndeviation of\n&gt;  &gt; about 0.1-0.5.    These cases seem to work=\r\n, but I can&#39;t seem \nhow \n&gt;  they\n&gt;  &gt; easily match up with the above code. =\r\n Even with the weight \ncaps, I\n&gt;  &gt; always seemed to have weights that want=\r\ned to hit the caps.  Any \n&gt;  good\n&gt;  &gt; suggestions on what are good weight =\r\nranges and adjustments to \n&gt;  make. \n&gt;  &gt; Another thing I noticed is that i=\r\nf the weight cap is too low, \nusing\n&gt;  &gt; the values for the species toleran=\r\nce calculation I give above, I\n&gt;  &gt; rarely got more than 1 or two species.\n=\r\n&gt;  &gt; \n&gt;  &gt; Additionally, I find it interesting in how one approaches the \ni=\r\nnput\n&gt;  &gt; data part.  I tried the standard XOR with a fixed set of data, \na=\r\nnd \n&gt;  it\n&gt;  &gt; can converge quickly, but when you then make the data random=\r\nly\n&gt;  &gt; presented (re-evaluating weights of genomes that carry over to \n&gt;  =\r\naccount\n&gt;  &gt; for repeated runs), the performance of the NEAT method is \npoo=\r\nr.   \n&gt;  That\n&gt;  &gt; is, when fixed, I almost always got a recurrent connecti=\r\non (I \ndon&#39;t\n&gt;  &gt; prevent these from being formed), suggesting it&#39;s trying =\r\nto \nmimic \n&gt;  the\n&gt;  &gt; input pattern order and not the actual data patterns=\r\n.  I found \n&gt;  that if\n&gt;  &gt; a random order was used, using a larger test pa=\r\nttern (in my \ncase, \n&gt;  50\n&gt;  &gt; initially randomly generated XOR cases and =\r\nthen presented in a \n&gt;  random\n&gt;  &gt; order to the networks) seemed to help. =\r\n I still get some \n&gt;  recurrancies,\n&gt;  &gt; but the network size seemed to sta=\r\ny small and avoid new node\n&gt;  &gt; formation.  (Obviously, when I use recurren=\r\nt data as like the \n&gt;  delayed\n&gt;  &gt; XOR problem, the data order will be fix=\r\ned, but the size of the \ndata\n&gt;  &gt; set needs to be large enough that it wou=\r\nld be impractical for \nthe\n&gt;  &gt; genome to try to learn the overall data set=\r\n pattern as opposed \nto \n&gt;  the\n&gt;  &gt; actual pattern that the dataset contai=\r\nns...)\n&gt;  &gt; \n&gt;  &gt; --- In neat@yahoogroups.com, &quot;Kenneth Stanley&quot; &lt;kstanley@=\r\n&gt; \nwrote:\n&gt;  &gt; &gt;\n&gt;  &gt; &gt; It sounds like the main issue before you can really=\r\n start \n&gt;  looking at \n&gt;  &gt; &gt; delayed-XOR is to get XOR working more closel=\r\ny to my own NEAT\n&gt;  &gt; &gt; (and others developed since).  Certainly the 500-10=\r\n00 \n&gt;  generations \n&gt;  &gt; &gt; it&#39;s taking you indicates somethign is likely wr=\r\nong in your \n&gt;  &gt; &gt; implementation.\n&gt;  &gt; &gt; \n&gt;  &gt; &gt; I would suggest the foll=\r\nowing experiment:  Start evolution \nwith  \n&gt;  a \n&gt;  &gt; &gt; population of netwo=\r\nrks that already have the correct topology \n&gt;  for a \n&gt;  &gt; &gt; solution to XO=\r\nR and turn off structural mutations.  Then run \n&gt;  your \n&gt;  &gt; &gt; version of =\r\nNEAT as usual, except in this case, it will only \nbe \n&gt;  &gt; &gt; searching over=\r\n weight space.  If it takes forever, it tells \nyou \n&gt;  that \n&gt;  &gt; &gt; the pro=\r\nblem is in the way your weights are being mutated, or \nthe \n&gt;  way \n&gt;  &gt; &gt; =\r\nthey are being combined in crossover.  It may also indicate a \n&gt;  &gt; &gt; probl=\r\nem in speciation (related to weight comparison).  In any \n&gt;  case, \n&gt;  &gt; &gt; =\r\nit will greatly narrow down the problem.\n&gt;  &gt; &gt; \n&gt;  &gt; &gt; Once you get it wor=\r\nking on pure weight-evolution, then you \ncan \n&gt;  move \n&gt;  &gt; &gt; to the normal=\r\n topology evolution, and you will either see it \n&gt;  work \n&gt;  &gt; &gt; right away=\r\n, or find that there is a problem in adding \nstructure \n&gt;  as \n&gt;  &gt; &gt; well.=\r\n\n&gt;  &gt; &gt; \n&gt;  &gt; &gt; About compatibility testing for speciation, when you \nmenio=\r\nn &quot;N,&quot; \n&gt;  do \n&gt;  &gt; &gt; you mean the normalization term in my papers?  Regret=\r\ntably, \nmany \n&gt;  &gt; &gt; people miss that I did not use N (for normalization) i=\r\nn \n&gt;  practice, \n&gt;  &gt; &gt; that is, I set N to 1 in all cases.  That may expla=\r\nin why you \n&gt;  using \n&gt;  &gt; &gt; the same coefficients as me does not work.  If=\r\n you look at my \n&gt;  papers \n&gt;  &gt; &gt; closely, they say that N can be set to 1=\r\n if genomes are not \ntoo \n&gt;  &gt; &gt; large.  I have found in practice that it a=\r\nlways works fine \nwith \n&gt;  it \n&gt;  &gt; &gt; set to 1, so that&#39;s what I&#39;ve done.  =\r\nThis confusion is my \nfault \n&gt;  and \n&gt;  &gt; &gt; I apologize for it- the papers =\r\nshould be more clear- but I \n&gt;  wanted \n&gt;  &gt; &gt; people to be aware of N and =\r\nthe option for normalization in \ncase \n&gt;  it \n&gt;  &gt; &gt; indeed does come into =\r\nplay with very large genomes.\n&gt;  &gt; &gt; \n&gt;  &gt; &gt; As for deviations from typical=\r\n values, you can see all the \n&gt;  values \n&gt;  &gt; &gt; I&#39;ve used in the appendix t=\r\no my dissertation.  There is some \n&gt;  &gt; &gt; explanation there too for why dif=\r\nferent values were chosen.  \nOne \n&gt;  &gt; &gt; consideration might be whether ver=\r\ny fine grained weight \nchanged \n&gt;  are \n&gt;  &gt; &gt; key or not in a particular p=\r\nroblem.  If they are, you might \nwant \n&gt;  the \n&gt;  &gt; &gt; coefficient of weight=\r\n differences to be higher.\n&gt;  &gt; &gt; \n&gt;  &gt; &gt; Once you get XOR working, please =\r\nlet the group know how \nthings \n&gt;  go \n&gt;  &gt; &gt; with the delayed XOR!  By the=\r\n way, what language/platform did \n&gt;  you \n&gt;  &gt; &gt; use for your version of NE=\r\nAT?\n&gt;  &gt; &gt; \n&gt;  &gt; &gt; ken\n&gt;  &gt; &gt; \n&gt;  &gt; &gt; --- In neat@yahoogroups.com, &quot;mneylon=\r\n01&quot; &lt;mneylon01@&gt; wrote:\n&gt;  &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; I&#39;ve been working on my own impli=\r\nmentation of NEAT (having \nto \n&gt;  fit \n&gt;  &gt; &gt; to a\n&gt;  &gt; &gt; &gt; predescribed fr=\r\namework) and while it&#39;s mostly working, I&#39;m \n&gt;  looking \n&gt;  &gt; &gt; at\n&gt;  &gt; &gt; &gt;=\r\n a couple of questions.\n&gt;  &gt; &gt; &gt; \n&gt;  &gt; &gt; &gt; First, I know that the networks =\r\nthat NEAT generates can have\n&gt;  &gt; &gt; &gt; recurrency (feedback), and this is li=\r\nkely why some of the \n&gt;  robot \n&gt;  &gt; &gt; game\n&gt;  &gt; &gt; &gt; examples work well.  H=\r\nowever, I&#39;m looking at trying to use \n&gt;  NEAT to\n&gt;  &gt; &gt; &gt; find patterns in =\r\ntime series data, as one would use fully \n&gt;  recurrent\n&gt;  &gt; &gt; &gt; networks fo=\r\nr (In these, also known as Elmen networks, all \nof \n&gt;  the\n&gt;  &gt; &gt; &gt; hidden =\r\nlayer and output layer values are &#39;propagated&#39; into \nthe \n&gt;  next\n&gt;  &gt; &gt; &gt; =\r\ntime step to give the network memory, with full \nconnectivity \n&gt;  &gt; &gt; betwe=\r\nen\n&gt;  &gt; &gt; &gt; all the input and previous nodes to the hidden/output \nnodes).\n=\r\n&gt;  &gt; &gt; &gt; \n&gt;  &gt; &gt; &gt; Such a network should be possible to be generated by NEA=\r\nT, \n&gt;  though I\n&gt;  &gt; &gt; &gt; figure that not every recurrent type problem needs=\r\n a fully \n&gt;  &gt; &gt; recurrent\n&gt;  &gt; &gt; &gt; network.  So I&#39;m trying to use NEAT to =\r\ngenerate such, using \nthe\n&gt;  &gt; &gt; &gt; classic delayed-XOR problem (such that t=\r\nhe result of xor of \n&gt;  the two\n&gt;  &gt; &gt; &gt; current inputs will be the actual =\r\noutput some time steps \n&gt;  away).  \n&gt;  &gt; &gt; Fully\n&gt;  &gt; &gt; &gt; recurrent network=\r\ns can be trained to do this, but I want to \n&gt;  &gt; &gt; generate a\n&gt;  &gt; &gt; &gt; NEAT=\r\n network that, after running through the fixed data \nseries \n&gt;  a\n&gt;  &gt; &gt; &gt; =\r\nnumber of relaxation times, that the weights have already \nbeen \n&gt;  &gt; &gt; tra=\r\nined\n&gt;  &gt; &gt; &gt; through NEAT evolution such that I don&#39;t have to perform \n&gt;  =\r\nadditional\n&gt;  &gt; &gt; &gt; training on the network.  \n&gt;  &gt; &gt; &gt; \n&gt;  &gt; &gt; &gt; Has anyon=\r\ne had any success directly in generating such \n&gt;  recurrent\n&gt;  &gt; &gt; &gt; networ=\r\nks?  I know my fitnesses improve with time, but it \ntakes \n&gt;  a \n&gt;  &gt; &gt; lot=\r\n\n&gt;  &gt; &gt; &gt; of generations (1000+ with a 150 member population) to even \nsee\n=\r\n&gt;  &gt; &gt; &gt; something, and even then, it&#39;s not anywhere close to what \n&gt;  simp=\r\nle\n&gt;  &gt; &gt; &gt; recurrent training can provide.  (This may be also related \nto =\r\n\n&gt;  my\n&gt;  &gt; &gt; &gt; second question).\n&gt;  &gt; &gt; &gt; \n&gt;  &gt; &gt; &gt; The other question I h=\r\nad was about convergence times.  I&#39;m \n&gt;  trying \n&gt;  &gt; &gt; to\n&gt;  &gt; &gt; &gt; test my=\r\n network on the normal XOR problem (non-recurrent \nmode) \n&gt;  and\n&gt;  &gt; &gt; &gt; f=\r\nind that it takes many more evolution generations for the \n&gt;  fitness \n&gt;  &gt;=\r\n &gt; to\n&gt;  &gt; &gt; &gt; get to acceptable levels (based solely on the distance of \n&gt;=\r\n  expected \n&gt;  &gt; &gt; vs\n&gt;  &gt; &gt; &gt; observed output), exceptionaly more than lis=\r\nted in the NEAT \n&gt;  papers\n&gt;  &gt; &gt; &gt; (500-1000 evolution steps as opposed to=\r\n 10-30 steps) even \nwhen \n&gt;  &gt; &gt; using\n&gt;  &gt; &gt; &gt; what I believe are the same=\r\n values described by Kenneth in \nhis \n&gt;  &gt; &gt; papers.\n&gt;  &gt; &gt; &gt;  I&#39;ve tried n=\r\nearly every parameter, and the only one that I \n&gt;  know I\n&gt;  &gt; &gt; &gt; want to =\r\nkeep low is the new node probability to avoid \n&gt;  excessive \n&gt;  &gt; &gt; growth\n=\r\n&gt;  &gt; &gt; &gt; of the network.  Anyone have any pointers on what \nparameters \n&gt;  =\r\nare\n&gt;  &gt; &gt; &gt; critical to help with rapid convergence on the best \nnetwork? =\r\n \n&gt;  Mind\n&gt;  &gt; &gt; &gt; you, it could still be something in my code which I&#39;ve b=\r\neen \n&gt;  &gt; &gt; pounding\n&gt;  &gt; &gt; &gt; through to try to find differences.\n&gt;  &gt; &gt; &gt; =\r\n\n&gt;  &gt; &gt; &gt; Another related question is on the species comparison \n&gt;  express=\r\nion \n&gt;  &gt; &gt; and\n&gt;  &gt; &gt; &gt; tolerance.  I tend to use N=3Dnumber of genes in l=\r\nargest \nspecies\n&gt;  &gt; &gt; &gt; regardless of the case, and for that I have to pla=\r\ny with \nthe \n&gt;  &gt; &gt; tolerance\n&gt;  &gt; &gt; &gt; as to get 5 or more species in a pop=\r\nulation of 150.  Is \nthere \n&gt;  an \n&gt;  &gt; &gt; ideal\n&gt;  &gt; &gt; &gt; average number of =\r\nspecies that you want to carry through in \nthe\n&gt;  &gt; &gt; &gt; population in order=\r\n to take advantage of NEAT&#39;s use of \n&gt;  species?  \n&gt;  &gt; &gt; And\n&gt;  &gt; &gt; &gt; when=\r\n do people move away from the &#39;typical&#39; values of the \n&gt;  &gt; &gt; coefficients\n=\r\n&gt;  &gt; &gt; &gt; (1 and 1 for disjoint and excess elements, 0.4 for weight \n&gt;  &gt; &gt; =\r\ndifference\n&gt;  &gt; &gt; &gt; average)\n&gt;  &gt; &gt; &gt;\n&gt;  &gt; &gt;\n&gt;  &gt;\n&gt;  \n&gt;  \n&gt;      \n&gt;        =\r\n                \n&gt; \n&gt;  \t\t\n&gt; ---------------------------------\n&gt; Talk is che=\r\nap. Use Yahoo! Messenger to make PC-to-Phone calls.  \nGreat rates starting =\r\nat 1=A2/min.\n&gt;\n\n\n\n\n\n\n"}}