{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":127853030,"authorName":"Colin Green","from":"Colin Green &lt;cgreen@...&gt;","profile":"alienseedpod","replyTo":"LIST","senderId":"NnNPLQ5WcDQFa5Tlwsj2aQfIa-vHXM9hPqGYvHGBhT5e17Bqo2V5ihDTk7nVfDDP2CP3BScWJv-lxzULIzaBKKhvnAaHH9kY3Q","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [neat] Re: Learning How to Learn","postDate":"1067649327","msgId":187,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDNGQTMwOTJGLjgwMjAyMDBAZHNsLnBpcGV4LmNvbT4=","inReplyToHeader":"PGJucGdyaysyZm5lQGVHcm91cHMuY29tPg==","referencesHeader":"PGJucGdyaysyZm5lQGVHcm91cHMuY29tPg=="},"prevInTopic":183,"nextInTopic":189,"prevInTime":186,"nextInTime":188,"topicId":170,"numMessagesInTopic":15,"msgSnippet":"... I ve always thought of signals within a NN to be equivalent to short term memory. So for instance in your foraging food experiment the physical structure","rawEmail":"Return-Path: &lt;cgreen@...&gt;\r\nX-Sender: cgreen@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nReceived: (qmail 97263 invoked from network); 1 Nov 2003 01:15:30 -0000\r\nReceived: from unknown (66.218.66.218)\n  by m2.grp.scd.yahoo.com with QMQP; 1 Nov 2003 01:15:30 -0000\r\nReceived: from unknown (HELO colossus.systems.pipex.net) (62.241.160.73)\n  by mta3.grp.scd.yahoo.com with SMTP; 1 Nov 2003 01:15:30 -0000\r\nReceived: from dsl.pipex.com (81-86-175-101.dsl.pipex.com [81.86.175.101])\n\tby colossus.systems.pipex.net (Postfix) with ESMTP id C35DE160009A1\n\tfor &lt;neat@yahoogroups.com&gt;; Sat,  1 Nov 2003 01:15:27 +0000 (GMT)\r\nMessage-ID: &lt;3FA3092F.8020200@...&gt;\r\nDate: Sat, 01 Nov 2003 01:15:27 +0000\r\nUser-Agent: Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.4) Gecko/20030624\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: neat@yahoogroups.com\r\nSubject: Re: [neat] Re: Learning How to Learn\r\nReferences: &lt;bnpgrk+2fne@...&gt;\r\nIn-Reply-To: &lt;bnpgrk+2fne@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nFrom: Colin Green &lt;cgreen@...&gt;\r\nX-Yahoo-Group-Post: member; u=127853030\r\nX-Yahoo-Profile: alienseedpod\r\n\r\nKenneth Stanley wrote:\n\n&gt;Anyway, yes I&#39;ve been interested in &quot;learning how to learn&quot; for a long\n&gt;time.  (Or, in other words, evolving networks that can learn.) \n&gt;Interestingly, a lot of people think (and it makes sense) that\n&gt;learning requires synaptic plasticity, i.e. the ability to change\n&gt;weights dynamically during the network&#39;s lifetime.  Usually such\n&gt;plasticity would be Hebbian learning or something like it.  Your\n&gt;experiment, and others, show that this is not necessarily true;  in\n&gt;fact, recurrent activation alone can store state information.\n&gt;  \n&gt;\nI&#39;ve always thought of signals within a NN to be equivalent to short \nterm memory. So for instance in your foraging food experiment the \nphysical structure of the network (it&#39;s architecture and weights) \ndescribe a set of hard coded skills. One skill is to determine which \nobjects are poison and which are food, once this has been determined and \nthe information stored in a recurrant sub-network, the next skill of \nactually surveying the area and eating food is invoked.  So I basically \nthe NN is acting like a procedure written in code with local variables \nwhere it can store data to help it along.\n\nPlasticity can then be thought of as actually changing the lines of code \nwithin the procedure. Now you can still do some clever stuff without \nsynaptic plasticity, so a NN could actually have a great many different \nsub NN&#39;s that somehow become activated when different internal states \noccur, thus giving the impression that something more clever than the \nuse of simple short term memory is going on.\n\nSo then we have two different levels of learning. One where we learn a \nnew skill, and one where a skill is invoked has the capacity to store \ndata about the environment - and so it is learning about it&#39;s \nenvironment and storing knowledge of it purely as NN state. Systems that \nevolve NN weights and architectures are learning new skills through \nevolution, once the NN is evolved no more skills can be acquired, they \ncan only be invoked.\n\n&gt;(On a side note, for anyone wondering, yes this does mean there is\n&gt;a version of NEAT that can evolve Hebbian networks with plastic\n&gt;synapses, but I haven&#39;t released it yet.) \n&gt;  \n&gt;\nAny ideas on when it might be released? any time soon?\n\n&gt;Anyway, these are very interesting issues.  When do you need Hebbian\n&gt;connections to learn as opposed to only fixed-weight recurrent\n&gt;connections?  It isn&#39;t clear.  And what is the capacity of a\n&gt;fixed-weight recurrent network to remember information?\n&gt;\nWe can calculate the maximum very easily for a fixed weight NN. The \nquestion ( to my mind) is what kind of structures do you need to access \nand manipulate that memory in a useful way and are they scalable? So \ncould we evolve a memory that acts like a (sort of) computer memory chip \nwhere we ask for the data at a given address. Except instead of an \naddress and a piece of data think of applying a pattern and getting back \nthe corresponding pattern that has been stored against it.\n\nColin\n\n\n"}}