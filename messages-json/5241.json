{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":54567749,"authorName":"Ken","from":"&quot;Ken&quot; &lt;kstanley@...&gt;","profile":"kenstanley01","replyTo":"LIST","senderId":"pNkzN5Ef9b68IE6WUKxgMhR654pIkzqfvPe7i77OK2CZXn_8faGHcBb1QC-KP0EtcwHP6_RaZkyh2mwbrWGr9sbdCHsh","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: &#39;Boxes&#39; Visual Discrimination Task","postDate":"1274773563","msgId":5241,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGh0ZnY3citjbmVvQGVHcm91cHMuY29tPg==","inReplyToHeader":"PEFBTkxrVGluaGcwZUVhMms5ZDgtT21jcTFXSC1yaF91VDR3QW40dzBhcmR0cUBtYWlsLmdtYWlsLmNvbT4="},"prevInTopic":5239,"nextInTopic":5242,"prevInTime":5240,"nextInTime":5242,"topicId":5237,"numMessagesInTopic":10,"msgSnippet":"I see where you re coming from.  The multiplication examples are indeed interesting.  Have you tried running the inputs to neurons (after they run over their","rawEmail":"Return-Path: &lt;kstanley@...&gt;\r\nX-Sender: kstanley@...\r\nX-Apparently-To: neat@yahoogroups.com\r\nX-Received: (qmail 77898 invoked from network); 25 May 2010 22:05:48 -0000\r\nX-Received: from unknown (98.137.34.44)\n  by m1.grp.sp2.yahoo.com with QMQP; 25 May 2010 22:05:48 -0000\r\nX-Received: from unknown (HELO n38b.bullet.mail.sp1.yahoo.com) (66.163.168.152)\n  by mta1.grp.sp2.yahoo.com with SMTP; 25 May 2010 22:05:47 -0000\r\nX-Received: from [69.147.65.150] by n38.bullet.mail.sp1.yahoo.com with NNFMP; 25 May 2010 07:46:06 -0000\r\nX-Received: from [98.137.34.72] by t7.bullet.mail.sp1.yahoo.com with NNFMP; 25 May 2010 07:46:05 -0000\r\nDate: Tue, 25 May 2010 07:46:03 -0000\r\nTo: neat@yahoogroups.com\r\nMessage-ID: &lt;htfv7r+cneo@...&gt;\r\nIn-Reply-To: &lt;AANLkTinhg0eEa2k9d8-Omcq1WH-rh_uT4wAn4w0ardtq@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nFrom: &quot;Ken&quot; &lt;kstanley@...&gt;\r\nSubject: Re: &#39;Boxes&#39; Visual Discrimination Task\r\nX-Yahoo-Group-Post: member; u=54567749; y=d7NTEn1kKLiXXmJ-AHdC4adOgpuDVD75e4gbKVY-xl1qwA0_9cAo\r\nX-Yahoo-Profile: kenstanley01\r\n\r\n\n\nI see where you&#39;re coming from.  The multiplication examples are indeed i=\r\nnteresting.  Have you tried running the inputs to neurons (after they run o=\r\nver their incoming connections) through logarithms?  I wonder what networks=\r\n would do with logs at all their inputs.   Maybe the nodes would then only =\r\nneed to be linear summations (because logs provide nonlinearities).  Or may=\r\nbe they could still run their usual activation function set.\n\nThe other exa=\r\nmple of one neuron modulating another is also interesting.  It reminds me o=\r\nf neuromodulation work in adaptive ANNs, although that is applied to plasti=\r\nc synapse change, not to activation levels.\n\nIn general, there is definitel=\r\ny room for the neural model to be improved.  The main impediment to progres=\r\ns in this area is probably just that it is not very glorious work to be fid=\r\ndling with different function sets and network models and hoping to see a b=\r\nreakthrough, but it is true there could be a payoff.  \n\nAnother problem is =\r\nthat it&#39;s difficult to tell if the model should be better or not.  For exam=\r\nple, by providing distance to the CPPN in the boxes domain, you get an almo=\r\nst instant solution, but without that it takes maybe a few hundred generati=\r\nons.   So it still solves it, but it just takes longer.  But is that a good=\r\n or bad thing?  It&#39;s hard to say.  A few hundred generations could be a dro=\r\np in the bucket if we&#39;re worried about things that take a few million, so m=\r\naking an effort to whittle hundreds down to a couple generations under some=\r\n improved model might not be worth whatever cost such whittling incurs (sin=\r\nce there is usually some trade-off).  \n\nBut I&#39;m not saying there isn&#39;t some=\r\nthing better. I&#39;m just saying it&#39;s hard to tell if there is a problem or no=\r\nt that needs fixing.  There may be and I would be very interested if someth=\r\ning superior was demonstrated.  Will the new SharpNEAT provide an ability t=\r\no play with such ideas?\n\nken\n\n--- In neat@yahoogroups.com, Colin Green &lt;col=\r\nin.green1@...&gt; wrote:\n&gt;\n&gt; On 24 May 2010 08:45, Ken &lt;kstanley@...&gt; wrote:\n&gt;=\r\n &gt;\n&gt; &gt;\n&gt; &gt; Colin, yes, the idea of a powerful representation that can easil=\r\ny capture the most important\n&gt; &gt; operations (such as comparison or multipli=\r\ncation) is always lurking in the background. Yet\n&gt; &gt; trade-offs always seem=\r\n to emerge when we try to bestow such capabilities explicitly. For\n&gt; &gt; exam=\r\nple, if we added multiplication, it may theoretically allow certain key com=\r\nputations to be\n&gt; &gt; represented easily, yet it also gives evolution a new p=\r\nower to more easily destroy what it has\n&gt; &gt; built through trivial mutations=\r\n (because multiplication is more powerful than summation).\n&gt; \n&gt; Point ackno=\r\nwledged.\n&gt; \n&gt; &gt; I am sure that CPPNs as a representational formalism can be=\r\n improved, yet the presence of\n&gt; &gt; these nagging trade-offs makes me think =\r\nthat the idea of a super-powerful representation is a\n&gt; &gt; false ideal. It s=\r\neems to me at this point that evolution in nature succeeds despite the limi=\r\nted\n&gt; &gt; nature of its representation rather than because DNA is somehow nea=\r\nr perfect.\n&gt; \n&gt; I accept you chain of reasoning but I don&#39;t accept the\n&gt; fo=\r\nundation/axiom that natural neurons don&#39;t perform multiplication and\n&gt; comp=\r\narison (and possibly quite a few other operations) at the neuronal\n&gt; level.=\r\n Christoph Koch&#39;s work is a good starting point to learn more\n&gt; about what =\r\nfunctionality is present in individual neurons (see\n&gt; Biophysics of Computa=\r\ntion, section 21.1.1 Many Ways to Multiply). I&#39;ll\n&gt; pick one example which =\r\nI think is the most significant -\n&gt; multiplication via addition and logarit=\r\nhms:\n&gt; \n&gt;     log(x*y) =3D log(x) + log(y)\n&gt; \n&gt; Thus:\n&gt; \n&gt;    x*y =3D e^(lo=\r\ng(x) + log(y))\n&gt; \n&gt; Logarithms occur naturally by decay of some neurotransm=\r\nitter\n&gt; concentration over the length of an axon or dendrite.\n&gt; \n&gt; Another =\r\ncompelling example is the non-linear interation of\n&gt; neurotransmitters in d=\r\nendrites. Say we have incoming signals x and y\n&gt; which connect at sites X a=\r\nnd Y on the dendritic tree. Those two sites\n&gt; produce two different neurotr=\r\nansmitters when excited and the resulting\n&gt; excitation of the neuron is the=\r\n *product* of the two neurotransmitter\n&gt; concentrations. Why? Because one n=\r\neurotransmitter acts chemically and\n&gt; electrically in improving the flow of=\r\n the other (or more specifically,\n&gt; improving the flow of ions produced by =\r\nthe stimulating effects of the\n&gt; other). This setup is much like a transist=\r\nor, and one use of\n&gt; transitors is of course amplification AKA multiplicati=\r\non.\n&gt; \n&gt; \n&gt; &gt; Of course, these thoughts don&#39;t contradict anything you said.=\r\n The length input trick also\n&gt; &gt; shows that to some extent we can provide d=\r\nramatic bias through a priori knowledge, which is\n&gt; &gt; a good thing (e.g. if=\r\n it means skipping 10,000 years of evolution).\n&gt; \n&gt; My concern is that this=\r\n was a simple test case where simple euclidean\n&gt; distance applied in an obv=\r\nious way. Ideally I want HyperNEAT to learn\n&gt; what distance metrics apply t=\r\no the problem rather than me suppling\n&gt; them precomputed.\n&gt; \n&gt; I think ther=\r\ne&#39;s a strong case for taking a fresh look at what\n&gt; computations we perform=\r\n at the neuronal level and whether traditional\n&gt; sigmoid based ANNs are def=\r\nicient in some significant ways. I hope you\n&gt; can see where I&#39;m coming from=\r\n here.\n&gt; \n&gt; Colin.\n&gt;\n\n\n\n"}}